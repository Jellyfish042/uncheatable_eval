{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe646b8b-491e-4272-9cf6-a62e831e59a4",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cab115e-f9ff-49cd-92ce-5a5a971385ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pkg\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d65a62-ddae-45fa-90fb-19e1828e651c",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc237f4-836c-41fe-8b92-1c0c7e5ebd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# load from json file\n",
    "\n",
    "file_path = \"./arxiv_pdfs_cs_24_1_2000_to_7000.json\"\n",
    "\n",
    "\n",
    "def load_list_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads a list of strings from a JSON file.\n",
    "\n",
    "    :param file_path: Path of the JSON file to be loaded.\n",
    "    :return: List of strings loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "\n",
    "extracted_texts = load_list_from_json(file_path)\n",
    "\n",
    "print(len(extracted_texts))\n",
    "# print([len(x) for x in extracted_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for text in extracted_texts[:100]:\n",
    "    print(text)\n",
    "    print('-' * 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "aa3910df-8196-4953-906f-f4c3764dd222",
   "metadata": {},
   "source": [
    "## Now evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cfa16a-0ace-4aca-b026-63fe0aa107f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_log_sum(logits, target_token_ids):\n",
    "    shifted_logits = logits[:-1, :]\n",
    "    shifted_targets = target_token_ids[1:]\n",
    "    \n",
    "    log_probs = F.log_softmax(shifted_logits, dim=-1)\n",
    "    \n",
    "    target_log_probs = -log_probs.gather(1, shifted_targets.unsqueeze(1)).squeeze()\n",
    "    # print(target_log_probs)\n",
    "    \n",
    "    log_sum = torch.sum(target_log_probs, dim=-1)\n",
    "    # print(perplexity_sum)\n",
    "\n",
    "    return log_sum.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c55ca0-30dd-4331-9dbc-531d37a445f3",
   "metadata": {},
   "source": [
    "## Evaluate RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56930e55-3876-4a1e-bcf6-798f287caa15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n",
      "\n",
      "Loading ../rwkv5_7b/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module wkv_cuda...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model detected: v5.2\n",
      "Strategy: (total 32+1=33 layers)\n",
      "* cuda [float16, float16], store 33 layers\n",
      "0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 13-cuda-float16-float16 14-cuda-float16-float16 15-cuda-float16-float16 16-cuda-float16-float16 17-cuda-float16-float16 18-cuda-float16-float16 19-cuda-float16-float16 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 25-cuda-float16-float16 26-cuda-float16-float16 27-cuda-float16-float16 28-cuda-float16-float16 29-cuda-float16-float16 30-cuda-float16-float16 31-cuda-float16-float16 32-cuda-float16-float16 \n",
      "emb.weight                        f16      cpu  65536  4096 \n",
      "blocks.0.ln1.weight               f16   cuda:0   4096       \n",
      "blocks.0.ln1.bias                 f16   cuda:0   4096       \n",
      "blocks.0.ln2.weight               f16   cuda:0   4096       \n",
      "blocks.0.ln2.bias                 f16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_k           f16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_v           f16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_r           f16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_g           f16   cuda:0   4096       \n",
      "blocks.0.att.time_decay           f32   cuda:0     64    64 \n",
      "blocks.0.att.time_first           f32   cuda:0     64    64 \n",
      "blocks.0.att.receptance.weight    f16   cuda:0   4096  4096 \n",
      "blocks.0.att.key.weight           f16   cuda:0   4096  4096 \n",
      "blocks.0.att.value.weight         f16   cuda:0   4096  4096 \n",
      "blocks.0.att.output.weight        f16   cuda:0   4096  4096 \n",
      "blocks.0.att.gate.weight          f16   cuda:0   4096  4096 \n",
      "blocks.0.att.ln_x.weight          f32   cuda:0   4096       \n",
      "blocks.0.att.ln_x.bias            f32   cuda:0   4096       \n",
      "blocks.0.ffn.time_mix_k           f16   cuda:0   4096       \n",
      "blocks.0.ffn.time_mix_r           f16   cuda:0   4096       \n",
      "blocks.0.ffn.key.weight           f16   cuda:0   4096 14336 \n",
      "blocks.0.ffn.receptance.weight    f16   cuda:0   4096  4096 \n",
      "blocks.0.ffn.value.weight         f16   cuda:0  14336  4096 \n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.31.ln1.weight              f16   cuda:0   4096       \n",
      "blocks.31.ln1.bias                f16   cuda:0   4096       \n",
      "blocks.31.ln2.weight              f16   cuda:0   4096       \n",
      "blocks.31.ln2.bias                f16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_k          f16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_v          f16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_r          f16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_g          f16   cuda:0   4096       \n",
      "blocks.31.att.time_decay          f32   cuda:0     64    64 \n",
      "blocks.31.att.time_first          f32   cuda:0     64    64 \n",
      "blocks.31.att.receptance.weight   f16   cuda:0   4096  4096 \n",
      "blocks.31.att.key.weight          f16   cuda:0   4096  4096 \n",
      "blocks.31.att.value.weight        f16   cuda:0   4096  4096 \n",
      "blocks.31.att.output.weight       f16   cuda:0   4096  4096 \n",
      "blocks.31.att.gate.weight         f16   cuda:0   4096  4096 \n",
      "blocks.31.att.ln_x.weight         f32   cuda:0   4096       \n",
      "blocks.31.att.ln_x.bias           f32   cuda:0   4096       \n",
      "blocks.31.ffn.time_mix_k          f16   cuda:0   4096       \n",
      "blocks.31.ffn.time_mix_r          f16   cuda:0   4096       \n",
      "blocks.31.ffn.key.weight          f16   cuda:0   4096 14336 \n",
      "blocks.31.ffn.receptance.weight   f16   cuda:0   4096  4096 \n",
      "blocks.31.ffn.value.weight        f16   cuda:0  14336  4096 \n",
      "ln_out.weight                     f16   cuda:0   4096       \n",
      "ln_out.bias                       f16   cuda:0   4096       \n",
      "head.weight                       f16   cuda:0   4096 65536 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu118/rwkv5/build.ninja...\n",
      "Building extension module rwkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module rwkv5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    }
   ],
   "source": [
    "# load rwkv model\n",
    "rwkv5_7b_path = r'../rwkv5_7b/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth'\n",
    "max_length = 4096\n",
    "truncate = True\n",
    "\n",
    "os.environ['RWKV_JIT_ON'] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
    "\n",
    "from rwkv.model import RWKV\n",
    "from rwkv.utils import PIPELINE\n",
    "\n",
    "rwkv5_7b = RWKV(model=rwkv5_7b_path, strategy='cuda fp16')\n",
    "pipeline = PIPELINE(rwkv5_7b, r\"rwkv_vocab_v20230424\")\n",
    "rwkv_tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f0edc1-cfcb-49f8-9118-a1644421ea8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99621844dd641a083415b8d4e8b81c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6124/3488090245.py:14: UserWarning: seq-160 length > 4096\n",
      "  warnings.warn(f'seq-{idx} length > {max_length}')\n",
      "/tmp/ipykernel_6124/3488090245.py:14: UserWarning: seq-470 length > 4096\n",
      "  warnings.warn(f'seq-{idx} length > {max_length}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log probability sum: 5171.20875805664\n",
      "avg tokens: 2368.164\n",
      "overlong_seq: 2\n"
     ]
    }
   ],
   "source": [
    "# eval rwkv\n",
    "rwkv_test_data = []\n",
    "rwkv_token_length_list = []\n",
    "overlong_seq = 0\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        input_seq = rwkv_tokenizer.encode(sample)\n",
    "\n",
    "        if len(input_seq) > max_length:\n",
    "            overlong_seq += 1\n",
    "            warnings.warn(f'seq-{idx} length > {max_length}')\n",
    "            \n",
    "            if truncate:\n",
    "                input_seq = input_seq[:max_length]\n",
    "                factor = len(input_seq) / max_length\n",
    "        else:\n",
    "            factor = 1\n",
    "            \n",
    "\n",
    "        logit = rwkv5_7b.forward(input_seq, None, full_output=True)[0]\n",
    "\n",
    "        log_sum = calculate_log_sum(logit, torch.tensor(input_seq).cuda())\n",
    "        log_sum *= factor\n",
    "\n",
    "        rwkv_token_length_list.append(len(input_seq))\n",
    "        rwkv_test_data.append(log_sum)\n",
    "        \n",
    "print(f'log probability sum: {sum(rwkv_test_data) / len(rwkv_test_data)}')\n",
    "print(f'avg tokens: {sum(rwkv_token_length_list) / len(rwkv_token_length_list)}')\n",
    "print(f'overlong_seq: {overlong_seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6fb1dc-f35c-4abc-ac0b-c0b0eba2c3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del rwkv5_7b, pipeline, rwkv_tokenizer, logit\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5180f32-7e40-410a-a492-b44051afca5f",
   "metadata": {},
   "source": [
    "## Evaluate Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b06ba51-0c73-41a5-a2d6-27ea674ab632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5360075bc044f4dab3f777fa55d944f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "\n",
    "# llama2\n",
    "llama2_7b_path = r\"../Llama-2-7b-hf/\"\n",
    "max_length = 4096\n",
    "truncate = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama2_7b_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(llama2_7b_path, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "\n",
    "# mistral\n",
    "# mistral_7b_path = r\"../mistral_7b/\"\n",
    "# max_length = 8192\n",
    "# truncate = True\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(mistral_7b_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(mistral_7b_path, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "\n",
    "# mpt\n",
    "# mpt_7b_path = r\"../mpt_7b/\"\n",
    "# max_length = 2048\n",
    "# truncate = True\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(mpt_7b_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(mpt_7b_path, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "\n",
    "# yi\n",
    "# yi_6b_path = r\"../yi_6b/\"\n",
    "# max_length = 4096\n",
    "# truncate = True\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(yi_6b_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(yi_6b_path, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "\n",
    "\n",
    "# falcon\n",
    "# falcon_7b_path = r\"../falcon_7b/\"\n",
    "# max_length = 2048\n",
    "# truncate = True\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(falcon_7b_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(falcon_7b_path, device_map=\"auto\", trust_remote_code=False).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67253f-59c7-4930-974f-baf5298ddb43",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb054699f7a34f29aa58738a230f7a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval\n",
    "data = []\n",
    "token_length_list = []\n",
    "overlong_seq = 0\n",
    "\n",
    "for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        inputs = tokenizer(sample, return_tensors='pt')\n",
    "        inputs = inputs.to(model.device)\n",
    "\n",
    "        seq_length = inputs['input_ids'].shape[-1]\n",
    "\n",
    "        if seq_length > max_length:\n",
    "            overlong_seq += 1\n",
    "            if truncate:\n",
    "                inputs = tokenizer(sample, return_tensors='pt', max_length=max_length, truncation=True)\n",
    "                inputs = inputs.to(model.device)\n",
    "                warnings.warn(f'seq-{idx} length {seq_length} > truncation_length({max_length}) truncated')\n",
    "                factor = (seq_length / max_length)\n",
    "        else:\n",
    "            factor = 1\n",
    "                \n",
    "\n",
    "        logit = model.forward(**inputs).logits[0, :, :]\n",
    "\n",
    "        log_sum = calculate_log_sum(logit, inputs['input_ids'].squeeze(0))\n",
    "        log_sum *= factor\n",
    "        # print(log_sum)\n",
    "\n",
    "        token_length_list.append(seq_length)\n",
    "        data.append(log_sum)\n",
    "\n",
    "print(f'log probability sum: {sum(data) / len(data)}')\n",
    "print(f'avg tokens: {sum(token_length_list) / len(token_length_list)}')\n",
    "print(f'overlong_seq: {overlong_seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e9bce-34bf-4ef7-913c-76252cd0fa97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model, tokenizer, logit, inputs\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b4438-149d-4b2b-9ba1-b037f2d1e558",
   "metadata": {},
   "source": [
    "## Evaluate Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa902d91-01ef-4e91-bc1f-037e425ceb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "# model_name = \"state-spaces/mamba-1.4b\"\n",
    "# max_length = 2048\n",
    "\n",
    "# mamba_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "# mamba_1b4 = MambaLMHeadModel.from_pretrained(model_name, device=\"cuda\", dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558946b-913e-42ed-9855-44c0d444e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mamba_test_data = []\n",
    "# mamba_token_length_list = []\n",
    "# overlong_seq = 0\n",
    "\n",
    "# for idx, sample in tqdm(enumerate(extracted_texts), total=len(extracted_texts)):\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         inputs = mamba_tokenizer(sample, return_tensors=\"pt\").input_ids.to(device=device)\n",
    "\n",
    "#         seq_length = inputs.shape[-1]\n",
    "\n",
    "#         if seq_length > max_length:\n",
    "#             # print(f'length > {max_length}')\n",
    "#             overlong_seq += 1\n",
    "#             warnings.warn(f'seq-{idx} length {seq_length} > truncation_length({max_length})')\n",
    "\n",
    "#         mamba_output = mamba_1b4.forward(inputs)\n",
    "#         logit = mamba_output.logits[0, :, :]\n",
    "\n",
    "#         log_sum = calculate_log_sum(logit, inputs[0])\n",
    "\n",
    "#         mamba_token_length_list.append(seq_length)\n",
    "#         mamba_test_data.append(log_sum)\n",
    "        \n",
    "\n",
    "# mamba_1b4_log_sum = sum(mamba_test_data) / len(mamba_test_data)\n",
    "# mamba_1b4_avg_length = sum(mamba_token_length_list) / len(mamba_token_length_list)\n",
    "# print(f'log probability sum: {mamba_1b4_log_sum}')\n",
    "# print(f'avg tokens: {mamba_1b4_avg_length}')\n",
    "# print(f'overlong_seq: {overlong_seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9139aa-a57c-4c06-9214-455fa8a4b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del mamba_1b4, inputs, logit\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
